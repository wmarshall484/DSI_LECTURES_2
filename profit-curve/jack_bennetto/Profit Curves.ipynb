{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profit Curves\n",
    "\n",
    "### Jack Bennetto\n",
    "#### March 21, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    " * Explain why you would use a cost-benefit matrix and profit curve\n",
    " * Generate a profit curve and use it to make decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some (fake) data, with two features (one continuous, one integer) and a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npts = 10000\n",
    "y = scs.bernoulli(0.2).rvs(npts)\n",
    "X = pd.DataFrame({'a':scs.norm(0, 1).rvs(npts) + 0.3*y,\n",
    "                  'b':scs.poisson(2*y + 2).rvs(npts)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(X.a, X.b, color=np.array(['b', 'r'])[y], alpha=0.05, s=100)\n",
    "ax.set_xlabel('feature a')\n",
    "ax.set_ylabel('feature b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fraud\n",
    "\n",
    "To make this a little more realistic, let's imagine that you have a website to which people can sign up, and some small fraction of the signups are fraud. You're building a system to detect whether the account is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(['Not', 'Fraud'])[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a train-test split and fit a model. We'll do logistic regression, since that's all we know right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict the results on the test set and see how often we are correct. For the moment we'll use the `predict` method, i.e., assume a 0.5 threshold, not because it's a good choice but just as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xtest) == ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(Xtest) == ytest).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the accuracy.\n",
    "\n",
    "Question: did we do a good job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An aside on class balance\n",
    "\n",
    "We talked briefly about imbalanced classes earlier, situations where there are far more of one class than the other. Ordinarily this isn't an issue, but there are points that do require special consideration.\n",
    "\n",
    "1. Hard classifiers are particularly problematic with imbalanced classes. They're a problem in general, but if you use a hard classifier it may predict everything to be in the majority class.\n",
    "2. You should never use accuracy with imbalanced classes. You generally shouldn't use accuracy anyway, but again, it's easy to get a deceptively high accuracy by predicting everything is in the majority class.\n",
    "3. You should be aware of situations when the class balance of the training data is different than that for which the model will be used in the real world.\n",
    "4. The ability to predict or infer from your data is more a function of the size of the minority class than the size of all the data.\n",
    "4. Some people believe other things, and some of them will be interviewing you. They may want you to talk about some of the ways to deal with them. There are a few techniques used to \"balance\" classes, including **undersampling**, **oversampling**, and **SMOTE**.\n",
    "\n",
    "Oversampling my be appropriate when the training data doesn't match the actual data, but a class weight is generally better.\n",
    "\n",
    "Undersampling may be appropriate in cases when you have more data than you can process; in these cases data in the majority class is less important to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the aside.\n",
    "\n",
    "Let's consider the full matrix of predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(Xtest), ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(Xtest)\n",
    "actual = ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(ytest, model.predict(Xtest), rownames=['actual'], colnames=['predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, except I want to be about to look at different thresholds. Here are a couple helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X, threshold=0.5):\n",
    "    '''Return prediction of the fitted binary-classifier model model on X using\n",
    "    the specifed `threshold`. NB: class 0 is the positive class'''\n",
    "    return np.where(model.predict_proba(X)[:, 0] > threshold,\n",
    "                    model.classes_[0],\n",
    "                    model.classes_[1])\n",
    "\n",
    "def confusion_matrix(model, X, threshold=0.5):\n",
    "    cf = pd.crosstab(ytest, predict(model, Xtest, threshold))\n",
    "    cf = cf.add(pd.DataFrame([[0,0],[0,0]], columns=['Fraud', 'Not'], index=['Fraud', 'Not']), fill_value=0)\n",
    "    cf.index.name = 'actual'\n",
    "    cf.columns.name = 'predicted'\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that good?\n",
    "\n",
    "Let's try a couple other thresholds. Which is best?\n",
    "\n",
    "How do we decide the best value for a threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making hard decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, bad predictions have actual costs, often in money but sometimes in some other metric (e.g., years of life).\n",
    "\n",
    "In this case, let's assume that we might want to have a human investigate every account, but that's expensive so we're going to use our model. If the fitted model thinks it's fraud, we have a human investigate; if not we just approve it.\n",
    "\n",
    "So there are four possibilities:\n",
    "\n",
    " * True positive: the account is fraudulent and we flag it for review.\n",
    " * True negative: the account is good and we create it automatically.\n",
    " * False positive: the account is good but we flag it, so a human has to do some unnecessary work checking the account.\n",
    " * False negative: the account is fraud but we let it through. Oops.\n",
    "\n",
    "We that assign each of these a value, in dollars or whatever. Let's assume that it costs \\$5 to review an account, a good account is worth \\$10 to us, a fraudulent account costs us \\$20, and reviews always catch fradulant accounts. So we can display all this in a cost-benefit matrix.\n",
    "\n",
    "Questions: what are the values for the various cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cb_matrix = pd.DataFrame([[-5, -20], [5, 10]], columns=[\"Fraud\", \"Not\"], index=[\"Fraud\", \"Not\"])\n",
    "cb_matrix.index.name = 'actual'\n",
    "cb_matrix.columns.name = 'predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just multiply stuff. For example, the total payout from the true positives is the number of true positives times the payout of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(model, X, 0.3) * cb_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the total payout is the sum of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(confusion_matrix(model, X, 0.3) * cb_matrix).values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at that for a bunch of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in np.arange(0.0, 1.01, 0.05):\n",
    "    print(\"The payout for a threshold of {:3.2f} is ${:>5.0f}\".\n",
    "            format(threshold,\n",
    "            (confusion_matrix(model, X, threshold) * cb_matrix).values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what should we choose for a threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The profit curve\n",
    "\n",
    "We have a bunch of numbers so we'll make a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_payout(cb_matrix, model, X, threshold):\n",
    "    return (confusion_matrix(model, X, threshold) * cb_matrix).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "profits = []\n",
    "for threshold in thresholds:\n",
    "    profits.append(calculate_payout(cb_matrix, model, X, threshold))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(thresholds, profits)\n",
    "ax.set_xlabel('thresholds')\n",
    "ax.set_ylabel('profits')\n",
    "ax.set_title('Profit Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What's going on at a threshold of 0?\n",
    "\n",
    "Question: What's going on at a threshold of 1?\n",
    "\n",
    "Question: Why is this jagged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a baseline\n",
    "\n",
    "It's pretty common to get confused on cost or benefit of the different boxes. The important thing is to be consistant with the what counts as zero for each of the actual options.\n",
    "\n",
    "Although there are four degrees of freedom in the cost-benefit matrix, really only two of them affect the shape of the profit curve, as we don't have control of the counts of the actual classes. Suppose, for example, in the old days every single account used to be checked. We might decide to have our profit matrix reflect how much our new approach we saves per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_matrix2 = cb_matrix + 5\n",
    "cb_matrix2.iloc[1] -= 10\n",
    "cb_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the original cost-benefit matrix.\n",
    "\n",
    "Question: what's the baseline in the case of Fraud? What's the baseline if in the case of Not-Fraud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 0.9, 0.01)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, matrix in zip(axes, [cb_matrix, cb_matrix2]):\n",
    "    profits = []\n",
    "    for threshold in thresholds:\n",
    "        profits.append(calculate_payout(matrix, model, X, threshold))\n",
    "    ax.plot(thresholds, profits)\n",
    "    ax.set_xlabel('thresholds')\n",
    "    ax.set_ylabel('profits')\n",
    "    ax.set_title('Profit Curve')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what's the difference between these?\n",
    "\n",
    "Or maybe originally we didn't check any of the accounts, but wrote off the cost of fraud cases, assuming they existed in some number and including them in the accounting. Then the cost-benefit matrix looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_matrix3 = cb_matrix.copy()\n",
    "cb_matrix3.iloc[0] += 20\n",
    "cb_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for ax, matrix in zip(axes, [cb_matrix, cb_matrix2, cb_matrix3]):\n",
    "    profits = []\n",
    "    for threshold in thresholds:\n",
    "        profits.append(calculate_payout(matrix, model, X, threshold))\n",
    "    ax.plot(thresholds, profits)\n",
    "    ax.set_xlabel('thresholds')\n",
    "    ax.set_ylabel('profits')\n",
    "    ax.set_title('Profit Curve')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These three graphs have the same overall shape. What do these cost benefit matrices have in common?\n",
    "\n",
    "Create another cost-benefit matrix for which the profit curve will have the same shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(cb_matrix)\n",
    "IPython.display.display(cb_matrix2)\n",
    "IPython.display.display(cb_matrix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
