{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "52ce26f3-3de8-41ac-83bb-971e2325e85c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Fittin' Lines to Data\n",
    "#### Dan Wiesenthal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f46f2d18-c938-48fc-b93d-2a99b2ba9e0e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives - Morning\n",
    "\n",
    "- Linear regression overview\n",
    "    - Fitting a line\n",
    "    - Linear relationshiops, Exact vs Inexact\n",
    "    - Where do we put the line? What is our error metric?\n",
    "- Expressing our model\n",
    "    - $y = Mx + b$\n",
    "    - $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "    - Matrix representation\n",
    "- How good is our model?\n",
    "    - $RSS$\n",
    "    - $RSE$ aka $RMSE$\n",
    "    - $R^2$  \n",
    "        - Fraction of variance explained, or, $1 - FUV$\n",
    "- Compare models (against null)\n",
    "    - Are all my coefficients not zero?\n",
    "    - Are some new coefficients not zero?\n",
    "- Interpreting model output\n",
    "    - Interpret coefficients from statsmodels\n",
    "- Assumptions of Linear Regression\n",
    "    - Linearity\n",
    "    - Constant Variance (Homoscedasticity)\n",
    "    - Independence of Errors\n",
    "    - Normality of Errors\n",
    "    - Lack of Multicollinearity\n",
    "- Outliers\n",
    "    - Types of outliers\n",
    "    - Detecting outliers\n",
    "        - Leverage (change in prediction over change in actual)\n",
    "        - Studentized Residuals\n",
    "    - Residual plots\n",
    "- Multicollinearity\n",
    "    - Perfect\n",
    "    - Partial\n",
    "- QQ Plots\n",
    "    - (Video, ragequit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Overview\n",
    "### Fitting a line\n",
    "### Linear relationships, Exact vs Inexact\n",
    "### Where do we put the line?  What is our error metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Overview\n",
    "### Linear relationships, Exact vs Inexact\n",
    "<img src=\"images/exact_inexact.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Overview\n",
    "### Where do we put the line? What is our error metric?\n",
    "<img src=\"images/line_placement.png\"></img>\n",
    "- Notes on image:\n",
    "    - There are many lines possible\n",
    "    - We don't want to minimize the absolute error, since that could tell us that we have 0 error when we clearly don't have 0 error (upper right)\n",
    "    - We'll minimize squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expressing our model\n",
    "## Simple (Bivariate) Linear Regression\n",
    "\n",
    "- The World\n",
    "    - what you're presuming the world looks like:\n",
    "    $$Y = \\beta_0 + \\beta_1X + \\epsilon$$\n",
    "\n",
    "\n",
    "- $\\beta_0$ and $\\beta_1$ are unknown constants that represent the intercept and slope\n",
    "- $\\epsilon$, the error term, is i.i.d $N(0, \\sigma^2)$\n",
    "_______\n",
    "- The Model\n",
    "    - what you've created from data to estimate the world:\n",
    "    $$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x$$\n",
    "- $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ are model coefficient estimates\n",
    "- $\\hat{y}$ indicates the prediction of $Y$ based on $X=x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Expressing our model\n",
    "## Multiple Linear Regression\n",
    "- Model in matrix form\n",
    "<img src=\"images/matrix_form.png\"></img>\n",
    "- Design Matrix $X$\n",
    "<img src=\"images/design_matrix.png\"></img>\n",
    "- Coefficient Matrix $\\beta$\n",
    "<img src=\"images/coefficient_matrix.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How good is our model?\n",
    "### RSS\n",
    "$RSS = SSE = \\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}$\n",
    "- How far off were we? In, um, raw units (squared).\n",
    " \n",
    "### RSE (aka  RMSE)\n",
    "$RSE = RMSE = \\sqrt{\\frac{RSS}{n - p - 1}} = \\sqrt{\\frac{\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}}{n - p - 1}}$\n",
    "- How far off were we? For, y'know, a typical point. On average. (Controlling for degrees of freedom lost.)\n",
    "- Similarly, we have the standard error (biased or not depending on dof), $\\frac{\\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}}{n - p - 1}$\n",
    " \n",
    "### R^2\n",
    "$1 - FUV = 1 - \\frac{Deviation_u}{Deviation_t}$\n",
    "- Total deviation, $Deviation_t$: $(y_1 - \\bar{y})$\n",
    "- Explained deviation, $Deviation_e$: $(\\hat{y_1} - \\bar{y})$\n",
    "- Unexplained deviation, $Deviation_u$: $(y_1 - \\hat{y_1})$\n",
    "\n",
    "<img src=\"images/tight_loose_fit.png\"></img>\n",
    "<img src=\"images/rss.png\"></img>\n",
    "<img src=\"images/r_squared_examples.png\"></img>\n",
    "- Notes on images:\n",
    "    - We can have two lines with the same slope and intercept, each of which is the best fit line, and they can still be differently \"good\" fits if the data is spread out\n",
    "    - If we didn't have a model, we would just predict the mean, so that's what we compare ourselves against to see how good our model is\n",
    "    - We could have data that clearly has a signal but a low $R^2$ if we aren't properly modeling it; low $R^2$ isn't necessarily grounds for concluding there's no signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing Models\n",
    "### Did I add a useful feature?\n",
    "1) Set up hypotheses, comparison\n",
    "\n",
    "$H_0: \\beta_{height} = \\beta_{color} = 0$\n",
    "\n",
    "$H_A:$ at least one of $\\beta_{height}$ or $\\beta_{color}$ is nonzero\n",
    "\n",
    "\n",
    "\n",
    "Reduced Model: $$Y = \\beta_0 + \\beta_{weight} + \\beta_{modelyear} + \\beta_{cartype}$$\n",
    "\n",
    "Full Model: $$Y = \\beta_0 + \\beta_{weight} + \\beta_{modelyear} + \\beta_{cartype} + \\beta_{height} + \\beta_{color}$$\n",
    "\n",
    "(X's omitted for clarity)\n",
    "\n",
    "2) Compute F-statistic\n",
    "$$F = \\frac{\\frac{(RSS_{reduced} - RSS_{full})}{(p_{full} - p_{reduced})}}{\\frac{RSS_{full}}{(n - p_{full} -1)}}$$\n",
    "\n",
    "3) Compute p-value\n",
    "\n",
    "p_val = 1 - scipy.stats.f.cdf(calculated_F, p_full - p_reduced, n - p_full - 1)\n",
    "\n",
    "______\n",
    "### Are any of my features useful?\n",
    "1) Set up hypotheses\n",
    "$H_0: \\beta_1 = \\beta_2 = \\beta_3 = ... = \\beta_p = 0$\n",
    "\n",
    "$H_A:$ at least one $\\beta_j$ is nonzero\n",
    "\n",
    "2) $F = \\frac{\\frac{TSS - RSS}{p}}{\\frac{RSS}{n - p - 1}}$ ~ $F_{p, n - p - 1}$\n",
    "\n",
    "_______\n",
    "<img src=\"images/significance_testing.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Interpreting Model Output\n",
    "\n",
    "### Interpreting coefficients from statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/.virtualenvs/linear_regression_lecture/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GNP.deflator</th>\n",
       "      <th>GNP</th>\n",
       "      <th>Unemployed</th>\n",
       "      <th>Armed.Forces</th>\n",
       "      <th>Population</th>\n",
       "      <th>Year</th>\n",
       "      <th>Employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>83.0</td>\n",
       "      <td>234.289</td>\n",
       "      <td>235.6</td>\n",
       "      <td>159.0</td>\n",
       "      <td>107.608</td>\n",
       "      <td>1947</td>\n",
       "      <td>60.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>88.5</td>\n",
       "      <td>259.426</td>\n",
       "      <td>232.5</td>\n",
       "      <td>145.6</td>\n",
       "      <td>108.632</td>\n",
       "      <td>1948</td>\n",
       "      <td>61.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>88.2</td>\n",
       "      <td>258.054</td>\n",
       "      <td>368.2</td>\n",
       "      <td>161.6</td>\n",
       "      <td>109.773</td>\n",
       "      <td>1949</td>\n",
       "      <td>60.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>89.5</td>\n",
       "      <td>284.599</td>\n",
       "      <td>335.1</td>\n",
       "      <td>165.0</td>\n",
       "      <td>110.929</td>\n",
       "      <td>1950</td>\n",
       "      <td>61.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>96.2</td>\n",
       "      <td>328.975</td>\n",
       "      <td>209.9</td>\n",
       "      <td>309.9</td>\n",
       "      <td>112.075</td>\n",
       "      <td>1951</td>\n",
       "      <td>63.221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GNP.deflator      GNP  Unemployed  Armed.Forces  Population  Year  \\\n",
       "1947          83.0  234.289       235.6         159.0     107.608  1947   \n",
       "1948          88.5  259.426       232.5         145.6     108.632  1948   \n",
       "1949          88.2  258.054       368.2         161.6     109.773  1949   \n",
       "1950          89.5  284.599       335.1         165.0     110.929  1950   \n",
       "1951          96.2  328.975       209.9         309.9     112.075  1951   \n",
       "\n",
       "      Employed  \n",
       "1947    60.323  \n",
       "1948    61.122  \n",
       "1949    60.171  \n",
       "1950    61.187  \n",
       "1951    63.221  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gnp = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df_gnp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_gnp.GNP  # Set up our design matrix, X\n",
    "X = sm.add_constant(X)  # We need to manually add a constant, aka intercept, when working with statsmodels\n",
    "y = df_gnp.Employed # Set up our target, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   51.8436</td> <td>    0.681</td> <td>   76.087</td> <td> 0.000</td> <td>   50.382</td> <td>   53.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GNP</th>   <td>    0.0348</td> <td>    0.002</td> <td>   20.374</td> <td> 0.000</td> <td>    0.031</td> <td>    0.038</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = sm.OLS(y, X)\n",
    "est = est.fit()\n",
    "est.summary()  # whoa, too much output for the moment\n",
    "\n",
    "# a utility function to only show the coefficient section of summary\n",
    "from IPython.core.display import HTML\n",
    "def short_summary(est):\n",
    "    return HTML(est.summary().tables[1].as_html())\n",
    "\n",
    "short_summary(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talking points on short summary:\n",
    "- sanity check\n",
    "- constant, GNP coefficients\n",
    "- do they point the right direction\n",
    "- rough magnitude check (if you have a sense)\n",
    "- are the p values significant\n",
    "- oh hey look we have confidence intervals, that's nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Employed</td>     <th>  R-squared:         </th> <td>   0.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   415.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 15 May 2017</td> <th>  Prob (F-statistic):</th> <td>8.36e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:10:49</td>     <th>  Log-Likelihood:    </th> <td> -14.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    16</td>      <th>  AIC:               </th> <td>   33.81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    14</td>      <th>  BIC:               </th> <td>   35.35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   51.8436</td> <td>    0.681</td> <td>   76.087</td> <td> 0.000</td> <td>   50.382</td> <td>   53.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GNP</th>   <td>    0.0348</td> <td>    0.002</td> <td>   20.374</td> <td> 0.000</td> <td>    0.031</td> <td>    0.038</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.925</td> <th>  Durbin-Watson:     </th> <td>   1.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.382</td> <th>  Jarque-Bera (JB):  </th> <td>   1.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.664</td> <th>  Prob(JB):          </th> <td>   0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.759</td> <th>  Cond. No.          </th> <td>1.66e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               Employed   R-squared:                       0.967\n",
       "Model:                            OLS   Adj. R-squared:                  0.965\n",
       "Method:                 Least Squares   F-statistic:                     415.1\n",
       "Date:                Mon, 15 May 2017   Prob (F-statistic):           8.36e-12\n",
       "Time:                        23:10:49   Log-Likelihood:                -14.904\n",
       "No. Observations:                  16   AIC:                             33.81\n",
       "Df Residuals:                      14   BIC:                             35.35\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         51.8436      0.681     76.087      0.000      50.382      53.305\n",
       "GNP            0.0348      0.002     20.374      0.000       0.031       0.038\n",
       "==============================================================================\n",
       "Omnibus:                        1.925   Durbin-Watson:                   1.619\n",
       "Prob(Omnibus):                  0.382   Jarque-Bera (JB):                1.215\n",
       "Skew:                           0.664   Prob(JB):                        0.545\n",
       "Kurtosis:                       2.759   Cond. No.                     1.66e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.66e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.summary()  # now let's look at the full summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1078e6f50>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD5IezEY5ayb3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d6adf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We pick 100 hundred points equally spaced from the min to the max\n",
    "X_prime = np.linspace(X.GNP.min(), X.GNP.max(), 100)[:, np.newaxis]\n",
    "X_prime = sm.add_constant(X_prime)  # add constant as we did before\n",
    "\n",
    "# Now we calculate the predicted values\n",
    "y_hat = est.predict(X_prime)\n",
    "\n",
    "plt.scatter(X.GNP, y, alpha=0.3)  # Plot the raw data\n",
    "plt.xlabel(\"Gross National Product\")\n",
    "plt.ylabel(\"Total Employment\")\n",
    "plt.plot(X_prime[:, 1], y_hat, 'r', alpha=0.9)  # Add the regression line, colored in red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "44f566e4-975c-4dd9-bf74-e18c516385e3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assumptions of Linear Regression\n",
    "- Assumptions of Linear Regression\n",
    "    - Linearity\n",
    "        - We assume it's possible\n",
    "    - Constant Variance (Homoscedasticity)\n",
    "        - Our variance shouldn't change as y or X gets bigger\n",
    "    - Independence of Errors\n",
    "        - We should gain no information from knowing the error of a different data point\n",
    "    - Normality of Errors\n",
    "        - Errors should be normally distributed\n",
    "    - Lack of Multicollinearity\n",
    "        - We shouldn't be measuring the same thing in multiple ways\n",
    "        \n",
    "We can't always meet these assumptions, and often have to find ways to combat that reality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/residual_plots.png\"></img>\n",
    "Notes on image:\n",
    "- a looks lovely\n",
    "- b has some extreme outliers\n",
    "- c is curvilinear; a change in x does not lead to the same response in y across all values of x\n",
    "- d violates homoscedasticity; the error variance is not constant but rather depends on x\n",
    "- e indicates a linear relationship between the residuals and a variable not in the model; we probably want to try to find/measure that variable and add it into our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "- Outliers\n",
    "    - Types of outliers\n",
    "    - Detecting outliers\n",
    "        - Leverage (change in prediction over change in actual)\n",
    "        - Studentized Residuals\n",
    "    - Residual plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "### Types of outliers\n",
    "<img src=\"images/types_of_outliers.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "## Detecting outliers\n",
    "\n",
    "### Visually\n",
    "We can make a **residual plot** to help identify outliers visually.  In this case, we plot the residuals by the fitted/predicted value, y.\n",
    "\n",
    "Even better, we can **standardize** by dividing by the standard error.  This allows us to see which points are outliers in a common way, perhaps by checking whether they're above or below 2 (roughly 95% of our data is covered by that region).\n",
    "\n",
    "<img src=\"images/standardized_residuals.png\"></img>\n",
    "\n",
    "Even better still, we can **\"Studentize\"** the errors by dividing, not by the \"global\" standard error for our model, but by the standard error of our model at the particular value of y where the residual occurred.  Our confidence intervals change depending on how much data we have seen in a particular region.  If we've seen a lot of data, our intervals are tight; otherwise, they are wide.  So, it takes \"more\" for a data point to be considered an outlier if it is in a region in which we have little data.\n",
    "\n",
    "<img src=\"images/variable_confidence_intervals.png\"></img>\n",
    "\n",
    "\n",
    "<img src=\"images/studentized_residual_plot.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "## Detecting outliers\n",
    "### Programmatically\n",
    "\n",
    "Leverage\n",
    "- A high-leverage point is an observation with an unusual X value.\n",
    "- It does not necessarily have a large effect on the regression model (it could lie right along the best fit line of a model fit without it).\n",
    "- Most common measure is the \"hat value\": $h_{ii}=(H)_{ii}$\n",
    "- The $i$th diagonal of the hat matrix $$ H = X(X^TX)^{-1}X^T $$ is the leverage of the particular data point\n",
    "- Heuristic explanation: dragging a single point upwards and seeing how much the prediction line follows. If, for a fixed $x$, we perturb $y$ and $\\hat{y_i}$ moves a little, then it is low leverage; if it moves a lot, then it is high leverage. So you can think of it as $h_{ii} = \\frac{d\\hat{y_i}}{dy_i}$\n",
    "\n",
    "We can use Leverage to Studentize our errors\n",
    "- The variance of the $i$th residual is $$ var(\\hat{\\epsilon_i}) = \\sigma^2(1 - \\frac{1}{n} - \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}{(x_j - \\bar{x})^2}}) $$\n",
    "- The corresponding Studentized residual is then $$ t_i = \\frac{\\hat{\\epsilon_i}}{ \\hat{\\sigma}\\sqrt{1 - h_{ii}} } $$ where $\\hat{\\sigma}$ is an appropriate estimate of $\\sigma$ (basically square root of RSE)\n",
    "\n",
    "\n",
    "Now that we have either standardized (divided by \"global\" standard error) or studentized (divided by \"local\" standard error) residuals, we can use them to programmatically identify outlier points.  Roughly 95% of our data will fall within $\\pm2$ standardized/studentized errors.  We likely don't want to throw out 1 in 20 data points, though, so perhaps we'll cut anything off if it's outside the $\\pm3$ bounds.  Note, if we do find outliers we usually train two models, one with them and one without.  Also we might chase down the story behind the outlier data points, if we can (was data entered incorrectly? was there a tsunami that day? did the servers go down?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity \n",
    "Measuring the same thing two different ways.\n",
    "\n",
    "If we had the grade of every class for a student, as well as their GPA, we have multicollinearity because GPA is a linear function of their grades.\n",
    "\n",
    "Could also just be married_is_True and married_is_False as features.\n",
    "\n",
    "- Perfect vs Partial\n",
    "  - Perfect\n",
    "      - Your model will often fail to run/converge (some libraries will still be ok, but don't count on it)\n",
    "      - Unlikely to occur in practice, unless you goof\n",
    "          - Consider a case where $\\beta_1$ and $\\beta_2$ measure \"years since graduating Galvanize\" and \"years as a data scientist\", and of course these are the same ;)\n",
    "          - If the true \"value\" for the coefficient is 10, our model could assign weights to $\\beta_1$ and $\\beta_2$ of (10, 0), (0, 10), (5, 5)... or any combination between these, and get the same values.\n",
    "  - Partial\n",
    "      - Uncertainty in the model coefficients becomes large\n",
    "      - Does not necessarily affect model accuracy or bias the coefficients\n",
    "      \n",
    "<img src=\"images/partial_multicollinearity.png\"></img>\n",
    "Notes on image: \n",
    "- Here $x_1$ and $x_2$ are both correlated with the dependent variable, and are correlated with each other.\n",
    "- If they are relatively less correlated with each other (top), then the \"black\" region of their overlap is smaller.  If they're relatively more correlated with each other (bottom), then the \"black\" region of their overlap is larger.  The variation that each variable captures of the target is its circle; when the circles overlap heavily, we don't know which variable to give credit to, as their overlapping region could go to either.  One of them could get full credit for it, no credit for it, or somewhere in between, which means the coefficient values are going to vary widely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity\n",
    "\n",
    "- Correlation Matrix / Scatterplot Matrix / Bivariate Correlations\n",
    "    - Can only pick up pairwise effects\n",
    "- Variance Inflation Factors, VIF\n",
    "    - I heard you liked regression, so I put some regression in your regression so you can regress while you regress\n",
    "    - Regress each of the independent variables on all the other independent variables\n",
    "        - If any of these auxiliary $R^2$ values are near 1, there is high multicollinearity\n",
    "        - Makes sense: if we can predict one variable with the others... it's collinear with them\n",
    "    - For each of the predictor variables j: $$ VIF = \\frac{1}{1 - R_j^2} $$\n",
    "    - This measures how much the variance of the regression coefficients is inflated relative to the noninflated baseline of linearly independent predictors\n",
    "    - In the best case:\n",
    "        - We can't predict our $j$th feature with the others, so $R_j^2$ is 0\n",
    "        - This means VIF is 1, and $x_j$ is linearly independent of the other independent variables\n",
    "    - If $VIF > 10$, then multicollinearity is probably a problem\n",
    "        - 10 is a rule of thumb; you may be more or less conservative\n",
    "\n",
    "Why do we subtract and invert rather than just talking about auxiliary $R^2$ values, where lower is better? Good. Question. (Perhaps fancy terms are correlated with salary for statisticians... a Salary Inflation Factor ;)\n",
    "\n",
    "- What should I do if I have high multicollinearity?\n",
    "    - Try to gather more data\n",
    "    - Consider combining multiple intercorrelated variables into one\n",
    "    - Don't interpret coefficients, just use your model to predict\n",
    "    - Discard the offending variable(s)\n",
    "    - What if I created multicollinearity by adding polynomial terms?\n",
    "        - Say I added $x_{40}$ and $x_{40}^2$ because there was a quadratic trend in $x_{40}$\n",
    "        - Now I have two variables that exhibit high multicollinearity\n",
    "        - I can subtract the mean and use the deviations (and squared deviations) as variables in the model; this reduces multicollinearity, but also changes the interpretation of the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QQ Plots\n",
    "Watch this: https://www.youtube.com/watch?v=X9_ISJ0YpGw\n",
    "\n",
    "(ragequit QQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "jupyter nbconvert Linear_Regression.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
