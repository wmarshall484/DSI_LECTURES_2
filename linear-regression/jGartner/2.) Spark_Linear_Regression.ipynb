{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Regression using Spark</h1>\n",
    "<p>We'll be going over a short case study of using Spark to perform a linear regression.  I'll be using data from the <a href=\"https://rptsvr1.tea.texas.gov/perfreport/tapr/2013/download/DownloadData.html\">Texas Education Agency</a>, particularly focusing on campus level data from STAAR Phase-in 1 Level II (Grades 3 to 8, End of Course).  As with all data tasks, we'll need to clean and view data before we can make meaningful progress.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2059\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "path = \"/Users/joesphgartner/Desktop/data/texaseduagency/Starr_phase1_Grades3to8.txt\"\n",
    "spk = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "df = spk.read.csv(path, header=True)\n",
    "cols = df.columns\n",
    "print(len(cols))\n",
    "#df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gross</h2>\n",
    "<p>The above columns bum me out.  I have no clue what this means.  We do have access to the <a href=\"https://rptsvr1.tea.texas.gov/perfreport/tapr/2013/download/campstaar1a.html\">data dictionary</a>. Have a look at the provided link. You can see that there is data for many grades, ethnicities, and schools.  For obvious reasons, the schools have been anonomysed; this is unfortunate as fusing this data with meadian household income would be an interesting way of enriching this dataset.<br/><br/>\n",
    "\n",
    "Lets see if we can't use our web scraping skills to get descriptions programatically.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import ssl\n",
    "\n",
    "# This performs SSL certificate verification\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Fake connection coming from a browser\n",
    "hdr = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "\n",
    "# URL for the data dict\n",
    "url = \"https://rptsvr1.tea.texas.gov/perfreport/tapr/2013/download/campstaar1a.html\"\n",
    "\n",
    "# Build a request\n",
    "req = request.Request(url, headers=hdr)\n",
    "gcontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)  # Only for gangstars -> Stole this line from S.O.\n",
    "res = request.urlopen(req, context=gcontext)\n",
    "\n",
    "# Create a BS object\n",
    "soup = BeautifulSoup(res.read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Limit the data</h2>\n",
    "<p>Let's start simple (generally a good idea for large messy data).  Looking at the table it's self, we can seen there are many columns specific to a particular grade.  Let's start by simply selecting those columns that pertain to the 3rd grade.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the table, and then the table row objects\n",
    "table = soup.find(\"tbody\")\n",
    "rows = table.find_all(\"tr\")\n",
    "print(len(rows)) # <- number of rows in the html table\n",
    "\n",
    "# Find only colums with \"Grade 3\" in the description, get the column name\n",
    "g3_cols = []\n",
    "for row in rows:\n",
    "    if str(row).find(\"Grade 3\") != -1:\n",
    "        g3_cols.append(row.find_all(\"td\")[0].text.strip())\n",
    "        \n",
    "len(g3_cols) # <- number of column names we extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CB03AMA1512N='.', CB03AMA1012D='.', CB03AMA1512R='.', CA03AMA1512N='.', CA03AMA1012D='.', CA03AMA1512R='.', CI03AMA1512N='.', CI03AMA1012D='.', CI03AMA1512R='.', C303AMA1512N='.', C303AMA1012D='.', C303AMA1512R='.', CR03AMA1512N='.', CR03AMA1012D='.', CR03AMA1512R='.', CL03AMA1512N='.', CL03AMA1012D='.', CL03AMA1512R='.', CE03AMA1512N='.', CE03AMA1012D='.', CE03AMA1512R='.', CF03AMA1512N='.', CF03AMA1012D='.', CF03AMA1512R='.', CH03AMA1512N='.', CH03AMA1012D='.', CH03AMA1512R='.', CM03AMA1512N='.', CM03AMA1012D='.', CM03AMA1512R='.', C403AMA1512N='.', C403AMA1012D='.', C403AMA1512R='.', CS03AMA1512N='.', CS03AMA1012D='.', CS03AMA1512R='.', C203AMA1512N='.', C203AMA1012D='.', C203AMA1512R='.', CW03AMA1512N='.', CW03AMA1012D='.', CW03AMA1512R='.', CB03ARE1512N='.', CB03ARE1012D='.', CB03ARE1512R='.', CA03ARE1512N='.', CA03ARE1012D='.', CA03ARE1512R='.', CI03ARE1512N='.', CI03ARE1012D='.', CI03ARE1512R='.', C303ARE1512N='.', C303ARE1012D='.', C303ARE1512R='.', CR03ARE1512N='.', CR03ARE1012D='.', CR03ARE1512R='.', CL03ARE1512N='.', CL03ARE1012D='.', CL03ARE1512R='.', CE03ARE1512N='.', CE03ARE1012D='.', CE03ARE1512R='.', CF03ARE1512N='.', CF03ARE1012D='.', CF03ARE1512R='.', CH03ARE1512N='.', CH03ARE1012D='.', CH03ARE1512R='.', CM03ARE1512N='.', CM03ARE1012D='.', CM03ARE1512R='.', C403ARE1512N='.', C403ARE1012D='.', C403ARE1512R='.', CS03ARE1512N='.', CS03ARE1012D='.', CS03ARE1512R='.', C203ARE1512N='.', C203ARE1012D='.', C203ARE1512R='.', CW03ARE1512N='.', CW03ARE1012D='.', CW03ARE1512R='.', CB03AMA1013D='.', CB03ARE1013D='.', CA03AMA1013D='.', CA03ARE1013D='.', CI03AMA1013D='.', CI03ARE1013D='.', C303AMA1013D='.', C303ARE1013D='.', CR03AMA1013D='.', CR03ARE1013D='.', CL03AMA1013D='.', CL03ARE1013D='.', CE03AMA1013D='.', CE03ARE1013D='.', CF03AMA1013D='.', CF03ARE1013D='.', CH03AMA1013D='.', CH03ARE1013D='.', CM03AMA1013D='.', CM03ARE1013D='.', C403AMA1013D='.', C403ARE1013D='.', CS03AMA1013D='.', CS03ARE1013D='.', C203AMA1013D='.', C203ARE1013D='.', CW03AMA1013D='.', CW03ARE1013D='.', CB03AMA1513N='.', CB03AMA1513R='.', CB03ARE1513N='.', CB03ARE1513R='.', CA03AMA1513N='.', CA03AMA1513R='.', CA03ARE1513N='.', CA03ARE1513R='.', CI03AMA1513N='.', CI03AMA1513R='.', CI03ARE1513N='.', CI03ARE1513R='.', C303AMA1513N='.', C303AMA1513R='.', C303ARE1513N='.', C303ARE1513R='.', CR03AMA1513N='.', CR03AMA1513R='.', CR03ARE1513N='.', CR03ARE1513R='.', CL03AMA1513N='.', CL03AMA1513R='.', CL03ARE1513N='.', CL03ARE1513R='.', CE03AMA1513N='.', CE03AMA1513R='.', CE03ARE1513N='.', CE03ARE1513R='.', CF03AMA1513N='.', CF03AMA1513R='.', CF03ARE1513N='.', CF03ARE1513R='.', CH03AMA1513N='.', CH03AMA1513R='.', CH03ARE1513N='.', CH03ARE1513R='.', CM03AMA1513N='.', CM03AMA1513R='.', CM03ARE1513N='.', CM03ARE1513R='.', C403AMA1513N='.', C403AMA1513R='.', C403ARE1513N='.', C403ARE1513R='.', CS03AMA1513N='.', CS03AMA1513R='.', CS03ARE1513N='.', CS03ARE1513R='.', C203AMA1513N='.', C203AMA1513R='.', C203ARE1513N='.', C203ARE1513R='.', CW03AMA1513N='.', CW03AMA1513R='.', CW03ARE1513N='.', CW03ARE1513R='.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df.select(g3_cols)\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Progress</h2>\n",
    "<p>OK, now let's try to answer some questions.  Let's start by getting those rows that have a percent for both white, black, and hispanic students.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8555\n",
      "912\n"
     ]
    }
   ],
   "source": [
    "print(df_small.count())\n",
    "df_filt = df_small.filter((df_small['CB03AMA1512R']!=\".\") & (df_small['CB03AMA1512R']!=\"-1\") & \\\n",
    "                          (df_small['CW03AMA1512R']!=\".\") & (df_small['CW03AMA1512R']!=\"-1\") & \\\n",
    "                          (df_small['CH03AMA1512R']!=\".\") & (df_small['CH03AMA1512R']!=\"-1\"))\n",
    "print(df_filt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>More Sophisticated Transformations</h2>\n",
    "<p>The data we have is on a per facility basis, which means the tagets (i.e. the passage rate for the various students) are in the same rows.  What we need to do is make separate rows for each category, and create a variable to distinguis between the three.  Let's have hispanic students be our base class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Passage=68.0, is_AA=0, is_CA=0)\n",
      "Row(Passage=46.0, is_AA=1, is_CA=0)\n",
      "Row(Passage=66.0, is_AA=0, is_CA=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(2, {}), label=68.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#Create limited DF with only hispanic students as our targets\n",
    "df_hs = df_filt.withColumn(\"Passage\", df_filt[\"CH03AMA1512R\"].cast(DoubleType()))\\\n",
    "                 .withColumn('is_AA', lit(0))\\\n",
    "                 .withColumn('is_CA', lit(0))\\\n",
    "                 .select([\"Passage\", \"is_AA\", \"is_CA\"])\n",
    "            \n",
    "print(df_hs.head())\n",
    "\n",
    "#Create limited DF with only black students as our targets\n",
    "df_aa = df_filt.withColumn(\"Passage\", df_filt[\"CB03AMA1512R\"].cast(DoubleType()))\\\n",
    "                 .withColumn('is_AA', lit(1))\\\n",
    "                 .withColumn('is_CA', lit(0))\\\n",
    "                 .select([\"Passage\", \"is_AA\", \"is_CA\"])\n",
    "            \n",
    "print(df_aa.head())\n",
    "\n",
    "#Create limited DF with only white students as our targets\n",
    "df_ca = df_filt.withColumn(\"Passage\", df_filt[\"CW03AMA1512R\"].cast(DoubleType()))\\\n",
    "                 .withColumn('is_AA', lit(0))\\\n",
    "                 .withColumn('is_CA', lit(1))\\\n",
    "                 .select([\"Passage\", \"is_AA\", \"is_CA\"])\n",
    "            \n",
    "print(df_ca.head())\n",
    "\n",
    "#Combined into single DF and format for ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df_all = df_hs.union(df_aa).union(df_ca)\n",
    "df_all.count()\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"is_AA\", \"is_CA\"], outputCol=\"features\")\n",
    "data = vecAssembler.transform(df_all).selectExpr(\"features as features\", \"Passage as label\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------------+\n",
      "| features|label|       prediction|\n",
      "+---------+-----+-----------------+\n",
      "|(2,[],[])| 45.0|71.54918032786885|\n",
      "|(2,[],[])| 46.0|71.54918032786885|\n",
      "+---------+-----+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "model = lr.fit(train)\n",
    "\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.54918032786885\n",
      "[-7.49385529329,9.05301445951]\n"
     ]
    }
   ],
   "source": [
    "print(model.intercept)\n",
    "print(model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
