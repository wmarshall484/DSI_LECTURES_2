{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Text Indexing in Python\n",
    "\n",
    "**Objective of text indexing**: to create a representation (vector) for each document that we can use to relate documents one to the other. This representation should encapsulate the overall \"meaning\" of the document.\n",
    "\n",
    "In this sequence, we don't care much about obtaining every detail and subtleties of that meaning, we want to obtain a synthetic representation to process a large amount of documents and extract the emerging global trends.\n",
    "\n",
    "So, there are two sides to NLP:\n",
    "- indexing text: reducing complexity to be able to process large corpus of documents, for information retrieval\n",
    "- extracting information from text: recognizing details in the meaning of each document, paragraph, sentence, to be able to extract relevant entities from these documents.\n",
    "\n",
    "These two purposes can be mutually beneficial, but the more you get into the details, the more complex the processing will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Text sources and possible text mining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = u\"My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt – sleeveless, white eyelet lace; I was wearing it as a farewell gesture. My carry-on item was a parka. In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds. It rains on this inconsequential town more than any other place in the United States of America. It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old. It was in this town that I’d been compelled to spend a month every summer until I was fourteen. That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.\"\n",
    "\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_string = paragraph.encode('utf-8')\n",
    "\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', unicode(input_str))\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii\n",
    "\n",
    "input_string = remove_accents(paragraph)\n",
    "\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating bag-of-words for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenize document\n",
    "\n",
    "**\"Tokenize\"** means creating \"tokens\" which are atomic units of the text. These tokens are usually words we extract from the document by splitting it (using punctuations as a separator). We can also consider sentences as tokens (and words as sub-tokens of sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n",
    "\n",
    "for sent in sent_tokens:\n",
    "    print(\"--- sentence: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = map(word_tokenize, sent_tokens)\n",
    "#tokens = word_tokenize(input_string)\n",
    "for sent in tokens:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))\n",
    "#print(\"--- nltk tokens from paragraph:\\n{}\".format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tokens_lower = [map(string.lower, sent) for sent in tokens]\n",
    "\n",
    "for sent in tokens_lower:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Filtering stopwords (and punctuation)\n",
    "\n",
    "**Stopwords** are words that should be stopped at this step because they do not carry much information about the actual meaning of the document. Usually, they are \"common\" words you use. You can find lists of such **stopwords** online, or embedded within the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list found at http://www.textfixer.com/resources/common-english-words.txt\n",
    "# 'not' has been removed (do you know why ?)\n",
    "\n",
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to filter punctuation tokens: tokens made of punctuation marks. We can find a list of those punctuations in string.punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_ = set(string.punctuation)\n",
    "print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "tokens_filtered = map(filter_tokens, tokens_lower)\n",
    "\n",
    "for sent in tokens_filtered:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stemming and lemmatization\n",
    "\n",
    "**Stemming** means reducing each word to a **stem**. That is, reducing each word in all its diversity to a root common to all its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [map(stemmer_porter.stem, sent) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (porter): {}\".format(tokens_stemporter[0]))\n",
    "\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "tokens_stemsnowball = [map(stemmer_snowball.stem, sent) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (snowball): {}\".format(tokens_stemsnowball[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. N-Grams\n",
    "\n",
    "<span style=\"color:red\">To capture sequences of tokens</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens_stemporter[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def join_sent_ngrams(input_tokens, n):\n",
    "    # first add the 1-gram tokens\n",
    "    ret_list = list(input_tokens)\n",
    "    \n",
    "    #then for each n\n",
    "    for i in range(2,n+1):\n",
    "        # add each n-grams to the list\n",
    "        ret_list.extend(['-'.join(tgram) for tgram in ngrams(input_tokens, n)])\n",
    "    \n",
    "    return(ret_list)\n",
    "\n",
    "tokens_ngrams = map(lambda x : join_sent_ngrams(x, 3), tokens_stemporter)\n",
    "\n",
    "for sent in tokens_ngrams:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Part-of-Speech tagging\n",
    "\n",
    "This is an alternative process that relies on machine learning to tag each word in a sentence with its function. In libraries such as NLTK, there are embedded tools to do that. Tags detected depend on the corpus used for training. In NLTK, the function `nltk.pos_tag()` uses the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sent_tags = map(pos_tag, tokens)\n",
    "\n",
    "for sent in sent_tags:\n",
    "    print(\"--- sentence tags: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter verbs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in sent_tags:\n",
    "    tags_filtered = [t for t in sent if t[1].startswith('VB')]\n",
    "    print(\"--- verbs:\\n{}\".format(tags_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NPB: {<DT|PP\\$>?<JJ|NN|,>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "  V2V: {<V.*> <TO> <V.*>}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(sent_tags[1])\n",
    "\n",
    "#print result\n",
    "\n",
    "for sent in sent_tags:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NPB': print(subtree)\n",
    "        if subtree.label() == 'V2V': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Creating a vector for each document in a corpus\n",
    "\n",
    "Our objective now is, from a corpus of documents, to build a vector representation of each of these documents. This vector representation will be used for:\n",
    "\n",
    "- mine the features that can caracterize classes of documents (supervised learning)\n",
    "- mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens).\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "## 2.1. Vocabulary and term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"Aunt Mary is going to town\"\n",
    "s2 = \"Mary and you meet at the town hall \"\n",
    "s3 = \"We should meet in town\"\n",
    "s4 = \"I will meet Mary in town\"\n",
    "\n",
    "corpus = [s1,s2,s3,s4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = ['is', 'and', 'we', 'you', 'will',\n",
    "                                   'the','going', 'to', 'having', 'a',\n",
    "                                   'should', 'in', 'i', 'm', 'at'],\n",
    "                    strip_accents='ascii',\n",
    "                    min_df=0.0,\n",
    "                    max_df=1.0)\n",
    "\n",
    "document_term_matrix = cv.fit_transform(corpus)\n",
    "\n",
    "print(\"--vocabulary: {}\".format(cv.vocabulary_))\n",
    "print(\"--doc matrix/count:\\n{}\".format(document_term_matrix.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. TF-IDF\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "$tf(term,document) = \\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document$\n",
    "\n",
    "The idf part is defined in terms of the document frequency. The document frequency is \n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "It is called the inverse document frequency but really it is the log of the inverse document frequency. Finally tf-idf is just\n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = ['is', 'and', 'we', 'you', 'will',\n",
    "                                   'the','going', 'to', 'having', 'a',\n",
    "                                   'should', 'in', 'i', 'm', 'at'],\n",
    "                    strip_accents='ascii',\n",
    "                    min_df=0.0,\n",
    "                    max_df=1.0)\n",
    "document_tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "\n",
    "print(\"--vocabulary: {}\".format(tfidf.vocabulary_))\n",
    "print(\"--doc matrix/count:\\n{}\".format(document_tfidf_matrix.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Measure similarity between documents\n",
    "\n",
    "Recall that for two vector $\\vec{x}$ and $\\vec{y}$ that $\\vec{x} \\cdot \\vec{y} = ||\\vec{x}|| ||\\vec{y}|| \\cos{\\theta}$. And so,\n",
    "\n",
    "$\\frac{\\vec{x} \\cdot \\vec{y} }{||\\vec{x}|| ||\\vec{y}||} = \\cos{\\theta}$\n",
    "\n",
    "This is called the cosine similarity of two vectors because it is the cosine of the angle between two vectors. Intuitively, the more similar two documents, the smaller the angle between them and the more dissimilar the larger the angle. An extreme example is when two documents share no words in common; the dot product is zero and therefor the cosine is zero. On the opposite extreme, when two documents are identical they share all of the same words with the same frequencies so their cosine is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
