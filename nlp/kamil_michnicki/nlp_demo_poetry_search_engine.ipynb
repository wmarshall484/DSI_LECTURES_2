{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## sklearn.feature_extraction.text has vectorizers in there. The vectorizers are\n",
    "## bag of words and tf-idf essentially. There is one option to do bag of words vectorization\n",
    "## with hashing.\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# COMPILE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = 'the cat in the hat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc2 = 'the cat in the tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc3 = 'the cat ate my hat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc4 = 'the cat in the hat the cat in the hat the cat in the hat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "documents = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FEATURIZE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This just splits on spaces and returns all of the words as a list. \n",
    " \n",
    "vocabulary = [word for doc in documents for word in doc.split(' ')]\n",
    "\n",
    "# Remove duplicates in the vocabulary. \n",
    "\n",
    "vocabulary = sorted(list(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (features): ['ate', 'cat', 'hat', 'in', 'my', 'the', 'tree']\n"
     ]
    }
   ],
   "source": [
    "print 'Vocabulary (features):',vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def vectorize(doc, vocabulary):\n",
    "    # It just splits the string on a space and then counts the number of times that terms happen. \n",
    "    bag_of_words = Counter(doc.split(' '))\n",
    "    # The counter object will be translated into a vector.\n",
    "    # And the next line creates a numpy vector with space for those entries. \n",
    "    doc_vector = np.zeros(len(vocabulary))\n",
    "    # The next for loop is very interesting. Although the doc may have words that are not in \n",
    "    # the vocabulary, it is only for words in the vocabulary that the vector is made. \n",
    "    # word_index is an integer and indexes the doc_vector. \n",
    "    for word_index, word in enumerate(vocabulary):\n",
    "        if word in bag_of_words:\n",
    "            doc_vector[word_index] += bag_of_words[word]\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The -1 is sort of a place holder. The first number, 1, says how many rows we will have. \n",
    "# the -1 means that we don't know how many columns a priori but that the function is to \n",
    "# fill in that information after it has created the vector. \n",
    "\n",
    "doc1_vectorized = vectorize(doc1, vocabulary).reshape(1, -1)\n",
    "doc2_vectorized = vectorize(doc2, vocabulary).reshape(1, -1)\n",
    "doc3_vectorized = vectorize(doc3, vocabulary).reshape(1, -1)\n",
    "doc4_vectorized = vectorize(doc4, vocabulary).reshape(1, -1)\n",
    "\n",
    "# vstack() combines numpy vectors into a matrix. \n",
    "tf_matrix = np.vstack((doc1_vectorized,\n",
    "                       doc2_vectorized,\n",
    "                       doc3_vectorized,\n",
    "                       doc4_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['ate', 'cat', 'hat', 'in', 'my', 'the', 'tree']\n",
      "\"the cat in the hat\": [ 0.  1.  1.  1.  0.  2.  0.]\n",
      "\"the cat in the tree\": [ 0.  1.  0.  1.  0.  2.  1.]\n",
      "\"the cat ate my hat\": [ 1.  1.  1.  0.  1.  1.  0.]\n",
      "\"the cat in the hat the cat in the hat the cat in the hat\":\n",
      "     [ 0.  3.  3.  3.  0.  6.  0.]\n",
      "\n",
      "feature matrix:\n",
      "[[ 0.  1.  1.  1.  0.  2.  0.]\n",
      " [ 0.  1.  0.  1.  0.  2.  1.]\n",
      " [ 1.  1.  1.  0.  1.  1.  0.]\n",
      " [ 0.  3.  3.  3.  0.  6.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'features:',vocabulary\n",
    "print '\"%s\":'%doc1, tf_matrix[0]\n",
    "print '\"%s\":'%doc2, tf_matrix[1]\n",
    "print '\"%s\":'%doc3, tf_matrix[2]\n",
    "print '\"%s\":\\n'%doc4, '    ', tf_matrix[3]\n",
    "print\n",
    "print 'feature matrix:'\n",
    "print tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sklearn can do this for you\n",
    "count_vectorizer = CountVectorizer(stop_words=None,\n",
    "                                  vocabulary=vocabulary)\n",
    "\n",
    "# The count_vectorizer has a sparse representation by default. todense() turns it into a dense matrix.\n",
    "feature_matrix = count_vectorizer.fit_transform([doc1]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize: the cat in the hat\n",
      "sklearn result [[0 1 1 1 0 2 0]]\n",
      "our result [ 0.  1.  1.  1.  0.  2.  0.]\n",
      "\n",
      "feature matrix\n",
      "[[0 1 1 1 0 2 0]\n",
      " [0 1 0 1 0 2 1]\n",
      " [1 1 1 0 1 1 0]\n",
      " [0 3 3 3 0 6 0]]\n"
     ]
    }
   ],
   "source": [
    "print 'Vectorize:',doc1\n",
    "print 'sklearn result',feature_matrix\n",
    "print 'our result',vectorize(doc1, vocabulary)\n",
    "print\n",
    "print 'feature matrix'\n",
    "print count_vectorizer.fit_transform(documents).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# COMPARE DOCUMENT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COSINE SIMILARITY COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare \"the cat in the hat\" \n",
      "with \"the cat in the tree\"\n",
      "[[ 0.85714286]]\n"
     ]
    }
   ],
   "source": [
    "print 'Compare \"%s\" \\nwith \"%s\"'%(doc1, doc2)\n",
    "print cosine_similarity(doc1_vectorized, doc2_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare \"the cat in the hat\" \n",
      "with \"the cat ate my hat\"\n",
      "[[ 0.6761234]]\n"
     ]
    }
   ],
   "source": [
    "print 'Compare \"%s\" \\nwith \"%s\"'%(doc1, doc3)\n",
    "print cosine_similarity(doc1_vectorized, doc3_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare \"the cat in the hat\" \n",
      "with \"the cat in the tree\"\n",
      "[[ 0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "# EUCLIDEAN DISTANCE COMPARISON\n",
    "print 'Compare \"%s\" \\nwith \"%s\"'%(doc1, doc2)\n",
    "eu = euclidean_distances\n",
    "\n",
    "def normalize(v):\n",
    "    return v/np.linalg.norm(v)\n",
    "\n",
    "v1 = normalize(doc1_vectorized)\n",
    "v2 = normalize(doc2_vectorized)\n",
    "\n",
    "print eu(v1,v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Poetry search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hope\" is the thing with feathers - \n",
      "That perches in the soul - \n",
      "And sings the tune without the words - \n",
      "And never stops - at all - \n",
      "\n",
      "And sweetest - in the Gale - is heard - \n",
      "And sore must be the storm - \n",
      "That could abash the little Bird \n",
      "That kept so many warm - \n",
      "\n",
      "I've heard it in the chillest land - \n",
      "And on the strangest Sea - \n",
      "Yet - never - in Extremity, \n",
      "It asked a crumb - of me.\n",
      "\n",
      "hope is the thing with feathers that perches in the soul and sings the tune without the words and never stops at all and sweetest in the gale is heard and sore must be the storm that could abash the little bird that kept so many warm ive heard it in the chillest land and on the strangest sea yet never in extremity it asked a crumb of me \n"
     ]
    }
   ],
   "source": [
    "# This box contains code for creating a corpus of poems. \n",
    "\n",
    "frost_poem = \\\n",
    "\"\"\"The Road Not Taken \n",
    "Robert Frost \n",
    "1874-1963\n",
    "\n",
    "U.S., New, England\n",
    "Time & Brevity, Nature, Landscapes & Pastorals, Living, Midlife, Fall\n",
    "\n",
    "Rhymed Stanza\n",
    "\n",
    "Two roads diverged in a yellow wood, \n",
    "And sorry I could not travel both \n",
    "And be one traveler, long I stood \n",
    "And looked down one as far as I could \n",
    "To where it bent in the undergrowth; \n",
    "\n",
    "Then took the other, as just as fair, \n",
    "And having perhaps the better claim, \n",
    "Because it was grassy and wanted wear; \n",
    "Though as for that the passing there \n",
    "Had worn them really about the same, \n",
    "\n",
    "And both that morning equally lay \n",
    "In leaves no step had trodden black. \n",
    "Oh, I kept the first for another day! \n",
    "Yet knowing how way leads on to way, \n",
    "I doubted if I should ever come back. \n",
    "\n",
    "I shall be telling this with a sigh \n",
    "Somewhere ages and ages hence: \n",
    "Two roads diverged in a wood, and I-- \n",
    "I took the one less traveled by, \n",
    "And that has made all the difference.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Poem:\n",
    "    \n",
    "    def __init__(self,poem):\n",
    "        self.title = poem[0].strip()\n",
    "        self.author = poem[1].strip()\n",
    "        self.region = poem[4].strip()\n",
    "        self.lifespan = poem[2].strip()\n",
    "        self.topics = [topic.strip() for topic in poem[5].split(',')]\n",
    "        self.type = poem[7]\n",
    "        self.text = \"\".join(poem[9:])\n",
    "        self.clean_text = \"\"\n",
    "        \n",
    "        \n",
    "    def clean(self):\n",
    "        # Get rid of line breaks. \n",
    "        x = self.text.split(\"\\n\")\n",
    "        x = [i.strip() for i in x]\n",
    "        self.clean_text = \" \".join(x)\n",
    "        \n",
    "        # Lower case.\n",
    "        self.clean_text = self.clean_text.lower()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        punctuations='.:,;!()?\\'\"-=[]'\n",
    "        for p in punctuations:\n",
    "            self.clean_text = self.clean_text.replace(p,'')\n",
    "            \n",
    "        # Remove spaces. \n",
    "        while \"  \" in self.clean_text:\n",
    "            self.clean_text = self.clean_text.replace(\"  \",\" \")\n",
    "    \n",
    "    def lemmatize(self):\n",
    "        \n",
    "        # Lemmatize.\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        temp=\"\"\n",
    "        for word in self.clean_text.split():\n",
    "            temp=temp+\" \"+wordnet_lemmatizer.lemmatize(word)\n",
    "        \n",
    "        self.clean_text = temp.strip() # get rid of white space. \n",
    "        \n",
    "    def stem(self):\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "        porter = PorterStemmer()\n",
    "        temp=\"\"\n",
    "        for word in self.clean_text.split():\n",
    "            temp=temp+\" \"+porter.stem(word)\n",
    "        \n",
    "        self.clean_text = temp.strip() # get rid of white space. \n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "import os\n",
    "\n",
    "# Load the corpus of poems from a subdirectory. \n",
    "poetry_corpus = []\n",
    "for filename in os.listdir('./poems'):\n",
    "    if filename[-3:] == 'txt':\n",
    "        f = open('./poems/'+filename)\n",
    "        poem = Poem(f.readlines())\n",
    "        f.close()\n",
    "        poetry_corpus.append(poem)\n",
    " \n",
    "# Preprocess the text\n",
    "for poem in poetry_corpus:\n",
    "    poem.clean()\n",
    "    temp = poem.clean_text\n",
    "    poem.lemmatize()\n",
    "    poem.stem()\n",
    "    #if temp!=poem.clean_text:\n",
    "    #    print temp[0:30]\n",
    "    #    print poem.clean_text[0:30]+\"\\n\"\n",
    "print poetry_corpus[10].text\n",
    "poetry_corpus[10].clean()    \n",
    "print poetry_corpus[10].clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words and cosine similarity. \n",
    "### Finding the poem most like Hope is the Thing with Feathers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for poem in poetry_corpus:\n",
    "    poem.clean()\n",
    "    poem.lemmatize()\n",
    "    #poem.stem()\n",
    "\n",
    "documents = []\n",
    "vocabulary = set()\n",
    "for poem in poetry_corpus:\n",
    "    documents.append(poem.clean_text)\n",
    "    for word in poem.clean_text.split():\n",
    "        vocabulary.add(word)    \n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords.words('english'),\n",
    "                                  vocabulary=vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# gives us a matrix with rows being the keys of words.\n",
    "count_vectorizer.fit(documents)\n",
    "vectorized_corpus = count_vectorizer.transform(documents).todense()\n",
    "\n",
    "# gives us a dictionary with keys as words and values as keys.\n",
    "#print count_vectorizer.vocabulary_\n",
    "\n",
    "print vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3138, array([[ 0.24452905]])]\n",
      "Three lovely notes he whistled, too soft to be heard\n",
      " If others sang; but others never sang\n",
      " In the great beech-wood all that May and June.\n",
      " No one saw him: I alone could hear him\n",
      " Though many listened. Was it but four years\n",
      " Ago? or five? He never came again.\n",
      "  \n",
      " Oftenest when I heard him I was alone,\n",
      " Nor could I ever make another hear.\n",
      " La-la-la! he called, seeming far-off--\n",
      " As if a cock crowed past the edge of the world,\n",
      " As if the bird or I were in a dream.\n",
      " Yet that he travelled through the trees and sometimes\n",
      " Neared me, was plain, though somehow distant still\n",
      " He sounded. All the proof is--I told men\n",
      " What I had heard.\n",
      "  \n",
      "                                    I never knew a voice,\n",
      " Man, beast, or bird, better than this. I told\n",
      " The naturalists; but neither had they heard\n",
      " Anything like the notes that did so haunt me,\n",
      " I had them clear by heart and have them still.\n",
      " Four years, or five, have made no difference. Then\n",
      " As now that La-la-la! was bodiless sweet:\n",
      " Sad more than joyful it was, if I must say\n",
      " That it was one or other, but if sad\n",
      " 'Twas sad only with joy too, too far off\n",
      " For me to taste it. But I cannot tell\n",
      " If truly never anything but fair\n",
      " The days were when he sang, as now they seem.\n",
      " This surely I know, that I who listened then,\n",
      " Happy sometimes, sometimes suffering\n",
      " A heavy body and a heavy heart,\n",
      " Now straightway, if I think of it, become\n",
      " Light as that bird wandering beyond my shore.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roads_index = 10\n",
    "\n",
    "search_results=list()\n",
    "for i,docvec in enumerate(vectorized_corpus):\n",
    "    sim = cosine_similarity(vectorized_corpus[roads_index],docvec)\n",
    "    search_results.append([i,sim])\n",
    "    \n",
    "search_results = sorted(search_results,key=lambda x:-x[1])\n",
    "print search_results[1]\n",
    "print poetry_corpus[search_results[1][0]].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and cosine similarity. Finding the poem most like Two Roads Diverged in Yellow Wood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                                  vocabulary=vocabulary)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3267, array([[ 0.14775207]])]\n",
      "The palm at the end of the mind,\n",
      "Beyond the last thought, rises\n",
      "In the bronze decor,\n",
      "\n",
      "A gold-feathered bird\n",
      "Sings in the palm, without human meaning,\n",
      "Without human feeling, a foreign song.\n",
      "\n",
      "You know then that it is not the reason \n",
      "That makes us happy or unhappy. \n",
      "The bird sings. Its feathers shine.\n",
      "\n",
      "The palm stands on the edge of space. \n",
      "The wind moves slowly in the branches. \n",
      "The bird's fire-fangled feathers dangle down.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roads_index = 10\n",
    "search_results=list()\n",
    "for i,docvec in enumerate(tfidf_matrix):\n",
    "    sim = cosine_similarity(tfidf_matrix[roads_index],docvec)\n",
    "    search_results.append([i,sim])\n",
    "    \n",
    "search_results = sorted(search_results,key=lambda x:-x[1])\n",
    "print search_results[1]\n",
    "print poetry_corpus[search_results[1][0]].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Father, where do the wild swans go?\n",
      "         Far, far. Ceaselessly winging,\n",
      "         Their necks outstraining, they haste them singing\n",
      "         Far, far. Whither, none may know.\n",
      "\n",
      "Father, where do the cloud-ships go?\n",
      "         Far, far. The winds pursue them,\n",
      "         And over the shining heaven strew them\n",
      "         Far, far. Whither, none may know.\n",
      "\n",
      "Father, where do the days all go?\n",
      "         Far, far. Each runs and races--\n",
      "         No one can catch them, they leave no traces--\n",
      "         Far, far. Whither, none may know.\n",
      "\n",
      "But father, we--where do we then go?\n",
      "         Far, far. Our dim eyes veiling,\n",
      "         With bended head we go sighing, wailing\n",
      "         Far, far. Whither none may know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the best matching poem with the worst matching poem.\n",
    "print poetry_corpus[search_results[-1][0]].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating poetry with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate poetry using an n gram we first have to count the number of times certain combinations of words occur and then find the one the one which maximizes the probability. We should use the cleaned version of the text to do this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for poem in poetry_corpus:\n",
    "    poem.clean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of,the', 4297),\n",
       " ('in,the', 4046),\n",
       " ('and,the', 2870),\n",
       " ('to,the', 2208),\n",
       " ('on,the', 1922)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram(n):\n",
    "    words = list()\n",
    "    for poem in poetry_corpus:\n",
    "        x = poem.clean_text.split()\n",
    "        for i in range(len(x)-(n-1)):\n",
    "            if '' not in x[i:i+n]:\n",
    "                words.append(\",\".join(x[i:i+n]))\n",
    "\n",
    "    return Counter(words)\n",
    "\n",
    "bigram = ngram(2)\n",
    "trigram = ngram(3)\n",
    "fourgram = ngram(4)\n",
    "bigram.most_common(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dictionary with counts in them and bigrams, we can use this to generate poetry. We seed it with the word \"the\" and then choose the word combo that has highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def most_likely_word(seed,gram,n):\n",
    "    \"\"\" input: list, string, int\"\"\"\n",
    "    best = -1\n",
    "    best_word=\"\"\n",
    "    for i in gram:\n",
    "        if i.split(',')[0:n-1]==seed:\n",
    "            if gram[i]>best:\n",
    "                best = gram[i]\n",
    "                best_word = i.split(',')[-1]\n",
    "    if best == -1:\n",
    "        n=len(gram.keys())\n",
    "        rand = random.randint(0,n)\n",
    "        m=0\n",
    "        for i in gram:\n",
    "            if m == rand:\n",
    "                return i.split(',')[-1]\n",
    "            m += 1\n",
    "        \n",
    "    return best_word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "world\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print \"the\"\n",
    "print most_likely_word(['the'],bigram,2)\n",
    "print most_likely_word(['the','world'],trigram,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_poem(seed,length,gram):\n",
    "    \"\"\"input: list,int,Counter\"\"\"\n",
    "    n = len(seed)+1\n",
    "    poem = seed\n",
    "    for i in xrange(length):\n",
    "        poem.append(most_likely_word(poem[1-n:],gram,n))\n",
    "    return \" \".join(poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world and the world and the world and the world and the world and the world and the world and the world and the world and the world and the'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_poem(['the'],30,bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world and all the world and all the world and all the world and all the world and all the world and all the world and all the world and all'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_poem(['the','world'],30,trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world is all his own mischance mute with a glassy countenance did she look to camelot and as the bird it left no trace in the heaven of your face in your stupidity i found the sweet hush after a sweet sound'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_poem(['the','world','is'],40,fourgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "incorpus = False\n",
    "for poem in poetry_corpus:\n",
    "    if \"in your stupidity i found the sweet hush after a sweet sound\" in poem.clean_text:\n",
    "        incorpus = True\n",
    "\n",
    "print incorpus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is over fit. It is just regenerating the same poetry that is in the text, which is not what we want. We want new poems. For fix that, we'll have to do something like randomize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_poem(seed,length,grams):\n",
    "    \"\"\"input: list,int,Counter\"\"\"\n",
    "    from random import randint\n",
    "    n = len(seed)+1\n",
    "    poem = seed\n",
    "    for i in xrange(length):\n",
    "        m = randint(1,(n-1)**2) # 4-grams more likley than 3-grams, etc...\n",
    "        if m<=4:\n",
    "            m=3\n",
    "        elif m<=9:\n",
    "            m=4\n",
    "        poem.append(most_likely_word(poem[1-m:],grams[m-2],m))\n",
    "    return \" \".join(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world is all i can see the face of the sea and the sky and the sun and the rain a weary heart went thankful to rest and what the waur am i gin a body kiss a body meet a body meet a body meet a body comin thro the rye'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_poem(['the','world','is'],50,[bigram,trigram,fourgram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print(clf.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Living', 1276),\n",
       " ('Nature', 921),\n",
       " ('Relationships', 901),\n",
       " ('Love', 817),\n",
       " ('Social Commentaries', 784),\n",
       " ('Arts & Sciences', 620),\n",
       " ('Religion', 515),\n",
       " ('Death', 462),\n",
       " ('Time & Brevity', 361),\n",
       " ('Romantic Love', 322)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes' can help us predict a category. The poem types are not mutually exclusive and are therefor not categories. A poem may have multiple types. Hence we cannot use Naive Bayes to predict the type of a poem. What we can do though, is predict whether a poem is a particular type or not. Say, if we wanted to tell if a poem was about nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word_count = {\"Nature\":{},\"Not Nature\":{}}\n",
    "for poem in poetry_corpus:\n",
    "    if 'Nature' in poem.topics:\n",
    "        topic=\"Nature\"\n",
    "    else:\n",
    "        topic=\"Not Nature\"\n",
    "        \n",
    "    #count the words\n",
    "    words = poem.clean_text.split()\n",
    "    for word in words:\n",
    "        if word in topic_word_count[topic]:\n",
    "            topic_word_count[topic][word]+=1\n",
    "        else:\n",
    "            topic_word_count[topic][word]=1\n",
    "\n",
    "topic_count={}\n",
    "for topic in topic_word_count:\n",
    "    count=0\n",
    "    for word in topic_word_count[topic]:\n",
    "        count+=topic_word_count[topic][word]\n",
    "    topic_count[topic]=count\n",
    "     \n",
    "total_count = 0\n",
    "for topic in topic_count:\n",
    "    total_count += topic_count[topic]\n",
    "    \n",
    "import numpy as np\n",
    "def get_nb(doc,topic):\n",
    "    score=np.log(1.0 * topic_count[topic]/total_count) #the base probability.\n",
    "    for word in doc.split():\n",
    "        if word in topic_word_count[topic]:\n",
    "            score += np.log(1.0*topic_word_count[topic][word]/topic_count[topic])\n",
    "        else:\n",
    "            score += np.log(0.001)\n",
    "    return score\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes 2.66666666667\n"
     ]
    }
   ],
   "source": [
    "def is_it_nature(doc):\n",
    "    score = (get_nb(doc,\"Nature\")-get_nb(doc,\"Not Nature\"))\n",
    "    if score > 0:\n",
    "        print \"Yes\", np.exp(score)\n",
    "    else:\n",
    "        print \"No\", np.exp(score)\n",
    "        \n",
    "is_it_nature(\"orange\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
