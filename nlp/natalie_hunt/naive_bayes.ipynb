{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of these subject lines are from spam emails?\n",
    "\n",
    "- Need just a little help? Independent senior living might be the answer\n",
    "- Hard sql from today's interview\n",
    "- Beautiful Women, Discrete Service\n",
    "- Ever consider driving with Lyft? Apply here.\n",
    "- 3/2/18 All Hands Unanswered Questions\n",
    "- Sizwe Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tipped you off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a collection of labeled text documents, each one belonging to a category (for example, news articles & which section of the paper they're in). Now we get a new, unlabeled article. How do use what we've seen so far to predict which category it belongs to?\n",
    "\n",
    "- Do a bunch of complicated part of speech tagging & account for the sequence of words?\n",
    "- Make the dumbest \"bag of words\" assumption possible and hope it works?\n",
    "  - yep this is what we do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is an extremely simple amazingly effective machine learning technique.\n",
    "\n",
    "## When do we use it?\n",
    "1. n << p\n",
    "2. n small\n",
    "3. n large\n",
    "4. streams of input data (online learning)\n",
    "5. multi-class\n",
    "6. low memory applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it really?\n",
    "\n",
    "It is just [Maximum A Posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), with a fun approximation to make it easy to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation: a document $\\vec{x}$ is a list of words $\\{w_1, w_2, \\ldots, w_k\\}$, and we want the probability that it belongs to class $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: \n",
    "- e-mail: spam or not spam\n",
    "- news article: is it from the World sections or Sport or Arts, etc\n",
    "- essay from anonymous author: is the author really a famous person (whose corpus I have access to)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(y|\\vec{x}) = \\frac{P(\\vec{x}|y)P(y)}{P(\\vec{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the wacky part: let's assume the features (words) are totally independent of each other. Then we can write\n",
    "\n",
    "## $$P(\\vec{x}|y) = P(w_1|y)\\times P(w_2|y)\\times\\cdots\\times P(w_k|y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty brazenly naive assumption. It is assuming, for example, that the probability of seeing the word \"ball\" in an article about sports $P(ball|\\text{sports})$ is totally independent of the presence of the word \"soccer\" in the article.\n",
    "\n",
    "But if it works, it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a document $\\vec{x}$, we just need to calculate the posterior probabilities for each class $\\{y_j\\}$, then see which class has the highest posterior probability.\n",
    "\n",
    "# $$P(y_j|\\vec{x}) \\propto P(w_1|y_j)\\times P(w_2|y_j)\\times\\cdots\\times P(w_k|y_j)\\times P(y_j)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've dropped the denominator $P(\\vec{x})$ since it doesn't depend on $y_j$, and we're only interested in the $y_j$ that maximizes $P(y_j|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the prior, $P(y_j)$ ?\n",
    "\n",
    "The prior probability of class $y$ is simply how frequent that class is among our training documents:\n",
    "$$P(y_j) = \\frac{\\text{# of documents of class}\\, y_j}{\\text{total # of documents}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about these likelihoods? How do we calculate 'em?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a document $\\vec{x}$ of class $y_j$ is generated by drawing words from a bag (with replacement) with probabilities $P(w_i | y_j)$\n",
    "\n",
    "Let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word counts: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can write our posterior (which, remember, is the prior times the likelihood) as\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j)\\times\\prod_{i=1}^p P(w_i|y_j)^{x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{N_{ji}}{N_j} = \\frac{\\text{total count of w_i across all documents of class}\\, y_j}{\\text{total count of all words across all documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our new document contains a word we've never seen before? Then our estimate of $P(w|y)$ for that word would be zero, and then the entire product $P(y|\\vec{x})$ would be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, we employ *Laplace Smoothing*\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + N_{ji}}{\\alpha p + N_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, $\\alpha = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Bernoulli Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, now, that a document of class $y_j$ has a probability $P(w_i|y_j)$ of containing word $w_i$ at least once (regardless of word count).\n",
    "\n",
    "Again let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word *occurrences*: $x_i = 1$ if the $w_i$ is in that document at all, $x_i = 0$ if not: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can think of our document as a series of Bernoulli trials, one for each word, and our posterior is\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j) \\times\\prod_{i=1}^p P(w_i|y_j)^{x_i}(1 - P(w_i|y_j))^{1 - x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j)) + (1 - x_i)\\text{log}(1 - P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{D_{ji}}{D_j} = \\frac{\\text{total # of documents of class}\\,y_j\\text{ containing}\\, w_i}{\\text{total number of documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again we'd hit the \"unseen word\" problem. So our smoothing here looks like\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + D_{ji}}{2\\alpha + D_j} $$\n",
    "\n",
    "Again with $\\alpha = 1$ usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "- Bernoulli tends to work better with shorter documents...\n",
    "- ...but sklearn says \"It is advisable to evaluate both models, if time permits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[sklearn User Guide: Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "[Spam Filtering with Naive Bayes â€“ Which Naive Bayes?](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=E870DFA148786F5A27F88CF80FB4D73B?doi=10.1.1.61.5542&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other flavors: Gaussian Naive Bayes\n",
    "When my features are continuous (but my target is still a class), I can go ahead and assume that data for a given class has normally distributed values for each feature.\n",
    "\n",
    "You cannot stop me from making this assumption.\n",
    "\n",
    "Then my likelihood above is just a product of Gaussian probability density functions. Same process!\n",
    "\n",
    "[See here for bugs](http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['when its time to party we will party hard',\n",
    "          'theres a party in my mind',\n",
    "           'i need something to change my mind']\n",
    "y = ['andrew wk', 'talking heads', 'talking heads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = ['a change in the weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for row in X:\n",
    "    for word in row.split():\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'change',\n",
       " 'hard',\n",
       " 'i',\n",
       " 'in',\n",
       " 'its',\n",
       " 'mind',\n",
       " 'my',\n",
       " 'need',\n",
       " 'party',\n",
       " 'something',\n",
       " 'theres',\n",
       " 'time',\n",
       " 'to',\n",
       " 'we',\n",
       " 'when',\n",
       " 'will'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'change',\n",
       " 'hard',\n",
       " 'i',\n",
       " 'in',\n",
       " 'its',\n",
       " 'mind',\n",
       " 'my',\n",
       " 'need',\n",
       " 'party',\n",
       " 'something',\n",
       " 'theres',\n",
       " 'time',\n",
       " 'to',\n",
       " 'we',\n",
       " 'when',\n",
       " 'will'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(' '.join(X).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'andrew wk': 1, 'talking heads': 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = sum(class_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_class = defaultdict(int)\n",
    "word_count_per_class = defaultdict(Counter)\n",
    "\n",
    "for row, label in zip(X,y):\n",
    "    words_per_class[label] += len(row.split())\n",
    "    word_count_per_class[label].update(Counter(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'andrew wk': 9, 'talking heads': 13})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'andrew wk': Counter({'hard': 1,\n",
       "                      'its': 1,\n",
       "                      'party': 2,\n",
       "                      'time': 1,\n",
       "                      'to': 1,\n",
       "                      'we': 1,\n",
       "                      'when': 1,\n",
       "                      'will': 1}),\n",
       "             'talking heads': Counter({'a': 1,\n",
       "                      'change': 1,\n",
       "                      'i': 1,\n",
       "                      'in': 1,\n",
       "                      'mind': 2,\n",
       "                      'my': 2,\n",
       "                      'need': 1,\n",
       "                      'party': 1,\n",
       "                      'something': 1,\n",
       "                      'theres': 1,\n",
       "                      'to': 1})})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'a change in the weather': Counter({'a': 1,\n",
       "                      'change': 1,\n",
       "                      'hard': 0,\n",
       "                      'i': 0,\n",
       "                      'in': 1,\n",
       "                      'its': 0,\n",
       "                      'mind': 0,\n",
       "                      'my': 0,\n",
       "                      'need': 0,\n",
       "                      'party': 0,\n",
       "                      'something': 0,\n",
       "                      'theres': 0,\n",
       "                      'time': 0,\n",
       "                      'to': 0,\n",
       "                      'we': 0,\n",
       "                      'when': 0,\n",
       "                      'will': 0})})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_vocab_counts = defaultdict(Counter)\n",
    "for elm in x_test:\n",
    "    x_test_word_counts = Counter(elm.split())\n",
    "    for word in vocab:\n",
    "        x_test_vocab_counts[elm][word] = x_test_word_counts[word] # x_test_word_counts[elm] = Counter(elm.split())\n",
    "\n",
    "x_test_vocab_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a prediction using Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andrew wk 1.8965255651646187e-05\n",
      "talking heads 0.00019753086419753083\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "\n",
    "for x_test in x_test_vocab_counts:\n",
    "    for y_class in class_counts:\n",
    "        prior = class_counts[y_class]/total_docs\n",
    "        posterior = prior\n",
    "        for word, word_count in x_test_vocab_counts[x_test].items():\n",
    "            N_ji = word_count_per_class[y_class][word] # total count of word_i across all documents of class y_j\n",
    "            N_j = words_per_class[y_class] # total count of all words across all documents of class y_j\n",
    "            posterior *= ((alpha + N_ji)/(alpha*len(vocab) + N_j))**word_count\n",
    "        print(y_class, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Which do we predict?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
