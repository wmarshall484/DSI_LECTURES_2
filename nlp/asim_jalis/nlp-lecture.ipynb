{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing / Naive Bayes\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Make computers read text.\n",
    "\n",
    "- Turn raw text into features we can feed into our classifiers.\n",
    "\n",
    "- Classify email into ham and spam.\n",
    "\n",
    "- Find out which documents match a search most closely.\n",
    "\n",
    "- Build a Naive Bayes classifier.\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "<br><details><summary>\n",
    "What could computers do if they could read?\n",
    "</summary>\n",
    "\n",
    "1. Filter out email spam.<br>\n",
    "2. Scan resumes.<br>\n",
    "3. Detect plagiarism.<br>\n",
    "4. Classify software bugs into different categories.<br>\n",
    "5. Cluster news like Google News.<br>\n",
    "6. Find out which headlines will get the most clicks.<br>\n",
    "7. Find out which ad copy will get the most clicks.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is NLP?\n",
    "</summary>\n",
    "\n",
    "1. NLP transforms unstructured text into vectors.<br>\n",
    "2. These vectors can be used for machine learning applications.<br>\n",
    "3. E.g. classification, clustering, regression, recommender systems, etc.<br>\n",
    "4. Instead of just apply ML to numeric data with NLP we can apply it to text.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is unstructured text? How is it different from structured text?\n",
    "</summary>\n",
    "\n",
    "1. Unstructured text is text without a schema.<br>\n",
    "2. Structured text is text with a schema, e.g. CSV, JSON.<br>\n",
    "3. A schema specifies column names and types for the data.<br>\n",
    "4. Unstructured text has no columns, no specified types.<br>\n",
    "5. Usually is just a blob of text.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What are some sources of unstructured text?\n",
    "</summary>\n",
    "\n",
    "1. Twitter, Facebook, social media.<br>\n",
    "2. Legal documents.<br>\n",
    "3. News stories, blogs, online comments.<br>\n",
    "4. Voice recognition software , OCR, digitized documents.<br>\n",
    "5. Bugs, code comments, documentation.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "Suppose we want to classify email into spam and not spam. Consider\n",
    "these two email messages.\n",
    "\n",
    "### Email 1\n",
    "\n",
    "> From: Joe    \n",
    "> Subject: R0lex    \n",
    ">     \n",
    "> Want to buy cheap R0leXX watches?    \n",
    "\n",
    "### Email 2\n",
    "\n",
    "> From: Jim    \n",
    "> Subject: Coffee    \n",
    ">     \n",
    "> Want to grab coffee at 4?    \n",
    "\n",
    "<br><details><summary>\n",
    "Which one of these is likely to be spam?\n",
    "</summary>\n",
    "\n",
    "Email 1.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "<br><details><summary>\n",
    "Why?\n",
    "</summary>\n",
    "\n",
    "1. It contains words that are spammy.<br>\n",
    "2. It contains misspellings.<br>\n",
    "</details>\n",
    "\n",
    "## Vectorizing Text\n",
    "\n",
    "Now as humans we can figure this out pretty easily. Our brains have\n",
    "amazing NLP. But we are not scalable. \n",
    "\n",
    "<br><details><summary>\n",
    "How can we automate this process?\n",
    "</summary>\n",
    "\n",
    "1. Convert text into feature vectors.<br>\n",
    "2. Train classifier on these feature vectors.<br>\n",
    "3. Use output vectors `[1]` to mean spam, and `[0]` to mean not-spam.<br>\n",
    "</details>\n",
    "\n",
    "What is *vectorizing*?\n",
    "\n",
    "- Vectorizing is converting unstructured raw text to feature vectors. \n",
    "\n",
    "Consider these two texts. \n",
    "\n",
    "- Text 1: `Want to buy cheap R0leXX watches?`\n",
    "\n",
    "- Text 2: `Want to grab coffee at 4?`\n",
    "\n",
    "<br><details><summary>\n",
    "How can we vectorize them?\n",
    "</summary>\n",
    "\n",
    "1. Lowercase all words.<br>\n",
    "2. Replace words with common base words.<br>\n",
    "3. Count how many times each word occurs.<br>\n",
    "4. Store as vector.<br>\n",
    "\n",
    "</details>\n",
    "\n",
    "## Vectorized Texts\n",
    "\n",
    "Here is what the emails look like vectorized. This is also known as a\n",
    "*bag of words*.\n",
    "\n",
    "Index |Word     |Text 1   |Text 2\n",
    "----- |----     |------   |------\n",
    "0     |want     |1        |1\n",
    "1     |to       |1        |1\n",
    "2     |buy      |1        |0\n",
    "3     |cheap    |1        |0\n",
    "4     |r0lexx   |1        |0\n",
    "5     |watches  |1        |0\n",
    "6     |grab     |0        |1\n",
    "7     |coffee   |0        |1\n",
    "8     |at       |0        |1\n",
    "9     |4        |0        |1\n",
    "\n",
    "## Terms\n",
    "\n",
    "Term         |Meaning\n",
    "----         |-------\n",
    "Corpus       |Collection of documents (collection of articles).\n",
    "Document     |A single document (a tweet, an email, an article).\n",
    "Vocabulary   |Set of words in your corpus, or maybe the entire English dictionary. \n",
    "Bag of Words |Vector representation of words in a document.\n",
    "Token        |Single word.\n",
    "Stop Words   |Common ignored words because not useful in distinguishing text.\n",
    "Vectorizing  |Converting text into a bag-of-words.\n",
    "\n",
    "## Building an NLP Pipeline\n",
    "\n",
    "Lets build an NLP Pipeline to turn unstructured text data into\n",
    "something we can train a classifier on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using newsgroups data set, let's fetch all of it.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Subset the data, this dataset is huge.\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "The first step is turning your raw text documents into lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"This is a sentence in English. This is another one.\"\n",
    "\n",
    "# Split the raw text into individual words.\n",
    "document = word_tokenize(text)\n",
    "print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizing\n",
    "\n",
    "Sometimes you want to treat each sentence as a separate document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split document in individual sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print sentences\n",
    "\n",
    "# Split individual sentences into words\n",
    "corpus = []\n",
    "for sentence in sentences:\n",
    "    corpus.append(word_tokenize(sentence))\n",
    "print corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "<br><details><summary>\n",
    "What are some use cases for sentence tokenizing?\n",
    "</summary>\n",
    "\n",
    "1. Classifying repetitive text that uses the same sentences.<br>\n",
    "2. Classifying conversation between an airplane and a control tower.<br>\n",
    "3. Classifying text generated by filling out a form.<br>\n",
    "4. Breaking document down into sentences and treating each sentence as\n",
    "   a document. For example, to give MPAA rating to a movie script.<br>\n",
    "</details>\n",
    "\n",
    "## Stop Words\n",
    "\n",
    "NLTK has functionality for removing common words that show up in\n",
    "almost every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "document = [w \n",
    "  for w in map(lambda x: x.lower(), document) \n",
    "    if not w in stopwords.words('english')]\n",
    "print document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "<br><details><summary>\n",
    "What might be some applications where you don't want to remove stop\n",
    "words from your text?\n",
    "</summary>\n",
    "\n",
    "1. Plagiarism detection.<br>\n",
    "2. Investigating if dusty papers found in attic are a lost Shakespeare play.<br>\n",
    "3. Identifying document types in Data Loss Prevention. E.g. resume, \n",
    "   insider information, legal contract, etc.<br> \n",
    "</details>\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "<br><details><summary>\n",
    "How will our code treat words like \"watch\" and \"watches\"?\n",
    "</summary>\n",
    "\n",
    "It will treat them as different words.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How can we fix this?\n",
    "</summary>\n",
    "\n",
    "1. Remove inflectional endings and return base form of word (known as the lemma).<br>\n",
    "2. This is known as stemming or lemmatization.<br>\n",
    "</details>\n",
    "\n",
    "What is the difference between stemming and lemmatization?\n",
    "\n",
    "- Lemmatization is more general.\n",
    "\n",
    "- Converting *cars* to *car* is stemming.\n",
    "\n",
    "- Converting *automobile* to *car* is lemmatization.\n",
    "\n",
    "- Behavior depends on your toolkit.\n",
    "\n",
    "In Python's NLTK:\n",
    "\n",
    "- Stemming converts *cars* to *car*, but does not convert *children*\n",
    "  to *child*.\n",
    "\n",
    "- Lemmatization converts both.\n",
    "\n",
    "## Stemming and Lemmatization Computation\n",
    "\n",
    "Removing morphoglical affixes from words, and also replacing words with their\n",
    "\"lemma\" (or base words: \"good\" is the lemma of \"better\").\n",
    "\n",
    "- running -> run\n",
    "\n",
    "- generously -> generous\n",
    "\n",
    "- better -> good\n",
    "\n",
    "- dogs -> dog\n",
    "\n",
    "- mice -> mouse\n",
    "\n",
    "Here is how to do this using NLTK. Don't try to write your own\n",
    "functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter   import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "\n",
    "print SnowballStemmer('english').stem('running')\n",
    "print WordNetLemmatizer().lemmatize('mice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "<br><details><summary>\n",
    "Consider \"rolex watch\" vs \"lets watch the game this weekend\". Which\n",
    "one is spam?\n",
    "</summary>\n",
    "\n",
    "The first one.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Why use N-grams?\n",
    "</summary>\n",
    "\n",
    "1. Sometimes context makes a difference.<br>\n",
    "2. You want to see the words before and after.<br>\n",
    "3. N-grams are strings consecutive words in your corpus.<br>\n",
    "4. These are extra \"features\" in your data that contain more information that individual words.<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# An example of 2-grams in our \"document\" above.\n",
    "document_bigrams = ngrams(document,2)\n",
    "for p in document_bigrams: print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "\n",
    "How do you turn a document into a bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Divide our original text into sentences.\n",
    "text = \"This is a sentence in English. This is another one.\"\n",
    "corpus = sent_tokenize(text)\n",
    "print corpus\n",
    "\n",
    "# Build a bag of words model.\n",
    "c = CountVectorizer(stop_words='english')\n",
    "print c.fit(corpus).vocabulary_\n",
    "print c.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Building bag of words from newgroups_train text.\n",
    "c = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "bag_of_words = c.fit_transform(newsgroups_train['data'])\n",
    "print c.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Limitations\n",
    "\n",
    "<br><details><summary>\n",
    "What are some limitations of the bag-of-words approach and\n",
    "CountVectorizer?\n",
    "</summary>\n",
    "\n",
    "1. Longer documents weigh more than short documents.<br>\n",
    "2. Does not consider uniqueness of words. Unique words should weigh more.<br>\n",
    "3. We are losing a lot of structure. This is like a giant word grinder.<br>\n",
    "4. We will address the first two issues. The third issue is part of the bargain we struck with the bag-of-words approach.<br>\n",
    "5. Note: bag-of-words is not the only way to featurize text. It is simple, and surprisingly powerful.<br>\n",
    "</details>\n",
    "\n",
    "## L2 Normalization\n",
    "\n",
    "<br><details><summary>\n",
    "What is L2 normalization?\n",
    "</summary>\n",
    "\n",
    "1. Divide each vector by its L2-norm.<br>\n",
    "2. Divide each vector by its magnitude.<br>\n",
    "3. Divide each vector by square root of sum of squares of all elements.<br>\n",
    "4. Makes long and short documents weigh the same.<br>\n",
    "$$\\frac{\\vec{v}}{||v_i||}$$<br>\n",
    "$$\\frac{\\vec{v}}{\\sqrt[2]{\\sum{v_i^2}}}$$\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is L1 normalization? \n",
    "</summary>\n",
    "\n",
    "Divide each vector by its L1-norm.<br>\n",
    "\n",
    "$$\\frac{\\vec{v}}{\\sum{|v_i|}}$$<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is L(n) normalization?\n",
    "</summary>\n",
    "\n",
    "Divide each vector by its L(n)-norm.<br>\n",
    "\n",
    "$$\\frac{\\vec{v}}{\\sqrt[n]{\\sum{|v_i|^n}}}$$<br>\n",
    "</details>\n",
    "\n",
    "## Why L2?\n",
    "\n",
    "<br><details><summary>\n",
    "Why use the L2-norm?\n",
    "</summary>\n",
    "\n",
    "1. It makes dot products between vectors meaningful.<br>\n",
    "2. Will see this later with cosine similarity.<br>\n",
    "</details>\n",
    "\n",
    "## Dot Product\n",
    "\n",
    "<br><details><summary>\n",
    "If two documents are highly similar what will the dot product of their\n",
    "L2-normalized vectors be?\n",
    "</summary>\n",
    "\n",
    "The dot product will be 1.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What does a dot product of 0 indicate?\n",
    "</summary>\n",
    "\n",
    "There is no similarity between the documents.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What does a dot product of -1 indicate?\n",
    "</summary>\n",
    "\n",
    "This is not possible since all vector values are zero or positive.<br>\n",
    "</details>\n",
    "\n",
    "## TF-IDF Intuition\n",
    "\n",
    "Intuitively, the idea of TF-IDF is this:\n",
    "\n",
    "- Words that occur in every document are less useful than words that\n",
    "  only occur in some documents.\n",
    "\n",
    "- Instead of looking at the term frequency for each word in a document\n",
    "  we want to scale up terms that are rare.\n",
    "\n",
    "## Applications\n",
    "\n",
    "<br><details><summary>\n",
    "Which term is likely to be more significant in spam detection:\n",
    "\"hey\", \"rolex\", and why?\n",
    "</summary>\n",
    "\n",
    "1. \"Rolex\" is going to be more significant.<br>\n",
    "2. Because this is a unique word that does not occur in a lot of documents.<br>\n",
    "3. \"Hey\" is a lot more common and is less likely to be a useful feature.<br>\n",
    "</details>\n",
    "\n",
    "## Inverse Document Frequency\n",
    "\n",
    "How can we increase the weight of tokens that are rare in our corpus\n",
    "and decrease the weight of tokens that are common?\n",
    "\n",
    "- Use Inverse Document Frequency.\n",
    "\n",
    "- Inverse Document Frequency or IDF is a measure of how unique a term\n",
    "  is. \n",
    "  \n",
    "- So we want to weigh terms high if they are unique.\n",
    "\n",
    "What is the formula for IDF?\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- *t* is a token\n",
    "\n",
    "- *d* is a document\n",
    "\n",
    "- *D* is a corpus\n",
    "\n",
    "- *N* is the total number of documents in *D*\n",
    "\n",
    "- *n(t)* is the number of documents containing *t*\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N}{n}\\right)}$$\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "What is TF-IDF?\n",
    "\n",
    "- TF-IDF combines TF or the normalized token counts.\n",
    "- Then it multiplies it with IDF.\n",
    "\n",
    "What is the formula for TF-IDF?\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- *t* is a token\n",
    "\n",
    "- *d* is a document\n",
    "\n",
    "- *D* is a corpus\n",
    "\n",
    "- *N* is the total number of documents in *D*\n",
    "\n",
    "- *n(t)* is the number of documents containing *t*\n",
    "\n",
    "Then, TF-IDF is:\n",
    "\n",
    "$$tfidf(t,d) = tf(t,d) * idf(t,d)$$\n",
    "\n",
    "What is TF?\n",
    "\n",
    "- TF is the number of times that the token $t$ appears in $d$ (often\n",
    "  normalized by dividing by the total length of $d$).\n",
    "\n",
    "$$tf(t, d) = freq(t, d)$$\n",
    "\n",
    "What is IDF?\n",
    "\n",
    "- IDF is a score for how unique a token is across the corpus.\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N}{n}\\right)}$$\n",
    "\n",
    "- This is sometimes written with some smoothers like this:\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N + 1}{n + 1}\\right)} + 1$$\n",
    "\n",
    "## Adding Ones\n",
    "\n",
    "<br><details><summary>\n",
    "Why are we adding 1's?\n",
    "</summary>\n",
    "\n",
    "1. This is called smoothing.\n",
    "2. Adding 1 inside the log ensures that we never divide by 0.<br>\n",
    "3. Adding 1 at the end ensures that *idf* is always non-zero.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider a very small library with these books:\n",
    "\n",
    "- Hadoop Handbook (HH)\n",
    "- Beekeeping Bible (BB)\n",
    "\n",
    "Here are the word frequencies.\n",
    "\n",
    "Terms  |HH   |BB\n",
    "-----  |--   |--\n",
    "hadoop |100  |0\n",
    "bees   |0    |150\n",
    "hive   |20   |50\n",
    "\n",
    "<br><details><summary>\n",
    "Intuitively what do you expect the IDF scores of hadoop, bees, and\n",
    "hive to be?\n",
    "</summary>\n",
    "\n",
    "1. Hadoop and bees should have a higher score.<br>\n",
    "2. Hive should have a low score because it is not rare.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What are the IDF scores of hadoop, bees, and hive? Assume log base 2.\n",
    "</summary>\n",
    "\n",
    "1. Note these are independent of document.<br>\n",
    "2. For hadoop: $N = 2, n = 1, \\log(N/n) = \\log(2) = 1$.<br>\n",
    "3. For bees: $N = 2, n = 1, \\log(N/n) = \\log(2) = 1$.<br>\n",
    "4. For hive: $N = 2, n = 2, \\log(N/n) = \\log(1) = 0$.<br>\n",
    "</details>\n",
    "\n",
    "## Computing  TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bag_of_words_matrix = bag_of_words.toarray()\n",
    "document_freq = np.sum(bag_of_words_matrix > 0, axis=0)\n",
    "\n",
    "# N is the number of documents\n",
    "N = bag_of_words_matrix.shape[0]\n",
    "\n",
    "# Divide each row by its l2 norm\n",
    "l2_rows = np.sqrt(np.sum(bag_of_words_matrix**2, axis=1)).reshape(N, 1)\n",
    "bag_of_words_matrix = bag_of_words_matrix / l2_rows\n",
    "\n",
    "# Want a IDF value for words that don't appear in a document\n",
    "idf = np.log(float(N+1) / (1.0 + document_freq)) + 1.0\n",
    "print idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "tfidf = np.multiply(bag_of_words_matrix, idf)\n",
    "tfidf = normalize(tfidf, norm='l2', axis=1)\n",
    "print tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably easier just to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "z = TfidfTransformer(norm='l2')\n",
    "print z.fit_transform(bag_of_words_matrix).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "t = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l2')\n",
    "print t.fit_transform(newsgroups_train['data']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "We need a way to compare our documents. Use the cosine similary metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use cosine similarity to measure similarity of 3 sentences in a corpus\n",
    "corpus.append(\"English is my favorite language.\")\n",
    "cosine_similarity(t.transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick\n",
    "\n",
    "One of the limitations of CountVectorizer is that the vectors it\n",
    "produces can be very large. \n",
    "\n",
    "<br><details><summary>\n",
    "How can we fix this?\n",
    "</summary>\n",
    "\n",
    "1. Use `HashingVectorizer`.<br>\n",
    "2. Hash words to collapse the vector.<br>\n",
    "3. Vector still retains enough uniqueness to be useful.<br>\n",
    "</details>\n",
    "\n",
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "features = hv.transform(corpus)\n",
    "print features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "<br><details><summary>\n",
    "What are some steps in vectorizing text?\n",
    "</summary>\n",
    "\n",
    "1. Tokenize.<br>\n",
    "2. Stemming, lemmatization, lowercasing, etc.<br>\n",
    "3. Count frequencies.<br>\n",
    "4. Modify feature weights using TF-IDF<br>\n",
    "5. Divide by L2 norm.<br>\n",
    "6. Use hashing trick.<br>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
