{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of these subject lines are from spam emails?\n",
    "\n",
    "- Need just a little help? Independent senior living might be the answer\n",
    "- Hard sql from today's interview\n",
    "- Beautiful Women, Discrete Service\n",
    "- Ever consider driving with Lyft? Apply here.\n",
    "- 3/2/18 All Hands Unanswered Questions\n",
    "- Sizwe Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tipped you off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a collection of labeled text documents, each one belonging to a category (for example, news articles & which section of the paper they're in). Now we get a new, unlabeled article. How do use what we've seen so far to predict which category it belongs to?\n",
    "\n",
    "- Do a bunch of complicated part of speech tagging & account for the sequence of words?\n",
    "- Make the dumbest \"bag of words\" assumption possible and hope it works?\n",
    "  - yep this is what we do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is an extremely simple amazingly effective machine learning technique.\n",
    "\n",
    "## When do we use it?\n",
    "1. n << p\n",
    "2. n small\n",
    "3. n large\n",
    "4. streams of input data (online learning)\n",
    "5. multi-class\n",
    "6. low memory applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it really?\n",
    "\n",
    "It is just [Maximum A Posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), with a fun approximation to make it easy to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation: a document $\\vec{x}$ is a list of words $\\{w_1, w_2, \\ldots, w_k\\}$, and we want the probability that it belongs to class $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: \n",
    "- e-mail: spam or not spam\n",
    "- news article: is it from the World sections or Sport or Arts, etc\n",
    "- essay from anonymous author: is the author really a famous person (whose corpus I have access to)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(y|\\vec{x}) = \\frac{P(\\vec{x}|y)P(y)}{P(\\vec{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the wacky part: let's assume the features (words) are totally independent of each other. Then we can write\n",
    "\n",
    "## $$P(\\vec{x}|y) = P(w_1|y)\\times P(w_2|y)\\times\\cdots\\times P(w_k|y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty brazenly naive assumption. It is assuming, for example, that the probability of seeing the word \"ball\" in an article about sports $P(ball|\\text{sports})$ is totally independent of the presence of the word \"soccer\" in the article.\n",
    "\n",
    "But if it works, it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a document $\\vec{x}$, we just need to calculate the posterior probabilities for each class $\\{y_j\\}$, then see which class has the highest posterior probability.\n",
    "\n",
    "# $$P(y_j|\\vec{x}) \\propto P(w_1|y_j)\\times P(w_2|y_j)\\times\\cdots\\times P(w_k|y_j)\\times P(y_j)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've dropped the denominator $P(\\vec{x})$ since it doesn't depend on $y_j$, and we're only interested in the $y_j$ that maximizes $P(y_j|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the prior, $P(y_j)$ ?\n",
    "\n",
    "The prior probability of class $y$ is simply how frequent that class is among our training documents:\n",
    "$$P(y_j) = \\frac{\\text{# of documents of class}\\, y_j}{\\text{total # of documents}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about these likelihoods? How do we calculate 'em?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a document $\\vec{x}$ of class $y_j$ is generated by drawing words from a bag (with replacement) with probabilities $P(w_i | y_j)$\n",
    "\n",
    "Let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word counts: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can write our posterior (which, remember, is the prior times the likelihood) as\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j)\\times\\prod_{i=1}^p P(w_i|y_j)^{x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{N_{ji}}{N_j} = \\frac{\\text{total count of w_i across all documents of class}\\, y_j}{\\text{total count of all words across all documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our new document contains a word we've never seen before? Then our estimate of $P(w|y)$ for that word would be zero, and then the entire product $P(y|\\vec{x})$ would be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, we employ *Laplace Smoothing*\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + N_{ji}}{\\alpha p + N_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, $\\alpha = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Bernoulli Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, now, that a document of class $y_j$ has a probability $P(w_i|y_j)$ of containing word $w_i$ at least once (regardless of word count).\n",
    "\n",
    "Again let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word *occurrences*: $x_i = 1$ if the $w_i$ is in that document at all, $x_i = 0$ if not: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can think of our document as a series of Bernoulli trials, one for each word, and our posterior is\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j) \\times\\prod_{i=1}^p P(w_i|y_j)^{x_i}(1 - P(w_i|y_j))^{1 - x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j)) + (1 - x_i)\\text{log}(1 - P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{D_{ji}}{D_j} = \\frac{\\text{total # of documents of class}\\,y_j\\text{ containing}\\, w_i}{\\text{total number of documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again we'd hit the \"unseen word\" problem. So our smoothing here looks like\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + D_{ji}}{2\\alpha + D_j} $$\n",
    "\n",
    "Again with $\\alpha = 1$ usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "- Bernoulli tends to work better with shorter documents...\n",
    "- ...but sklearn says \"It is advisable to evaluate both models, if time permits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[sklearn User Guide: Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "[Spam Filtering with Naive Bayes â€“ Which Naive Bayes?](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=E870DFA148786F5A27F88CF80FB4D73B?doi=10.1.1.61.5542&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other flavors: Gaussian Naive Bayes\n",
    "When my features are continuous (but my target is still a class), I can go ahead and assume that data for a given class has normally distributed values for each feature.\n",
    "\n",
    "You cannot stop me from making this assumption.\n",
    "\n",
    "Then my likelihood above is just a product of Gaussian probability density functions. Same process!\n",
    "\n",
    "[See here for bugs](http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['when its time to party we will party hard',\n",
    "     'theres a party in my mind',\n",
    "     'i need something to change my mind']\n",
    "y = ['andrew wk', \n",
    "     'talking heads', \n",
    "     'talking heads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = ['a change in the weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more than one way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'time', 'my', 'need', 'i', 'its', 'will', 'hard', 'in', 'theres', 'we', 'a', 'change', 'when', 'something', 'party', 'mind'}\n"
     ]
    }
   ],
   "source": [
    "# Simple, readable, efficient, bulky\n",
    "\n",
    "vocab = set()\n",
    "for row in X:\n",
    "    for word in row.split():\n",
    "        vocab.add(word)\n",
    "\n",
    "print( vocab )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'time', 'my', 'need', 'i', 'its', 'will', 'hard', 'in', 'theres', 'we', 'a', 'change', 'when', 'something', 'party', 'mind'}\n"
     ]
    }
   ],
   "source": [
    "# Simple, clever, memory inefficient\n",
    "\n",
    "print( set(' '.join(X).split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'time', 'my', 'need', 'i', 'its', 'will', 'hard', 'in', 'theres', 'we', 'a', 'change', 'when', 'something', 'party', 'mind'}\n"
     ]
    }
   ],
   "source": [
    "# Compact, efficient, unreadable\n",
    "\n",
    "print( reduce( lambda a,b: a|b, [set(x.split()) for x in X] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'andrew wk': 1, 'talking heads': 2})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = sum(class_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_class = defaultdict(int)\n",
    "word_count_per_class = defaultdict(Counter)\n",
    "\n",
    "for doc, label in zip(X,y):\n",
    "    doc_words = doc.split()\n",
    "    \n",
    "    words_per_class[label] += len(doc_words)\n",
    "    \n",
    "    word_count_per_class[label].update(Counter(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'andrew wk': 9, 'talking heads': 13})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'andrew wk': Counter({'when': 1,\n",
       "                      'its': 1,\n",
       "                      'time': 1,\n",
       "                      'to': 1,\n",
       "                      'party': 2,\n",
       "                      'we': 1,\n",
       "                      'will': 1,\n",
       "                      'hard': 1}),\n",
       "             'talking heads': Counter({'theres': 1,\n",
       "                      'a': 1,\n",
       "                      'party': 1,\n",
       "                      'in': 1,\n",
       "                      'my': 2,\n",
       "                      'mind': 2,\n",
       "                      'i': 1,\n",
       "                      'need': 1,\n",
       "                      'something': 1,\n",
       "                      'to': 1,\n",
       "                      'change': 1})})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{x}$ = \"a change in the weather\"\n",
    "\n",
    "$y_1$ = andrew wk\n",
    "\n",
    "$y_2$ = talking heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y_1 | \\vec{x}) \\propto P(y_1) \\prod P( w_i | y_1 )^{x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing the likelihoods $$P(w_i | y_j) = \\frac{\\alpha + N_{ji}}{\\alpha p + N_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_w_given_class(word, classname, alpha=1):\n",
    "    \n",
    "    p = len(vocab)\n",
    "    \n",
    "    N_ji = word_count_per_class[classname][word]\n",
    "    N_j = words_per_class[classname]\n",
    "    \n",
    "    return (alpha + N_ji) / (alpha*p + N_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'change', 'in', 'the', 'weather']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_test[0].split()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll do andrew wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.038461538461538464,\n",
       " 0.038461538461538464,\n",
       " 0.038461538461538464,\n",
       " 0.038461538461538464,\n",
       " 0.038461538461538464]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_w_given_andrewwk = [ P_w_given_class(w_i, \"andrew wk\") for w_i in x ]\n",
    "P_w_given_andrewwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_andrewwk = class_counts[\"andrew wk\"]/total_docs\n",
    "P_andrewwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.38909497877552"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_x_given_andrewwk = np.log(P_andrewwk) + np.log(P_w_given_andrewwk).sum()\n",
    "P_x_given_andrewwk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now the talking heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.06666666666666667,\n",
       " 0.03333333333333333,\n",
       " 0.03333333333333333]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_w_given_th = [ P_w_given_class(w_i, \"talking heads\") for w_i in x ]\n",
    "P_w_given_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_th = class_counts[\"talking heads\"]/total_docs\n",
    "P_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.332010474739105"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_x_given_th = np.log(P_th) + np.log(P_w_given_th).sum()\n",
    "P_x_given_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement over andrew wk, and so we declare talking heads the most likely origin of the lyric \"a change in the weather\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
