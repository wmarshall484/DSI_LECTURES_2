{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough : obtaining Bag-of-Words from raw documents\n",
    "\n",
    "This Walkthrough will lead us from raw documents to bag-of-words representations using **Natural Language Processing** functions.\n",
    "\n",
    "There are two sides to NLP:\n",
    "- **indexing** text: reducing complexity to be able to process large corpus of documents, for information retrieval\n",
    "- **extracting information** from text: recognizing details in the meaning of each document, paragraph, sentence, to be able to extract relevant entities from these documents.\n",
    "\n",
    "In our case, this walkthrough is a preliminary step of a pipeline for **indexing** documents.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **signature** (vector) for each document.\n",
    "\n",
    "This **signature** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Text sources and possible text mining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = u\"My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt – sleeveless, white eyelet lace; I was wearing it as a farewell gesture. My carry-on item was a parka. In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds. It rains on this inconsequential town more than any other place in the United States of America. It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old. It was in this town that I’d been compelled to spend a month every summer until I was fourteen. That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.\"\n",
    "\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_string = paragraph.encode('utf-8')\n",
    "\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return str(only_ascii, 'utf-8')\n",
    "\n",
    "input_string = remove_accents(paragraph)\n",
    "\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating bag-of-words for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenize document\n",
    "\n",
    "**\"Tokenize\"** means creating \"tokens\" which are atomic units of the text. These tokens are usually words we extract from the document by splitting it (using punctuations as a separator). We can also consider sentences as tokens (and words as sub-tokens of sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n",
    "\n",
    "for sent in sent_tokens:\n",
    "    print(\"--- sentence: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [sent for sent in map(word_tokenize, sent_tokens)]\n",
    "\n",
    "#tokens = word_tokenize(input_string)\n",
    "for sent in tokens:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))\n",
    "#print(\"--- nltk tokens from paragraph:\\n{}\".format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tokens_lower = [list(map(lambda s : s.lower(), sent)) for sent in tokens]\n",
    "\n",
    "for sent in tokens_lower:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Filtering stopwords (and punctuation)\n",
    "\n",
    "**Stopwords** are words that should be stopped at this step because they do not carry much information about the actual meaning of the document. Usually, they are \"common\" words you use. You can find lists of such **stopwords** online, or embedded within the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list found at http://www.textfixer.com/resources/common-english-words.txt\n",
    "# 'not' has been removed (do you know why ?)\n",
    "\n",
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to filter punctuation tokens: tokens made of punctuation marks. We can find a list of those punctuations in string.punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_ = set(string.punctuation)\n",
    "print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "\n",
    "for sent in tokens_filtered:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stemming and lemmatization\n",
    "\n",
    "**Stemming** means reducing each word to a **stem**. That is, reducing each word in all its diversity to a root common to all its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [list(map(stemmer_porter.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (porter): {}\".format(tokens_stemporter[0]))\n",
    "\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "tokens_stemsnowball = [list(map(stemmer_snowball.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (snowball): {}\".format(tokens_stemsnowball[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. N-Grams\n",
    "\n",
    "<span style=\"color:red\">To capture sequences of tokens</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens_stemsnowball[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def join_sent_ngrams(input_tokens, n):\n",
    "    # first add the 1-gram tokens\n",
    "    ret_list = list(input_tokens)\n",
    "    \n",
    "    #then for each n\n",
    "    for i in range(2,n+1):\n",
    "        # add each n-grams to the list\n",
    "        ret_list.extend(['-'.join(tgram) for tgram in ngrams(input_tokens, i)])\n",
    "    \n",
    "    return(ret_list)\n",
    "\n",
    "tokens_ngrams = list(map(lambda x : join_sent_ngrams(x, 3), tokens_stemporter))\n",
    "\n",
    "print(\"--- sentence tokens: {}\".format(tokens_ngrams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Part-of-Speech tagging\n",
    "\n",
    "This is an alternative process that relies on machine learning to tag each word in a sentence with its function. In libraries such as NLTK, there are embedded tools to do that. Tags detected depend on the corpus used for training. In NLTK, the function `nltk.pos_tag()` uses the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "for sent in sent_tags:\n",
    "    print(\"--- sentence tags: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter verbs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in sent_tags:\n",
    "    tags_filtered = [t for t in sent if t[1].startswith('VB')]\n",
    "    print(\"--- verbs:\\n{}\".format(tags_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NPB: {<DT|PP\\$>?<JJ|NN|,>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "  V2V: {<V.*> <TO> <V.*>}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(sent_tags[1])\n",
    "\n",
    "#print result\n",
    "\n",
    "for sent in sent_tags:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NPB': print(subtree)\n",
    "        if subtree.label() == 'V2V': print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
