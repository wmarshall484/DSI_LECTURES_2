{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough : indexing Bag-of-Words into a vector table\n",
    "\n",
    "This Walkthrough will lead us from bag-of-words representations of documents to **vector signatures** (indexes) using the **TF-IDF** formula.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **vector representation** (signature) for each document. This vector representation will be used for:\n",
    "\n",
    "- mine the features that can caracterize classes of documents (supervised learning using **labels**)\n",
    "- mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough2.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading some input data from the Amazon Reviews\n",
    "\n",
    "To try this indexing walkthrough, we will get 5 reviews from the Amazon Reviews dataset. We will apply a function for extracting bag-of-words representations from these 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os               # for environ variables in Part 3\n",
    "from nlp_pipeline import extract_bow_from_raw_text\n",
    "import json\n",
    "\n",
    "docs = []\n",
    "with open('./reviews.json', 'r') as data_file:    \n",
    "    for line in data_file:\n",
    "        docs.append(json.loads(line))\n",
    "\n",
    "# extracting bows\n",
    "bows = list(map(lambda row: extract_bow_from_raw_text(row['reviewText']), docs))\n",
    "\n",
    "# displaying bows\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Obtaining vocabulary and term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# term occurence = counting distinct words in each bag\n",
    "term_occ = list(map(lambda bow : Counter(bow), bows))\n",
    "\n",
    "# term frequency = occurences over length of bag\n",
    "term_freq = list()\n",
    "for i in range(len(docs)):\n",
    "    term_freq.append( {k: (v / float(len(bows[i])))\n",
    "                       for k, v in term_occ[i].items()} )\n",
    "\n",
    "# displaying occurences\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- term_occ: {}\".format(term_occ[i]))\n",
    "    print(\"--- term_freq: {}\".format(term_freq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document occurence = number of documents having this word\n",
    "# term frequency = occurences over length of bag\n",
    "\n",
    "doc_occ = Counter( [word for bow in bows for word in set(bow)] )\n",
    "\n",
    "# document frequency = occurences over length of corpus\n",
    "doc_freq = {k: (v / float(len(docs)))\n",
    "            for k, v in doc_occ.items()}\n",
    "\n",
    "# displaying vocabulary\n",
    "print(\"\\n--- full vocabulary: {}\".format(doc_occ))\n",
    "print(\"\\n--- doc freq: {}\".format(doc_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transforming frequencies into a TF-IDF vector\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "$tf(term,document) = \\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document$\n",
    "\n",
    "The idf part is defined in terms of the document frequency. The document frequency is \n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "It is called the inverse document frequency but really it is the log of the inverse document frequency. Finally tf-idf is just\n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "min_df = 0.3\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n",
    "\n",
    "# print vocabulary\n",
    "print (\"-- vocabulary (len={}): {}\".format(len(vocabulary),vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a dense matrix of vectors for each document\n",
    "# each vector has the length of the vocabulary\n",
    "vectors = np.zeros((len(docs),len(vocabulary)))\n",
    "\n",
    "# fill these vectors with tf-idf values\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        term     = vocabulary[j]\n",
    "        term_tf  = term_freq[i].get(term, 0.0)   # 0.0 if term not found in doc\n",
    "        term_idf = np.log(1 + 1 / doc_freq[term]) # smooth formula\n",
    "        vectors[i,j] = term_tf * term_idf\n",
    "\n",
    "# displaying results\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- tfidf vector: {}\".format( vectors[i] ) )\n",
    "    print(\"--- tfidf sorted: {}\".format( \n",
    "            sorted( zip(vocabulary,vectors[i]), key=lambda x:-x[1] )\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
