{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\text{NLP: Natural Language Processing}$$\n",
    "\n",
    "\n",
    "# OBJECTIVES:\n",
    "# 1. Tokens/Bag of Words\n",
    "# 2. Text Normalization: stemming/lemmatizing, stop words, punctuation/whitespace\n",
    "# 3. TF-IDF\n",
    "# 4. Cosine Similarity\n",
    "\n",
    "<br>\n",
    "\n",
    "## BONUS (https://www.youtube.com/watch?v=6zm9NC9uRkk):\n",
    "## 1. spacy\n",
    "## $\\quad$ a. Entity Identification\n",
    "## $\\quad$ b. Part of Speech Tagging\n",
    "## $\\quad$ c. Text \"Normalization\"\n",
    "## 2. gensim \n",
    "## $\\quad$ a. Phrase Modeling\n",
    "## $\\quad$ b. Topic Modeling\n",
    "## $\\quad$ c. Word2Vec\n",
    "## 2. textacy\n",
    "## $\\quad$ \"C\". is for \"Charley\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.nyt_dump\n",
    "coll = db.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unicodetoascii(text):# \\xe2\\x80\\x9c#replace(u'\\xe2\\xad',\"\").\n",
    "    TEXT = text.replace(u'\\xe2\\x80\\x99', \"'\").replace(u'\\xc3\\xa9', 'e').replace(u'\\xe2\\x80\\x90', '-').replace(u'\\xe2\\x80\\x91', '-').replace(u'\\xe2\\x80\\x92', '-').replace(u'\\xe2\\x80\\x93', '-').replace(u'\\xe2\\x80\\x94', '-').replace(u'\\xe2\\x80\\x94', '-').replace(u'\\xe2\\x80\\x98', \"'\").replace(u'\\xe2\\x80\\x9b', \"'\").replace(u'\\xe2\\x80\\x9c', '\"').replace(u'\\xe2\\x80\\x9c', '\"').replace(u'\\xe2\\x80\\x9d', '\"').replace(u'\\xe2\\x80\\x9e', '\"').replace(u'\\xe2\\x80\\x9f', '\"').replace(u'\\xe2\\x80\\xa6', '...').replace(u'\\xe2\\x80\\xb2', \"'\").replace(u'\\xe2\\x80\\xb3', \"'\").replace(u'\\xe2\\x80\\xb4', \"'\").replace(u'\\xe2\\x80\\xb5', \"'\").replace(u'\\xe2\\x80\\xb6', \"'\").replace(u'\\xe2\\x80\\xb7', \"'\").replace(u'\\xe2\\x81\\xba', \"+\").replace(u'\\xe2\\x81\\xbb', \"-\").replace(u'\\xe2\\x81\\xbc', \"=\").replace(u'\\xe2\\x81\\xbd', \"(\").replace(u'\\xe2\\x81\\xbe', \")\")\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [unicodetoascii(' '.join(article['content']).lower()) for article in coll.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [d for d in documents if len(d) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(documents)\n",
    "documents[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs = []\n",
    "for document in documents:\n",
    "    tokenized_docs.append(nltk.word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://text-processing.com/demo/tokenize/\n",
    "tokenized_docs[2][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing \"documents\" as a *Bag of Words*\n",
    "# $ \n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "& \\text{token 1} &  \\text{token 2} & \\cdots &\\text{token p} \\\\ \\hline \n",
    "\\text{document 1} &&&& \\\\ \\hline\n",
    "\\text{document 2} &&&& \\\\ \\hline\n",
    "\\vdots &&&& \\\\ \\hline\n",
    "\\text{document n} &&&& \\\\ \\hline\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "# How big is $n$?\n",
    "\n",
    "\n",
    "# How big is $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing $p$:\n",
    "# - stop words \n",
    "# - punctuation/whitespace\n",
    "# - stemming/lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toc_doc = []\n",
    "for document in tokenized_docs:\n",
    "    doc = []\n",
    "    for token in document:\n",
    "        if token not in sw:\n",
    "            doc.append(token)\n",
    "    toc_doc.append(doc)\n",
    "    \n",
    "tokenized_docs = toc_doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "sp = set(string.punctuation)\n",
    "sp.add('``')\n",
    "sp.add(\"''\")\n",
    "\n",
    "toc_doc = []\n",
    "for document in tokenized_docs:\n",
    "    doc = []\n",
    "    for token in document:\n",
    "        if token not in sp:\n",
    "            doc.append(token)\n",
    "    toc_doc.append(doc)\n",
    "    \n",
    "tokenized_docs = toc_doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "tokenized_docs_porter = copy.deepcopy(tokenized_docs)\n",
    "tokenized_docs_snowball = copy.deepcopy(tokenized_docs)\n",
    "tokenized_docs_lemmatize = copy.deepcopy(tokenized_docs)\n",
    "for document in range(len(tokenized_docs)):\n",
    "    for token in range(len(tokenized_docs[document])):\n",
    "        tokenized_docs_porter[document][token] = porter.stem(tokenized_docs[document][token])\n",
    "        tokenized_docs_snowball[document][token] = snowball.stem(tokenized_docs[document][token])\n",
    "        tokenized_docs_lemmatize[document][token] = wordnet.lemmatize(tokenized_docs[document][token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs_porter[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs_snowball[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs_lemmatize[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set([t for d in tokenized_docs_porter for t in d ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {v:i for i,v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "TF = np.zeros([len(tokenized_docs_porter), len(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in range(len(tokenized_docs_porter)):\n",
    "    for t in range(len(tokenized_docs_porter[d])):\n",
    "        TF[d,vocab[tokenized_docs_porter[d][t]]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary, Corpus\n",
    "# Term Frequency, Presence/Absence, Document Frequency\n",
    "# $ \n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "& \\text{token 1} &  \\text{token 2} & \\cdots &\\text{token p} \\\\ \\hline \n",
    "\\text{document 1} &&&& \\\\ \\hline\n",
    "\\text{document 2} &&&& \\\\ \\hline\n",
    "\\vdots &&&& \\\\ \\hline\n",
    "\\text{document n} &&&& \\\\ \\hline\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<font color=\"red\">\n",
    "# What makes two documents similar?\n",
    "<br>\n",
    "\n",
    "<img src=\"stuff/Image530.gif\",width=500px, align=\"left\", Image529.gif>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequencing Weighting\n",
    "\n",
    "# $$TF\\text{-}IDF_{ij} = \\frac{TF_{ij}}{\\log\\left(1 + \\frac{n}{1+DF_{i}}\\right)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing\n",
    "\n",
    "# $$^*TF\\text{-}IDF_{ij} = \\frac{TF\\text{-}IDF_{ij}}{\\sum_{j=1}^p TF\\text{-}IDF_{ij}}$$\n",
    "\n",
    "# $$^{**}TF\\text{-}IDF_{ij} = \\frac{TF\\text{-}IDF_{ij}}{\\sqrt{\\sum_{j=1}^p TF\\text{-}IDF_{ij}^2}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF = (TF>0).sum(axis=0)#/len(TF)\n",
    "DF.shape\n",
    "# plt.hist(DF)\n",
    "# DF.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_IDF = TF*(1+np.log(len(TF)/(0.+DF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(999./7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tf in range(len(TF_IDF)):\n",
    "    TF_IDF[tf] = TF_IDF[tf]/np.sqrt(np.sum(TF_IDF[tf]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dot product of two vectors $X_1$ and $X_2$ depends on their magnitude and the angle $\\theta$ between them:\n",
    "# $$ \\frac{X_1 \\cdot X_2}{||X_1|| \\; ||X_2||} = Cos(\\theta)$$\n",
    "\n",
    "# *Cosine Similarity* measures if two vectors point in similar directions:\n",
    "# The angle between two vectors is $\\quad\\quad\\quad$ if they point in similar directions.\n",
    "# The angle between two vectors is $\\quad\\quad\\quad$ if they point in dissimilar directions.\n",
    "# The cosine similarity between two vectors is $\\quad\\quad\\quad$ if they point in similar directions.\n",
    "# The cosine similarity between two vectors is $\\quad\\quad\\quad$ if they point in dissimilar directions.\n",
    "# The maximum angle between *vectorized documents is* $\\quad\\quad\\quad$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_IDF.dot(TF_IDF.T)[2].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents[782]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "word_counts = vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    tokenized_doc = nltk.word_tokenize(doc)\n",
    "\n",
    "    sw = set(stopwords.words('english'))\n",
    "    \n",
    "    toc_doc = []\n",
    "    for token in tokenized_doc:\n",
    "        if token not in sw:\n",
    "            toc_doc.append(token)\n",
    "    tokenized_doc = toc_doc  \n",
    "\n",
    "    sp = set(string.punctuation)\n",
    "    sp.add('``')\n",
    "    sp.add(\"''\")\n",
    "\n",
    "    toc_doc = []\n",
    "    for token in tokenized_doc:\n",
    "        if token not in sp:\n",
    "            toc_doc.append(token)\n",
    "    tokenized_doc = toc_doc       \n",
    "\n",
    "    tokenized_doc_porter = copy.deepcopy(tokenized_doc)\n",
    "    for token in range(len(tokenized_doc)):\n",
    "        tokenized_doc_porter[token] = porter.stem(tokenized_doc[token])\n",
    "    \n",
    "    return tokenized_doc_porter\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "word_counts = vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "j=1\n",
    "plt.plot([TF[j][vocab[i]] for i in vect.get_feature_names()],word_counts.toarray()[j],'.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=tokenize, smooth_idf=False)\n",
    "word_counts = vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j=1\n",
    "plt.plot([TF_IDF[j][vocab[i]] for i in vect.get_feature_names()],word_counts.toarray()[j],'.') \n",
    "plt.plot([0,.5],[0,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conda install spacy\n",
    "# pip install spacy && python -m spacy.en.download\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_doc = nlp(documents[2])\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,s in enumerate(parsed_doc.sents):\n",
    "    print i\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,e in enumerate(parsed_doc.ents):\n",
    "    print i, e, \": (\", e.label_, \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = [token.orth_ for token in parsed_doc]\n",
    "post = [token.pos_ for token in parsed_doc]\n",
    "\n",
    "pd.DataFrame(zip(text, post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ent_type = [token.ent_type_ for token in parsed_doc]\n",
    "ent_iob = [token.ent_iob_ for token in parsed_doc]\n",
    "\n",
    "pd.DataFrame(zip(text, ent_type, ent_iob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemma = [token.lemma_ for token in parsed_doc]\n",
    "shape = [token.shape_ for token in parsed_doc]\n",
    "\n",
    "pd.DataFrame(zip(text, lemma, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stuffs = [(token.orth_, token.prob, token.is_stop, token.is_punct, token.is_space, token.like_num, token.is_oov) for token in parsed_doc]\n",
    "ent_iob = [token.ent_iob_ for token in parsed_doc]\n",
    "\n",
    "pd.DataFrame(stuffs, columns=[\"orth\",\"log prob\",\"is_stop\",\"is_punct\",\"is_space\",\"like_num\",\"is_oov\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# gensim \n",
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Phrase Modeling e.g., \"Happy Hour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#is_stop\n",
    "def norm_lem():\n",
    "    for doc in documents:\n",
    "        parsed_doc = nlp(doc)\n",
    "        for sent in parsed_doc.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent if not token.is_punct and not token.is_space])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"LineSentenceFile.txt\", 'w', encoding='utf_8') as f:\n",
    "    for s in norm_lem():\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "for s in it.islice(LineSentence(\"LineSentenceFile.txt\"),230,233):\n",
    "    print u' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(LineSentence(\"LineSentenceFile.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_model.save(\"BigramModel.txt\")\n",
    "with codecs.open(\"BigramSentenceFile.txt\", 'w', encoding='utf_8') as f:\n",
    "    for s in LineSentence(\"LineSentenceFile.txt\"):\n",
    "        f.write(u' '.join(bigram_model[s]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(LineSentence(\"BigramSentenceFile.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model.save(\"TrigramModel.txt\")\n",
    "with codecs.open(\"TrigramSentenceFile.txt\", 'w', encoding='utf_8') as f:\n",
    "    for s in LineSentence(\"BigramSentenceFile.txt\"):\n",
    "        f.write(u' '.join(trigram_model[s]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "for s in it.islice(LineSentence(\"TrigramSentenceFile.txt\"),139,146):\n",
    "    print u' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"Trigram_documents.txt\", 'w', encoding='utf_8') as f:\n",
    "    for doc in [d for d in documents if len(d) > 0]:\n",
    "        parsed_doc = nlp(doc)\n",
    "        tri = trigram_model[bigram_model[[token.lemma_ for token in parsed_doc if not token.is_punct and not token.is_space]]]         \n",
    "        f.write(u' '.join([t for t in tri if t not in spacy.en.STOPWORDS])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# gensim \n",
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "#pip install pyldavis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#from cPickle import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrigramDictionary = Dictionary(LineSentence(\"Trigram_documents.txt\"))\n",
    "TrigramDictionary.filter_extremes(no_below=10, no_above=.4)\n",
    "TrigramDictionary.filter_n_most_frequent(300)\n",
    "TrigramDictionary.compactify()\n",
    "#TrigramDictionary.save(\"TrigramDictionary.txt\")\n",
    "#TrigramDictionary = Dictionary.load(\"TrigramDictionary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow():\n",
    "    for doc in LineSentence(\"Trigram_documents.txt\"):\n",
    "        yield TrigramDictionary.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize(\"Trigram_documents_bow.txt\", bow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(MmCorpus(\"Trigram_documents_bow.txt\"), num_topics=6, workers=1, id2word=TrigramDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore topics\n",
    "for term, freq in lda.show_topic(0,10):\n",
    "    print term, \":\", round(freq, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(lda, MmCorpus(\"Trigram_documents_bow.txt\"), TrigramDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 2\n",
    "print lda[list(bow())[d]]\n",
    "print u' '.join(list(LineSentence(\"Trigram_documents.txt\"))[d])\n",
    "documents[d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# gensim \n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V = Word2Vec(LineSentence(\"Trigram_documents.txt\"), size=20, window=5, min_count=3, sg=1, seed=0)\n",
    "for i in range(5):\n",
    "    myW2V.train(LineSentence(\"Trigram_documents.txt\"))\n",
    "\n",
    "myW2V.init_sims()\n",
    "myW2V.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted([(t, v.count) for t, v in myW2V.vocab.iteritems()], key = lambda x: -x[1])[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1:16:42\n",
    "# vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=['child'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=[u'country'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=[u'united_states',u'middle_east'], negative=[u'democracy'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=[u'united_states'], negative=[u'food'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=[u'food',u'russia'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myW2V.most_similar(positive=[u'united_states',u'election'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ins = pd.DataFrame(myW2V.syn0, index=[t for t, v in myW2V.vocab.iteritems()]).drop(spacy.en.STOPWORDS, errors='ignore')\n",
    "ins = ins[:1000]\n",
    "tsne = TSNE()\n",
    "tsne_vects = tsne.fit_transform(ins.values)\n",
    "outs = pd.DataFrame(tsne_vects, index=pd.Index(ins.index), columns=['tsne_x','tsne_y'])\n",
    "outs['word'] = outs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data = ColumnDataSource(outs)\n",
    "tsne_plot = figure(title=\"tsne word encodings\",\n",
    "                   plot_width=800, plot_height=800,\n",
    "                   tools=('pan, wheel_zoom, box_zoom, box_select, resize, reset'),\n",
    "                   active_scroll='wheel_zoom')\n",
    "\n",
    "tsne_plot.add_tools( HoverTool(tooltips = '@word') )\n",
    "tsne_plot.circle('tsne_x','tsne_y', source=plot_data, color='blue', line_alpha=.2, fill_alpha=.1,\n",
    "                 size=10, hover_line_color='black')\n",
    "\n",
    "tsne_plot.title.text_font_size = value('16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stuff/bayes.tiff\",width=1000px, align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVES:\n",
    "# 1. NB4NLP (Naive Bayes for Natural Language Processing)\n",
    "<br>\n",
    "# Bonus:\n",
    "\n",
    "# 1. Generative Modeling\n",
    "# 2. Why NB4NLP?\n",
    "# $\\quad$ a. p>>n covariance matrix estimation\n",
    "# $\\quad$ b. Computation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB4NLP\n",
    "\n",
    "# $ \n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& \\text{token 1} &  \\text{token 2} & \\cdots &\\text{token p}& \\textbf{Class Label } (Y) \\\\ \\hline \n",
    "\\text{document 1} &&&& &\\\\ \\hline\n",
    "\\text{document 2} &&&& &\\\\ \\hline\n",
    "\\vdots &&&& &\\\\ \\hline\n",
    "\\text{document n} &&&& &\\\\ \\hline\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "\n",
    "# $$ \n",
    "\\begin{align*}\n",
    "\\textbf{X}_0 &= \\text{\"$\\textbf{X}_0$ is a 'document' that...\"} \\\\\n",
    "&= (t_1, t_2, \\cdots, t_K)\\\\\n",
    "&\\\\\n",
    "Pr(Y_0=y_0|\\textbf{X}_0) &= \\frac{Pr(\\textbf{X}_0|Y_0=y_0)Pr(Y_0=y_0)}{Pr(\\textbf{X}_0)}\\\\\n",
    "&\\propto  Pr(\\textbf{X}_0|Y_0=y_0)Pr(Y_0=y_0)\\\\\n",
    "{}&\\\\\n",
    "Pr(Y_0=y_0) &= \\sum_{i: Y_i=y_0}\\frac{1}{n}\\\\\n",
    "Pr(\\textbf{X}_0|Y_0=y_0) &= \\prod_{k=1}^{|\\textbf{X}_0|}Pr(X_{0k}|Y_0=y_0) \\\\\n",
    "&= \\prod_{k=1}^{|\\textbf{X}_0|} \\left( \\sum_{ \\underset{\\text{token}_{j}=X_{0k}}{\\overset{i,j: Y_i=y_0}{}} }TF_{ij} \\right)/\\left( \\sum_{ \\underset{\\;}{\\overset{i,j: Y_i=y_0}{}} }TF_{ij} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "# What is the assumption here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if $TF_{ij}$ = 0?\n",
    "# Laplace Smoothing: \n",
    "# $$ \\prod_{k=1}^{|\\textbf{X}_0|} \\left( \\sum_{ \\underset{\\text{token}_{j}=X_{0k}}{\\overset{i,j: Y_i=y_0}{}} }(TF_{ij}+\\alpha) \\right)/\\left( \\sum_{ \\underset{\\;}{\\overset{i,j: Y_i=y_0}{}} }(TF_{ij} +\\alpha) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we get rid of the *\"proportional to\"*?\n",
    "# $$\n",
    "\\begin{align*}\n",
    "Pr(Y_0=y_0|\\textbf{X}_0) &= \\frac{Pr(\\textbf{X}_0|Y_0=y_0)Pr(Y_0=y_0)}{\\underset{y}{\\sum} Pr(\\textbf{X}_0|Y_0=y)Pr(Y_0=y)}\\\\\n",
    "&= \\frac{\\left(\\prod_{k=1}^{|\\textbf{X}_0|} p_k^{y_0}\\right)Pr(Y_0=y_0)}{\\underset{y}{\\sum} \\left(\\prod_{k=1}^{|\\textbf{X}_0|} p_k^y\\right)Pr(Y_0=y)}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "# What about multiplying so many small probabilities together?\n",
    "# $$ \\log\\left(\\prod_{k=1}^{|\\textbf{X}_0|} p_k\\right) = \\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k $$\n",
    "\n",
    "# $$\n",
    "\\begin{align*}\n",
    "Pr(Y_0=y_0|\\textbf{X}_0) &= \\frac{\\exp\\left(\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y_0} \\right) Pr(Y_0=y_0)}{\\underset{y}{\\sum} \\exp\\left(\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y} \\right) Pr(Y_0=y)}\n",
    "\\end{align*}$$\n",
    "\n",
    "# Okay but this is still a really small number that we probably can't represent...\n",
    "# $$ Pr(\\textbf{X}_0|Y_0=y) = \\exp\\left(\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^y \\right) $$\n",
    "\n",
    "# But we can scale it!\n",
    "# $$\n",
    "\\begin{align*}\n",
    "Pr(Y_0=y_0|\\textbf{X}_0) &= \\frac{\\exp(C) \\exp\\left(\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y_0} \\right) Pr(Y_0=y_0)}{\\underset{y}{\\sum}\\exp(C) \\exp\\left(\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y} \\right) Pr(Y_0=y)}\\\\\n",
    "&= \\frac{exp\\left(C+\\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y_0} \\right) Pr(Y_0=y_0)}{\\underset{y}{\\sum}  \\exp\\left(C+ \\sum_{k=1}^{|\\textbf{X}_0|}  \\log p_k^{y} \\right) Pr(Y_0=y)}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminitive or Predictive (Conditional) Model:\n",
    "# $$ f(Y_0|\\textbf{X}_0)$$\n",
    "\n",
    "# Generative (Joint) Model\n",
    "# $$ f(\\textbf{X}_i,Y_i) \\quad \\sim \\quad f(\\textbf{X}_i|Y_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB4NLP\n",
    "# $$ \n",
    "\\begin{align*}\n",
    "Pr(\\textbf{X}_i^*|Y_i=y) &= Multinomial(\\hat \\pi_y)\\\\\n",
    " &= f_{\\hat \\pi_y}(\\textbf{X}_i^*)\\\\\n",
    "Pr(Y_i=y) &= \\hat w_y\\\\\n",
    "Pr(\\textbf{X}_i^*,Y_i) &= \\sum_y \\hat w_y f_{\\hat \\pi_y}(\\textbf{X}_i^*) \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \n",
    "\\begin{align*}\n",
    "\\textbf{X}_0 &= \\text{\"$\\textbf{X}_0$ is a 'document' that...\"} \\\\\n",
    "&= (t_1, t_2, \\cdots, t_K)\\\\\n",
    "\\textbf{X}_0^* &= (TF_{01}^*, TF_{02}^*, \\cdots, TF_{0p}^*)\\\\\n",
    "{}\\\\\n",
    "\\textbf{X}_i &= (X_{i1}, X_{i2}, \\cdots, X_{ip})\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB4continuousfeatures\n",
    "# $$ \n",
    "\\begin{align*}\n",
    "Pr(\\textbf{X}|Y=y) &= MVN(\\hat \\mu_y, \\hat \\Sigma_y)\\\\\n",
    " &= f_{\\hat \\mu_y, \\hat \\Sigma_y}(\\textbf{X})\\\\\n",
    "Pr(Y=y) &= \\hat w_y\\\\\n",
    "Pr(\\textbf{X},Y=y) &= \\sum_y \\hat w_y f_{\\hat \\mu_y, \\hat \\Sigma_y}(\\textbf{X})\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stuff/mixture.png\",width=500px, align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# $$\n",
    "\\left[\\begin{array}{cccc}\n",
    "X_{11}&X_{12} & \\cdots & X_{1p} \\\\\n",
    "X_{21}&X_{22} & \\cdots & X_{2p} \\\\\n",
    "\\vdots &\\vdots &\\ddots&\\vdots \\\\\\hline\n",
    "X_{i1}&X_{i2} & \\cdots & X_{ip} \\\\ \\hline\n",
    "\\vdots &\\vdots &\\ddots&\\vdots \\\\\n",
    "X_{n1}&X_{n2} & \\cdots & X_{np} \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "# $$ {\\boldsymbol X}_i = (X_{i1}, X_{i2}, \\cdots, X_{ip})^T \\sim MVN({\\boldsymbol \\mu}_p,\\Sigma_{p \\times p})$$ \n",
    "\n",
    "\n",
    "# $${\\boldsymbol X}_i = \\left(\\begin{array}{c} {X_{i1}} \\\\ {X_{i2}} \\\\ \\vdots\\\\ {X_{ip}} \\end{array} \\right) \\sim MVN\\left(\n",
    " \\left[\\begin{array}{c} \\mu_{{X_{i1}}} \\\\ \\mu_{{X_{i2}}} \\\\ \\vdots\\\\ \\mu_{{X_{ip}}} \\end{array} \\right]_,\n",
    "\\left[\\begin{array}{cccc}\\sigma^2_{{X_{i1}}}&\\sigma_{{X_{i1}X_{i2}}} & \\cdots & \\sigma_{{X_{i1}X_{ip}}}\\\\ \n",
    "\\sigma_{{X_{i2}X_{i1}}} &\\sigma^2_{{X_{i2}}}&  \\cdots & \\sigma_{{X_{i2}X_{ip}}}\\\\\n",
    "\\vdots &\\vdots&  \\ddots & \\vdots\\\\\n",
    "\\sigma_{{X_{ip}X_{i1}}} & \\sigma_{{X_{ip}X_{i2}}} &   \\cdots & \\sigma^2_{{X_{ip}}}\\\\ \\end{array}\\right]\\right)$$\n",
    "\n",
    "# Can't estimate the covariance matrix above when $p>n$... so instead we'll use:\n",
    "\n",
    "# $${\\boldsymbol X}_i = \\left(\\begin{array}{c} {X_{i1}} \\\\ {X_{i2}} \\\\ \\vdots\\\\ {X_{ip}} \\end{array} \\right) \\sim MVN\\left(\n",
    " \\left[\\begin{array}{c} \\mu_{{X_{i1}}} \\\\ \\mu_{{X_{i2}}} \\\\ \\vdots\\\\ \\mu_{{X_{ip}}} \\end{array} \\right]_,\n",
    "\\left[\\begin{array}{cccc}\\sigma^2_{{X_1}}&\\;\\;\\;0\\;\\;\\;& \\cdots & \\;\\;\\;0\\;\\;\\;\\\\ \n",
    "0&\\sigma^2_{{X_2}}&  \\cdots & 0\\\\\n",
    "\\vdots &\\vdots&  \\ddots & \\vdots\\\\\n",
    "\\;\\;\\;\\;0\\;\\;\\;\\; & 0 &   \\cdots & \\sigma^2_{{X_p}}  \\\\ \\end{array}\\right]\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stuff/nb.tiff\",width=1000px, align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"stuff/mix2b.png\",width=600px, align='center'></td>\n",
    "<td><img src=\"stuff/mix2.png\",width=600px, align='center'></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highly correlated features will ruin classification\n",
    "# Probability estimates are not particularly reliable\n",
    "# Less naive methodologies will outperform NB\n",
    "# But NB is FAST and can give acceptable results\n",
    "# Especially in HUGE data sets where it's the only way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Austin: NLP\n",
    "# \n",
    "# onsite info collected\n",
    "\n",
    "# Noah: Product Hunt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
