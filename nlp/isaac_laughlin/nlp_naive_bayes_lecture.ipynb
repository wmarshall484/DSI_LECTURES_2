{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Natural Language Processing\n",
    "\n",
    "* NLP for short.\n",
    "* Large field devoted to using computation to understand text data.\n",
    " * Information Retrieval\n",
    " * Document classification\n",
    " * Machine translation\n",
    " * Semantic Orientation\n",
    " \n",
    "Huge, complex field.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Terminology\n",
    "\n",
    "* _Corpus_: a dataset of text. e.g. Newspaper Articles, tweets, etc.\n",
    "* _Document_: A single entry from our corpus. e.g. Sentence, Tweet, Article, Complete Works of Shakespeare etc. \n",
    "* _Vocabulary_: All the words that appear in our corpus.\n",
    "* _Bag of Words_: A vector representation of a document based on word counts.\n",
    "* _Stop Words_: Words we ignore in our analysis that are too common to be useful.\n",
    "* _Token_: A single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'giving': 0,\n",
       " u'going': 1,\n",
       " u'is': 2,\n",
       " u'isaac': 3,\n",
       " u'lecture': 4,\n",
       " u'rafting': 5,\n",
       " u'whitewater': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#A really tiny example example.\n",
    "corpus = ['Isaac is giving a lecture', 'Isaac is going whitewater rafting']\n",
    "c = CountVectorizer()\n",
    "c = c.fit(corpus) #What happens here?\n",
    "c.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#TF-IDF\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency. The name is very descriptive:\n",
    "$$ tfidf(t, d) = tf(t, d) * idf(t, D) $$\n",
    "\n",
    "For some term $t$ some document $d$ and some corpus of documents $D$. This is pretty memorable, but what do it's parts mean?\n",
    "\n",
    "$$ tf(t, d) =freq(t, d) $$ \n",
    "\n",
    "Where $|d|$ is the number of terms in the document.\n",
    "\n",
    "$$ idf(t, d) = log\\frac{N}{|\\{d \\in D : t \\in d\\}|} $$\n",
    "\n",
    "Which is the log total number of documents in the corpus, divided by the number of documents in which the term appears--_the inverse document frequency_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Back to example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = c.transform(corpus).todense()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we get the idf from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 2, 2, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency = np.sum(d > 0, axis=0)\n",
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = d.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#IDF\n",
    "\n",
    "IDF is calculated _almost_ like you expect, but with a few tweaks to deal with the possible presence of zeros in our data.\n",
    "\n",
    "If it's possible we'll want to calculate idf for new terms that don't appear in our corpus, we add one two prevent division by zero in the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.40546511,  1.40546511,  1.        ,  1.        ,  1.40546511,\n",
       "          1.40546511,  1.40546511]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(float(N+1) /(1.+document_frequency)) + 1\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#TFIDF (at last)\n",
    "\n",
    "There are variety of competing normalization strategies for tfidf. This choice could change your results, and so is something you should be aware of. Most will probably have the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.40546511,  0.        ,  1.        ,  1.        ,  1.40546511,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  1.40546511,  1.        ,  1.        ,  0.        ,\n",
       "          1.40546511,  1.40546511]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf  = np.multiply(d, idf)\n",
    "#tfidf = normalize(tfidf, norm='l2')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or... more easily..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.40546511,  0.        ,  1.        ,  1.        ,  1.40546511,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  1.40546511,  1.        ,  1.        ,  0.        ,\n",
       "          1.40546511,  1.40546511]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = TfidfTransformer(norm=None)\n",
    "z.fit_transform(d).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Cosine similarity\n",
    "##Or, what to do with these vectors\n",
    "\n",
    "Cosine similarity is a way of measuring the similarity between vectors.\n",
    "\n",
    "$$ a \\cdot b = ||a||\\,||b||\\,cos(\\theta) $$\n",
    "\n",
    "We can rearrange the definition of the definition of the dot product above, to get \n",
    "\n",
    "$$ cos(\\theta) = \\frac{a \\cdot b}{||a||\\,||b||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Isaac is giving a lecture',\n",
       " 'Isaac is going whitewater rafting',\n",
       " 'Isaac is giving a lecture on whitewater rafting']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = corpus + ['Isaac is giving a lecture on whitewater rafting']\n",
    "new_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.29121942,  0.77523967],\n",
       "       [ 0.29121942,  1.        ,  0.67172541],\n",
       "       [ 0.77523967,  0.67172541,  1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = z.transform(c.transform(new_document))\n",
    "cosine_similarity(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#How do we evaluate the quality of our method?\n",
    "\n",
    "It's not supervised!\n",
    "\n",
    "* Compare agaist something else.\n",
    "* Design an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#What else is in NLP?\n",
    "\n",
    "1. POS-Tagging\n",
    "2. word2vec\n",
    "3. Stemming e.g. hosting/host, lying/lie etc.\n",
    "4. chunking\n",
    "5. grammars\n",
    "6. semantic orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Naive Bayes\n",
    "\n",
    "Naive Bayes is an extremely simple but, still amazingly effective machine learning technique.\n",
    "\n",
    "##Where/When?\n",
    "1. n << p\n",
    "2. n small\n",
    "3. n large\n",
    "4. streams of input data (online learning)\n",
    "5. multi-class\n",
    "6. low memory applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Bayes\n",
    "\n",
    "$$ P(C|X) = \\frac{P(X|C)P(C)}{P(X)} $$\n",
    "\n",
    "For $C$ a class, and X some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#The algorithm.\n",
    "\n",
    "$$ P(c|X) \\propto P(c) \\times P(x_1|c) \\times P(x_2|c) \\times ... \\times P(x_p|c) $$ \n",
    "\n",
    "Cool, looks good, lets just go ahead and implement that...\n",
    "\n",
    "![](https://mathspig.files.wordpress.com/2011/01/6-angry-teacher.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Naivete\n",
    "\n",
    "$$ P(X|c) \\propto \\prod_i P(x_i | c) \\iff  P(x_i|c, x_j) = P(x_i|c)\\,\\forall\\, i, j $$\n",
    "\n",
    "So our model assumes this is true, but think about how ridiculous this is in practice.\n",
    "\n",
    "$$ P(\\text{'ball' appears in article}\\,|\\, \\text{article is about sports}) = P(\\text{'ball' appears in article}\\,|\\,\\text{article is about sports and 'soccer' appears in article})$$\n",
    "\n",
    "##But, if it works...\n",
    "\n",
    "Then fine, but don't go claiming the number that results is _actually_ a probability or anything.\n",
    "\n",
    "<img src='https://therealryanbell.files.wordpress.com/2013/12/pouting-child.jpg' width=400px />\n",
    "\n",
    "In practice, relative rank of probabilities will be preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Laplace smoothing\n",
    "\n",
    "What if $P(x_i|c) = 0$? This means that we see no instances of a feature for a given class in our training data. What consequence does this have for prediction?\n",
    "\n",
    "So what can we do?\n",
    "\n",
    "$$ P(x_i | c) = \\frac{count_c(x_i) + \\alpha}{count_c(X) + \\alpha \\cdot count_C(X))} $$\n",
    "\n",
    "Add some $\\alpha$, often $\\alpha=1$, to prevent our probabilities from going to all the way to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MLE Estimation\n",
    "\n",
    "$$ P(X|c) \\propto P(c)\\prod_i P(x_i | c) $$\n",
    "\n",
    "So our bayes classifier would choose:\n",
    "\n",
    "$$ argmax_c{P(c)\\prod_i P(x_i | c)} $$\n",
    "\n",
    "But in a high dimensional problems like text, $P(x_i|c)$ is likely to be small, and the product _very_ small which can result in underflow.\n",
    "\n",
    "So how about maximizing log?\n",
    "\n",
    "$$ argmax_c{P(c)\\prod_i P(x_i | c)} = argmax_c{\\log P(c)\\prod_i P(x_i | c)} = argmax_c \\log P(c) + \\sum_i \\log P(x_i | c) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Types of features for Naive Bayes\n",
    "\n",
    "Easy to see how we would compute this for categorical features $X$. What about numerical features?\n",
    "\n",
    "Assume some distribution on those features, and then use the pdf of that distribution for $P(x_i|c)$. Normal is a pretty popular choice (especially if you normalize your features).\n",
    "\n",
    "$$ P(x_i | c) \\sim N(\\mu_c, \\sigma_c) $$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
