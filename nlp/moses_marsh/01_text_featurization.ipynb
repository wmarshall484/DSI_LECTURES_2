{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text featurization: turning a bunch of words into a vector of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "- think about how to featurize text\n",
    "- make a dumb count vector\n",
    "- make a text processing pipeline\n",
    " - split documents into words or \"tokens\"\n",
    " - drop \"filler\" or \"stop\" words\n",
    " - drop punctuation, capitalization\n",
    "- make a word frequency vector\n",
    "- make a word frequency / document frequency (TF-IDF) vector\n",
    "- compare documents with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "- So far we've been handed a lot of data that looks like big tables of numbers: many features and a target. All the machine learning techniques we've learned so far treat each data point as a *vector in feature space*: every data point has some numerical extension along each feature axis. And each algorithm looks for some pattern in that feature space, minimizing some loss function. \n",
    "- BUT WHAT ABOUT TEXT. Text is made of words, letters, strange characters like üêò... a poem is not a vector... or is it?\n",
    "- Let's imagine we have many texts (for example, email subject lines) and some corresponding labels (for example, spam or not spam). How can we turn each **text** into a **vector of features** about that text (for use in a machine learning algorithm)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some common NLP tasks\n",
    "- Information retrieval: How do you find a document or a particular fact within a document?\n",
    "- Document classification: What category of documents does the text belong to?\n",
    "- Machine translation: How do you write an English phrase in Chinese?\n",
    "- Sentiment analysis: Was a product review positive or negative? \n",
    "\n",
    "Natural language processing is a huge field and we will just touch on some of the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some terminology\n",
    "- **token**: the smallest meaningful unit of text. Usually, \"token\" means \"word\".\n",
    "- **document**: a group of words (tokens). Depending on the problem at hand, this could be a tweet, or a sentence, or a paragraph, or an article.\n",
    "- **corpus**: a set of documents. \n",
    "- **vocabulary**: the set of unique words that appear in the corpus.\n",
    "- **bag of words**: today we are ignoring word order; we simply treat each document as a collection of words. This is called a **bag of words** approach. It is surprisingly effective for how dumb it is. \n",
    "- **n-grams**: a sequence of **n** words in a row, to be treated as a single token. For example, the phrase \"What a sad clown painting\" contains the 2-grams \"what a\", \"a sad\", \"sad clown\" and \"clown painting\". Including 2-, 3-, and 4-gram tokenization in your pipeline below can begin to account for signal encoded in word order via multi-word phrases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST TRY: Word Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus: these three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"Jeff stole my octopus sandwich.\", \n",
    "    \"'My hammy!' I sobbed, sandwichlessly.\", \n",
    "    \"'Drop the sandwiches!' said the sandwich police.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the simplest possible approach: a vector for a document (sentence) should encode the word count in that document for each word in the corpus vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's split the documents into words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jeff', 'stole', 'my', 'octopus', 'sandwich.'],\n",
       " [\"'My\", \"hammy!'\", 'I', 'sobbed,', 'sandwichlessly.'],\n",
       " [\"'Drop\", 'the', \"sandwiches!'\", 'said', 'the', 'sandwich', 'police.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = [doc.split() for doc in corpus]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "for doc in tokenized_documents:\n",
    "    vocab_set.update(set(doc))\n",
    "vocabulary = sorted(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Drop\",\n",
       " \"'My\",\n",
       " 'I',\n",
       " 'Jeff',\n",
       " \"hammy!'\",\n",
       " 'my',\n",
       " 'octopus',\n",
       " 'police.',\n",
       " 'said',\n",
       " 'sandwich',\n",
       " 'sandwich.',\n",
       " \"sandwiches!'\",\n",
       " 'sandwichlessly.',\n",
       " 'sobbed,',\n",
       " 'stole',\n",
       " 'the']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a word count vector for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(len(corpus), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(tokenized_documents):\n",
    "    for word in doc:\n",
    "        j = vocabulary.index(word)\n",
    "        X[i,j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'Drop</th>\n",
       "      <th>'My</th>\n",
       "      <th>I</th>\n",
       "      <th>Jeff</th>\n",
       "      <th>hammy!'</th>\n",
       "      <th>my</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police.</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwich.</th>\n",
       "      <th>sandwiches!'</th>\n",
       "      <th>sandwichlessly.</th>\n",
       "      <th>sobbed,</th>\n",
       "      <th>stole</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   'Drop  'My    I  Jeff  hammy!'   my  octopus  police.  said  sandwich  \\\n",
       "0    0.0  0.0  0.0   1.0      0.0  1.0      1.0      0.0   0.0       0.0   \n",
       "1    0.0  1.0  1.0   0.0      1.0  0.0      0.0      0.0   0.0       0.0   \n",
       "2    1.0  0.0  0.0   0.0      0.0  0.0      0.0      1.0   1.0       1.0   \n",
       "\n",
       "   sandwich.  sandwiches!'  sandwichlessly.  sobbed,  stole  the  \n",
       "0        1.0           0.0              0.0      0.0    1.0  0.0  \n",
       "1        0.0           0.0              1.0      1.0    0.0  0.0  \n",
       "2        0.0           1.0              0.0      0.0    0.0  2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=X, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anything unsatisfactory about the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a better text processing pipeline:\n",
    "- split each document into tokens\n",
    "- lowercase & drop punctuation\n",
    "- drop **stop words** (filler, like \"the\")\n",
    "- reduce words to their root form using a huge dictionary of rules that many generations of linguists have compiled\n",
    " - **stemming**: simple, context-free rules like turning *sandwiches* into *sandwich*\n",
    " - **lemmatization**: edge cases and part-of-speech based rules like turning *better* into *good*\n",
    "- then turn your corpus of cleaned, tokenized documents into an array of feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/moses/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/moses/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/moses/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: split each document into a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jeff', 'stole', 'my', 'octopus', 'sandwich.'],\n",
       " [\"'My\", \"hammy!'\", 'I', 'sobbed,', 'sandwichlessly.'],\n",
       " [\"'Drop\", 'the', \"sandwiches!'\", 'said', 'the', 'sandwich', 'police.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [doc.split() for doc in corpus]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: lowercase, lose punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs_lowered = [[word.lower() for word in doc]\n",
    "                          for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'my', 'octopus', 'sandwich.'],\n",
       " [\"'my\", \"hammy!'\", 'i', 'sobbed,', 'sandwichlessly.'],\n",
       " [\"'drop\", 'the', \"sandwiches!'\", 'said', 'the', 'sandwich', 'police.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs_lowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercise: get rid of all punctuation in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'mr.peanut!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(word, symbol_set):\n",
    "    return ''.join([char for char in word if char not in symbol_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                for doc in tokenized_docs_lowered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
       " ['my', 'hammy', 'i', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'does', 'those', 'be', 'nor', 'if', 'a', 'an', 'below', 'did', 'are', 'about', \"it's\", \"doesn't\", \"you're\", 'of', 'yourselves', 'between', 'but', 'don', 'her', 'themselves', 'and', 'against', 'because', 'm', 'off', 'out', 'when', 'had', 'mustn', 'again', 'or', \"should've\", 'how', 'more', 'as', 'while', 'up', 'these', 'any', 'can', \"you'd\", 'under', 'couldn', 'we', 'whom', 'doesn', 'during', 'itself', 'for', 'myself', 'they', 'both', 'should', 'what', \"needn't\", 'he', 'have', 'where', 'your', 'him', 'to', 'aren', \"weren't\", 'hers', 'me', 'why', 'all', \"didn't\", 'down', 'their', 'mightn', 'i', \"shouldn't\", 've', 'by', 'only', 'do', 'being', \"hadn't\", \"hasn't\", 'too', 'here', 'ourselves', 'the', 'than', 'which', 'isn', 't', 'didn', 'wasn', 'having', 'has', \"she's\", 'on', 'now', 'haven', 'there', 'once', 'through', \"won't\", 'his', 'my', 'own', 'were', 'after', 'y', \"isn't\", 'each', \"wouldn't\", 'most', 'she', 'just', 'is', 'yours', 'some', 'o', 'ma', 'will', 'who', 'll', 'herself', 'above', 'further', \"shan't\", 'shouldn', \"you'll\", 'needn', 'that', 'am', 'until', 'this', 's', 'wouldn', 'same', \"you've\", 'yourself', \"wasn't\", 'its', 'you', 'no', 'ain', 'ours', \"haven't\", 'such', 'doing', \"mightn't\", \"aren't\", 'other', 'been', 'then', 'at', 'hasn', 'so', 'over', \"that'll\", 'shan', 'won', 'into', 'from', 'hadn', 'it', 'with', 'theirs', 'not', 'was', 'himself', 'them', \"don't\", 'weren', 're', 'd', 'before', 'few', 'our', \"couldn't\", 'very', \"mustn't\", 'in'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_no_stops = [[word for word in doc if word not in stop_words] \n",
    "                 for doc in cleaned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['hammy', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwiches', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: stemming / lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sandwich'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer.lemmatize('sandwiches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sandwich'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"sandwiches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['hammy', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lemmatized = [[lemmer.lemmatize(word) for word in doc]\n",
    "                  for doc in docs_no_stops]\n",
    "docs_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['hammi', 'sob', 'sandwichless'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'polic']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_stemmed = [[stemmer.stem(word) for word in doc]\n",
    "                  for doc in docs_no_stops]\n",
    "docs_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can combine all these steps into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_text_pipeline(doc, stops={}, lemmatize=False):\n",
    "    '''\n",
    "    Args:\n",
    "        doc (str): the text to be tokenized\n",
    "        stops (set): an optional set of words (tokens) to exclude\n",
    "        lemmatize (bool): if True, lemmatize the words\n",
    "    \n",
    "    Returns: \n",
    "        tokens (list of strings)\n",
    "    '''\n",
    "    doc = doc.lower().split()\n",
    "    punct = set(string.punctuation)\n",
    "    tokens = [''.join([char for char in tok if char not in punct]) \n",
    "              for tok in doc]\n",
    "    if stops:\n",
    "        tokens = [tok for tok in tokens if (tok not in stops)]\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['hammy', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[our_text_pipeline(doc, stops=stop_words, lemmatize=True) \n",
    " for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` has a `word_tokenize` function that's a bit more complicated than `doc.split()`, feel free to incorporate it into your pipeline. See the [documentation](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeff', 'stole', 'my', 'octopus', 'sandwich', '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the work above cleaned up our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jeff stole my octopus sandwich.',\n",
       " \"'My hammy!' I sobbed, sandwichlessly.\",\n",
       " \"'Drop the sandwiches!' said the sandwich police.\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeff', 'stole', 'octopus', 'sandwich'],\n",
       " ['hammy', 'sobbed', 'sandwichlessly'],\n",
       " ['drop', 'sandwich', 'said', 'sandwich', 'police']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_cleaned = [our_text_pipeline(doc, stops=stop_words, lemmatize=True) \n",
    "              for doc in corpus]\n",
    "docs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn our corpus into vectors of word counts. Let's put that (inefficient) counting code from earlier into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_count_vectorizer(docs):\n",
    "    '''\n",
    "    Args:\n",
    "        docs (list of lists of strings): corpus\n",
    "    Returns:\n",
    "        X_count (numpy array): count vectors\n",
    "        vocab (list of strings): alphabetical list \n",
    "                                 of unique words\n",
    "    '''\n",
    "    vocab_set = set()\n",
    "    for doc in docs:\n",
    "        vocab_set.update(doc)\n",
    "\n",
    "    vocab = sorted(vocab_set)\n",
    "\n",
    "    X_count = np.zeros(shape=(len(docs), len(vocab)))\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc:\n",
    "            j = vocab.index(word)\n",
    "            X_count[i,j] += 1\n",
    "    return X_count, vocab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count, vocab = our_count_vectorizer(docs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drop',\n",
       " 'hammy',\n",
       " 'jeff',\n",
       " 'octopus',\n",
       " 'police',\n",
       " 'said',\n",
       " 'sandwich',\n",
       " 'sandwichlessly',\n",
       " 'sobbed',\n",
       " 'stole']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 2., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop</th>\n",
       "      <th>hammy</th>\n",
       "      <th>jeff</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwichlessly</th>\n",
       "      <th>sobbed</th>\n",
       "      <th>stole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drop  hammy  jeff  octopus  police  said  sandwich  sandwichlessly  sobbed  \\\n",
       "0   0.0    0.0   1.0      1.0     0.0   0.0       1.0             0.0     0.0   \n",
       "1   0.0    1.0   0.0      0.0     0.0   0.0       0.0             1.0     1.0   \n",
       "2   1.0    0.0   0.0      0.0     1.0   1.0       2.0             0.0     0.0   \n",
       "\n",
       "   stole  \n",
       "0    1.0  \n",
       "1    0.0  \n",
       "2    0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors = pd.DataFrame(data=X_count, columns=vocab)\n",
    "count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequency (TF) vectors\n",
    "If you are comparing documents of different lengths, the *frequency* of a word in a document may be more useful than the raw count.\n",
    "\n",
    "\n",
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 2., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [3.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.25      , 0.25      , 0.        ,\n",
       "        0.        , 0.25      , 0.        , 0.        , 0.25      ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33333333, 0.33333333, 0.        ],\n",
       "       [0.2       , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.2       , 0.4       , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf = X_count / X_count.sum(axis=1, keepdims=True)\n",
    "X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop</th>\n",
       "      <th>hammy</th>\n",
       "      <th>jeff</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwichlessly</th>\n",
       "      <th>sobbed</th>\n",
       "      <th>stole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drop  hammy  jeff  octopus  police  said  sandwich  sandwichlessly  sobbed  \\\n",
       "0   0.0   0.00  0.25     0.25     0.0   0.0      0.25            0.00    0.00   \n",
       "1   0.0   0.33  0.00     0.00     0.0   0.0      0.00            0.33    0.33   \n",
       "2   0.2   0.00  0.00     0.00     0.2   0.2      0.40            0.00    0.00   \n",
       "\n",
       "   stole  \n",
       "0   0.25  \n",
       "1   0.00  \n",
       "2   0.00  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors = pd.DataFrame(data=X_tf,\n",
    "                          columns=vocab)\n",
    "tf_vectors.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency $*$ Inverse Document Frequency (TF-IDF) Vectors\n",
    "\n",
    "In many NLP tasks related to information retrieval or document classification, rare words may be especially helpful in categorizing documents. For example, if I'm trying to pick out the music articles from a database of ~1 million articles, the word _arpeggiate_, though rare, probably exclusively appears in music texts. Conversely, words like _the_ appear in every article, and have no discriminatory power.\n",
    "\n",
    "To quantify this idea, we first calculate the _document frequency_ of each word in the vocabulary\n",
    "$$ DF_{word} = \\frac{\\#\\_of\\_documents\\_containing\\_word}{total\\_\\#\\_of\\_documents} $$\n",
    "\n",
    "Then we give that word a logarithmically scaled _inverse document frequency_ weight\n",
    "$$ IDF_{word} = \\log\\left(\\frac{1}{DF_{word}}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply each word's term frequency (TF) in a document by its IDF weight to get a TF-IDF vector\n",
    "$$TFIDF_{word,document} = TF_{word, document}*IDF_{word}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop              0.33\n",
       "hammy             0.33\n",
       "jeff              0.33\n",
       "octopus           0.33\n",
       "police            0.33\n",
       "said              0.33\n",
       "sandwich          0.67\n",
       "sandwichlessly    0.33\n",
       "sobbed            0.33\n",
       "stole             0.33\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = np.zeros(shape=(len(vocab),))\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    for doc in docs_cleaned:\n",
    "        if word in doc:\n",
    "            doc_count[i] += 1\n",
    "\n",
    "doc_freq = doc_count/len(corpus)\n",
    "            \n",
    "doc_freq_series = pd.Series(data=doc_freq, index=vocab)\n",
    "doc_freq_series.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop              1.098612\n",
       "hammy             1.098612\n",
       "jeff              1.098612\n",
       "octopus           1.098612\n",
       "police            1.098612\n",
       "said              1.098612\n",
       "sandwich          0.405465\n",
       "sandwichlessly    1.098612\n",
       "sobbed            1.098612\n",
       "stole             1.098612\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_doc_freq = np.log(1/doc_freq_series)\n",
    "inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.25      , 0.25      , 0.        ,\n",
       "        0.        , 0.25      , 0.        , 0.        , 0.25      ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33333333, 0.33333333, 0.        ],\n",
       "       [0.2       , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.2       , 0.4       , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09861229, 1.09861229, 1.09861229, 1.09861229, 1.09861229,\n",
       "       1.09861229, 0.40546511, 1.09861229, 1.09861229, 1.09861229])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log(1/doc_freq)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.27465307, 0.27465307, 0.        ,\n",
       "        0.        , 0.10136628, 0.        , 0.        , 0.27465307],\n",
       "       [0.        , 0.3662041 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.3662041 , 0.3662041 , 0.        ],\n",
       "       [0.21972246, 0.        , 0.        , 0.        , 0.21972246,\n",
       "        0.21972246, 0.16218604, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop</th>\n",
       "      <th>hammy</th>\n",
       "      <th>jeff</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwichlessly</th>\n",
       "      <th>sobbed</th>\n",
       "      <th>stole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       drop     hammy      jeff   octopus    police      said  sandwich  \\\n",
       "0  0.000000  0.000000  0.274653  0.274653  0.000000  0.000000  0.101366   \n",
       "1  0.000000  0.366204  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.219722  0.000000  0.000000  0.000000  0.219722  0.219722  0.162186   \n",
       "\n",
       "   sandwichlessly    sobbed     stole  \n",
       "0        0.000000  0.000000  0.274653  \n",
       "1        0.366204  0.366204  0.000000  \n",
       "2        0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = pd.DataFrame(data=X_tf*idf, columns=vocab)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I've explicitly done the calculations using numpy, but we could have used pandas instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop</th>\n",
       "      <th>hammy</th>\n",
       "      <th>jeff</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwichlessly</th>\n",
       "      <th>sobbed</th>\n",
       "      <th>stole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drop  hammy  jeff  octopus  police  said  sandwich  sandwichlessly  sobbed  \\\n",
       "0   0.0   0.00  0.25     0.25     0.0   0.0      0.25            0.00    0.00   \n",
       "1   0.0   0.33  0.00     0.00     0.0   0.0      0.00            0.33    0.33   \n",
       "2   0.2   0.00  0.00     0.00     0.2   0.2      0.40            0.00    0.00   \n",
       "\n",
       "   stole  \n",
       "0   0.25  \n",
       "1   0.00  \n",
       "2   0.00  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop              1.098612\n",
       "hammy             1.098612\n",
       "jeff              1.098612\n",
       "octopus           1.098612\n",
       "police            1.098612\n",
       "said              1.098612\n",
       "sandwich          0.405465\n",
       "sandwichlessly    1.098612\n",
       "sobbed            1.098612\n",
       "stole             1.098612\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drop</th>\n",
       "      <th>hammy</th>\n",
       "      <th>jeff</th>\n",
       "      <th>octopus</th>\n",
       "      <th>police</th>\n",
       "      <th>said</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>sandwichlessly</th>\n",
       "      <th>sobbed</th>\n",
       "      <th>stole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.274653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.366204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       drop     hammy      jeff   octopus    police      said  sandwich  \\\n",
       "0  0.000000  0.000000  0.274653  0.274653  0.000000  0.000000  0.101366   \n",
       "1  0.000000  0.366204  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.219722  0.000000  0.000000  0.000000  0.219722  0.219722  0.162186   \n",
       "\n",
       "   sandwichlessly    sobbed     stole  \n",
       "0        0.000000  0.000000  0.274653  \n",
       "1        0.366204  0.366204  0.000000  \n",
       "2        0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors*inverse_doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we've featurized documents based on word count & frequency within a document, and using document frequency... now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have turned our TEXT DOCUMENTS into VECTORS, we can put them into whatever machine learning algorithm we want! We can use whatever kind of similarity measures we please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pop quiz: \n",
    "I'm trying to use logistic regression to tell spam e-mail from non-spam e-mails. How do I pick which text featurization / vectorization to use? What are my options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing document vectors using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: comparing count vectors of short articles & long articles (see whiteboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any two non-collinear vectors $\\vec{u}$ and $\\vec{v}$ form a plane (a 2D space), no matter how many dimensions $\\vec{u}$ and $\\vec{v}$ have. Let $\\theta$ be the angle between the vectors in that plane. The **cosine similarity** between $\\vec{u}$ and $\\vec{v}$ is defined as\n",
    "$$ \\text{cosine_sim}(\\vec{u},\\vec{v}) = \\cos{\\theta} =\\frac{\\vec{u}\\cdot\\vec{v}}{|\\vec{u}||\\vec{v}|} $$\n",
    "where the numerator is the **dot product** between the vectors and the denominator is the product of the vector **magnitudes** (L2 norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: a villanelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The highly structured villanelle is a nineteen-line poem with two repeating rhymes and two refrains. The form is made up of five tercets followed by a quatrain. The first and third lines of the opening tercet are repeated alternately in the last lines of the succeeding stanzas; then in the final stanza, the refrain serves as the poem‚Äôs two concluding lines. Using capitals for the refrains and lowercase letters for the rhymes, the form could be expressed as: A1 b A2 / a b A1 / a b A2 / a b A1 / a b A2 / a b A1 A2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the best known villanelle in English is \"Do not go gentle into that good night\" by Dylan Thomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_poem = '''Do not go gentle into that good night,\n",
    "Old age should burn and rave at close of day;\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "Though wise men at their end know dark is right,\n",
    "Because their words had forked no lightning they\n",
    "Do not go gentle into that good night.\n",
    "\n",
    "Good men, the last wave by, crying how bright\n",
    "Their frail deeds might have danced in a green bay,\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "Wild men who caught and sang the sun in flight,\n",
    "And learn, too late, they grieved it on its way,\n",
    "Do not go gentle into that good night.\n",
    "\n",
    "Grave men, near death, who see with blinding sight\n",
    "Blind eyes could blaze like meteors and be gay,\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "And you, my father, there on the sad height,\n",
    "Curse, bless, me now with your fierce tears, I pray.\n",
    "Do not go gentle into that good night.\n",
    "Rage, rage against the dying of the light.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's treat each line as a document and the poem as our corpus, then vectorize this baby into a matrix that would make Dylan Thomas proud (sorry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not go gentle into that good night,',\n",
       " 'Old age should burn and rave at close of day;',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'Though wise men at their end know dark is right,',\n",
       " 'Because their words had forked no lightning they',\n",
       " 'Do not go gentle into that good night.',\n",
       " '',\n",
       " 'Good men, the last wave by, crying how bright',\n",
       " 'Their frail deeds might have danced in a green bay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'Wild men who caught and sang the sun in flight,',\n",
       " 'And learn, too late, they grieved it on its way,',\n",
       " 'Do not go gentle into that good night.',\n",
       " '',\n",
       " 'Grave men, near death, who see with blinding sight',\n",
       " 'Blind eyes could blaze like meteors and be gay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'And you, my father, there on the sad height,',\n",
       " 'Curse, bless, me now with your fierce tears, I pray.',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Rage, rage against the dying of the light.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = raw_poem.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not go gentle into that good night,',\n",
       " 'Old age should burn and rave at close of day;',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'Though wise men at their end know dark is right,',\n",
       " 'Because their words had forked no lightning they',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Good men, the last wave by, crying how bright',\n",
       " 'Their frail deeds might have danced in a green bay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'Wild men who caught and sang the sun in flight,',\n",
       " 'And learn, too late, they grieved it on its way,',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Grave men, near death, who see with blinding sight',\n",
       " 'Blind eyes could blaze like meteors and be gay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'And you, my father, there on the sad height,',\n",
       " 'Curse, bless, me now with your fierce tears, I pray.',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Rage, rage against the dying of the light.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing empty lines\n",
    "lines = [l for l in lines if l]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['do', 'not', 'go', 'gentle', 'into', 'that', 'good', 'night'],\n",
       " ['old', 'age', 'should', 'burn', 'and', 'rave', 'at', 'close', 'of', 'day'],\n",
       " ['rage', 'rage', 'against', 'the', 'dying', 'of', 'the', 'light'],\n",
       " ['though',\n",
       "  'wise',\n",
       "  'men',\n",
       "  'at',\n",
       "  'their',\n",
       "  'end',\n",
       "  'know',\n",
       "  'dark',\n",
       "  'is',\n",
       "  'right'],\n",
       " ['because', 'their', 'words', 'had', 'forked', 'no', 'lightning', 'they'],\n",
       " ['do', 'not', 'go', 'gentle', 'into', 'that', 'good', 'night'],\n",
       " ['good', 'men', 'the', 'last', 'wave', 'by', 'crying', 'how', 'bright'],\n",
       " ['their',\n",
       "  'frail',\n",
       "  'deeds',\n",
       "  'might',\n",
       "  'have',\n",
       "  'danced',\n",
       "  'in',\n",
       "  'a',\n",
       "  'green',\n",
       "  'bay'],\n",
       " ['rage', 'rage', 'against', 'the', 'dying', 'of', 'the', 'light'],\n",
       " ['wild', 'men', 'who', 'caught', 'and', 'sang', 'the', 'sun', 'in', 'flight'],\n",
       " ['and', 'learn', 'too', 'late', 'they', 'grieved', 'it', 'on', 'its', 'way'],\n",
       " ['do', 'not', 'go', 'gentle', 'into', 'that', 'good', 'night'],\n",
       " ['grave', 'men', 'near', 'death', 'who', 'see', 'with', 'blinding', 'sight'],\n",
       " ['blind', 'eyes', 'could', 'blaze', 'like', 'meteors', 'and', 'be', 'gay'],\n",
       " ['rage', 'rage', 'against', 'the', 'dying', 'of', 'the', 'light'],\n",
       " ['and', 'you', 'my', 'father', 'there', 'on', 'the', 'sad', 'height'],\n",
       " ['curse',\n",
       "  'bless',\n",
       "  'me',\n",
       "  'now',\n",
       "  'with',\n",
       "  'your',\n",
       "  'fierce',\n",
       "  'tears',\n",
       "  'i',\n",
       "  'pray'],\n",
       " ['do', 'not', 'go', 'gentle', 'into', 'that', 'good', 'night'],\n",
       " ['rage', 'rage', 'against', 'the', 'dying', 'of', 'the', 'light']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem = [our_text_pipeline(line) for line in  lines]\n",
    "poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_vec, poem_vocab = our_count_vectorizer(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'against', 'age', 'and', 'at', 'bay', 'be', 'because', 'blaze', 'bless', 'blind', 'blinding', 'bright', 'burn', 'by', 'caught', 'close', 'could', 'crying', 'curse', 'danced', 'dark', 'day', 'death', 'deeds', 'do', 'dying', 'end', 'eyes', 'father', 'fierce', 'flight', 'forked', 'frail', 'gay', 'gentle', 'go', 'good', 'grave', 'green', 'grieved', 'had', 'have', 'height', 'how', 'i', 'in', 'into', 'is', 'it', 'its', 'know', 'last', 'late', 'learn', 'light', 'lightning', 'like', 'me', 'men', 'meteors', 'might', 'my', 'near', 'night', 'no', 'not', 'now', 'of', 'old', 'on', 'pray', 'rage', 'rave', 'right', 'sad', 'sang', 'see', 'should', 'sight', 'sun', 'tears', 'that', 'the', 'their', 'there', 'they', 'though', 'too', 'wave', 'way', 'who', 'wild', 'wise', 'with', 'words', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(poem_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poem_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 98)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_dist = squareform(pdist(poem_vec, metric='cosine'))\n",
    "pairwise_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not go gentle into that good night,\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Good men, the last wave by, crying how bright\n",
      "-----\n",
      "Old age should burn and rave at close of day;\n",
      "And you, my father, there on the sad height,\n",
      "Blind eyes could blaze like meteors and be gay,\n",
      "Wild men who caught and sang the sun in flight,\n",
      "Though wise men at their end know dark is right,\n",
      "-----\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "And you, my father, there on the sad height,\n",
      "-----\n",
      "Though wise men at their end know dark is right,\n",
      "Because their words had forked no lightning they\n",
      "Good men, the last wave by, crying how bright\n",
      "Grave men, near death, who see with blinding sight\n",
      "Wild men who caught and sang the sun in flight,\n",
      "-----\n",
      "Because their words had forked no lightning they\n",
      "Though wise men at their end know dark is right,\n",
      "Their frail deeds might have danced in a green bay,\n",
      "And learn, too late, they grieved it on its way,\n",
      "Do not go gentle into that good night,\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lines[:5])):\n",
    "    for ind in pairwise_dist[i].argsort()[:5]:\n",
    "        print(lines[ind])\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## well all of this is implemented in sklearn, too. the exercise is about using it. here's a taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=our_text_pipeline, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_vec_sk = count_vectorizer.fit_transform(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19x57 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 79 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_vec_sk.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gentle': 26, 'good': 27, 'night': 41, 'old': 42, 'age': 0, 'burn': 7, 'rave': 45, 'close': 9, 'day': 14, 'rage': 44, 'dying': 17, 'light': 35, 'wise': 55, 'men': 38, 'end': 18, 'know': 32, 'dark': 13, 'right': 46, 'words': 56, 'forked': 23, 'lightning': 36, 'wave': 52, 'crying': 10, 'bright': 6, 'frail': 24, 'deeds': 16, 'danced': 12, 'green': 29, 'bay': 1, 'wild': 54, 'caught': 8, 'sang': 48, 'sun': 50, 'flight': 22, 'learn': 34, 'late': 33, 'grieved': 30, 'way': 53, 'grave': 28, 'near': 40, 'death': 15, 'blinding': 5, 'sight': 49, 'blind': 4, 'eyes': 19, 'blaze': 2, 'like': 37, 'meteors': 39, 'gay': 25, 'father': 20, 'sad': 47, 'height': 31, 'curse': 11, 'bless': 3, 'fierce': 21, 'tears': 51, 'pray': 43}\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'bay', 'blaze', 'bless', 'blind', 'blinding', 'bright', 'burn', 'caught', 'close', 'crying', 'curse', 'danced', 'dark', 'day', 'death', 'deeds', 'dying', 'end', 'eyes', 'father', 'fierce', 'flight', 'forked', 'frail', 'gay', 'gentle', 'good', 'grave', 'green', 'grieved', 'height', 'know', 'late', 'learn', 'light', 'lightning', 'like', 'men', 'meteors', 'near', 'night', 'old', 'pray', 'rage', 'rave', 'right', 'sad', 'sang', 'sight', 'sun', 'tears', 'wave', 'way', 'wild', 'wise', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "livereveal": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
