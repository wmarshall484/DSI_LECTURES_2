{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of these subject lines are from spam emails?\n",
    "\n",
    "- Need just a little help? Independent senior living might be the answer\n",
    "- Hard sql from today's interview\n",
    "- Beautiful Women, Discrete Service\n",
    "- Ever consider driving with Lyft? Apply here.\n",
    "- 3/2/18 All Hands Unanswered Questions\n",
    "- Sizwe Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tipped you off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a collection of labeled text documents, each one belonging to a category (for example, news articles & which section of the paper they're in). Now we get a new, unlabeled article. How do use what we've seen so far to predict which category it belongs to?\n",
    "\n",
    "- Do a bunch of complicated part of speech tagging & account for the sequence of words?\n",
    "- Make the dumbest \"bag of words\" assumption possible and hope it works?\n",
    "  - yep this is what we do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is an extremely simple amazingly effective machine learning technique.\n",
    "\n",
    "## When do we use it?\n",
    "1. n << p\n",
    "2. n small\n",
    "3. n large\n",
    "4. streams of input data (online learning)\n",
    "5. multi-class\n",
    "6. low memory applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it really?\n",
    "\n",
    "It is just [Maximum A Posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), with a fun approximation to make it easy to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation: a document $\\vec{x}$ is a list of words $\\{w_1, w_2, \\ldots, w_k\\}$, and we want the probability that it belongs to class $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: \n",
    "- e-mail: spam or not spam\n",
    "- news article: is it from the World sections or Sport or Arts, etc\n",
    "- essay from anonymous author: is the author really a famous person (whose corpus I have access to)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(y|\\vec{x}) = \\frac{P(\\vec{x}|y)P(y)}{P(\\vec{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the wacky part: let's assume the features (words) are totally independent of each other. Then we can write\n",
    "\n",
    "## $$P(\\vec{x}|y) = P(w_1|y)\\times P(w_2|y)\\times\\cdots\\times P(w_k|y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty brazenly naive assumption. It is assuming, for example, that the probability of seeing the word \"ball\" in an article about sports $P(ball|\\text{sports})$ is totally independent of the presence of the word \"soccer\" in the article.\n",
    "\n",
    "But if it works, it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a document $\\vec{x}$, we just need to calculate the posterior probabilities for each class $\\{y_j\\}$, then see which class has the highest posterior probability.\n",
    "\n",
    "## $$P(y_j|\\vec{x}) \\propto P(w_1|y_j)\\times P(w_2|y_j)\\times\\cdots\\times P(w_k|y_j)\\times P(y_j)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've dropped the denominator $P(\\vec{x})$ since it doesn't depend on $y_j$, and we're only interested in the $y_j$ that maximizes $P(y_j|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the prior, $P(y_j)$ ?\n",
    "\n",
    "The prior probability of class $y$ is simply how frequent that class is among our training documents:\n",
    "$$P(y_j) = \\frac{\\text{# of documents of class}\\, y_j}{\\text{total # of documents}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about these likelihoods? How do we calculate 'em?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a document $\\vec{x}$ of class $y_j$ is generated by drawing words from a bag (with replacement) with probabilities $P(w_i | y_j)$\n",
    "\n",
    "Let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word counts: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can write our posterior (which, remember, is the prior times the likelihood) as\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j)\\times\\prod_{i=1}^p P(w_i|y_j)^{x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{N_{ji}}{N_j} = \\frac{\\text{total count of w_i across all documents of class}\\, y_j}{\\text{total count of all words across all documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our new document contains a word we've never seen before? Then our estimate of $P(w|y)$ for that word would be zero, and then the entire product $P(y|\\vec{x})$ would be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, we employ *Laplace Smoothing*\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + N_{ji}}{\\alpha p + N_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, $\\alpha = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Bernoulli Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, now, that a document of class $y_j$ has a probability $P(w_i|y_j)$ of containing word $w_i$ at least once (regardless of word count).\n",
    "\n",
    "Again let our vocabulary (the set of unique words observed across the entire training corpus) be $p$ words long. Let's write $\\vec{x}$ as a p-dimensional vector of word *occurrences*: $x_i = 1$ if the $w_i$ is in that document at all, $x_i = 0$ if not: $\\vec{x} = [x_1, x_2, \\ldots, x_p]$\n",
    "\n",
    "Then we can think of our document as a series of Bernoulli trials, one for each word, and our posterior is\n",
    "### $$P(y_j|\\vec{x}) \\propto P(y_j) \\times\\prod_{i=1}^p P(w_i|y_j)^{x_i}(1 - P(w_i|y_j))^{1 - x_i} $$\n",
    "\n",
    "$$\\text{log}(P(y_j|\\vec{x})) \\propto \\text{log}(P(y_j)) + \\sum_{i=1}^p x_i \\text{log}(P(w_i|y_j)) + (1 - x_i)\\text{log}(1 - P(w_i|y_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate $P(w_i|y_j)$, we could just use \n",
    "$$\\frac{D_{ji}}{D_j} = \\frac{\\text{total # of documents of class}\\,y_j\\text{ containing}\\, w_i}{\\text{total number of documents of class}\\,y_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again we'd hit the \"unseen word\" problem. So our smoothing here looks like\n",
    "### $$ P(w_i | y_j) = \\frac{\\alpha + D_{ji}}{2\\alpha + D_j} $$\n",
    "\n",
    "Again with $\\alpha = 1$ usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "- Bernoulli tends to work better with shorter documents...\n",
    "- ...but sklearn says \"It is advisable to evaluate both models, if time permits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[sklearn User Guide: Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "[Spam Filtering with Naive Bayes â€“ Which Naive Bayes?](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=E870DFA148786F5A27F88CF80FB4D73B?doi=10.1.1.61.5542&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
