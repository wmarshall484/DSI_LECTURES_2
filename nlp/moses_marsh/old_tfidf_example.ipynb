{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus: list of documents\n",
    "\n",
    "```\n",
    "    ['This roller coaster is too rickety.', \n",
    "    'Roller coasters give me hives.', \n",
    "    'That bee hive is approaching at an alarming speed.']```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency\n",
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse document frequency\n",
    "$$ IDF_{word} = \\log\\left(\\frac{total\\_\\#\\_of\\_documents}{\\#\\_of\\_documents\\_containing\\_word}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['This roller coaster is too, too rickety.', \n",
    "    'Roller coasters give me hives.', \n",
    "    'That bee hive is approaching at an alarming speed.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_doc = [doc.lower().split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_doc = [word_tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def our_tokenizer(doc):\n",
    "    doc = word_tokenize(doc.lower())\n",
    "    return [tok for tok in doc if tok not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_docs = [our_tokenizer(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'roller', 'coaster', 'is', 'too', 'too', 'rickety'],\n",
       " ['roller', 'coasters', 'give', 'me', 'hives'],\n",
       " ['that', 'bee', 'hive', 'is', 'approaching', 'at', 'an', 'alarming', 'speed']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in tokenized_docs:\n",
    "    vocab.update(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'roller', 'coaster', 'is', 'too', 'too', 'rickety'],\n",
       " ['roller', 'coasters', 'give', 'me', 'hives'],\n",
       " ['that', 'bee', 'hive', 'is', 'approaching', 'at', 'an', 'alarming', 'speed']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alarming',\n",
       " 'an',\n",
       " 'approaching',\n",
       " 'at',\n",
       " 'bee',\n",
       " 'coaster',\n",
       " 'coasters',\n",
       " 'give',\n",
       " 'hive',\n",
       " 'hives',\n",
       " 'is',\n",
       " 'me',\n",
       " 'rickety',\n",
       " 'roller',\n",
       " 'speed',\n",
       " 'that',\n",
       " 'this',\n",
       " 'too']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'roller', 'coaster', 'is', 'too', 'too', 'rickety']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'coaster': 1,\n",
       "         'is': 1,\n",
       "         'rickety': 1,\n",
       "         'roller': 1,\n",
       "         'this': 1,\n",
       "         'too': 2})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_vec = np.zeros(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word, count in Counter(tokenized_docs[0]).iteritems():\n",
    "    ind = vocab.index(word)\n",
    "    doc_vec[ind] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "        1.,  0.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english')\n",
    "vector_matrix = vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'alarming',\n",
       " u'approaching',\n",
       " u'bee',\n",
       " u'coaster',\n",
       " u'coasters',\n",
       " u'hive',\n",
       " u'hives',\n",
       " u'rickety',\n",
       " u'roller',\n",
       " u'speed']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'roller', 'coaster', 'is', 'too', ',', 'too', 'rickety', '.'],\n",
       " ['Roller', 'coasters', 'give', 'me', 'hives', '.'],\n",
       " ['That',\n",
       "  'bee',\n",
       "  'hive',\n",
       "  'is',\n",
       "  'approaching',\n",
       "  'at',\n",
       "  'an',\n",
       "  'alarming',\n",
       "  'speed',\n",
       "  '.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(doc, lemmatizer=WordNetLemmatizer(), stopwords=None):\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [t.lower() for t in tokens if t not in string.punctuation]\n",
    "    if lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    if stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'roller', 'coaster', 'is', 'too', 'too', 'rickety'],\n",
       " ['roller', 'coasters', 'give', 'me', 'hives'],\n",
       " ['that', 'bee', 'hive', 'is', 'approaching', 'at', 'an', 'alarming', 'speed']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[my_tokenizer(doc, lemmatizer=None) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'roller', 'coaster', 'is', 'too', 'too', 'rickety'],\n",
       " ['roller', u'coaster', 'give', 'me', u'hive'],\n",
       " ['that', 'bee', 'hive', 'is', 'approaching', 'at', 'an', 'alarming', 'speed']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[my_tokenizer(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english', tokenizer=my_tokenizer)\n",
    "vector_matrix = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'alarming', u'approaching', u'bee', u'coaster', u'hive', u'rickety', u'roller', u'speed']\n",
      "---\n",
      "{u'coaster': 3, u'alarming': 0, u'rickety': 5, u'hive': 4, u'bee': 2, u'speed': 7, u'roller': 6, u'approaching': 1}\n"
     ]
    }
   ],
   "source": [
    "print vect.get_feature_names()\n",
    "print '---'\n",
    "print vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.680918560399\n",
      "  (0, 3)\t0.517856116168\n",
      "  (0, 6)\t0.517856116168\n",
      "  (1, 4)\t0.57735026919\n",
      "  (1, 3)\t0.57735026919\n",
      "  (1, 6)\t0.57735026919\n",
      "  (2, 7)\t0.467350981811\n",
      "  (2, 0)\t0.467350981811\n",
      "  (2, 1)\t0.467350981811\n",
      "  (2, 2)\t0.467350981811\n",
      "  (2, 4)\t0.35543246785\n"
     ]
    }
   ],
   "source": [
    "print vector_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.51785612  0.          0.68091856\n",
      "   0.51785612  0.        ]\n",
      " [ 0.          0.          0.          0.57735027  0.57735027  0.\n",
      "   0.57735027  0.        ]\n",
      " [ 0.46735098  0.46735098  0.46735098  0.          0.35543247  0.          0.\n",
      "   0.46735098]]\n"
     ]
    }
   ],
   "source": [
    "print vector_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.57735027,  0.57735027,\n",
       "        0.        ,  0.57735027,  0.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_matrix.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.40203126,  1.        ],\n",
       "       [ 0.40203126,  0.        ,  0.79479097],\n",
       "       [ 1.        ,  0.79479097,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squareform(pdist(vector_matrix.toarray(), metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF example: a villanelle\n",
    "\"The highly structured villanelle is a nineteen-line poem with two repeating rhymes and two refrains. The form is made up of five tercets followed by a quatrain. The first and third lines of the opening tercet are repeated alternately in the last lines of the succeeding stanzas; then in the final stanza, the refrain serves as the poemâ€™s two concluding lines. Using capitals for the refrains and lowercase letters for the rhymes, the form could be expressed as: A1 b A2 / a b A1 / a b A2 / a b A1 / a b A2 / a b A1 A2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_poem = '''Do not go gentle into that good night,\n",
    "Old age should burn and rave at close of day;\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "Though wise men at their end know dark is right,\n",
    "Because their words had forked no lightning they\n",
    "Do not go gentle into that good night.\n",
    "\n",
    "Good men, the last wave by, crying how bright\n",
    "Their frail deeds might have danced in a green bay,\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "Wild men who caught and sang the sun in flight,\n",
    "And learn, too late, they grieved it on its way,\n",
    "Do not go gentle into that good night.\n",
    "\n",
    "Grave men, near death, who see with blinding sight\n",
    "Blind eyes could blaze like meteors and be gay,\n",
    "Rage, rage against the dying of the light.\n",
    "\n",
    "And you, my father, there on the sad height,\n",
    "Curse, bless, me now with your fierce tears, I pray.\n",
    "Do not go gentle into that good night.\n",
    "Rage, rage against the dying of the light.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's treat each line as a document and the poem as our corpus, then vectorize this baby into a matrix that would make Dylan Thomas proud (sorry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not go gentle into that good night,',\n",
       " 'Old age should burn and rave at close of day;',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'Though wise men at their end know dark is right,',\n",
       " 'Because their words had forked no lightning they',\n",
       " 'Do not go gentle into that good night.',\n",
       " '',\n",
       " 'Good men, the last wave by, crying how bright',\n",
       " 'Their frail deeds might have danced in a green bay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'Wild men who caught and sang the sun in flight,',\n",
       " 'And learn, too late, they grieved it on its way,',\n",
       " 'Do not go gentle into that good night.',\n",
       " '',\n",
       " 'Grave men, near death, who see with blinding sight',\n",
       " 'Blind eyes could blaze like meteors and be gay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " '',\n",
       " 'And you, my father, there on the sad height,',\n",
       " 'Curse, bless, me now with your fierce tears, I pray.',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Rage, rage against the dying of the light.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = raw_poem.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not go gentle into that good night,',\n",
       " 'Old age should burn and rave at close of day;',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'Though wise men at their end know dark is right,',\n",
       " 'Because their words had forked no lightning they',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Good men, the last wave by, crying how bright',\n",
       " 'Their frail deeds might have danced in a green bay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'Wild men who caught and sang the sun in flight,',\n",
       " 'And learn, too late, they grieved it on its way,',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Grave men, near death, who see with blinding sight',\n",
       " 'Blind eyes could blaze like meteors and be gay,',\n",
       " 'Rage, rage against the dying of the light.',\n",
       " 'And you, my father, there on the sad height,',\n",
       " 'Curse, bless, me now with your fierce tears, I pray.',\n",
       " 'Do not go gentle into that good night.',\n",
       " 'Rage, rage against the dying of the light.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing empty lines\n",
    "lines = [l for l in lines if l]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = nltk.corpus.stopwords.words('english')\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['their', 'frail', 'deeds', 'might', 'have', 'danced', 'in', 'a', 'green', 'bay']\n"
     ]
    }
   ],
   "source": [
    "print my_tokenizer(lines[7], lemmatizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['their', 'frail', u'deed', 'might', 'have', 'danced', 'in', 'a', 'green', 'bay']\n"
     ]
    }
   ],
   "source": [
    "print my_tokenizer(lines[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frail', 'deeds', 'might', 'danced', 'green', 'bay']\n"
     ]
    }
   ],
   "source": [
    "print my_tokenizer(lines[7], lemmatizer=None, stopwords=set(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frail', u'deed', 'might', 'danced', 'green', 'bay']\n"
     ]
    }
   ],
   "source": [
    "print my_tokenizer(lines[7], stopwords=set(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_vectorizer(corpus, stopwords=None, lemmatizer=None, kind='count'):\n",
    "    '''\n",
    "    INPUT: list of strings (documents)\n",
    "    OUTPUT: 2D numpy array (vector matrix), \n",
    "            1D numpy array (sorted vocabulary),\n",
    "            dictionary (keys: vocab words, values: indices in sorted vocab)\n",
    "    \n",
    "    '''\n",
    "    #Tokenize documents\n",
    "    tokenized_docs = [my_tokenizer(doc, stopwords=stopwords, lemmatizer=lemmatizer) for doc in corpus]\n",
    "    \n",
    "    #Make an array of unique words in the corpus\n",
    "    vocab_list = set()\n",
    "    for doc in tokenized_docs:\n",
    "        vocab_list.update(doc)\n",
    "    vocab_list = np.array(sorted(list(vocab_list)))\n",
    "    \n",
    "    #Make a dictionary mapping vocab tokens (words) to indices in this list\n",
    "    vocab_dict = dict()\n",
    "    for i, token in enumerate(vocab_list):\n",
    "        vocab_dict[token] = i\n",
    "    \n",
    "    #Vectorize each document!\n",
    "    vector_matrix = np.zeros((len(corpus), len(vocab_list)))\n",
    "    for i, doc in enumerate(tokenized_docs):\n",
    "        counter = Counter(doc)\n",
    "        for token, count in counter.iteritems():\n",
    "            vector_matrix[i,vocab_dict[token]] = count\n",
    "    \n",
    "    # TF-IDF code\n",
    "    if kind == 'tfidf':\n",
    "        # number of documents\n",
    "        N = len(corpus)\n",
    "        \n",
    "        # document frequency for each word in vocab\n",
    "        df = np.array([sum(1 for doc in tokenized_docs if word in doc)\\\n",
    "                       for word in vocab_list])\n",
    "        \n",
    "        # idf\n",
    "        idf_list = np.log(N/df)\n",
    "        \n",
    "        # vectorizin'\n",
    "        vector_matrix = np.zeros((len(corpus), len(vocab_list)))\n",
    "        for i, doc in enumerate(tokenized_docs):\n",
    "            counter = Counter(doc)\n",
    "            n = 1.*len(doc)\n",
    "            for token, count in counter.iteritems():\n",
    "                idf = idf_list[vocab_dict[token]]\n",
    "                vector_matrix[i,vocab_dict[token]] = (count/n)*idf\n",
    "                \n",
    "    return vector_matrix, vocab_list, vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_mat, voc_list, voc_dict = my_vectorizer(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'against', 'age', 'and', 'at', 'bay', 'be', 'because', 'blaze',\n",
       "       'bless', 'blind', 'blinding', 'bright', 'burn', 'by', 'caught',\n",
       "       'close', 'could', 'crying', 'curse', 'danced', 'dark', 'day',\n",
       "       'death', 'deeds', 'do', 'dying', 'end', 'eyes', 'father', 'fierce',\n",
       "       'flight', 'forked', 'frail', 'gay', 'gentle', 'go', 'good', 'grave',\n",
       "       'green', 'grieved', 'had', 'have', 'height', 'how', 'i', 'in',\n",
       "       'into', 'is', 'it', 'its', 'know', 'last', 'late', 'learn', 'light',\n",
       "       'lightning', 'like', 'me', 'men', 'meteors', 'might', 'my', 'near',\n",
       "       'night', 'no', 'not', 'now', 'of', 'old', 'on', 'pray', 'rage',\n",
       "       'rave', 'right', 'sad', 'sang', 'see', 'should', 'sight', 'sun',\n",
       "       'tears', 'that', 'the', 'their', 'there', 'they', 'though', 'too',\n",
       "       'wave', 'way', 'who', 'wild', 'wise', 'with', 'words', 'you', 'your'], \n",
       "      dtype='|S9')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'against': 1,\n",
       " 'age': 2,\n",
       " 'and': 3,\n",
       " 'at': 4,\n",
       " 'bay': 5,\n",
       " 'be': 6,\n",
       " 'because': 7,\n",
       " 'blaze': 8,\n",
       " 'bless': 9,\n",
       " 'blind': 10,\n",
       " 'blinding': 11,\n",
       " 'bright': 12,\n",
       " 'burn': 13,\n",
       " 'by': 14,\n",
       " 'caught': 15,\n",
       " 'close': 16,\n",
       " 'could': 17,\n",
       " 'crying': 18,\n",
       " 'curse': 19,\n",
       " 'danced': 20,\n",
       " 'dark': 21,\n",
       " 'day': 22,\n",
       " 'death': 23,\n",
       " 'deeds': 24,\n",
       " 'do': 25,\n",
       " 'dying': 26,\n",
       " 'end': 27,\n",
       " 'eyes': 28,\n",
       " 'father': 29,\n",
       " 'fierce': 30,\n",
       " 'flight': 31,\n",
       " 'forked': 32,\n",
       " 'frail': 33,\n",
       " 'gay': 34,\n",
       " 'gentle': 35,\n",
       " 'go': 36,\n",
       " 'good': 37,\n",
       " 'grave': 38,\n",
       " 'green': 39,\n",
       " 'grieved': 40,\n",
       " 'had': 41,\n",
       " 'have': 42,\n",
       " 'height': 43,\n",
       " 'how': 44,\n",
       " 'i': 45,\n",
       " 'in': 46,\n",
       " 'into': 47,\n",
       " 'is': 48,\n",
       " 'it': 49,\n",
       " 'its': 50,\n",
       " 'know': 51,\n",
       " 'last': 52,\n",
       " 'late': 53,\n",
       " 'learn': 54,\n",
       " 'light': 55,\n",
       " 'lightning': 56,\n",
       " 'like': 57,\n",
       " 'me': 58,\n",
       " 'men': 59,\n",
       " 'meteors': 60,\n",
       " 'might': 61,\n",
       " 'my': 62,\n",
       " 'near': 63,\n",
       " 'night': 64,\n",
       " 'no': 65,\n",
       " 'not': 66,\n",
       " 'now': 67,\n",
       " 'of': 68,\n",
       " 'old': 69,\n",
       " 'on': 70,\n",
       " 'pray': 71,\n",
       " 'rage': 72,\n",
       " 'rave': 73,\n",
       " 'right': 74,\n",
       " 'sad': 75,\n",
       " 'sang': 76,\n",
       " 'see': 77,\n",
       " 'should': 78,\n",
       " 'sight': 79,\n",
       " 'sun': 80,\n",
       " 'tears': 81,\n",
       " 'that': 82,\n",
       " 'the': 83,\n",
       " 'their': 84,\n",
       " 'there': 85,\n",
       " 'they': 86,\n",
       " 'though': 87,\n",
       " 'too': 88,\n",
       " 'wave': 89,\n",
       " 'way': 90,\n",
       " 'who': 91,\n",
       " 'wild': 92,\n",
       " 'wise': 93,\n",
       " 'with': 94,\n",
       " 'words': 95,\n",
       " 'you': 96,\n",
       " 'your': 97}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 98)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print vec_mat.shape\n",
    "print vec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rage, rage against the dying of the light.\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print lines[2]\n",
    "print vec_mat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sk_vectorizer = CountVectorizer()\n",
    "sk_vec_mat = sk_vectorizer.fit_transform(lines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'against',\n",
       " u'age',\n",
       " u'and',\n",
       " u'at',\n",
       " u'bay',\n",
       " u'be',\n",
       " u'because',\n",
       " u'blaze',\n",
       " u'bless',\n",
       " u'blind',\n",
       " u'blinding',\n",
       " u'bright',\n",
       " u'burn',\n",
       " u'by',\n",
       " u'caught',\n",
       " u'close',\n",
       " u'could',\n",
       " u'crying',\n",
       " u'curse',\n",
       " u'danced',\n",
       " u'dark',\n",
       " u'day',\n",
       " u'death',\n",
       " u'deeds',\n",
       " u'do',\n",
       " u'dying',\n",
       " u'end',\n",
       " u'eyes',\n",
       " u'father',\n",
       " u'fierce',\n",
       " u'flight',\n",
       " u'forked',\n",
       " u'frail',\n",
       " u'gay',\n",
       " u'gentle',\n",
       " u'go',\n",
       " u'good',\n",
       " u'grave',\n",
       " u'green',\n",
       " u'grieved',\n",
       " u'had',\n",
       " u'have',\n",
       " u'height',\n",
       " u'how',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'it',\n",
       " u'its',\n",
       " u'know',\n",
       " u'last',\n",
       " u'late',\n",
       " u'learn',\n",
       " u'light',\n",
       " u'lightning',\n",
       " u'like',\n",
       " u'me',\n",
       " u'men',\n",
       " u'meteors',\n",
       " u'might',\n",
       " u'my',\n",
       " u'near',\n",
       " u'night',\n",
       " u'no',\n",
       " u'not',\n",
       " u'now',\n",
       " u'of',\n",
       " u'old',\n",
       " u'on',\n",
       " u'pray',\n",
       " u'rage',\n",
       " u'rave',\n",
       " u'right',\n",
       " u'sad',\n",
       " u'sang',\n",
       " u'see',\n",
       " u'should',\n",
       " u'sight',\n",
       " u'sun',\n",
       " u'tears',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'there',\n",
       " u'they',\n",
       " u'though',\n",
       " u'too',\n",
       " u'wave',\n",
       " u'way',\n",
       " u'who',\n",
       " u'wild',\n",
       " u'wise',\n",
       " u'with',\n",
       " u'words',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'against': 0,\n",
       " u'age': 1,\n",
       " u'and': 2,\n",
       " u'at': 3,\n",
       " u'bay': 4,\n",
       " u'be': 5,\n",
       " u'because': 6,\n",
       " u'blaze': 7,\n",
       " u'bless': 8,\n",
       " u'blind': 9,\n",
       " u'blinding': 10,\n",
       " u'bright': 11,\n",
       " u'burn': 12,\n",
       " u'by': 13,\n",
       " u'caught': 14,\n",
       " u'close': 15,\n",
       " u'could': 16,\n",
       " u'crying': 17,\n",
       " u'curse': 18,\n",
       " u'danced': 19,\n",
       " u'dark': 20,\n",
       " u'day': 21,\n",
       " u'death': 22,\n",
       " u'deeds': 23,\n",
       " u'do': 24,\n",
       " u'dying': 25,\n",
       " u'end': 26,\n",
       " u'eyes': 27,\n",
       " u'father': 28,\n",
       " u'fierce': 29,\n",
       " u'flight': 30,\n",
       " u'forked': 31,\n",
       " u'frail': 32,\n",
       " u'gay': 33,\n",
       " u'gentle': 34,\n",
       " u'go': 35,\n",
       " u'good': 36,\n",
       " u'grave': 37,\n",
       " u'green': 38,\n",
       " u'grieved': 39,\n",
       " u'had': 40,\n",
       " u'have': 41,\n",
       " u'height': 42,\n",
       " u'how': 43,\n",
       " u'in': 44,\n",
       " u'into': 45,\n",
       " u'is': 46,\n",
       " u'it': 47,\n",
       " u'its': 48,\n",
       " u'know': 49,\n",
       " u'last': 50,\n",
       " u'late': 51,\n",
       " u'learn': 52,\n",
       " u'light': 53,\n",
       " u'lightning': 54,\n",
       " u'like': 55,\n",
       " u'me': 56,\n",
       " u'men': 57,\n",
       " u'meteors': 58,\n",
       " u'might': 59,\n",
       " u'my': 60,\n",
       " u'near': 61,\n",
       " u'night': 62,\n",
       " u'no': 63,\n",
       " u'not': 64,\n",
       " u'now': 65,\n",
       " u'of': 66,\n",
       " u'old': 67,\n",
       " u'on': 68,\n",
       " u'pray': 69,\n",
       " u'rage': 70,\n",
       " u'rave': 71,\n",
       " u'right': 72,\n",
       " u'sad': 73,\n",
       " u'sang': 74,\n",
       " u'see': 75,\n",
       " u'should': 76,\n",
       " u'sight': 77,\n",
       " u'sun': 78,\n",
       " u'tears': 79,\n",
       " u'that': 80,\n",
       " u'the': 81,\n",
       " u'their': 82,\n",
       " u'there': 83,\n",
       " u'they': 84,\n",
       " u'though': 85,\n",
       " u'too': 86,\n",
       " u'wave': 87,\n",
       " u'way': 88,\n",
       " u'who': 89,\n",
       " u'wild': 90,\n",
       " u'wise': 91,\n",
       " u'with': 92,\n",
       " u'words': 93,\n",
       " u'you': 94,\n",
       " u'your': 95}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_vec_mat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_dist = squareform(pdist(vec_mat, metric='cosine'))\n",
    "pairwise_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not go gentle into that good night,\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Good men, the last wave by, crying how bright\n",
      "-----\n",
      "Old age should burn and rave at close of day;\n",
      "And you, my father, there on the sad height,\n",
      "Blind eyes could blaze like meteors and be gay,\n",
      "Wild men who caught and sang the sun in flight,\n",
      "Though wise men at their end know dark is right,\n",
      "-----\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "And you, my father, there on the sad height,\n",
      "-----\n",
      "Though wise men at their end know dark is right,\n",
      "Because their words had forked no lightning they\n",
      "Good men, the last wave by, crying how bright\n",
      "Grave men, near death, who see with blinding sight\n",
      "Wild men who caught and sang the sun in flight,\n",
      "-----\n",
      "Because their words had forked no lightning they\n",
      "Though wise men at their end know dark is right,\n",
      "Their frail deeds might have danced in a green bay,\n",
      "And learn, too late, they grieved it on its way,\n",
      "Do not go gentle into that good night,\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(lines[:5])):\n",
    "    for ind in pairwise_dist[i].argsort()[:5]:\n",
    "        print lines[ind]\n",
    "    print '-----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_vec_mat, tf_voc_list, tf_voc_dict = my_vectorizer(lines, kind='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.1732868 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.1732868 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.1732868 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.13732654,  0.        ,\n",
       "        0.        ,  0.        ,  0.34657359,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.1732868 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec_mat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_tf_vec = TfidfVectorizer()\n",
    "sk_tf_mat = sk_tf_vec.fit_transform(lines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30960308,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.30960308,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.30960308,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.28594828,  0.        ,  0.        ,  0.        ,\n",
       "        0.61920616,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.49724755,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_tf_mat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sk_tf_mat[2]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_dist_tf = squareform(pdist(tf_vec_mat, metric='cosine'))\n",
    "pairwise_dist_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not go gentle into that good night,\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Good men, the last wave by, crying how bright\n",
      "-----\n",
      "Old age should burn and rave at close of day;\n",
      "Though wise men at their end know dark is right,\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "-----\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Old age should burn and rave at close of day;\n",
      "-----\n",
      "Though wise men at their end know dark is right,\n",
      "Old age should burn and rave at close of day;\n",
      "Because their words had forked no lightning they\n",
      "Their frail deeds might have danced in a green bay,\n",
      "Good men, the last wave by, crying how bright\n",
      "-----\n",
      "Because their words had forked no lightning they\n",
      "And learn, too late, they grieved it on its way,\n",
      "Though wise men at their end know dark is right,\n",
      "Their frail deeds might have danced in a green bay,\n",
      "Do not go gentle into that good night,\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# tfidf vector similarity\n",
    "for i in xrange(len(lines[:5])):\n",
    "    for ind in pairwise_dist_tf[i].argsort()[:5]:\n",
    "        print lines[ind]\n",
    "    print '-----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not go gentle into that good night,\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Do not go gentle into that good night.\n",
      "Good men, the last wave by, crying how bright\n",
      "-----\n",
      "Old age should burn and rave at close of day;\n",
      "And you, my father, there on the sad height,\n",
      "Blind eyes could blaze like meteors and be gay,\n",
      "Wild men who caught and sang the sun in flight,\n",
      "Though wise men at their end know dark is right,\n",
      "-----\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "Rage, rage against the dying of the light.\n",
      "And you, my father, there on the sad height,\n",
      "-----\n",
      "Though wise men at their end know dark is right,\n",
      "Because their words had forked no lightning they\n",
      "Good men, the last wave by, crying how bright\n",
      "Grave men, near death, who see with blinding sight\n",
      "Wild men who caught and sang the sun in flight,\n",
      "-----\n",
      "Because their words had forked no lightning they\n",
      "Though wise men at their end know dark is right,\n",
      "Their frail deeds might have danced in a green bay,\n",
      "And learn, too late, they grieved it on its way,\n",
      "Do not go gentle into that good night,\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# count vector similarity\n",
    "for i in xrange(len(lines[:5])):\n",
    "    for ind in pairwise_dist[i].argsort()[:5]:\n",
    "        print lines[ind]\n",
    "    print '-----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
