{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do I turn a corpus of documents into a feature matrix? \n",
    "# Words --> numbers?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus: list of documents\n",
    "\n",
    "```\n",
    " [\n",
    " \"Jeff stole my octopus sandwich.\", \n",
    " \"'Help!' I sobbed, sandwichlessly.\", \n",
    " \"'Drop the sandwiches!' said the sandwich police.\"\n",
    " ]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: lowercase, lose punction, split into tokens\n",
    "```\n",
    "[\n",
    " ['jeff', 'stole', 'my', 'octopus', 'sandwich'],\n",
    " ['help', 'i', 'sobbed', 'sandwichlessly'],\n",
    " ['drop', 'the', 'sandwiches', 'said', 'the', 'sandwich', 'police']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: remove stop words\n",
    "```\n",
    "[\n",
    " ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    " ['help', 'sobbed', 'sandwichlessly'],\n",
    " ['drop', 'sandwiches', 'said', 'sandwich', 'police']\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Stemming/Lemmatization\n",
    "```\n",
    "[\n",
    " ['jeff', 'stole', 'octopus', 'sandwich'],\n",
    " ['help', 'sobbed', 'sandwichlessly'],\n",
    " ['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "]\n",
    "```\n",
    "### OK now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vocabulary:\n",
    "```\n",
    "['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vocabulary:\n",
    "```\n",
    "['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "[0, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "['help', 'sobbed', 'sandwichlessly']\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "\n",
    "['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "[1, 0, 0, 0, 1, 1, 2, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequency\n",
    "$$TF_{word,document} = \\frac{\\#\\_of\\_times\\_word\\_appears\\_in\\_document}{total\\_\\#\\_of\\_words\\_in\\_document}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "[0, 0, 1/4, 1/4, 0, 0, 1/4, 0, 0, 1/4]\n",
    "\n",
    "['help', 'sobbed', 'sandwichlessly']\n",
    "[0, 1/3, 0, 0, 0, 0, 0, 1/3, 1/3, 0]\n",
    "\n",
    "['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "[1/5, 0, 0, 0, 1/5, 1/5, 2/5, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document frequency\n",
    "$$ DF_{word} = \\frac{\\#\\_of\\_documents\\_containing\\_word}{total\\_\\#\\_of\\_documents} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vocabulary:\n",
    "```\n",
    "['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "Document frequency for each word:\n",
    "```\n",
    "[1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 2/3, 1/3, 1/3, 1/3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse document frequency\n",
    "$$ IDF_{word} = \\log\\left(\\frac{total\\_\\#\\_of\\_documents}{\\#\\_of\\_documents\\_containing\\_word}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vocabulary:\n",
    "```\n",
    "['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "```\n",
    "\n",
    "IDF for each word:\n",
    "```\n",
    "[1.099, 1.099, 1.099, 1.099, 1.099, 1.099, 0.405, 1.099, 1.099, 1.099]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TFIDF\n",
    "\n",
    "Vocabulary:\n",
    "```\n",
    "['drop', 'help', 'jeff', 'octopus', 'police', 'said', 'sandwich', 'sandwichlessly', 'sobbed', 'stole']\n",
    "```\n",
    "TF * IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "['jeff', 'stole', 'octopus', 'sandwich']\n",
    "[0, 0, 0.275, 0.275, 0, 0, 0.101, 0, 0, 0.275]\n",
    "\n",
    "['help', 'sobbed', 'sandwichlessly']\n",
    "[0, 0.366, 0, 0, 0, 0, 0, 0.366, 0.366, 0]\n",
    "\n",
    "['drop', u'sandwich', 'said', 'sandwich', 'police']\n",
    "[0.22, 0, 0, 0, 0.22, 0.22, 0.162, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have turned our DOCUMENTS into VECTORS, we can put them into whatever machine learning algorithm we want! We can use whatever kind of similarity measure we please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wow!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
