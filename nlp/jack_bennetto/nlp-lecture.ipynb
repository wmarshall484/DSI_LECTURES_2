{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "J.F. Omhover, with thanks to Mari Pierce-Quinonez for some great enhancements.\n",
    "\n",
    "Updated for Python 3.6 by Miles Erickson\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You need to install the `nltk` module:\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "This module will need corporas that you need to download. This can take a very long time, for simplicity here's the minimal corporas for this lecture. In a terminal, open `ipython` and type:\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "```\n",
    "\n",
    "### General Introduction\n",
    "\n",
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. Text is inherently unstructured and has all sorts of tricks required for converting (vectorizing) text into a format that a machine learning algorithm can interpret. It is called Processing for a reason - most of what we'll be covering during this morning session are Data Processing operations that make it possible to plug test into other ML algorythms.\n",
    "\n",
    "### Overview of nlp\n",
    "\n",
    "Natural language processing is concerned with understanding text using computation. People working within the field are often concerned with:\n",
    "- Information retrieval. How do you find a document or a particular fact within a document?\n",
    "- Document classification. What is the document about amongst mutually exclusive categories?\n",
    "- Machine translation. How do you write an English phrase in Chinese? Think of Google translate.\n",
    "- Sentiment analysis. Was a product review positive or negative?\n",
    "Natural language processing is a huge field and we will just touch on some of the concepts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Name and describe the steps necessary for processing text in machine learning.\n",
    "- Implement a Natural Language Processing pipeline.\n",
    "- Explain the cosine similarity measure and why it is used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Featurization part 1 : Bags of Words\n",
    "\n",
    "This Walkthrough will lead us from raw documents to bag-of-words representations using **Natural Language Processing** functions.\n",
    "\n",
    "In our case, this walkthrough is a preliminary step of a pipeline for **indexing** documents.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **signature** (vector) for each document.\n",
    "\n",
    "This **signature** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Text sources and possible text mining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:04.475521",
     "start_time": "2018-04-10T09:43:04.454882"
    }
   },
   "outputs": [],
   "source": [
    "paragraph = \"My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt – sleeveless, white eyelet lace; I was wearing it as a farewell gesture. My carry-on item was a parka. In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds. It rains on this inconsequential town more than any other place in the United States of America. It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old. It was in this town that I’d been compelled to spend a month every summer until I was fourteen. That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.\"\n",
    "\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:04.490934",
     "start_time": "2018-04-10T09:43:04.476845"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "input_string = remove_accents(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating bag-of-words for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenize document\n",
    "\n",
    "**\"Tokenize\"** means creating \"tokens\" which are atomic units of the text. These tokens are usually words we extract from the document by splitting it (using punctuations as a separator). We can also consider sentences as tokens (and words as sub-tokens of sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.641474",
     "start_time": "2018-04-10T09:43:04.492348"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n",
    "\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.676022",
     "start_time": "2018-04-10T09:43:06.642876"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [sent for sent in map(word_tokenize, sent_tokens)]\n",
    "\n",
    "list(enumerate(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.708148",
     "start_time": "2018-04-10T09:43:06.677745"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tokens_lower = [[word.lower() for word in sent]\n",
    "                 for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Filtering stopwords (and punctuation)\n",
    "\n",
    "**Stopwords** are words that should be stopped at this step because they do not carry much information about the actual meaning of the document. Usually, they are \"common\" words you use. You can find lists of such **stopwords** online, or embedded within the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.742508",
     "start_time": "2018-04-10T09:43:06.709793"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.773638",
     "start_time": "2018-04-10T09:43:06.744111"
    }
   },
   "outputs": [],
   "source": [
    "# list found at http://www.textfixer.com/resources/common-english-words.txt\n",
    "# 'not' has been removed (do you know why ?)\n",
    "\n",
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to filter punctuation tokens: tokens made of punctuation marks. We can find a list of those punctuations in string.punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.812229",
     "start_time": "2018-04-10T09:43:06.776565"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_ = set(string.punctuation)\n",
    "print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "\n",
    "for sent in tokens_filtered:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stemming and lemmatization\n",
    "\n",
    "**Stemming** means reducing each word to a **stem**. That is, reducing each word in all its diversity to a root common to all its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.847750",
     "start_time": "2018-04-10T09:43:06.813833"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [list(map(stemmer_porter.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (porter): {}\".format(tokens_stemporter[0]))\n",
    "\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "tokens_stemsnowball = [list(map(stemmer_snowball.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (snowball): {}\".format(tokens_stemsnowball[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. N-Grams\n",
    "\n",
    "<span style=\"color:red\">To capture sequences of tokens</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.878775",
     "start_time": "2018-04-10T09:43:06.849349"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens_stemsnowball[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.917945",
     "start_time": "2018-04-10T09:43:06.880183"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def join_sent_ngrams(input_tokens, n):\n",
    "    # first add the 1-gram tokens\n",
    "    ret_list = list(input_tokens)\n",
    "    \n",
    "    #then for each n\n",
    "    for i in range(2,n+1):\n",
    "        # add each n-grams to the list\n",
    "        ret_list.extend(['-'.join(tgram) for tgram in ngrams(input_tokens, i)])\n",
    "    \n",
    "    return(ret_list)\n",
    "\n",
    "tokens_ngrams = list(map(lambda x : join_sent_ngrams(x, 3), tokens_stemporter))\n",
    "\n",
    "print(\"--- sentence tokens: {}\".format(tokens_ngrams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Part-of-Speech tagging\n",
    "\n",
    "This is an alternative process that relies on machine learning to tag each word in a sentence with its function. In libraries such as NLTK, there are embedded tools to do that. Tags detected depend on the corpus used for training. In NLTK, the function `nltk.pos_tag()` uses the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.105150",
     "start_time": "2018-04-10T09:43:06.919478"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "for sent in sent_tags:\n",
    "    print(\"--- sentence tags: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter verbs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.135456",
     "start_time": "2018-04-10T09:43:07.106679"
    }
   },
   "outputs": [],
   "source": [
    "for sent in sent_tags:\n",
    "    tags_filtered = [t for t in sent if t[1].startswith('VB')]\n",
    "    print(\"--- verbs:\\n{}\".format(tags_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.175528",
     "start_time": "2018-04-10T09:43:07.136878"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NPB: {<DT|PP\\$>?<JJ|NN|,>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "  V2V: {<V.*> <TO> <V.*>}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(sent_tags[1])\n",
    "\n",
    "#print result\n",
    "\n",
    "for sent in sent_tags:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NPB': print(subtree)\n",
    "        if subtree.label() == 'V2V': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Featurization part 2 : Indexing Bag-of-Words into a vector table\n",
    "\n",
    "This Walkthrough will lead us from bag-of-words representations of documents to **vector signatures** (indexes) using the **TF-IDF** formula.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **vector representation** (signature) for each document. This vector representation will be used for:\n",
    "\n",
    "- mine the features that can caracterize classes of documents (supervised learning using **labels**)\n",
    "- mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough2.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading some input data from the Amazon Reviews\n",
    "\n",
    "To try this indexing walkthrough, we will get 5 reviews from the Amazon Reviews dataset. We will apply a function for extracting bag-of-words representations from these 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.242123",
     "start_time": "2018-04-10T09:43:07.177077"
    }
   },
   "outputs": [],
   "source": [
    "import os               # for environ variables in Part 3\n",
    "from nlp_pipeline import extract_bow_from_raw_text\n",
    "import json\n",
    "\n",
    "docs = []\n",
    "with open('./reviews.json', 'r') as data_file:    \n",
    "    for line in data_file:\n",
    "        docs.append(json.loads(line))\n",
    "\n",
    "# extracting bows\n",
    "bows = list(map(lambda row: extract_bow_from_raw_text(row['reviewText']), docs))\n",
    "\n",
    "# displaying bows\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indexing Bag of Words into a Vector Matrix using Term Frequency / Inverse Document Frequency\n",
    "The ultimate goal of indexing is to create a vector representation (signature) for each document. This vector representation will be used for:\n",
    "mine the features that can caracterize classes of documents (supervised learning using labels)\n",
    "mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "## 1.1 Term Frequency\n",
    "\n",
    "The number of times a term occurs in a specific document:\n",
    "\n",
    "$tf(term,document) = \\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.280477",
     "start_time": "2018-04-10T09:43:07.243784"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# term occurence = counting distinct words in each bag\n",
    "term_occ = list(map(lambda bow : Counter(bow), bows))\n",
    "\n",
    "# term frequency = occurences over length of bag\n",
    "term_freq = list()\n",
    "for i in range(len(docs)):\n",
    "    term_freq.append( {k: (v / float(len(bows[i])))\n",
    "                       for k, v in term_occ[i].items()} )\n",
    "\n",
    "# displaying occurences\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- term_occ: {}\".format(term_occ[i]))\n",
    "    print(\"--- term_freq: {}\".format(term_freq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Obtaining document frequencies\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.313506",
     "start_time": "2018-04-10T09:43:07.281893"
    }
   },
   "outputs": [],
   "source": [
    "# document occurence = number of documents having this word\n",
    "# term frequency = occurences over length of bag\n",
    "\n",
    "doc_occ = Counter( [word for bow in bows for word in set(bow)] )\n",
    "\n",
    "# document frequency = occurences over length of corpus\n",
    "doc_freq = {k: (v / float(len(docs)))\n",
    "            for k, v in doc_occ.items()}\n",
    "\n",
    "# displaying vocabulary\n",
    "print(\"\\n--- full vocabulary: {}\".format(doc_occ))\n",
    "print(\"\\n--- doc freq: {}\".format(doc_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the vocabulary for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.346712",
     "start_time": "2018-04-10T09:43:07.315248"
    }
   },
   "outputs": [],
   "source": [
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "min_df = 0.3\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n",
    "\n",
    "# print vocabulary\n",
    "print (\"-- vocabulary (len={}): {}\".format(len(vocabulary),vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 the TFIDF vector\n",
    "\n",
    "Words might show up a lot in individual documents, but their relevace is less important if they're in every document! We need to take into account words that show up everywhere and reduce their relative importance. The document frequency does exactly that:\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.388629",
     "start_time": "2018-04-10T09:43:07.348353"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a dense matrix of vectors for each document\n",
    "# each vector has the length of the vocabulary\n",
    "vectors = np.zeros((len(docs),len(vocabulary)))\n",
    "\n",
    "# fill these vectors with tf-idf values\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        term     = vocabulary[j]\n",
    "        term_tf  = term_freq[i].get(term, 0.0)   # 0.0 if term not found in doc\n",
    "        term_idf = np.log(1 + 1 / doc_freq[term]) # smooth formula\n",
    "        vectors[i,j] = term_tf * term_idf\n",
    "\n",
    "# displaying results\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- tfidf vector: {}\".format( vectors[i] ) )\n",
    "    print(\"--- tfidf sorted: {}\".format( \n",
    "            sorted( zip(vocabulary,vectors[i]), key=lambda x:-x[1] )\n",
    "         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.418751",
     "start_time": "2018-04-10T09:43:07.390154"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [row['reviewText'] for row in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.463389",
     "start_time": "2018-04-10T09:43:07.420683"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer()\n",
    "\n",
    "document_tf_matrix = tf.fit_transform(corpus).todense()\n",
    "\n",
    "print(sorted(tf.vocabulary_))\n",
    "print(document_tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.493574",
     "start_time": "2018-04-10T09:43:07.464848"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def idf(frequency_matrix):\n",
    "    df =  float(len(document_tf_matrix)) / sum(frequency_matrix > 0)\n",
    "    return [log(i) for i in df.getA()[0]]\n",
    "print(sorted(tf.vocabulary_))\n",
    "print(idf(document_tf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.546190",
     "start_time": "2018-04-10T09:43:07.495291"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "document_tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(sorted(tfidf.vocabulary_))\n",
    "print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Comparing two documents / Similarity Measures\n",
    "\n",
    "## 3.1 Euclidean distance\n",
    "\n",
    "We could try the Euclidean distance $||\\vec{x}-\\vec{y}||$  \n",
    "What problems would we encounter with this? \n",
    "\n",
    "The euclidean distance goes up with the length of a document. Intuitively, duplicating each word in our bag of words generates a vector that points in exactly the same direction, however, the euclidean distance goes up. One solution is to normalize vectors before calculating the euclidean distance. Now increasing the length of a document does not change the Euclidean distance unless the direction of the term frequency vector changes. \n",
    "\n",
    "## 3.2 Cosine Similarity\n",
    "Recall that for two vector $\\vec{x}$ and $\\vec{y}$ that $\\vec{x} \\cdot \\vec{y} = ||\\vec{x}|| ||\\vec{y}|| \\cos{\\theta}$. And so,\n",
    "\n",
    "$\\frac{\\vec{x} \\cdot \\vec{y} }{||\\vec{x}|| ||\\vec{y}||} = \\cos{\\theta}$\n",
    "\n",
    "θ can only range from 0 to 90 degrees, because tf-idf vectors are non-negative. Therefore cos θ ranges from 0 to 1. Documents that are exactly identical will have cos θ = 1\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
