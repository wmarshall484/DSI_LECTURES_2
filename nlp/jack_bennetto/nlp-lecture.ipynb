{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "J.F. Omhover, with thanks to Mari Pierce-Quinonez for some great enhancements.\n",
    "\n",
    "Updated for Python 3.6 by Miles Erickson\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You need to install the `nltk` module:\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "This module will need corpora that you need to download. This can take a very long time, for simplicity here's the minimal corpora for this lecture. Run these from `ipython` or a jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jackbennetto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jackbennetto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jackbennetto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/jackbennetto/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Introduction\n",
    "\n",
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. Text is inherently unstructured and has all sorts of tricks required for converting (vectorizing) text into a format that a machine learning algorithm can interpret. It is called Processing for a reason - most of what we'll be covering during this morning session are Data Processing operations that make it possible to plug test into other ML algorithms.\n",
    "\n",
    "### Overview of nlp\n",
    "\n",
    "Natural language processing is concerned with understanding text using computation. People working within the field are often concerned with:\n",
    "- Information retrieval. How do you find a document or a particular fact within a document?\n",
    "- Document classification. What is the document about amongst mutually exclusive categories?\n",
    "- Machine translation. How do you write an English phrase in Chinese? Think of Google translate.\n",
    "- Sentiment analysis. Was a product review positive or negative?\n",
    "Natural language processing is a huge field and we will just touch on some of the concepts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Name and describe the steps necessary for processing text in machine learning.\n",
    "- Implement a Natural Language Processing pipeline.\n",
    "- Explain the cosine similarity measure and why it is used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Featurization part 1 : Bags of Words\n",
    "\n",
    "This Walkthrough will lead us from raw documents to bag-of-words representations using **Natural Language Processing** functions.\n",
    "\n",
    "In our case, this walkthrough is a preliminary step of a pipeline for **indexing** documents.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **signature** (vector) for each document.\n",
    "\n",
    "This **signature** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Text sources and possible text mining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:04.475521",
     "start_time": "2018-04-10T09:43:04.454882"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt – sleeveless, white eyelet lace; I was wearing it as a farewell gesture. My carry-on item was a parka. In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds. It rains on this inconsequential town more than any other place in the United States of America. It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old. It was in this town that I’d been compelled to spend a month every summer until I was fourteen. That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt – sleeveless, white eyelet lace; I was wearing it as a farewell gesture. My carry-on item was a parka. In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds. It rains on this inconsequential town more than any other place in the United States of America. It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old. It was in this town that I’d been compelled to spend a month every summer until I was fourteen. That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.\"\n",
    "\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:04.490934",
     "start_time": "2018-04-10T09:43:04.476845"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "input_string = remove_accents(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating bag-of-words for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenize document\n",
    "\n",
    "**\"Tokenize\"** means creating \"tokens\" which are atomic units of the text. These tokens are usually words we extract from the document by splitting it (using punctuations as a separator). We can also consider sentences as tokens (and words as sub-tokens of sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.641474",
     "start_time": "2018-04-10T09:43:04.492348"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My mother drove me to the airport with the windows rolled down.',\n",
       " 'It was seventy-five degrees in Phoenix, the sky a perfect, cloudless blue.',\n",
       " 'I was wearing my favorite shirt  sleeveless, white eyelet lace; I was wearing it as a farewell gesture.',\n",
       " 'My carry-on item was a parka.',\n",
       " 'In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a near-constant cover of clouds.',\n",
       " 'It rains on this inconsequential town more than any other place in the United States of America.',\n",
       " 'It was from this town and its gloomy, omnipresent shade that my mother escaped with me when I was only a few months old.',\n",
       " 'It was in this town that Id been compelled to spend a month every summer until I was fourteen.',\n",
       " 'That was the year I finally put my foot down; these past three summers, my dad, Charlie, vacationed with me in California for two weeks instead.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n",
    "\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.676022",
     "start_time": "2018-04-10T09:43:06.642876"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ['My',\n",
       "   'mother',\n",
       "   'drove',\n",
       "   'me',\n",
       "   'to',\n",
       "   'the',\n",
       "   'airport',\n",
       "   'with',\n",
       "   'the',\n",
       "   'windows',\n",
       "   'rolled',\n",
       "   'down',\n",
       "   '.']),\n",
       " (1,\n",
       "  ['It',\n",
       "   'was',\n",
       "   'seventy-five',\n",
       "   'degrees',\n",
       "   'in',\n",
       "   'Phoenix',\n",
       "   ',',\n",
       "   'the',\n",
       "   'sky',\n",
       "   'a',\n",
       "   'perfect',\n",
       "   ',',\n",
       "   'cloudless',\n",
       "   'blue',\n",
       "   '.']),\n",
       " (2,\n",
       "  ['I',\n",
       "   'was',\n",
       "   'wearing',\n",
       "   'my',\n",
       "   'favorite',\n",
       "   'shirt',\n",
       "   'sleeveless',\n",
       "   ',',\n",
       "   'white',\n",
       "   'eyelet',\n",
       "   'lace',\n",
       "   ';',\n",
       "   'I',\n",
       "   'was',\n",
       "   'wearing',\n",
       "   'it',\n",
       "   'as',\n",
       "   'a',\n",
       "   'farewell',\n",
       "   'gesture',\n",
       "   '.']),\n",
       " (3, ['My', 'carry-on', 'item', 'was', 'a', 'parka', '.']),\n",
       " (4,\n",
       "  ['In',\n",
       "   'the',\n",
       "   'Olympic',\n",
       "   'Peninsula',\n",
       "   'of',\n",
       "   'northwest',\n",
       "   'Washington',\n",
       "   'State',\n",
       "   ',',\n",
       "   'a',\n",
       "   'small',\n",
       "   'town',\n",
       "   'named',\n",
       "   'Forks',\n",
       "   'exists',\n",
       "   'under',\n",
       "   'a',\n",
       "   'near-constant',\n",
       "   'cover',\n",
       "   'of',\n",
       "   'clouds',\n",
       "   '.']),\n",
       " (5,\n",
       "  ['It',\n",
       "   'rains',\n",
       "   'on',\n",
       "   'this',\n",
       "   'inconsequential',\n",
       "   'town',\n",
       "   'more',\n",
       "   'than',\n",
       "   'any',\n",
       "   'other',\n",
       "   'place',\n",
       "   'in',\n",
       "   'the',\n",
       "   'United',\n",
       "   'States',\n",
       "   'of',\n",
       "   'America',\n",
       "   '.']),\n",
       " (6,\n",
       "  ['It',\n",
       "   'was',\n",
       "   'from',\n",
       "   'this',\n",
       "   'town',\n",
       "   'and',\n",
       "   'its',\n",
       "   'gloomy',\n",
       "   ',',\n",
       "   'omnipresent',\n",
       "   'shade',\n",
       "   'that',\n",
       "   'my',\n",
       "   'mother',\n",
       "   'escaped',\n",
       "   'with',\n",
       "   'me',\n",
       "   'when',\n",
       "   'I',\n",
       "   'was',\n",
       "   'only',\n",
       "   'a',\n",
       "   'few',\n",
       "   'months',\n",
       "   'old',\n",
       "   '.']),\n",
       " (7,\n",
       "  ['It',\n",
       "   'was',\n",
       "   'in',\n",
       "   'this',\n",
       "   'town',\n",
       "   'that',\n",
       "   'Id',\n",
       "   'been',\n",
       "   'compelled',\n",
       "   'to',\n",
       "   'spend',\n",
       "   'a',\n",
       "   'month',\n",
       "   'every',\n",
       "   'summer',\n",
       "   'until',\n",
       "   'I',\n",
       "   'was',\n",
       "   'fourteen',\n",
       "   '.']),\n",
       " (8,\n",
       "  ['That',\n",
       "   'was',\n",
       "   'the',\n",
       "   'year',\n",
       "   'I',\n",
       "   'finally',\n",
       "   'put',\n",
       "   'my',\n",
       "   'foot',\n",
       "   'down',\n",
       "   ';',\n",
       "   'these',\n",
       "   'past',\n",
       "   'three',\n",
       "   'summers',\n",
       "   ',',\n",
       "   'my',\n",
       "   'dad',\n",
       "   ',',\n",
       "   'Charlie',\n",
       "   ',',\n",
       "   'vacationed',\n",
       "   'with',\n",
       "   'me',\n",
       "   'in',\n",
       "   'California',\n",
       "   'for',\n",
       "   'two',\n",
       "   'weeks',\n",
       "   'instead',\n",
       "   '.'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(sent) for sent in sent_tokens]\n",
    "\n",
    "list(enumerate(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.708148",
     "start_time": "2018-04-10T09:43:06.677745"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tokens_lower = [[word.lower() for word in sent]\n",
    "                 for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Filtering stopwords (and punctuation)\n",
    "\n",
    "**Stopwords** are words that should be stopped at this step because they do not carry much information about the actual meaning of the document. Usually, they are \"common\" words you use. You can find lists of such **stopwords** online, or embedded within the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.742508",
     "start_time": "2018-04-10T09:43:06.709793"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stopwords in english: {'too', 'just', 'doing', 'her', \"couldn't\", \"haven't\", 'those', 'up', 'than', 'who', 'ain', 'off', 'to', 'most', \"weren't\", 'did', 'o', 'on', 'of', 'each', \"don't\", \"hadn't\", \"you'll\", 'has', 'some', 'y', \"hasn't\", \"you've\", 'them', 'or', 'same', 'while', 'for', 'against', \"isn't\", 'any', 'himself', 'yourselves', 'been', 'we', 'it', 'weren', \"shan't\", 'don', \"she's\", 'such', 'an', 'myself', 'once', \"shouldn't\", 'won', 'couldn', 'about', 'didn', 'then', 'a', 'shouldn', 'no', 'hasn', 'can', 'he', 'further', 'but', 'had', 'needn', \"won't\", 'that', 'down', 'i', 'our', 'if', 'isn', 'into', 'until', 'above', \"wouldn't\", 'again', \"mustn't\", 'both', 'what', \"that'll\", 'doesn', 'your', 'whom', 'in', 'hers', 'me', 'at', 'during', 'wouldn', 'when', 'him', 'and', 'below', 'ourselves', 's', 'which', 'mightn', \"it's\", 'with', 'being', 're', 'wasn', 'more', 'was', 'because', 'as', 'should', 'itself', \"you're\", 'having', 'you', 'from', 'how', 'will', 'aren', 'not', 'their', 'does', 'through', 'they', \"wasn't\", 'd', 've', 'my', 'is', 'over', 't', 'themselves', 'other', 'yourself', 'have', 'why', 'haven', 'am', 'are', 'hadn', \"you'd\", 'between', 'now', 'yours', 'his', 'be', 'all', 'before', 'so', \"aren't\", 'theirs', 'after', 'where', \"needn't\", 'few', 'under', 'mustn', 'there', 'were', 'own', 'shan', 'ma', 'the', \"mightn't\", 'only', 'these', 'nor', 'she', 'this', \"doesn't\", 'do', 'herself', 'by', 'm', \"didn't\", 'll', \"should've\", 'very', 'here', 'ours', 'out', 'its'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.773638",
     "start_time": "2018-04-10T09:43:06.744111"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stopwords in english: {'too', 'just', 'neither', 'her', 'than', 'who', 'off', 'to', 'most', 'did', 'on', 'of', 'has', 'some', 'or', 'them', 'get', 'while', 'for', 'every', 'any', 'been', 'we', 'must', 'it', 'ever', 'tis', 'either', 'an', 'else', 'about', 'then', 'a', 'no', 'can', 'he', 'but', 'had', 'least', 'since', 'that', 'i', 'our', 'if', 'however', 'into', 'could', 'yet', 'might', 'what', 'in', 'whom', 'like', 'hers', 'me', 'at', 'dear', 'said', 'when', 'him', 'and', 'wants', 'among', 'almost', 'which', 'say', 'with', 'because', 'should', 'as', 'was', 'may', 'twas', 'from', 'likely', 'how', 'you', 'will', 'their', 'does', 'us', 'they', 'often', 'my', 'is', 'other', 'have', 'why', 'am', 'also', 'are', 'across', 'rather', 'all', 'be', 'his', 'your]', 'let', 'so', 'after', 'where', 'able', 'would', 'there', 'were', 'own', 'the', 'only', 'these', 'she', 'this', 'got', 'do', 'by', 'says', 'its'}\n"
     ]
    }
   ],
   "source": [
    "# list found at http://www.textfixer.com/resources/common-english-words.txt\n",
    "# 'not' has been removed (do you know why ?)\n",
    "\n",
    "stopwords_ = set(\"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(','))\n",
    "\n",
    "print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to filter punctuation tokens: tokens made of punctuation marks. We can find a list of those punctuations in `string.punctuation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.812229",
     "start_time": "2018-04-10T09:43:06.776565"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "--- sentence tokens: ['mother', 'drove', 'airport', 'windows', 'rolled', 'down']\n",
      "--- sentence tokens: ['seventy-five', 'degrees', 'phoenix', 'sky', 'perfect', 'cloudless', 'blue']\n",
      "--- sentence tokens: ['wearing', 'favorite', 'shirt', 'sleeveless', 'white', 'eyelet', 'lace', 'wearing', 'farewell', 'gesture']\n",
      "--- sentence tokens: ['carry-on', 'item', 'parka']\n",
      "--- sentence tokens: ['olympic', 'peninsula', 'northwest', 'washington', 'state', 'small', 'town', 'named', 'forks', 'exists', 'under', 'near-constant', 'cover', 'clouds']\n",
      "--- sentence tokens: ['rains', 'inconsequential', 'town', 'more', 'place', 'united', 'states', 'america']\n",
      "--- sentence tokens: ['town', 'gloomy', 'omnipresent', 'shade', 'mother', 'escaped', 'few', 'months', 'old']\n",
      "--- sentence tokens: ['town', 'id', 'compelled', 'spend', 'month', 'summer', 'until', 'fourteen']\n",
      "--- sentence tokens: ['year', 'finally', 'put', 'foot', 'down', 'past', 'three', 'summers', 'dad', 'charlie', 'vacationed', 'california', 'two', 'weeks', 'instead']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation_ = set(string.punctuation)\n",
    "print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "\n",
    "for sent in tokens_filtered:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stemming and lemmatization\n",
    "\n",
    "**Stemming** means reducing each word to a **stem**. That is, reducing each word in all its diversity to a root common to all its variants. **Lemmatizing** is similar, but is slower and has a greater understanding of the words in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.847750",
     "start_time": "2018-04-10T09:43:06.813833"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- sentence tokens (porter): ['mother', 'drove', 'airport', 'window', 'roll', 'down']\n",
      "--- sentence tokens (snowball): ['mother', 'drove', 'airport', 'window', 'roll', 'down']\n",
      "--- sentence tokens (lemmatize): ['mother', 'drove', 'airport', 'window', 'rolled', 'down']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "sentence_number = 0\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [list(map(stemmer_porter.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (porter): {}\".format(tokens_stemporter[sentence_number]))\n",
    "\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "tokens_stemsnowball = [list(map(stemmer_snowball.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (snowball): {}\".format(tokens_stemsnowball[sentence_number]))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatize = [list(map(lemmatizer.lemmatize, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (lemmatize): {}\".format(tokens_lemmatize[sentence_number]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. N-Grams\n",
    "\n",
    "An n-gram is a sequence of (generally consecutive) words that are treated as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.878775",
     "start_time": "2018-04-10T09:43:06.849349"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 'drove'),\n",
       " ('drove', 'airport'),\n",
       " ('airport', 'window'),\n",
       " ('window', 'roll'),\n",
       " ('roll', 'down')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens_stemsnowball[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:06.917945",
     "start_time": "2018-04-10T09:43:06.880183"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- sentence tokens: ['mother', 'drove', 'airport', 'window', 'roll', 'down', 'mother-drove', 'drove-airport', 'airport-window', 'window-roll', 'roll-down', 'mother-drove-airport', 'drove-airport-window', 'airport-window-roll', 'window-roll-down']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def join_sent_ngrams(input_tokens, n):\n",
    "    # first add the 1-gram tokens\n",
    "    ret_list = list(input_tokens)\n",
    "    \n",
    "    #then for each n\n",
    "    for i in range(2,n+1):\n",
    "        # add each n-grams to the list\n",
    "        ret_list.extend(['-'.join(tgram) for tgram in ngrams(input_tokens, i)])\n",
    "    \n",
    "    return(ret_list)\n",
    "\n",
    "tokens_ngrams = list(map(lambda x : join_sent_ngrams(x, 3), tokens_stemporter))\n",
    "\n",
    "print(\"--- sentence tokens: {}\".format(tokens_ngrams[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Part-of-Speech tagging\n",
    "\n",
    "This is an alternative process that relies on machine learning to tag each word in a sentence with its function. In libraries such as NLTK, there are embedded tools to do that. Tags detected depend on the corpus used for training. In NLTK, the function `nltk.pos_tag()` uses the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.105150",
     "start_time": "2018-04-10T09:43:06.919478"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- sentence tags: [('My', 'PRP$'), ('mother', 'NN'), ('drove', 'VBD'), ('me', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('airport', 'NN'), ('with', 'IN'), ('the', 'DT'), ('windows', 'NNS'), ('rolled', 'VBD'), ('down', 'RB'), ('.', '.')]\n",
      "--- sentence tags: [('It', 'PRP'), ('was', 'VBD'), ('seventy-five', 'JJ'), ('degrees', 'NNS'), ('in', 'IN'), ('Phoenix', 'NNP'), (',', ','), ('the', 'DT'), ('sky', 'NN'), ('a', 'DT'), ('perfect', 'JJ'), (',', ','), ('cloudless', 'JJ'), ('blue', 'NN'), ('.', '.')]\n",
      "--- sentence tags: [('I', 'PRP'), ('was', 'VBD'), ('wearing', 'VBG'), ('my', 'PRP$'), ('favorite', 'JJ'), ('shirt', 'NN'), ('sleeveless', 'NN'), (',', ','), ('white', 'JJ'), ('eyelet', 'NN'), ('lace', 'NN'), (';', ':'), ('I', 'PRP'), ('was', 'VBD'), ('wearing', 'VBG'), ('it', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('farewell', 'NN'), ('gesture', 'NN'), ('.', '.')]\n",
      "--- sentence tags: [('My', 'PRP$'), ('carry-on', 'JJ'), ('item', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('parka', 'NN'), ('.', '.')]\n",
      "--- sentence tags: [('In', 'IN'), ('the', 'DT'), ('Olympic', 'NNP'), ('Peninsula', 'NNP'), ('of', 'IN'), ('northwest', 'JJS'), ('Washington', 'NNP'), ('State', 'NNP'), (',', ','), ('a', 'DT'), ('small', 'JJ'), ('town', 'NN'), ('named', 'VBN'), ('Forks', 'NNP'), ('exists', 'VBZ'), ('under', 'IN'), ('a', 'DT'), ('near-constant', 'JJ'), ('cover', 'NN'), ('of', 'IN'), ('clouds', 'NNS'), ('.', '.')]\n",
      "--- sentence tags: [('It', 'PRP'), ('rains', 'VBZ'), ('on', 'IN'), ('this', 'DT'), ('inconsequential', 'JJ'), ('town', 'NN'), ('more', 'RBR'), ('than', 'IN'), ('any', 'DT'), ('other', 'JJ'), ('place', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('of', 'IN'), ('America', 'NNP'), ('.', '.')]\n",
      "--- sentence tags: [('It', 'PRP'), ('was', 'VBD'), ('from', 'IN'), ('this', 'DT'), ('town', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('gloomy', 'NN'), (',', ','), ('omnipresent', 'NN'), ('shade', 'NN'), ('that', 'IN'), ('my', 'PRP$'), ('mother', 'NN'), ('escaped', 'VBD'), ('with', 'IN'), ('me', 'PRP'), ('when', 'WRB'), ('I', 'PRP'), ('was', 'VBD'), ('only', 'RB'), ('a', 'DT'), ('few', 'JJ'), ('months', 'NNS'), ('old', 'JJ'), ('.', '.')]\n",
      "--- sentence tags: [('It', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('this', 'DT'), ('town', 'NN'), ('that', 'WDT'), ('Id', 'NNP'), ('been', 'VBN'), ('compelled', 'VBN'), ('to', 'TO'), ('spend', 'VB'), ('a', 'DT'), ('month', 'NN'), ('every', 'DT'), ('summer', 'NN'), ('until', 'IN'), ('I', 'PRP'), ('was', 'VBD'), ('fourteen', 'JJ'), ('.', '.')]\n",
      "--- sentence tags: [('That', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('year', 'NN'), ('I', 'PRP'), ('finally', 'RB'), ('put', 'VBD'), ('my', 'PRP$'), ('foot', 'NN'), ('down', 'RP'), (';', ':'), ('these', 'DT'), ('past', 'IN'), ('three', 'CD'), ('summers', 'NNS'), (',', ','), ('my', 'PRP$'), ('dad', 'NN'), (',', ','), ('Charlie', 'NNP'), (',', ','), ('vacationed', 'VBD'), ('with', 'IN'), ('me', 'PRP'), ('in', 'IN'), ('California', 'NNP'), ('for', 'IN'), ('two', 'CD'), ('weeks', 'NNS'), ('instead', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "for sent in sent_tags:\n",
    "    print(\"--- sentence tags: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it isn't perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN')]\n",
      "[('Fruit', 'NNP'), ('flies', 'VBZ'), ('like', 'IN'), ('a', 'DT'), ('banana', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "print(pos_tag('Time flies like an arrow'.split()))\n",
    "print(pos_tag('Fruit flies like a banana'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter verbs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.135456",
     "start_time": "2018-04-10T09:43:07.106679"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- verbs:\n",
      "[('drove', 'VBD'), ('rolled', 'VBD')]\n",
      "--- verbs:\n",
      "[('was', 'VBD')]\n",
      "--- verbs:\n",
      "[('was', 'VBD'), ('wearing', 'VBG'), ('was', 'VBD'), ('wearing', 'VBG')]\n",
      "--- verbs:\n",
      "[('was', 'VBD')]\n",
      "--- verbs:\n",
      "[('named', 'VBN'), ('exists', 'VBZ')]\n",
      "--- verbs:\n",
      "[('rains', 'VBZ')]\n",
      "--- verbs:\n",
      "[('was', 'VBD'), ('escaped', 'VBD'), ('was', 'VBD')]\n",
      "--- verbs:\n",
      "[('was', 'VBD'), ('been', 'VBN'), ('compelled', 'VBN'), ('spend', 'VB'), ('was', 'VBD')]\n",
      "--- verbs:\n",
      "[('was', 'VBD'), ('put', 'VBD'), ('vacationed', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tags:\n",
    "    tags_filtered = [t for t in sent if t[1].startswith('VB')]\n",
    "    print(\"--- verbs:\\n{}\".format(tags_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.175528",
     "start_time": "2018-04-10T09:43:07.136878"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NPB mother/NN)\n",
      "(NPB the/DT airport/NN)\n",
      "(NPB Phoenix/NNP)\n",
      "(NPB the/DT sky/NN)\n",
      "(NPB a/DT perfect/JJ ,/, cloudless/JJ blue/NN)\n",
      "(NPB\n",
      "  favorite/JJ\n",
      "  shirt/NN\n",
      "  sleeveless/NN\n",
      "  ,/,\n",
      "  white/JJ\n",
      "  eyelet/NN\n",
      "  lace/NN)\n",
      "(NPB a/DT farewell/NN gesture/NN)\n",
      "(NPB carry-on/JJ item/NN)\n",
      "(NPB a/DT parka/NN)\n",
      "(NPB Olympic/NNP Peninsula/NNP)\n",
      "(NPB Washington/NNP State/NNP)\n",
      "(NPB a/DT small/JJ town/NN)\n",
      "(NPB Forks/NNP)\n",
      "(NPB a/DT near-constant/JJ cover/NN)\n",
      "(NPB this/DT inconsequential/JJ town/NN)\n",
      "(NPB any/DT other/JJ place/NN)\n",
      "(NPB United/NNP)\n",
      "(NPB America/NNP)\n",
      "(NPB this/DT town/NN)\n",
      "(NPB gloomy/NN ,/, omnipresent/NN shade/NN)\n",
      "(NPB mother/NN)\n",
      "(NPB this/DT town/NN)\n",
      "(NPB Id/NNP)\n",
      "(V2V compelled/VBN to/TO spend/VB)\n",
      "(NPB a/DT month/NN)\n",
      "(NPB every/DT summer/NN)\n",
      "(NPB the/DT year/NN)\n",
      "(NPB foot/NN)\n",
      "(NPB dad/NN)\n",
      "(NPB Charlie/NNP)\n",
      "(NPB California/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NPB: {<DT|PP\\$>?<JJ|NN|,>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "  V2V: {<V.*> <TO> <V.*>}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(sent_tags[1])\n",
    "\n",
    "#print result\n",
    "\n",
    "for sent in sent_tags:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NPB': print(subtree)\n",
    "        if subtree.label() == 'V2V': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Featurization part 2 : Indexing Bag-of-Words into a vector table\n",
    "\n",
    "This Walkthrough will lead us from bag-of-words representations of documents to **vector signatures** (indexes) using the **TF-IDF** formula.\n",
    "\n",
    "The ultimate goal of **indexing** is to create a **vector representation** (signature) for each document. This vector representation will be used for:\n",
    "\n",
    "- mine the features that can caracterize classes of documents (supervised learning using **labels**)\n",
    "- mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "<img src=\"img/pipeline-walkthrough2.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading some input data from the Amazon Reviews\n",
    "\n",
    "To try this indexing walkthrough, we will get 5 reviews from the Amazon Reviews dataset. We will apply a function for extracting bag-of-words representations from these 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.242123",
     "start_time": "2018-04-10T09:43:07.177077"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- review: Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\n",
      "--- bow: ['b', 'filter', 'pop', 'record', 'more', 'crisp', 'lowest', 'price', 'filter', 'amazon', 'same', 'price']\n",
      "\n",
      "--- review: The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\n",
      "--- bow: ['b', 'product', 'doubl', 'better', 'ad', 'bonus', 'screen', 'small', 'hint', 'smell', 'old', 'grape', 'candi', 'reminisc', 'sake', 'pop', 'filter', 'nose', 'dif', 'pop', 'filter', 'expens', 'one', 'aroma', 'mine', 'product', ']']\n",
      "\n",
      "--- review: The primary job of this device is to block the breath that would otherwise produce a popping sound, while allowing your voice to pass through with no noticeable reduction of volume or high frequencies. The double cloth filter blocks the pops and lets the voice through with no coloration. The metal clamp mount attaches to the mike stand secure enough to keep it attached. The goose neck needs a little coaxing to stay where you put it.\n",
      "--- bow: [\"b'the\", 'primari', 'job', 'devic', 'breath', 'pop', 'sound', 'voic', 'notic', 'reduct', 'volum', 'high', 'frequenc', 'doubl', 'cloth', 'filter', 'pop', 'voic', 'color', 'metal', 'clamp', 'mount', 'attach', 'mike', 'secur', 'enough', 'attach', 'goos', 'neck', 'littl', 'coax']\n",
      "\n",
      "--- review: Nice windscreen protects my MXL mic and prevents pops. Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging.\n",
      "--- bow: [\"b'nice\", 'windscreen', 'protect', 'mxl', 'mic', 'prevent', 'pop', 'thing', 'gooseneck', 'abl', 'screen', 'posit', 'care', 'posit', 'clamp']\n",
      "\n",
      "--- review: This pop filter is great. It looks and performs like a studio filter. If you're recording vocals this will eliminate the pops that gets recorded when you sing.\n",
      "--- bow: ['b', 'pop', 'filter', 'great', 'perform', 'studio', 'filter', 'vocal', 'pop']\n"
     ]
    }
   ],
   "source": [
    "import os               # for environ variables in Part 3\n",
    "from nlp_pipeline import extract_bow_from_raw_text\n",
    "import json\n",
    "\n",
    "docs = []\n",
    "with open('./reviews.json', 'r') as data_file:    \n",
    "    for line in data_file:\n",
    "        docs.append(json.loads(line))\n",
    "\n",
    "# extracting bows\n",
    "bows = list(map(lambda row: extract_bow_from_raw_text(row['reviewText']), docs))\n",
    "\n",
    "# displaying bows\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indexing Bag of Words into a Vector Matrix using Term Frequency / Inverse Document Frequency\n",
    "The ultimate goal of indexing is to create a vector representation (signature) for each document. This vector representation will be used for:\n",
    "mine the features that can caracterize classes of documents (supervised learning using labels)\n",
    "mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "To do that, we need:\n",
    "- a fixed number of features\n",
    "- a quantitative value for each feature.\n",
    "\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n",
    "## 1.1 Term Frequency\n",
    "\n",
    "The number of times a term occurs in a specific document:\n",
    "\n",
    "$tf(term,document) = \\frac{\\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document}{\\#\\ of\\ terms\\ in\\ the\\ document|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.280477",
     "start_time": "2018-04-10T09:43:07.243784"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- review: Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\n",
      "--- bow: ['b', 'filter', 'pop', 'record', 'more', 'crisp', 'lowest', 'price', 'filter', 'amazon', 'same', 'price']\n",
      "--- term_occ: Counter({'filter': 2, 'price': 2, 'b': 1, 'pop': 1, 'record': 1, 'more': 1, 'crisp': 1, 'lowest': 1, 'amazon': 1, 'same': 1})\n",
      "--- term_freq: {'b': 0.08333333333333333, 'filter': 0.16666666666666666, 'pop': 0.08333333333333333, 'record': 0.08333333333333333, 'more': 0.08333333333333333, 'crisp': 0.08333333333333333, 'lowest': 0.08333333333333333, 'price': 0.16666666666666666, 'amazon': 0.08333333333333333, 'same': 0.08333333333333333}\n",
      "\n",
      "--- review: The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\n",
      "--- bow: ['b', 'product', 'doubl', 'better', 'ad', 'bonus', 'screen', 'small', 'hint', 'smell', 'old', 'grape', 'candi', 'reminisc', 'sake', 'pop', 'filter', 'nose', 'dif', 'pop', 'filter', 'expens', 'one', 'aroma', 'mine', 'product', ']']\n",
      "--- term_occ: Counter({'product': 2, 'pop': 2, 'filter': 2, 'b': 1, 'doubl': 1, 'better': 1, 'ad': 1, 'bonus': 1, 'screen': 1, 'small': 1, 'hint': 1, 'smell': 1, 'old': 1, 'grape': 1, 'candi': 1, 'reminisc': 1, 'sake': 1, 'nose': 1, 'dif': 1, 'expens': 1, 'one': 1, 'aroma': 1, 'mine': 1, ']': 1})\n",
      "--- term_freq: {'b': 0.037037037037037035, 'product': 0.07407407407407407, 'doubl': 0.037037037037037035, 'better': 0.037037037037037035, 'ad': 0.037037037037037035, 'bonus': 0.037037037037037035, 'screen': 0.037037037037037035, 'small': 0.037037037037037035, 'hint': 0.037037037037037035, 'smell': 0.037037037037037035, 'old': 0.037037037037037035, 'grape': 0.037037037037037035, 'candi': 0.037037037037037035, 'reminisc': 0.037037037037037035, 'sake': 0.037037037037037035, 'pop': 0.07407407407407407, 'filter': 0.07407407407407407, 'nose': 0.037037037037037035, 'dif': 0.037037037037037035, 'expens': 0.037037037037037035, 'one': 0.037037037037037035, 'aroma': 0.037037037037037035, 'mine': 0.037037037037037035, ']': 0.037037037037037035}\n",
      "\n",
      "--- review: The primary job of this device is to block the breath that would otherwise produce a popping sound, while allowing your voice to pass through with no noticeable reduction of volume or high frequencies. The double cloth filter blocks the pops and lets the voice through with no coloration. The metal clamp mount attaches to the mike stand secure enough to keep it attached. The goose neck needs a little coaxing to stay where you put it.\n",
      "--- bow: [\"b'the\", 'primari', 'job', 'devic', 'breath', 'pop', 'sound', 'voic', 'notic', 'reduct', 'volum', 'high', 'frequenc', 'doubl', 'cloth', 'filter', 'pop', 'voic', 'color', 'metal', 'clamp', 'mount', 'attach', 'mike', 'secur', 'enough', 'attach', 'goos', 'neck', 'littl', 'coax']\n",
      "--- term_occ: Counter({'pop': 2, 'voic': 2, 'attach': 2, \"b'the\": 1, 'primari': 1, 'job': 1, 'devic': 1, 'breath': 1, 'sound': 1, 'notic': 1, 'reduct': 1, 'volum': 1, 'high': 1, 'frequenc': 1, 'doubl': 1, 'cloth': 1, 'filter': 1, 'color': 1, 'metal': 1, 'clamp': 1, 'mount': 1, 'mike': 1, 'secur': 1, 'enough': 1, 'goos': 1, 'neck': 1, 'littl': 1, 'coax': 1})\n",
      "--- term_freq: {\"b'the\": 0.03225806451612903, 'primari': 0.03225806451612903, 'job': 0.03225806451612903, 'devic': 0.03225806451612903, 'breath': 0.03225806451612903, 'pop': 0.06451612903225806, 'sound': 0.03225806451612903, 'voic': 0.06451612903225806, 'notic': 0.03225806451612903, 'reduct': 0.03225806451612903, 'volum': 0.03225806451612903, 'high': 0.03225806451612903, 'frequenc': 0.03225806451612903, 'doubl': 0.03225806451612903, 'cloth': 0.03225806451612903, 'filter': 0.03225806451612903, 'color': 0.03225806451612903, 'metal': 0.03225806451612903, 'clamp': 0.03225806451612903, 'mount': 0.03225806451612903, 'attach': 0.06451612903225806, 'mike': 0.03225806451612903, 'secur': 0.03225806451612903, 'enough': 0.03225806451612903, 'goos': 0.03225806451612903, 'neck': 0.03225806451612903, 'littl': 0.03225806451612903, 'coax': 0.03225806451612903}\n",
      "\n",
      "--- review: Nice windscreen protects my MXL mic and prevents pops. Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging.\n",
      "--- bow: [\"b'nice\", 'windscreen', 'protect', 'mxl', 'mic', 'prevent', 'pop', 'thing', 'gooseneck', 'abl', 'screen', 'posit', 'care', 'posit', 'clamp']\n",
      "--- term_occ: Counter({'posit': 2, \"b'nice\": 1, 'windscreen': 1, 'protect': 1, 'mxl': 1, 'mic': 1, 'prevent': 1, 'pop': 1, 'thing': 1, 'gooseneck': 1, 'abl': 1, 'screen': 1, 'care': 1, 'clamp': 1})\n",
      "--- term_freq: {\"b'nice\": 0.06666666666666667, 'windscreen': 0.06666666666666667, 'protect': 0.06666666666666667, 'mxl': 0.06666666666666667, 'mic': 0.06666666666666667, 'prevent': 0.06666666666666667, 'pop': 0.06666666666666667, 'thing': 0.06666666666666667, 'gooseneck': 0.06666666666666667, 'abl': 0.06666666666666667, 'screen': 0.06666666666666667, 'posit': 0.13333333333333333, 'care': 0.06666666666666667, 'clamp': 0.06666666666666667}\n",
      "\n",
      "--- review: This pop filter is great. It looks and performs like a studio filter. If you're recording vocals this will eliminate the pops that gets recorded when you sing.\n",
      "--- bow: ['b', 'pop', 'filter', 'great', 'perform', 'studio', 'filter', 'vocal', 'pop']\n",
      "--- term_occ: Counter({'pop': 2, 'filter': 2, 'b': 1, 'great': 1, 'perform': 1, 'studio': 1, 'vocal': 1})\n",
      "--- term_freq: {'b': 0.1111111111111111, 'pop': 0.2222222222222222, 'filter': 0.2222222222222222, 'great': 0.1111111111111111, 'perform': 0.1111111111111111, 'studio': 0.1111111111111111, 'vocal': 0.1111111111111111}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# term occurence = counting distinct words in each bag\n",
    "term_occ = list(map(lambda bow : Counter(bow), bows))\n",
    "\n",
    "# term frequency = occurences over length of bag\n",
    "term_freq = list()\n",
    "for i in range(len(docs)):\n",
    "    term_freq.append( {k: (v / float(len(bows[i])))\n",
    "                       for k, v in term_occ[i].items()} )\n",
    "\n",
    "# displaying occurences\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- term_occ: {}\".format(term_occ[i]))\n",
    "    print(\"--- term_freq: {}\".format(term_freq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Obtaining document frequencies\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.313506",
     "start_time": "2018-04-10T09:43:07.281893"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- full vocabulary: Counter({'pop': 5, 'filter': 4, 'b': 3, 'doubl': 2, 'screen': 2, 'clamp': 2, 'record': 1, 'price': 1, 'same': 1, 'crisp': 1, 'lowest': 1, 'amazon': 1, 'more': 1, 'reminisc': 1, 'expens': 1, 'better': 1, 'sake': 1, 'nose': 1, 'dif': 1, 'old': 1, ']': 1, 'hint': 1, 'ad': 1, 'mine': 1, 'candi': 1, 'grape': 1, 'bonus': 1, 'smell': 1, 'small': 1, 'product': 1, 'one': 1, 'aroma': 1, 'high': 1, 'enough': 1, 'reduct': 1, 'frequenc': 1, 'littl': 1, 'mount': 1, 'goos': 1, 'breath': 1, 'cloth': 1, 'secur': 1, 'neck': 1, 'devic': 1, 'voic': 1, 'primari': 1, 'coax': 1, 'job': 1, 'color': 1, 'attach': 1, 'notic': 1, \"b'the\": 1, 'metal': 1, 'volum': 1, 'mike': 1, 'sound': 1, 'abl': 1, 'protect': 1, 'prevent': 1, 'thing': 1, 'gooseneck': 1, 'care': 1, 'posit': 1, 'windscreen': 1, 'mxl': 1, 'mic': 1, \"b'nice\": 1, 'great': 1, 'studio': 1, 'perform': 1, 'vocal': 1})\n",
      "\n",
      "--- doc freq: {'record': 0.2, 'pop': 1.0, 'price': 0.2, 'same': 0.2, 'crisp': 0.2, 'lowest': 0.2, 'b': 0.6, 'filter': 0.8, 'amazon': 0.2, 'more': 0.2, 'reminisc': 0.2, 'expens': 0.2, 'better': 0.2, 'sake': 0.2, 'nose': 0.2, 'doubl': 0.4, 'dif': 0.2, 'old': 0.2, ']': 0.2, 'screen': 0.4, 'hint': 0.2, 'ad': 0.2, 'mine': 0.2, 'candi': 0.2, 'grape': 0.2, 'bonus': 0.2, 'smell': 0.2, 'small': 0.2, 'product': 0.2, 'one': 0.2, 'aroma': 0.2, 'high': 0.2, 'enough': 0.2, 'reduct': 0.2, 'frequenc': 0.2, 'littl': 0.2, 'mount': 0.2, 'goos': 0.2, 'breath': 0.2, 'cloth': 0.2, 'secur': 0.2, 'neck': 0.2, 'devic': 0.2, 'voic': 0.2, 'primari': 0.2, 'coax': 0.2, 'job': 0.2, 'clamp': 0.4, 'color': 0.2, 'attach': 0.2, 'notic': 0.2, \"b'the\": 0.2, 'metal': 0.2, 'volum': 0.2, 'mike': 0.2, 'sound': 0.2, 'abl': 0.2, 'protect': 0.2, 'prevent': 0.2, 'thing': 0.2, 'gooseneck': 0.2, 'care': 0.2, 'posit': 0.2, 'windscreen': 0.2, 'mxl': 0.2, 'mic': 0.2, \"b'nice\": 0.2, 'great': 0.2, 'studio': 0.2, 'perform': 0.2, 'vocal': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# document occurence = number of documents having this word\n",
    "# term frequency = occurences over length of bag\n",
    "\n",
    "doc_occ = Counter( [word for bow in bows for word in set(bow)] )\n",
    "\n",
    "# document frequency = occurences over length of corpus\n",
    "doc_freq = {k: (v / float(len(docs)))\n",
    "            for k, v in doc_occ.items()}\n",
    "\n",
    "# displaying vocabulary\n",
    "print(\"\\n--- full vocabulary: {}\".format(doc_occ))\n",
    "print(\"\\n--- doc freq: {}\".format(doc_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the vocabulary for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.346712",
     "start_time": "2018-04-10T09:43:07.315248"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- vocabulary (len=6): ['pop', 'b', 'filter', 'doubl', 'screen', 'clamp']\n"
     ]
    }
   ],
   "source": [
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "min_df = 0.3\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n",
    "\n",
    "# print vocabulary\n",
    "print (\"-- vocabulary (len={}): {}\".format(len(vocabulary),vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 the TFIDF vector\n",
    "\n",
    "Words might show up a lot in individual documents, but their relevace is less important if they're in every document! We need to take into account words that show up everywhere and reduce their relative importance. The document frequency does exactly that:\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.388629",
     "start_time": "2018-04-10T09:43:07.348353"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- review: Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\n",
      "--- bow: ['b', 'filter', 'pop', 'record', 'more', 'crisp', 'lowest', 'price', 'filter', 'amazon', 'same', 'price']\n",
      "--- tfidf vector: [ 0.05776227  0.08173577  0.13515504  0.          0.          0.        ]\n",
      "--- tfidf sorted: [('filter', 0.13515503603605478), ('b', 0.081735771084310516), ('pop', 0.057762265046662105), ('doubl', 0.0), ('screen', 0.0), ('clamp', 0.0)]\n",
      "\n",
      "--- review: The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\n",
      "--- bow: ['b', 'product', 'doubl', 'better', 'ad', 'bonus', 'screen', 'small', 'hint', 'smell', 'old', 'grape', 'candi', 'reminisc', 'sake', 'pop', 'filter', 'nose', 'dif', 'pop', 'filter', 'expens', 'one', 'aroma', 'mine', 'product', ']']\n",
      "--- tfidf vector: [ 0.05134424  0.03632701  0.0600689   0.04639863  0.04639863  0.        ]\n",
      "--- tfidf sorted: [('filter', 0.060068904904913241), ('pop', 0.051344235597032981), ('doubl', 0.046398628462791407), ('screen', 0.046398628462791407), ('b', 0.036327009370804679), ('clamp', 0.0)]\n",
      "\n",
      "--- review: The primary job of this device is to block the breath that would otherwise produce a popping sound, while allowing your voice to pass through with no noticeable reduction of volume or high frequencies. The double cloth filter blocks the pops and lets the voice through with no coloration. The metal clamp mount attaches to the mike stand secure enough to keep it attached. The goose neck needs a little coaxing to stay where you put it.\n",
      "--- bow: [\"b'the\", 'primari', 'job', 'devic', 'breath', 'pop', 'sound', 'voic', 'notic', 'reduct', 'volum', 'high', 'frequenc', 'doubl', 'cloth', 'filter', 'pop', 'voic', 'color', 'metal', 'clamp', 'mount', 'attach', 'mike', 'secur', 'enough', 'attach', 'goos', 'neck', 'littl', 'coax']\n",
      "--- tfidf vector: [ 0.04471917  0.          0.02615904  0.04041171  0.          0.04041171]\n",
      "--- tfidf sorted: [('pop', 0.044719172939351307), ('doubl', 0.040411708661140903), ('clamp', 0.040411708661140903), ('filter', 0.026159039232784797), ('b', 0.0), ('screen', 0.0)]\n",
      "\n",
      "--- review: Nice windscreen protects my MXL mic and prevents pops. Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging.\n",
      "--- bow: [\"b'nice\", 'windscreen', 'protect', 'mxl', 'mic', 'prevent', 'pop', 'thing', 'gooseneck', 'abl', 'screen', 'posit', 'care', 'posit', 'clamp']\n",
      "--- tfidf vector: [ 0.04620981  0.          0.          0.          0.08351753  0.08351753]\n",
      "--- tfidf sorted: [('screen', 0.083517531233024536), ('clamp', 0.083517531233024536), ('pop', 0.046209812037329684), ('b', 0.0), ('filter', 0.0), ('doubl', 0.0)]\n",
      "\n",
      "--- review: This pop filter is great. It looks and performs like a studio filter. If you're recording vocals this will eliminate the pops that gets recorded when you sing.\n",
      "--- bow: ['b', 'pop', 'filter', 'great', 'perform', 'studio', 'filter', 'vocal', 'pop']\n",
      "--- tfidf vector: [ 0.15403271  0.10898103  0.18020671  0.          0.          0.        ]\n",
      "--- tfidf sorted: [('filter', 0.18020671471473973), ('pop', 0.15403270679109896), ('b', 0.10898102811241403), ('doubl', 0.0), ('screen', 0.0), ('clamp', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a dense matrix of vectors for each document\n",
    "# each vector has the length of the vocabulary\n",
    "vectors = np.zeros((len(docs),len(vocabulary)))\n",
    "\n",
    "# fill these vectors with tf-idf values\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        term     = vocabulary[j]\n",
    "        term_tf  = term_freq[i].get(term, 0.0)   # 0.0 if term not found in doc\n",
    "        term_idf = np.log(1 + 1 / doc_freq[term]) # smooth formula\n",
    "        vectors[i,j] = term_tf * term_idf\n",
    "\n",
    "# displaying results\n",
    "for i in range(len(docs)):\n",
    "    print(\"\\n--- review: {}\".format(docs[i]['reviewText']))\n",
    "    print(\"--- bow: {}\".format(bows[i]))\n",
    "    print(\"--- tfidf vector: {}\".format( vectors[i] ) )\n",
    "    print(\"--- tfidf sorted: {}\".format( \n",
    "            sorted( zip(vocabulary,vectors[i]), key=lambda x:-x[1] )\n",
    "         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.418751",
     "start_time": "2018-04-10T09:43:07.390154"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [row['reviewText'] for row in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.463389",
     "start_time": "2018-04-10T09:43:07.420683"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'about', 'added', 'affordable', 'after', 'allowing', 'amazon', 'an', 'and', 'are', 'aroma', 'arrived', 'as', 'attached', 'attaches', 'avoid', 'better', 'block', 'blocks', 'bonus', 'breath', 'but', 'buy', 'candy', 'cannot', 'careful', 'carries', 'clamp', 'cloth', 'coaxing', 'coloration', 'come', 'crisp', 'despite', 'device', 'did', 'dif', 'does', 'double', 'eliminate', 'enough', 'even', 'exactly', 'expected', 'expensive', 'filter', 'filters', 'for', 'frequencies', 'gets', 'goose', 'gooseneck', 'grape', 'great', 'had', 'here', 'high', 'hint', 'hold', 'honestly', 'if', 'in', 'is', 'it', 'job', 'just', 'keep', 'lets', 'like', 'little', 'looks', 'lowest', 'marginally', 'may', 'metal', 'mic', 'might', 'mike', 'mine', 'more', 'mount', 'much', 'mxl', 'my', 'neck', 'needed', 'needs', 'next', 'nice', 'no', 'nose', 'not', 'noticeable', 'now', 'of', 'old', 'on', 'one', 'ones', 'only', 'or', 'otherwise', 'out', 'pass', 'performs', 'pleasing', 'pop', 'popping', 'pops', 'position', 'positioning', 'prevents', 'prices', 'pricing', 'primary', 'produce', 'product', 'protects', 'put', 'putting', 'quite', 're', 'realized', 'recorded', 'recording', 'recordings', 'reduction', 'reminiscent', 'requires', 'sagging', 'sake', 'same', 'screen', 'screened', 'screens', 'secure', 'should', 'sing', 'small', 'smell', 'smelling', 'so', 'sound', 'sounds', 'stand', 'stay', 'stop', 'studio', 'supposed', 'than', 'that', 'the', 'their', 'they', 'thing', 'this', 'through', 'to', 'until', 'used', 'vocals', 'voice', 'volume', 'was', 'well', 'what', 'when', 'where', 'while', 'will', 'windscreen', 'with', 'work', 'would', 'write', 'you', 'your']\n",
      "[[0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      "  1 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 4 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 1 0 0 1 0 2 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 2 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      "  1 0 0 3 1 1 0 0 0 2 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 1 1 1 0 0 2 3 0 1 1 4 0 0 0 1 0 0 1 0 0 2 1 1 0 1 0 0 0 0 1 0 0 0 2 1\n",
      "  1 1 0 0 2 1 1 1 2 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 6 0 1 0 0 1 0 0 0 0 1\n",
      "  0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 3 1 0 1 1 0 0 0 0 0 0 1 2 0 0 0 0\n",
      "  0 0 0 0 0 2 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 2 0 0 0 0 1 0\n",
      "  0 1 0 5 0 0 0 2 0 2 1 1 0 0 0 2 1 0 0 0 0 1 0 1 1 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
      "  0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 2 1 0 1 1 0 1 0 0 0 0\n",
      "  1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 2 0 0 1 0 2 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0\n",
      "  0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
      "  0 0 1 8 0 0 0 1 2 5 0 0 0 2 1 0 0 0 0 1 1 0 0 2 0 1 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 2 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 1 1 1\n",
      "  1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 3 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 2 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 1 1 0 0 0 2 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer()\n",
    "\n",
    "document_tf_matrix = tf.fit_transform(corpus).todense()\n",
    "\n",
    "print(sorted(tf.vocabulary_))\n",
    "print(document_tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.493574",
     "start_time": "2018-04-10T09:43:07.464848"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'about', 'added', 'affordable', 'after', 'allowing', 'amazon', 'an', 'and', 'are', 'aroma', 'arrived', 'as', 'attached', 'attaches', 'avoid', 'better', 'block', 'blocks', 'bonus', 'breath', 'but', 'buy', 'candy', 'cannot', 'careful', 'carries', 'clamp', 'cloth', 'coaxing', 'coloration', 'come', 'crisp', 'despite', 'device', 'did', 'dif', 'does', 'double', 'eliminate', 'enough', 'even', 'exactly', 'expected', 'expensive', 'filter', 'filters', 'for', 'frequencies', 'gets', 'goose', 'gooseneck', 'grape', 'great', 'had', 'here', 'high', 'hint', 'hold', 'honestly', 'if', 'in', 'is', 'it', 'job', 'just', 'keep', 'lets', 'like', 'little', 'looks', 'lowest', 'marginally', 'may', 'metal', 'mic', 'might', 'mike', 'mine', 'more', 'mount', 'much', 'mxl', 'my', 'neck', 'needed', 'needs', 'next', 'nice', 'no', 'nose', 'not', 'noticeable', 'now', 'of', 'old', 'on', 'one', 'ones', 'only', 'or', 'otherwise', 'out', 'pass', 'performs', 'pleasing', 'pop', 'popping', 'pops', 'position', 'positioning', 'prevents', 'prices', 'pricing', 'primary', 'produce', 'product', 'protects', 'put', 'putting', 'quite', 're', 'realized', 'recorded', 'recording', 'recordings', 'reduction', 'reminiscent', 'requires', 'sagging', 'sake', 'same', 'screen', 'screened', 'screens', 'secure', 'should', 'sing', 'small', 'smell', 'smelling', 'so', 'sound', 'sounds', 'stand', 'stay', 'stop', 'studio', 'supposed', 'than', 'that', 'the', 'their', 'they', 'thing', 'this', 'through', 'to', 'until', 'used', 'vocals', 'voice', 'volume', 'was', 'well', 'what', 'when', 'where', 'while', 'will', 'windscreen', 'with', 'work', 'would', 'write', 'you', 'your']\n",
      "[1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.22314355131420976, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.0, 0.22314355131420976, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 0.22314355131420976, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 0.0, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003, 0.22314355131420976, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 1.6094379124341003, 0.9162907318741551, 1.6094379124341003, 0.9162907318741551, 0.9162907318741551, 1.6094379124341003, 1.6094379124341003, 0.5108256237659907, 1.6094379124341003]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def idf(frequency_matrix):\n",
    "    df =  float(len(document_tf_matrix)) / sum(frequency_matrix > 0)\n",
    "    return [log(i) for i in df.getA()[0]]\n",
    "print(sorted(tf.vocabulary_))\n",
    "print(idf(document_tf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T09:43:07.546190",
     "start_time": "2018-04-10T09:43:07.495291"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'about', 'added', 'affordable', 'after', 'allowing', 'amazon', 'an', 'and', 'are', 'aroma', 'arrived', 'as', 'attached', 'attaches', 'avoid', 'better', 'block', 'blocks', 'bonus', 'breath', 'but', 'buy', 'candy', 'cannot', 'careful', 'carries', 'clamp', 'cloth', 'coaxing', 'coloration', 'come', 'crisp', 'despite', 'device', 'did', 'dif', 'does', 'double', 'eliminate', 'enough', 'even', 'exactly', 'expected', 'expensive', 'filter', 'filters', 'for', 'frequencies', 'gets', 'goose', 'gooseneck', 'grape', 'great', 'had', 'here', 'high', 'hint', 'hold', 'honestly', 'if', 'in', 'is', 'it', 'job', 'just', 'keep', 'lets', 'like', 'little', 'looks', 'lowest', 'marginally', 'may', 'metal', 'mic', 'might', 'mike', 'mine', 'more', 'mount', 'much', 'mxl', 'my', 'neck', 'needed', 'needs', 'next', 'nice', 'no', 'nose', 'not', 'noticeable', 'now', 'of', 'old', 'on', 'one', 'ones', 'only', 'or', 'otherwise', 'out', 'pass', 'performs', 'pleasing', 'pop', 'popping', 'pops', 'position', 'positioning', 'prevents', 'prices', 'pricing', 'primary', 'produce', 'product', 'protects', 'put', 'putting', 'quite', 're', 'realized', 'recorded', 'recording', 'recordings', 'reduction', 'reminiscent', 'requires', 'sagging', 'sake', 'same', 'screen', 'screened', 'screens', 'secure', 'should', 'sing', 'small', 'smell', 'smelling', 'so', 'sound', 'sounds', 'stand', 'stay', 'stop', 'studio', 'supposed', 'than', 'that', 'the', 'their', 'they', 'thing', 'this', 'through', 'to', 'until', 'used', 'vocals', 'voice', 'volume', 'was', 'well', 'what', 'when', 'where', 'while', 'will', 'windscreen', 'with', 'work', 'would', 'write', 'you', 'your']\n",
      "[[ 0.          0.14280355  0.          0.          0.          0.\n",
      "   0.14280355  0.          0.          0.14280355  0.          0.\n",
      "   0.11521301  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.14280355  0.11521301  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.14280355  0.14280355  0.          0.          0.          0.11521301\n",
      "   0.          0.          0.          0.          0.11521301  0.          0.\n",
      "   0.          0.2856071   0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.14280355  0.          0.          0.\n",
      "   0.14280355  0.          0.          0.06804666  0.32181212  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.14280355\n",
      "   0.          0.          0.          0.          0.14280355  0.          0.\n",
      "   0.14280355  0.          0.2856071   0.          0.0956372   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.11521301\n",
      "   0.          0.14280355  0.08045303  0.          0.14280355  0.11521301\n",
      "   0.          0.          0.          0.          0.14280355  0.          0.\n",
      "   0.          0.1912744   0.          0.          0.          0.          0.\n",
      "   0.14280355  0.14280355  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.14280355  0.          0.          0.          0.          0.\n",
      "   0.14280355  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.11521301  0.          0.14280355\n",
      "   0.          0.          0.          0.          0.14280355  0.          0.\n",
      "   0.20413997  0.14280355  0.14280355  0.          0.          0.\n",
      "   0.16090606  0.          0.          0.          0.          0.          0.\n",
      "   0.11521301  0.14280355  0.          0.          0.          0.          0.\n",
      "   0.          0.11521301  0.          0.14280355  0.          0.        ]\n",
      " [ 0.          0.          0.09309924  0.09309924  0.09309924  0.          0.\n",
      "   0.18619848  0.15735146  0.          0.09309924  0.09309924  0.30044752\n",
      "   0.          0.          0.          0.09309924  0.          0.\n",
      "   0.09309924  0.          0.          0.15022376  0.09309924  0.09309924\n",
      "   0.          0.09309924  0.          0.          0.          0.\n",
      "   0.09309924  0.          0.          0.          0.18619848  0.09309924\n",
      "   0.07511188  0.07511188  0.          0.          0.18619848  0.07511188\n",
      "   0.09309924  0.09309924  0.12469929  0.          0.09309924  0.          0.\n",
      "   0.          0.          0.09309924  0.          0.09309924  0.          0.\n",
      "   0.09309924  0.          0.          0.          0.          0.04436229\n",
      "   0.31470293  0.          0.09309924  0.          0.          0.07511188\n",
      "   0.          0.          0.          0.          0.09309924  0.          0.\n",
      "   0.          0.          0.09309924  0.          0.          0.          0.\n",
      "   0.06234965  0.          0.09309924  0.          0.09309924  0.          0.\n",
      "   0.09309924  0.07511188  0.          0.          0.15735146  0.09309924\n",
      "   0.          0.07511188  0.09309924  0.          0.          0.          0.\n",
      "   0.          0.          0.09309924  0.12469929  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.18619848  0.          0.          0.09309924  0.09309924  0.\n",
      "   0.09309924  0.          0.07511188  0.          0.          0.09309924\n",
      "   0.          0.          0.09309924  0.          0.          0.09309924\n",
      "   0.09309924  0.          0.09309924  0.          0.09309924  0.09309924\n",
      "   0.09309924  0.15022376  0.          0.          0.          0.\n",
      "   0.09309924  0.          0.          0.09309924  0.          0.22181143\n",
      "   0.          0.          0.          0.12469929  0.          0.10490098\n",
      "   0.09309924  0.09309924  0.          0.          0.          0.18619848\n",
      "   0.07511188  0.          0.          0.          0.          0.07511188\n",
      "   0.          0.07511188  0.07511188  0.          0.          0.06234965\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.10865818\n",
      "   0.          0.          0.06121612  0.          0.          0.          0.\n",
      "   0.10865818  0.10865818  0.          0.          0.10865818  0.10865818\n",
      "   0.          0.10865818  0.          0.          0.          0.          0.\n",
      "   0.          0.08766473  0.10865818  0.10865818  0.10865818  0.          0.\n",
      "   0.          0.10865818  0.          0.          0.          0.08766473\n",
      "   0.          0.10865818  0.          0.          0.          0.\n",
      "   0.07276965  0.          0.          0.10865818  0.          0.10865818\n",
      "   0.          0.          0.          0.          0.          0.10865818\n",
      "   0.          0.          0.          0.          0.          0.0517762\n",
      "   0.12243225  0.10865818  0.          0.10865818  0.10865818  0.\n",
      "   0.10865818  0.          0.          0.          0.          0.10865818\n",
      "   0.          0.          0.10865818  0.          0.          0.10865818\n",
      "   0.          0.          0.          0.10865818  0.          0.10865818\n",
      "   0.          0.          0.21731636  0.          0.          0.10865818\n",
      "   0.          0.12243225  0.          0.          0.          0.          0.\n",
      "   0.10865818  0.10865818  0.          0.10865818  0.          0.          0.\n",
      "   0.10865818  0.07276965  0.          0.          0.          0.          0.\n",
      "   0.10865818  0.10865818  0.          0.          0.10865818  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.10865818\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.10865818  0.          0.          0.          0.          0.\n",
      "   0.          0.10865818  0.          0.10865818  0.10865818  0.          0.\n",
      "   0.          0.          0.07276965  0.41420964  0.          0.          0.\n",
      "   0.07276965  0.21731636  0.30608061  0.          0.          0.\n",
      "   0.21731636  0.10865818  0.          0.          0.          0.\n",
      "   0.10865818  0.10865818  0.          0.          0.17532947  0.\n",
      "   0.10865818  0.          0.07276965  0.10865818]\n",
      " [ 0.18017345  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.20301316  0.          0.          0.          0.          0.\n",
      "   0.          0.18017345  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.18017345  0.\n",
      "   0.1453628   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.18017345  0.          0.          0.\n",
      "   0.          0.          0.          0.18017345  0.          0.\n",
      "   0.18017345  0.17170723  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.18017345  0.          0.\n",
      "   0.18017345  0.          0.          0.          0.          0.          0.\n",
      "   0.18017345  0.12066426  0.          0.          0.          0.\n",
      "   0.18017345  0.          0.          0.          0.          0.\n",
      "   0.10150658  0.          0.          0.          0.          0.3603469\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.12066426  0.18017345  0.18017345  0.18017345  0.          0.\n",
      "   0.          0.          0.          0.18017345  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.18017345  0.18017345  0.          0.          0.18017345  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.12066426  0.25756084  0.          0.          0.18017345\n",
      "   0.          0.          0.20301316  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.18017345  0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.12074815  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.2143271   0.          0.\n",
      "   0.          0.          0.          0.28707471  0.          0.          0.\n",
      "   0.2143271   0.          0.          0.          0.2143271   0.          0.\n",
      "   0.          0.          0.          0.          0.2143271   0.\n",
      "   0.10212801  0.12074815  0.          0.          0.          0.\n",
      "   0.17291775  0.          0.2143271   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.2143271   0.          0.14353736  0.          0.14353736\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.2143271\n",
      "   0.          0.2143271   0.17291775  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.2143271   0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.2143271   0.          0.\n",
      "   0.14353736  0.10212801  0.          0.          0.          0.28707471\n",
      "   0.          0.          0.          0.          0.2143271   0.          0.\n",
      "   0.          0.          0.          0.2143271   0.          0.\n",
      "   0.17291775  0.          0.          0.          0.          0.\n",
      "   0.28707471  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "document_tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(sorted(tfidf.vocabulary_))\n",
    "print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Comparing two documents / Similarity Measures\n",
    "\n",
    "## 3.1 Euclidean distance\n",
    "\n",
    "We could try the Euclidean distance $||\\vec{x}-\\vec{y}||$  \n",
    "What problems would we encounter with this? \n",
    "\n",
    "The euclidean distance goes up with the length of a document. Intuitively, duplicating each word in our bag of words generates a vector that points in exactly the same direction, however, the euclidean distance goes up. One solution is to normalize vectors before calculating the euclidean distance. Now increasing the length of a document does not change the Euclidean distance unless the direction of the term frequency vector changes. \n",
    "\n",
    "## 3.2 Cosine Similarity\n",
    "Recall that for two vector $\\vec{x}$ and $\\vec{y}$ that $\\vec{x} \\cdot \\vec{y} = ||\\vec{x}|| ||\\vec{y}|| \\cos{\\theta}$. And so,\n",
    "\n",
    "$\\frac{\\vec{x} \\cdot \\vec{y} }{||\\vec{x}|| ||\\vec{y}||} = \\cos{\\theta}$\n",
    "\n",
    "$\\theta$ can only range from 0 to 90 degrees, because tf-idf vectors are non-negative. Therefore cos $\\theta$ ranges from 0 to 1. Documents that are exactly identical will have $\\cos{\\theta} = 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
