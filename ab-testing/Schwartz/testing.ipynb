{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\textit{testing}$$\n",
    "$$\\text{Schwartz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Significance testing_ is largely the product of _Karl Pearson_ (1857--1936), _William Sealy Gosset_ (1876--1937), and _Ronald Fisher_ (1890--1962), although evidence of its use dates back to _Pierre-Simon Laplace_ (1749--1827) in the 1770's. _Pearson_ created the notion of a _p-value_ and (Pearson's) _chi-squared test_ and founded the world's first statistics department at University College London in 1911. _Gosset_ developed and penned the _t-distribution_ and _t-test_ under the \n",
    "pseudonym _Student_ due to the objections of his employer -- the original Guinness Brewery in Dublin, Ireland -- regarding publication of internal practices.  And _Fisher_ created _Analysis of Variance_ and popularized the use of a _null hypothesis_ for the so-called _significance test_. In addition to being regarded as the father of modern statistical science and experimental design, _Fisher_ also made significant contributions to agricultural biology and genetics.  Indeed,  Richard Dawkins named him \"the greatest biologist since Darwin\". \n",
    "\n",
    "_Hypothesis testing_ was developed by _Jerzy Neyman_ (1894 -- 1981) and _Egon Pearson_ (1895--1980, son of Karl Pearson). Building on these ideas, Neyman later introduced _Confidence Intervals_ into the statistics landscape.   At the time of the publication of their work on hypothesis testing in 1933, Neyman and Pearson (along with Fisher) were faculty members at the University College London in the department of statistics (founded by the older Pearson).   While Fisher as a result of his agricultural background emphasized rigorous experimental design and methods to extract a result from few samples assuming Gaussian distributions, Neyman (who teamed with the younger Pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions. \n",
    "\n",
    "Initially a _Bayesian_, Fisher sought to provide a more \"objective\" approach to inference. The significance testing he developed did not use the notion of an alternative hypothesis -- only a null hypothesis -- and hence did not involve the notion of _Type II error_. Fisher's interpretation of p-values was informal: p-values were only meant to provide guidance for potential future experiments.  Neyman and Pearson on the other hand formalized hypothesis testing with _Type I/II errors_ and developed a procedure to choose between competing hypotheses. They considered their formulation to be an improved and more objective generalization of significance testing as it provided a decision making tool to determine researcher behavior without requiring any inductive inference on the part of the researcher.  \n",
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/Karl_Pearson.jpg\" width=\"180px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/William_Sealy_Gosset.jpg\" width=\"188px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/ronald-fisher-5.jpg\" width=\"199px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/neyman.jpg\" width=\"162px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/pearson.jpg\" width=\"201px\" align=\"left\">\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "K. Pearson\n",
    "</td>\n",
    "<td>\n",
    "W. Gosset\n",
    "</td>\n",
    "<td>\n",
    "R. Fisher\n",
    "</td>\n",
    "<td>\n",
    "J. Neyman\n",
    "</td>\n",
    "<td>\n",
    "E. Pearson\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fisher and Neyman/Pearson clashed bitterly, and often.  As they all shared the same building at the University College London they had ample opportunity to cross paths (and swords -- although only Fisher was ever knighted -- and not until many years later -- and Neyman was, after all, Polish, not English).  They disagreed about the proper role of models in statistical inference. Fisher thought the Neyman/Pearson approach was not applicable to scientific research because (1) initial assumptions about the null hypothesis are often discovered to be questionable as unexpected sources of error appear over the course of the experiment and (2) rigid reject/accept decisions based on models formulated before data is collected are incompatible with the real-world scenario faced by scientists and attempts to apply such formulations to scientific research would lead to mass confusion (as it has). \n",
    "\n",
    "In 1938 Neyman left University College London and moved to the University of California, Berkeley. This put much of the planetary diameter between both his partnership with Pearson and his dispute with Fisher. A further respite in the debate was provided by World War II.  Nonetheless, the disagreement between Fisher and Neyman only terminated (unresolved after 27 years) with Fisher's death in 1962.  Neyman wrote a well-regarded eulogy of Fisher upon his death.  And some of Neyman's later publications reported p-values and significance levels.\n",
    "\n",
    "Afterword:\n",
    "\n",
    "In an apparent effort to provide a \"non-controversial\" theory (as well as likely from confusion and misunderstanding of the topic, _per se_) the modern version of hypothesis testing used today is an inconsistent hybrid of the \"Fisher versus Neyman/Pearson\" formulations developed in the early 20th century.  Rather than comparing two directly competing realistic hypotheses, one of the hypotheses is made to be a \"no effect null hypothesis\" so (despite great conceptual differences and caveats) p-values can be interpreted from both the Fisher and the Neyman/Pearson perspectives.  Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the hypothesis testing used today has more similarities with Fisher's method than theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- Hypothesis Testing\n",
    "    - Null Hypothesis\n",
    "    - Test Statistic\n",
    "    - Significance Level/Rejection Region\n",
    "        - One/Two-Tailed\n",
    "    - P-Values (never get this wrong...)\n",
    "    - Relationship to Confidence Intervals\n",
    "- Multiple Testing Adjustment\n",
    "    - Bonferroni\n",
    "    - False Discovery Rate (FDR)\n",
    "- Alternative Hypothesis\n",
    "    - Power\n",
    "- Type I/II Errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Common Tests (and when to apply them)\n",
    "    - Z/T-Test\n",
    "    - Pearson's Chi-Squared Test\n",
    "    - Fisher's Exact Test\n",
    "    - Other common Non-Parametric Tests \n",
    "    - Kolmogorov-Smirnov (K-S) Test\n",
    "    - F-Test\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Workflow\n",
    "\n",
    "0. Specify the _Population(s)_\n",
    "1. Specify a _Null Hypothesis_ about _Population Parameters_ that we are interested evaluating\n",
    "2. Identify a _Test Statistic_ that is _informative about the Null Hypotheis_ and whose _distribution is known when the null hypothesis is true_ \n",
    "3. Specify a _Rejection Region_ (i.e., _Significance Level_) for the \"Distribution of the Test Statistic under the Null Hypothesis\" which includes hypothetical values of the _Test Statistic_ for which we would _Reject the Null Hypothesis_\n",
    "4. Take a (hopefully _Representative_) _Sample_ from the _Population_ on which to base your inference and calculate the _Test Statistic_\n",
    "5. Determine if the _Test Statistic_ lies in the _Rejection region, and\n",
    "    \n",
    "     1. if it does, _Reject_ the _Null Hypothesis_\n",
    "     2. if it does not, _Fail to Reject_ the _Null Hypothesis_\n",
    "\n",
    "\n",
    "\n",
    "*Note: The _Null Hypothesis_ $H_0$ is often simply a placeholder or straw man which we fully expect to reject.  Confidence intervals give us a much better tool for getting a feel for actually making *inferences* about what a population parameter might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "- Suppose we'd like to know _Spurs_ fans and _Rockets_ fans are represented in the same proportion here in Austin, TX.\n",
    "- Suppose we'd like to independently verify that dogs are indeed heavier than cats.\n",
    "- [A/B Testing] Suppose we'd like to assess which of two different versions of a website are more effective for business outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Hints\n",
    "\n",
    "# $$X_i = Bernoulli(p_X)$$ \n",
    "\n",
    "# $$Var[X_i] = p_X(1-p_X)$$\n",
    "\n",
    "# $$X_i, i=1,\\cdots,n_X, \\;\\; Y_i, i=1,\\cdots,n_Y$$\n",
    "\n",
    "# $$H_0: p_X = p_Y = p$$\n",
    "\n",
    "# $$\\bar X_i - \\bar Y_i \\sim ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Level $\\alpha$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\Large \\text{Pr}(\\text{rejecting $H_0$} | H_0 \\text{ is true}) = \\Large \\text{Pr}(\\text{Type I error})\n",
    "\\end{align*}\n",
    "\n",
    "# P-Value $p$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\Large \\text{Pr}(\\text{Observing a test statistic as or more extreme than yours} | H_0\\text{ is true})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/tests_1.png\" width=\"1000px\" align=\"center\">\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<img src=\"stuff/tests_2.png\" width=\"1000px\" align=\"center\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Value blunders for which _I'll never forgive you_ (and which will _haunt_ you for the _rest of your natural and unnatural life_)\n",
    "\n",
    "<br>\n",
    "X: A p-value _is not_ the probability $H_0$ is False\n",
    "\n",
    "$\\checkmark$: $H_0$ is True, or it is not -- there is no ''sometimes/probability''\n",
    "\n",
    "<br>\n",
    "X: A p-value _is not_ the probability of incorrectly rejecting $H_0$\n",
    "\n",
    "$\\checkmark$: Significance level $\\alpha$ is the probability of wrongly rejecting $H_0$\n",
    "\n",
    "<br>\n",
    "X: A p-value _is not anything else except_ \n",
    "\n",
    "\\begin{align*}\n",
    "\\Large \\text{Pr}(\\text{Observing a test statistic as or more extreme than yours} | H_0\\text{ is true})\n",
    "\\end{align*}\n",
    "\n",
    "$\\checkmark$: A p-value is, _at all times, ever only and EXACTLY ONLY_ \n",
    "\n",
    "\\begin{align*}\n",
    "\\Large \\text{Pr}(\\text{Observing a test statistic as or more extreme than yours} | H_0\\text{ is true})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals (via the pivot) and Hypothesis Testing\n",
    "\n",
    "- A $100(1-\\alpha)\\%$ confidence interval _does not contain_ $\\mu_0$ $\\iff$ A two-sided test _rejects_ $H_0$ at the $\\alpha$-significance level\n",
    "\n",
    "- A $100(1-\\alpha)\\%$ confidence interval _contains_ $\\mu_0$ $\\iff$ A two-sided test _fails to reject_ $H_0$ at the $\\alpha$-significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/conf1.png\" width=\"500px\" align=\"center\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If $H_0$ is true, then \n",
    "\\begin{align*}\n",
    "\\Pr\\left( -t_{n-1}^{\\alpha/2} < \\frac{\\bar x - \\mu_0} {\\hat \\sigma /\\sqrt{n}} < t_{n-1}^{\\alpha/2}\\right) &=  \\Pr\\left( -\\bar x -t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} < - \\mu_0 <  -\\bar x  +t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} \\right)\\\\\n",
    "&=  \\Pr\\left( \\bar x +t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} > \\mu_0 >  \\bar x  - t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} \\right)\\\\\n",
    "&= \\Pr\\left( \\bar x - t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} < \\mu_0 <  \\bar x  + t_{n-1}^{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\"captures\" $\\mu_0$ in $100(1-\\alpha)\\%$ of hypothetically repeated experiments \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof\n",
    "\n",
    "If $H_0$ is true, then \n",
    "$\\alpha = \\text{Pr}_{\\bar x}\\left(\\left\\lvert  \\frac{\\bar x - \\mu_0} {\\hat \\sigma /\\sqrt{n}}\\right\\rvert > Z_{\\alpha/2} \\right)$\n",
    "and the observed p-value under $H_0$ is \n",
    "$p = \\text{Pr}_Z\\left( Z > \\left\\lvert  \\frac{\\bar x - \\mu_0} {\\hat \\sigma /\\sqrt{n}}\\right\\rvert \\right)$\n",
    "\n",
    "And if $p < \\alpha$ then\n",
    "\n",
    "$\\quad\\;\\;\\; \\left\\lvert \\frac{\\bar x - \\mu_0} {\\hat \\sigma /\\sqrt{n}}\\right\\rvert > Z_{\\alpha/2} $\n",
    "\n",
    "$\\Longrightarrow \\mu_0 < \\bar x - Z_{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}} \\text{ or } \n",
    "\\mu_0 > \\bar x + Z_{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}}$\n",
    "\n",
    "\n",
    "$\\Longrightarrow \\mu_0 \\not \\in \\left(\\bar x - Z_{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}}, \n",
    "\\bar x + Z_{\\alpha/2} \\frac{\\hat \\sigma}{\\sqrt{n}}\\right)$\n",
    "\n",
    "<br> \n",
    "So the $100(1-\\alpha)\\%$ confidence interval _does not_ contain $\\mu_0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Testing\n",
    "\n",
    "- Each time we do a hypothesis test \\textcolor{gray}{[what?]}\n",
    "- There's a chance we are wrong about our decision\n",
    "- If $H_0$ is true, an $\\alpha$ chance of being wrong\n",
    "- So if we do $N$ tests, and $H_0$ is true for all of them\n",
    "- _we still expect to wrongly reject $H_0$ about $\\alpha \\times N$ times!_\n",
    "- Testing at $\\alpha' = \\alpha/N$ gives an $\\alpha$ chance all tests are right\n",
    "- This is called _Bonferroni correction_ \n",
    "- and it guarantees a $\\alpha$ _familly-wise error rate_ \n",
    "\n",
    "\n",
    "\n",
    "- Bonferroni correction is really quite stringent... \n",
    "\n",
    "\n",
    "- An alternative is the _False Discovery Rate (FDR)_ $q$\n",
    "- which for a set of tests (e.g., tests significant at the $\\alpha$-level)\n",
    "- is the proportion $q$ of the tests called incorrectly (i.e., the \"FDR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Testing in A/B Testing Contexts\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/figure_1.png\" width=\"1000px\" align=\"center\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Hypotheses $H_A$\n",
    "\n",
    "Unlike the _Null Hypothesis_ which provides a formal straw man, the _Alternative Hypothesis_ $H_A$ is actually our \"Best Guess\" at what we think might actually be true. The _Alternative Hypothesis_ $H_A$ is used to assess the _Power_ of an experiment to _Reject_ $H_0$: \n",
    "\n",
    "the specification of $H_A$ allows us to perform probabilistic cacluations under the distribution of the test statistic as we think it truly is (as opposed to how we think it is not, i.e., $H_0$).\n",
    "\n",
    "# Test Power $\\beta$\n",
    "\n",
    "\\begin{align*}\n",
    "\\Large \\text{Pr}(\\text{Fail to rejecting $H_0$} | H_A \\text{ is true}) = \\text{Pr}(\\text{Type II error})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/tests_3.png\" width=\"1000px\" align=\"center\">\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<img src=\"stuff/tests_4.png\" width=\"1000px\" align=\"center\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that p-values are not tightly related to relative likelihoods\n",
    "- Compared to all possible _Alternative Hypotheses_, p-values $\\sim .05$ are very strong evidence for $H_0$\n",
    "- If some extreme $H_A$ is true then you'd never see anything like $.05$, but if $H_0$ is true then you would..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A quick wiki read of Karl Pearson or Sir Ronald Fisher shows the staggering and breathtaking extent of their contributions to the field of statistics. Even Neyman, bitter rival of Fisher, who tends to receive the short end of the stick in historical restrospectives and is generally dismissed as the the foil to Fisher's genius has been an absolute pillar of modern statistical thought.  \n",
    "\n",
    "So who then is this upstart pretender William Sealy Gosset -- a mere beer maker -- who could dare to cross paths with these giants and be considered one of the major individuals responsible for hypthosis testing?  \n",
    "\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/William_Sealy_Gosset.jpg\" width=\"188px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/ronald-fisher-5.jpg\" width=\"199px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/neyman.jpg\" width=\"162px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/pearson.jpg\" width=\"201px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/Karl_Pearson.jpg\" width=\"180px\" align=\"left\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Gosset was an Oxford man -- a trained Chemist -- and he ended up working at Guinness because of Claude Guinness's policy of recruiting the best graduates from Oxford and Cambridge to apply biochemistry and statistics to Guinness's industrial processes. Never formally trained in Statistics, Gosset was self-taught but steadily incorporated and endeared himself with the statisticians working at University College London. Fisher himself even lauded Gosset as \"one of the most original minds in contemporary science\" and a contemporary of Gosset and Fisher even went so far as to say \"I think he [Gosset] was really the big influence in statistics... he asked the questions and Pearson and Fisher put them into statistical language, and then Neyman came to work with the mathematics. But I think\n",
    "most of it came from Gosset.\"\n",
    "\n",
    "http://statistics.berkeley.edu/sites/default/files/tech-reports/541.pdf\n",
    "\n",
    "http://faculty.fiu.edu/~blissl/GuinessGossetFisher.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Let $X_i \\overset{i.i.d.}{\\sim}f(\\theta)$, for $i = 1, \\cdots n$,\n",
    "and suppose we are interested in testing:}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\mu_0\\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\mu_0\n",
    "\\end{array} \\right. $\n",
    "\n",
    "\n",
    "$\\text{If $X_i$ is distributed normally or the CLT applies$^*$ than if $H_0$ is true}$\n",
    "\n",
    "$$\\bar X - \\mu_0 \\overset{\\tiny approx}{\\sim} N\\left(0, \n",
    "\\frac{\\text{Var}[X]}{n}\\right)$$\n",
    "\n",
    "$\\text{However, if we }\\textit{do not know } \\text{Var}[X] \\text{ we would need to estimate it,}$\n",
    "\n",
    "$\\text{and can do so in an unbiased manner with}$\n",
    "\n",
    "$$ s^2 = \\frac{\\sum (X_i - \\bar X)^2}{n-1} $$\n",
    "\n",
    "$\\text{Unfortunately, if the CLT has not yet ``kicked in'' }$ \n",
    "$\\text{then }\\textit{only if } f(\\theta)\\sim N(\\mu,\\sigma^2)\n",
    "\\text{ do we have that}$\n",
    "\n",
    "$$\\frac{\\bar X - \\mu_0}{\\sqrt{s^2/n}} \\sim t_{n-1}$$\n",
    "\n",
    "\n",
    "$\\text{But as $n \\rightarrow \\infty$, }$\n",
    "\n",
    "$$\n",
    "t_{n}\n",
    "\\longrightarrow\n",
    "N\\left(0, 1\\right)\n",
    "$$\n",
    "\n",
    "$\\text{and the CLT applies anyway at that point (so the use case for the t-test is}$ $\\textit{extremely} limited)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of Gosset’s analyses focused on malt extract, which was measured in “degrees saccharine” per barrel of 168 lbs. malt. At the time, an extract in the neighborhood of 133° gave the targeted level of alcohol content for Guinness’s beer. A higher extract affected the life of the beer, and also the alcohol content which in turn affected the excise tax paid on alcoholic beverages. \n",
    "\n",
    "In Gosset’s view, +/-0.5° was a difference or error in malt extract level which Guinness and its customers could swallow, and he determined that \"in order to get the accuracy we require we must take the mean of at least four determinations.”\n",
    "\n",
    "http://fmmh.ycdsb.ca/teachers/fmmh_mcmanaman/pages/tok_ziliak1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "H0_mu0 = 133\n",
    "degrees_saccharine = np.array([133.72, 137.02, 140.88, 135.45])\n",
    "n = len(degrees_saccharine)\n",
    "\n",
    "xbar = np.mean(degrees_saccharine)\n",
    "s2 = np.var(degrees_saccharine, ddof = 1)\n",
    "df = n - 1\n",
    "\n",
    "print 2 * (1 - stats.t.cdf((xbar - H0_mu0)/np.sqrt(s2/n), df = df))\n",
    "\n",
    "stats.ttest_1samp(degrees_saccharine, H0_mu0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{If we had $paired$ samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_i \\overset{i.i.d.}{\\sim}f(\\theta_Y)$$ \n",
    "\n",
    "$\\text{such that $X_i$ and $Y_i$ shared some dependencey for $i = 1, \\cdots n$, and were interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_i] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_i]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{we could let $Z_i = X_i - Y_i$ so that}$\n",
    "\n",
    "$$\\text{E}[Z_i] = 0 \\text{ and } \\text{Var}[Z_i] = \\text{Var}[X_i] + \\text{Var}[Y_i] - 2 \\text{Cov}[X_i,Y_i]$$\n",
    "\n",
    "$\\text{Then, if $H_0$ is true}$\n",
    "\n",
    "$$\\text{E}[Z_i] = 0$$\n",
    "\n",
    "$\\text{And if we don't know $\\text{Var}[Z_i]$, we could estimate it in the usual way with}$\n",
    "\n",
    "$$ s^2_Z = \\frac{\\sum (Z_i - \\bar Z)^2}{n-1} $$\n",
    "\n",
    "$\\text{Then, if $n$ is sufficiently large to invoke the CLT approximation we have that}$\n",
    "\n",
    "$$\\bar Z \\overset{\\tiny approx}{\\sim} N\\left(0, \n",
    "\\frac{s_Z^2}{n}\\right)$$\n",
    "\n",
    "$\\text{Or, if $Z_i \\sim N(\\text{E}[Z_i],\\text{Var}[Z_i])$}$ \n",
    "\n",
    "$$\\frac{\\bar Z - 0}{\\sqrt{s_Z^2/n}} \\sim t_{n-1}$$\n",
    "\n",
    "$\\text{Note that this is the same as the previous single sample case}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Student's actual publication \n",
    "http://seismo.berkeley.edu/~kirchner/eps_120/Odds_n_ends/Students_original_paper.pdf\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td>\n",
    "<img src=\"stuff/t2.png\" width=\"900px\" align=\"left\">\n",
    "</tr></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soft_corn_yield = np.array([7.85, 8.89, 14.81, 13.55, 7.48, 15.39])\n",
    "hard_corn_yield = np.array([7.27, 8.32, 13.81, 13.36, 7.97, 13.13])\n",
    "diffs = soft_corn_yield - hard_corn_yield\n",
    "\n",
    "H0_mu0 = 0 # soft_corn_yield - hard_corn_yield >= 0\n",
    "n = len(diffs)\n",
    "\n",
    "xbar = np.mean(diffs)\n",
    "s2b = np.var(diffs.tolist() + [.685], ddof = 0)\n",
    "print \"Gosset's variance estimator SD_G =\", np.sqrt(s2b), \"was BIASED!\"\n",
    "s2w = np.var(diffs, ddof = 0)\n",
    "print \"Gosset's variance estimate SD_W =\", np.sqrt(s2w), \"was WRONG!\"\n",
    "s2 = np.var(diffs, ddof = 1)\n",
    "print \"We use the unbiased (and correctly calculated) estimator, SD_U =\", np.sqrt(s2), \"\\n\"\n",
    "df = n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# V0 -- fully manual \n",
    "print 1 - stats.t.cdf((xbar - H0_mu0)/np.sqrt(s2/n), df = df) # one-sided test\n",
    "\n",
    "# V1 -- manual differences then single sample test\n",
    "one_sided_diffs_test = list(stats.ttest_1samp(diffs, H0_mu0))\n",
    "one_sided_diffs_test[1] = one_sided_diffs_test[1]/2\n",
    "print one_sided_diffs_test\n",
    "\n",
    "# V2 -- automated two related samples test\n",
    "paired_sample_test = list(stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "paired_sample_test[1] = paired_sample_test[1]/2\n",
    "print paired_sample_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{What if we again had $paired$ samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}f(\\theta_Y)$$ \n",
    "\n",
    "$\\text{such that $X_i$ and $Y_i$ shared some dependencey for $i = 1, \\cdots n$, and were again interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{but we } \\textit{did not } \\text{take the difference $Z_i = X_i - Y_i$}$\n",
    "\n",
    "$\\text{and instead directly estimated the means and difference therein? That is, we examined}$\n",
    "\n",
    "$$\\bar X - \\bar Y$$\n",
    "\n",
    "$\\text{Depending on $n$ and/or the distributions of $X_i$ and $Y_i$ ($how?$),}$\n",
    "$\\text{and after estimating $s_X^2$ and $s_Y^2$ in the usual way, we have either}$\n",
    "\n",
    "$$ \\bar X \\overset{\\tiny approx}{\\sim} N\\left(\\text{E}[X], \\frac{s_X^2}{n}\\right) \\text{ or }\n",
    "\\frac{\\bar X-\\text{E}[X]}{s_X^2/n} \\sim t_{n-1} \n",
    "\\overset{n\\rightarrow\\infty}{\\longrightarrow}\n",
    "N\\left(0, 1\\right)  \n",
    "$$\n",
    "\n",
    "$\\text{and the analogous situation holds for $\\bar Y$ as well. Subsequently, we have}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Var}[\\bar X - \\bar Y] &=& \\text{Var}[\\bar X] + \\text{Var}[\\bar Y] - 2\\text{Cov}[\\bar X, \\bar Y]\\\\\n",
    "&\\overset{?}{=}& \\text{Var}[\\bar X] + \\text{Var}[\\bar Y] \\\\\n",
    "&=& \\frac{\\text{Var}[X]}{n} + \\frac{\\text{Var}[Y]}{n} \\\\\n",
    "&=& \\frac{\\text{Var}[X] + \\text{Var}[Y]}{n} \\\\\n",
    "&\\overset{?}{=}& \\frac{\\text{Var}[X] + \\text{Var}[Y] −2Cov[X,Y]}{n} \\\\\n",
    "&=& \\frac{\\text{Var}[Z]}{n} \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$\\text{So if there's _positive (negative)_ dependency between $X_i$ and $Y_i$ estimating variance as }$\n",
    "\n",
    "$$\\text{Var}[\\bar X - \\bar Y] = \\text{Var}[\\bar X] + \\text{Var}[\\bar Y]$$\n",
    "\n",
    "$\\text{is wrong and _less efficient (overly optimistic)_; on the other hand, testing arbitrarily paired independent variables}$ \n",
    "\n",
    "$$\\frac{\\text{Var}[Z]}{n} = \\frac{\\text{Var}[X] + \\text{Var}[Y] −2Cov[X,Y]}{n}$$\n",
    "\n",
    "$\\text{could produce chance covariation in $X_i$ and $Y_i$ which spuriously effected variance estimates}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If data is paired, paired testing is more powerful and efficient in terms of significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paired_sample_test = list(stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "paired_sample_test[1] = paired_sample_test[1]/2\n",
    "print \"Correctly treated as dependent samples:\", paired_sample_test\n",
    "\n",
    "two_independent_samples_test = list(stats.ttest_ind(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "two_independent_samples_test[1] = two_independent_samples_test[1]/2\n",
    "print \"Incorrectly treated as independent samples:\", two_independent_samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Guinness really does taste better in Ireland\n",
    "\n",
    "https://www.theguardian.com/science/blog/2011/may/27/barack-obama-guinness-taste-ireland\n",
    "http://blog.minitab.com/blog/michelle-paret/guinness-t-tests-and-proving-a-pint-really-does-taste-better-in-ireland\n",
    "\n",
    "$\\begin{array}{c|ccc}\n",
    "       & n & \\bar X & s_x \\\\\\hline\n",
    "Ireland &42 & 74 & 7.4 \\\\\n",
    "Elsewhere & 61 & 57 & 7.1 \\\\\n",
    "\\end{array}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xbar1 = 74.\n",
    "s21 = 7.4**2\n",
    "n1 = 42\n",
    "\n",
    "xbar2 = 57.\n",
    "s22 = 7.1**2\n",
    "n2 = 61\n",
    "\n",
    "# Prove (xbar1-xbar2)/np.sqrt(s21/n1 + s22/n2) ~ N(0,1)...\n",
    "1 - stats.norm.cdf((xbar1-xbar2)/np.sqrt(s21/n1 + s22/n2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Making this concrete, suppose that we have two independent samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}Bern(p_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}Bern(p_Y)$$ \n",
    "\n",
    "$\\text{for $i = 1, \\cdots n$ and $j = 1, \\cdots m$ and we are again interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{We will test this with $\\bar X - \\bar Y$ which we notate as\n",
    " $\\hat p_X - \\hat p_Y$}$\n",
    "\n",
    "$\\text{Suppose $H_0$ is true and $\\text{E}[X_i] = \\text{E}[Y_j] = p$}$\n",
    "$\\text{Then, under $H_0$ we have } \\textit{common variances}$\n",
    "\n",
    "$$\\text{Var}[X_i] = \\text{Var}[Y_i] = p(1-p)$$\n",
    "\n",
    "$\\text{And we estimate $p$ with $\\hat p = \\frac{\\sum X_i + \\sum Y_i}{n+m}$ and $\\text{Var}\\left[\\hat p_X - \\hat p_Y\\right] = \\frac{\\hat p(1- \\hat p)}{n} + \\frac{\\hat p(1- \\hat p)}{m}$}$\n",
    "\n",
    "$\\text{For large $n$ and $m$ then, under $H_0$ the CLT and normality of added normal variables imply}$ \n",
    "\n",
    "$$ \\hat p_X - \\hat p_Y \\overset{\\tiny approx}{\\sim} N\\left(0, \\frac{\\hat p(1- \\hat p)}{n} + \\frac{\\hat p(1- \\hat p)}{m}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We did this in the morning, plus you'll be doing a bit of this type of stuff in your sprint so I'll leave it for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Consider now the case of the two independent normally distributed samples}$\n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}N(\\mu_X,\\sigma^2) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}N(\\mu_Y,\\sigma^2) $$ \n",
    "\n",
    "$\\text{ for } i = 1, \\cdots n \\text{ and } j = 1, \\cdots m$\n",
    "\n",
    "$\\text{where we explicitly consider the case of a shared } \\textit{common variance } \\sigma^2$ \n",
    "\n",
    "$\\text{and where we are again interested in}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{If $H_0$ is true we can estimate the shared variance in an unbiased manner as}$\n",
    "\n",
    "\n",
    "$$ s^2_{XY} = \\frac{\\sum (X_i - \\bar X)^2 + \\sum (Y_i - \\bar Y)^2}{n + m - 2} $$\n",
    "\n",
    "$\\text{with the associated degrees of freedom $n + m - 2$ where}$\n",
    "\n",
    "\n",
    "$$\\text{Var}[\\bar X - \\bar Y] = \\frac{s^2_{XY}}{n} + \\frac{s^2_{XY}}{m}$$\n",
    "\n",
    "$\\text{and}$\n",
    "\n",
    "$$\\frac{\\bar X - \\bar Y}{\\sqrt{\\frac{s^2_{XY}}{n} + \\frac{s^2_{XY}}{m}}} \\sim t_{n+m-2}$$\n",
    "\n",
    "\n",
    "\n",
    "$\\textit{When the variances are not assumed to be equal, and CLT normality has not yet kicked in,}$\n",
    "$\\textit{the degrees of freedom for the t-distirubiton is given by the Welch–Satterthwaite equation}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In 1897, Thomas B. Case -- Guinness’s first scientific brewer -- and a cooperating scientist named Briant were examining the amount of soft and hard resins found in 50 gram samples of several seasons of American and Kent hops. For one lot of Kent hops Case had examined 11 samples and Briant had examined 14 and they observed soft resin averages of 4.05 grams and 4.2 grams, respectively. They observed even greater differences in their two samples of “American, 1895” at 0.35 grams and .5 grams for soft resins and hard resins, respectively.  Case wrote that “We could not... support the conclusion that there are no differences between pockets of the same lot” but he had no basis for evaluating whether observed differences represented random error from the samples or actual differences in the population.  In 1899 Case hired Gosset, who in 1908 arrived at his small sample inference theory.  \n",
    "\n",
    "http://fmmh.ycdsb.ca/teachers/fmmh_mcmanaman/pages/tok_ziliak1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soft_resin_Case = np.array([3.33,3.39,3.85,4.16,3.59,4.25,4.94,4.06,4.06,4.55,4.37])\n",
    "soft_resin_Briant = np.array([4.11,5.48,2.94,1.52,3.87,5.64,4.85,4.68,5.11,4.13,5.44,2.34,3.50,5.19])\n",
    "\n",
    "print stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = True), \"\\n(with common variances)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = False), \"\\n(with unique variances)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Observed variance for soft_resin_Case was\", np.var(soft_resin_Case, ddof = 1)\n",
    "print \"Observed variance for soft_resin_Briant was\", np.var(soft_resin_Briant, ddof = 1)\n",
    "\n",
    "print \"\\nObserved standard deviation for soft_resin_Case was\", \n",
    "print np.var(soft_resin_Case, ddof = 1)**.5\n",
    "print \"Observed standard deviation for soft_resin_Briant was\", \n",
    "print np.var(soft_resin_Briant, ddof = 1)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.ttest_rel(soft_resin_Case, soft_resin_Briant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{If $n$ is small enough that the CLT does not yet provide a good approximation, and}$\n",
    "\n",
    "$$X_i \\sim f(\\theta) \\not = N\\left(\\mu,\\sigma^2\\right)$$\n",
    "\n",
    "$\\text{for $i = 1, \\cdots n,$ then according to scikit-learn you need to get more data.}$\n",
    "\n",
    "<table align=\"center\">\n",
    "<tr><td>\n",
    "<img src=\"stuff/ml_map.png\" width=\"750px\" align=\"center\">\n",
    "</tr></td>\n",
    "</table>\n",
    "\n",
    "\n",
    "$\\text{However, one thing you } \\textit{could } \\text{do is test the median, i.e.,}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{Median}(X_i) = m \\\\\n",
    "H_a: \\text{Median}(X_i) \\not = m\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{since if $H_0$ holds}$\n",
    "\n",
    "$$\\sum 1_{[X_i>m]} \\sim Binom(0.5, n)$$\n",
    "\n",
    "$\\text{This approach belongs to a class of tests known as } \\textit{nonparametric tests}$ \n",
    "\n",
    "$\\textit{Nonparametric tests } \\text{do not rely on any distributional assumptions about the data}$\n",
    "\n",
    "$- \\; \\text{They therefore are much more generally applicable than their parametric counterparts}$\n",
    "\n",
    "$- \\; \\text{And they are } \\textit{still } \\text{often nearly as powerful as their parametric counterparts}$\n",
    "\n",
    "$- \\; \\text{One then wonders why these more general purpose tools are not more heavily emphesized... }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H0_mu0 = 133\n",
    "\n",
    "print \"Single Sample degrees saccharine test p-values\"\n",
    "print \"Parametric: \", stats.ttest_1samp(degrees_saccharine, H0_mu0)\n",
    "print \"Nonparametric: \", 2*stats.binom.pmf(np.sum((degrees_saccharine-H0_mu0)>0),4,0.5)\n",
    "# Show that this is the correct p-value based on the binomial idea in the last block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Some very useful nonparametric tests are }$\n",
    "\n",
    "$\\quad - \\; \\textbf{The Wilcoxon signed-rank test} \\text{, which addresses paired samples like the paired t-test}$\n",
    "\n",
    "$\\quad - \\; \\textbf{Mann–Whitney U} \\text{, which addresses independent samples, like the two-sample t-test}$\n",
    "\n",
    "$\\quad - \\; \\textbf{Fisher's exact test} \\text{, which addresses independence in (often, $2\\times2$) contingency tables} (we will return to this soon)$\n",
    "\n",
    "$\\quad - \\; \\textbf{Kolmogorov-Smirnov test} \\text{, which addresses distributional assumptions (we will return to this later)}$\n",
    "\n",
    "$\\quad - \\; \\text{And many many more... sign test, median test, Wilcoxon rank sum test, etc....} $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diffs = soft_corn_yield - hard_corn_yield\n",
    "H0_mu0 = 0\n",
    "\n",
    "print \"Paired Soft/Hard corn yield test p-values\"\n",
    "print \"Parametric: \", stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0)\n",
    "print \"Nonparametric: \", stats.wilcoxon(soft_corn_yield, hard_corn_yield, correction = True) \n",
    "\n",
    "print \"\\nCase/Briant Independent soft resin test p-values\"\n",
    "print \"Parametric: \", stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = False)\n",
    "print \"Nonparametric: \", stats.mannwhitneyu(soft_resin_Case, soft_resin_Briant,alternative = 'two-sided') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dr. Muriel Bristol, an acquaintance of Fisher was a British algologist who took tea with milk and (as was the usual practice of high class British aristocrats) prefered that the milk be poured into the cup before the tea. One day Fisher was having afternoon tea with Muriel and William Roach (who would later marry Muriel) but Muriel politely declined tea because (on this particular occasion) the milk had not been poured first. To this Fisher responded \"Nonsense. Surely it makes no difference,\" but Muriel insisted that it did, to which William proposed, \"Let's test her!\"  On their next afternoon tea engagement, Fisher brought back a newly invented test for the task at hand, a nonparametric test now known as \"Fisher's Exact Test\".  \n",
    "\n",
    "Together with William's help, Fisher tested tested Muriel using a sample of eight cups of tea. In a random order, William prepared four cups of tea with the milk poured first, and four cups of tea with the milk poured second -- all of which Muriel was able to correctly identify the correct milk/tea order for. \n",
    "\n",
    "$\\begin{array}{c|cc}\n",
    "&\\text{William poured milk first} & \\text{William poured milk second} \\\\\\hline\n",
    "\\text{Muriel said, \"Milk poured first\"} & 4 & 0\\\\\n",
    "\\text{Muriel said, \"Milk poured second\"} & 0 & 4 \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " stats.fisher_exact([[4,0],[0,4]], alternative = 'greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{As with Fisher's Exact test, interest in contingency tables often lies in the independence of the marginal variables }$\n",
    "\n",
    "$\\text{That is, for discrete variables }\n",
    "X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}f(\\theta_Y) \n",
    "\\text{ for } i = 1, \\cdots n \\text{ and } j = 1, \\cdots m$\n",
    "\n",
    "$\\text{we are often interested in }$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{$X_i$ and $Y_i$ are }independent \\\\\n",
    "H_a: \\text{$X_i$ and $Y_i$ are }dependent\n",
    "\\end{array} \\right. $\n",
    "\n",
    "\n",
    "\n",
    "$\\text{A } \\textit{chi-squared } \\text{test based on a large sample distributional approximation can be used to evaluate $H_0$ }$\n",
    "\n",
    "$\\text{The approximation is (conservatively) recommended for use } \\textit{only when }$\n",
    "\n",
    "$\\quad - \\; \\text{The number of counts in each cell is $\\geq 5$, or}$\n",
    "\n",
    "$\\quad - \\; \\text{The number of counts in each cell is $\\geq 10$ if the degrees of freedom are 1}$\n",
    "\n",
    "\n",
    "$\\text{The chi-squared test statistic itself is}$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "x^2 &=& \\sum_{r=1}^R \\sum_{c=1}^C \\frac{(O_{rc} - E_{rc})^2}{E_{rc}} \\text{ where }\\\\\n",
    "E_{rc} &=& \\frac{\\left(\\sum_i O_{ic}\\right)\\left(\\sum_j O_{rj}\\right)}{\\sum O_{ij}} \\text{ and} \\\\\\\\\n",
    "x^2 &\\overset{\\tiny approx}{\\sim}& \\chi^2_{df} \\;\\; where \\; df = (R-1)(C-1)\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Does grunting help McEnroe serve more aces?\n",
    "\n",
    "* McEnroe lost only one Collegiate match, which occured at Trinity University in San Antonio, TX\n",
    "\n",
    "\n",
    "$\\begin{array}{c|ccc}\n",
    "&\\text{McEnroe serves an ace} & \\text{McEnroe faults} & \\text{McEnroe's serve is retruned} \\\\\\hline\n",
    "\\text{McEnroe grunts on serve} & 61 & 32 & 144\\\\\n",
    "\\text{McEnroe silent on serve} & 35 & 8 & 53 \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi2, p, ddof, expected = stats.chi2_contingency([[61,32,144],[35,8,53]])\n",
    "\n",
    "print \"Observed:\\n\", [[61,32,144],[35,8,53]],\"\\n\"\n",
    "print \"Expected (if independent):\\n\", expected,\"\\n\"\n",
    "\n",
    "msg = \"Test Statistic: {}\\np-value: {}\\nDegrees of Freedom: {}\"\n",
    "print msg.format( chi2, p, ddof ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{A } \\textit{chi-squared test } \\text{can also be used to examine distributional assumptions }in \\; toto$\n",
    "\n",
    "$\\text{For any binned random variable $X_i$ for $i = 1, \\cdots, n$ we can test}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: X_i \\sim f(\\theta) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{where $\\Pr(X_i = k) = \\theta_k$ }$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "x^2 &=& \\sum_{k=1}^K \\frac{(O_{k} - E_{k})^2}{E_{k}} \\text{ where }\\\\\n",
    "O_{k} &=& \\sum^n_{i=1} 1_{[X_i=k]}  \\\\\n",
    "E_{k} &=& n \\cdot \\theta \\text{ and} \\\\\n",
    "x^2 &\\overset{approx.}{\\sim}& \\chi^2_{df} \\;\\; where \\; df = n - K\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No example here... this is type of test is a total relic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{A } \\textit{chi-squared test } \\text{can also test variance in normally distributed populations }$\n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}N(\\mu_X,\\sigma_X^2), \\; i = 1, \\cdots n$$\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\sigma_X^2 = \\sigma_0^2 \\\\\n",
    "H_a: \\sigma_X^2 \\not = \\sigma_0^2\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{since, if $H_0$ is True, then}$\n",
    "$$ \\frac{\\sum (X_i - \\bar X)^2}{\\sigma_0^2} \\sim \\chi_{n-1}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(soft_resin_Case)\n",
    "print \"Estimated variance \" + str(np.var(soft_resin_Case, ddof = 0))\n",
    "print \"p-value of H_0: variance=1 test \" + str(2*stats.chi2.cdf((np.var(soft_resin_Case, ddof = 0)*n)/1, df = n - 1)) + \"\\n\"\n",
    "\n",
    "n = len(soft_resin_Briant)\n",
    "print \"Estimated variance \" + str(np.var(soft_resin_Briant, ddof = 0))\n",
    "print \"p-value of H_0: variance=1 test \" + str(2*(1-stats.chi2.cdf((np.var(soft_resin_Briant, ddof = 0)*n)/1, df = n - 1))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Further, the variances of two samples can be compared using an } \\textit{F-test:}$\n",
    "\n",
    "$\\text{If $\\chi^2_1$ and $\\chi^2_2$ are chi-squared random variables with $v$ and $w$ degrees of freedom}$\n",
    "\n",
    "$\\text{then $F = \\frac{\\chi^2_1}{v}\\div\\frac{\\chi^2_2}{w}$ is distributed as an } \\textit{F distribution } \\text{with degrees of freedom $v$ and $w$}$ \n",
    "\n",
    "$\\text{Therefore, for $Y_j \\overset{i.i.d.}{\\sim} N(\\mu_Y,\\sigma_Y^2), \\; j = 1, \\cdots m$, we can test}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\sigma_X^2 = \\sigma_Y^2 \\\\\n",
    "H_a: \\sigma_X^2 \\not = \\sigma_Y^2\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$ \\frac{\\sum (X_i - \\bar X)^2\\big/(n-1)}{\\sum (Y_i - \\bar Y)^2\\big/(m-1)} \\sim F_{n-1,m-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 - stats.f.cdf(np.var(soft_resin_Briant, ddof = 1)/np.var(soft_resin_Case, ddof = 1),\n",
    "               len(soft_resin_Briant)-1, len(soft_resin_Case)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{The F-test is most often encountered in model selection contexts, and in particular in the } \\textit{multiple regression } \\text{context}$\n",
    "\n",
    "$\\text{Here, it is used to test if the $k$ features (covariates) } \\textbf{$X$}_i \\text{ have any explanatory power with respect to outcome $Y_i$}$\n",
    "\n",
    "$\\text{Under the usual multiple regression assumptions, we can evaluate}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: E[Y_i] = \\beta_0 \\\\\n",
    "H_a: E[Y_i] \\not = \\beta_0\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$ \\frac{\\sum (Y_i - \\beta_0)^2\\big/(n - 1 -1)}{\\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\big/(n-k-1)} \n",
    "=\n",
    "\\frac{RSS_{2}\\big/(n - 2)}{RSS_{k + 1}\\big/(n - k - 1)} \n",
    "\\sim F_{n-2,n-k-1} \n",
    "$$\n",
    "\n",
    "$\\text{ or, using the preferred method }$\n",
    "\n",
    "$$\\frac{\\left(\\sum (X_i - \\beta_0)^2 - \\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\right)\\big/(k - 1)}{\\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\big/(n-k-1)} \n",
    "= \n",
    "\\frac{\\left(RSS_{2}- RSS_{k + 1}\\right)\\big/(k - 1)}{RSS_{k + 1}\\big/(n - k - 1)} \n",
    "\\sim F_{k-1,n-k-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will note this again in another lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{An interesting result we will revisit with } \\textit{logistic regression }\\text{is that } \\textit{deviance } \\text{for a $true$ $model$ $M$ with $k$ parameters}$\n",
    "\n",
    "$$D_M \\overset{\\tiny approx}{\\sim} \\chi^2_{n-k}$$\n",
    "\n",
    "$\\text{where for true model $M$ and saturated model $Y$}$\n",
    "\n",
    "$$D_M = -2\\left(\\log f\\left(Y|\\hat \\theta^M\\right) - \\log f\\left(Y|\\hat \\theta^Y\\right)\\right)$$\n",
    "\n",
    "$\\text{Hence for Model $R$ with $m-k$ parameters nested within Model $F$ with $m$ parameters we can evaluate}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{Models $R$ fits the data fairly well}  \\\\\n",
    "H_a: H_0 \\text{ is False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$D_R - D_F \\overset{\\tiny approx}{\\sim} \\chi^2_{k}$$\n",
    "\n",
    "$\\text{since the sum (i.e., difference) of two $\\chi^2$ random variables remains $\\chi^2$ and if model $R$ does not fit the data $D_R$ will be large}$\n",
    "\n",
    "$\\text{Becaues there is no residual sums of squares ($RSS$) in GLM contexts and only the deviance approximation is available,}$ \n",
    "$\\text{in GLM contexts (such as logistic regression) this result plays the same role as the $F$ test in multiple linear regression}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will note this again in another lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{The } \\textit{Kolmogorov-Smirnov (K-S) test } \\text{provides another test of distributional assumptions}$\n",
    "\n",
    "$\\text{Interestingly, while the test tests distributional (often, parametric) assumptions,\n",
    "the test itself is } \\textit{nonparametric}$\n",
    "\n",
    "$\\text{The K-S test can examine distributional assumptions of a single sample}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: X_i \\sim f(\\theta) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{of or compare the distributions of two samples} $ \n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: f_X(\\theta_X) = f_Y(\\theta_Y) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{Like all other tests, the K-S test has a test statistic (traditionally noted as $D_n$), and }$\n",
    "$\\text{is based on a null distribution (called the $Kolmogorov \\; distribution$ for the K-S test)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td><img src=\"stuff/ks1.png\" width=\"300px\" align=\"center\"></td>\n",
    "<td><img src=\"stuff/ks2.png\" width=\"300px\" align=\"center\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# print stats.ks_2samp(soft_resin_Case, soft_resin_Briant)\n",
    "\n",
    "print stats.kstest(stats.t.rvs(df=30,size=1000),'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parametric Versus Nonparametric\n",
    "\n",
    "#### We might characterize an analysis framework as parametric if\n",
    "\n",
    "### Results are buttressed or bolstered by modeling assumptions \n",
    "* E.g., leveraging the structure of normality via a t-test increases power but comes at a cost of loss of robustness compared to nonparametric tests free of distributional assumptions\n",
    "\n",
    "### Predicted values are based on \"parameters'' \n",
    "* E.g., the $\\beta$ coefficients in linear regression\n",
    "\n",
    "### Parameter estimation  determines the specific instance of a model within a \"model class'' defined by those parameters\n",
    "* E.g., the CLT is based on a normal distribution which is determined by estimating $\\mu$ and $\\sigma^2$\n",
    "\n",
    "###  The complexity of the model does grows as data size $n$ grows\n",
    "* E.g., trees grow in complexity as data becomes richer while a normal distribution is defined by $\\mu$ and $\\sigma^2$ regardless of $n$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
