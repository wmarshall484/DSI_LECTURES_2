{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\textit{testing}$$\n",
    "$$\\text{Schwartz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A quick wiki read of Karl Pearson or Sir Ronald Fisher shows the staggering and breathtaking extent of their contributions to the field of statistics. Even Neyman, bitter rival of Fisher, who tends to receive the short end of the stick in historical restrospectives and is generally dismissed as the the foil to Fisher's genius has been an absolute pillar of modern statistical thought.  \n",
    "\n",
    "So who then is this upstart pretender William Sealy Gosset -- a mere beer maker -- who could dare to cross paths with these giants and be considered one of the major individuals responsible for hypthosis testing?  \n",
    "\n",
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"stuff/William_Sealy_Gosset.jpg\" width=\"188px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/ronald-fisher-5.jpg\" width=\"199px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/neyman.jpg\" width=\"162px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/pearson.jpg\" width=\"201px\" align=\"left\">\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"stuff/Karl_Pearson.jpg\" width=\"180px\" align=\"left\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Gosset was an Oxford man -- a trained Chemist -- and he ended up working at Guinness because of Claude Guinness's policy of recruiting the best graduates from Oxford and Cambridge to apply biochemistry and statistics to Guinness's industrial processes. Never formally trained in Statistics, Gosset was self-taught but steadily incorporated and endeared himself with the statisticians working at University College London. Fisher himself even lauded Gosset as \"one of the most original minds in contemporary science\" and a contemporary of Gosset and Fisher even went so far as to say \"I think he [Gosset] was really the big influence in statistics... he asked the questions and Pearson and Fisher put them into statistical language, and then Neyman came to work with the mathematics. But I think\n",
    "most of it came from Gosset.\"\n",
    "\n",
    "http://statistics.berkeley.edu/sites/default/files/tech-reports/541.pdf\n",
    "\n",
    "http://faculty.fiu.edu/~blissl/GuinessGossetFisher.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Let $X_i \\overset{i.i.d.}{\\sim}f(\\theta)$, for $i = 1, \\cdots n$,\n",
    "and suppose we are interested in testing:}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\mu_0\\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\mu_0\n",
    "\\end{array} \\right. $\n",
    "\n",
    "\n",
    "$\\text{If $X_i$ is distributed normally or the CLT applies$^*$ than if $H_0$ is true}$\n",
    "\n",
    "$$\\bar X - \\mu_0 \\overset{\\tiny approx}{\\sim} N\\left(0, \n",
    "\\frac{\\text{Var}[X]}{n}\\right)$$\n",
    "\n",
    "$\\text{However, if we }\\textit{do not know } \\text{Var}[X] \\text{ we would need to estimate it,}$\n",
    "\n",
    "$\\text{and can do so in an unbiased manner with}$\n",
    "\n",
    "$$ s^2 = \\frac{\\sum (X_i - \\bar X)^2}{n-1} $$\n",
    "\n",
    "$\\text{Unfortunately, if the CLT has not yet ``kicked in'' }$ \n",
    "$\\text{then }\\textit{only if } f(\\theta)\\sim N(\\mu,\\sigma^2)\n",
    "\\text{ do we have that}$\n",
    "\n",
    "$$\\frac{\\bar X - \\mu_0}{\\sqrt{s^2/n}} \\sim t_{n-1}$$\n",
    "\n",
    "\n",
    "$\\text{But as $n \\rightarrow \\infty$, }$\n",
    "\n",
    "$$\n",
    "t_{n}\n",
    "\\longrightarrow\n",
    "N\\left(0, 1\\right)\n",
    "$$\n",
    "\n",
    "$\\text{and the CLT applies anyway at that point (so the use case for the t-test is}$ $\\textit{extremely limited})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of Gosset’s analyses focused on malt extract, which was measured in “degrees saccharine” per barrel of 168 lbs. malt. At the time, an extract in the neighborhood of 133° gave the targeted level of alcohol content for Guinness’s beer. A higher extract affected the life of the beer, and also the alcohol content which in turn affected the excise tax paid on alcoholic beverages. \n",
    "\n",
    "In Gosset’s view, +/-0.5° was a difference or error in malt extract level which Guinness and its customers could swallow, and he determined that \"in order to get the accuracy we require we must take the mean of at least four determinations.”\n",
    "\n",
    "http://fmmh.ycdsb.ca/teachers/fmmh_mcmanaman/pages/tok_ziliak1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0903500656754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=2.4664308272896882, pvalue=0.090350065675395053)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "H0_mu0 = 133\n",
    "degrees_saccharine = np.array([133.72, 137.02, 140.88, 135.45])\n",
    "n = len(degrees_saccharine)\n",
    "\n",
    "xbar = np.mean(degrees_saccharine)\n",
    "s2 = np.var(degrees_saccharine, ddof = 1)\n",
    "df = n - 1\n",
    "\n",
    "print 2 * (1 - stats.t.cdf((xbar - H0_mu0)/np.sqrt(s2/n), df = df))\n",
    "\n",
    "stats.ttest_1samp(degrees_saccharine, H0_mu0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{If we had $paired$ samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_i \\overset{i.i.d.}{\\sim}f(\\theta_Y)$$ \n",
    "\n",
    "$\\text{such that $X_i$ and $Y_i$ shared some dependencey for $i = 1, \\cdots n$, and were interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_i] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_i]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{we could let $Z_i = X_i - Y_i$ so that}$\n",
    "\n",
    "$$\\text{E}[Z_i] = 0 \\text{ and } \\text{Var}[Z_i] = \\text{Var}[X_i] + \\text{Var}[Y_i] - 2 \\text{Cov}[X_i,Y_i]$$\n",
    "\n",
    "$\\text{Then, if $H_0$ is true}$\n",
    "\n",
    "$$\\text{E}[Z_i] = 0$$\n",
    "\n",
    "$\\text{And if we don't know $\\text{Var}[Z_i]$, we could estimate it in the usual way with}$\n",
    "\n",
    "$$ s^2_Z = \\frac{\\sum (Z_i - \\bar Z)^2}{n-1} $$\n",
    "\n",
    "$\\text{Then, if $n$ is sufficiently large to invoke the CLT approximation we have that}$\n",
    "\n",
    "$$\\bar Z \\overset{\\tiny approx}{\\sim} N\\left(0, \n",
    "\\frac{s_Z^2}{n}\\right)$$\n",
    "\n",
    "$\\text{Or, if $Z_i \\sim N(\\text{E}[Z_i],\\text{Var}[Z_i])$}$ \n",
    "\n",
    "$$\\frac{\\bar Z - 0}{\\sqrt{s_Z^2/n}} \\sim t_{n-1}$$\n",
    "\n",
    "$\\text{Note that this is the same as the previous single sample case}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Student's actual publication \n",
    "http://seismo.berkeley.edu/~kirchner/eps_120/Odds_n_ends/Students_original_paper.pdf\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td>\n",
    "<img src=\"stuff/t2.png\" width=\"900px\" align=\"left\">\n",
    "</tr></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gosset's variance estimator SD_G = 0.777335191536 was BIASED!\n",
      "Gosset's variance estimate SD_W = 0.839617968682 was WRONG!\n",
      "We use the unbiased (and correctly calculated) estimator, SD_U = 0.919755402267 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "soft_corn_yield = np.array([7.85, 8.89, 14.81, 13.55, 7.48, 15.39])\n",
    "hard_corn_yield = np.array([7.27, 8.32, 13.81, 13.36, 7.97, 13.13])\n",
    "diffs = soft_corn_yield - hard_corn_yield\n",
    "\n",
    "H0_mu0 = 0 # soft_corn_yield - hard_corn_yield >= 0\n",
    "n = len(diffs)\n",
    "\n",
    "xbar = np.mean(diffs)\n",
    "s2b = np.var(diffs.tolist() + [.685], ddof = 0)\n",
    "print \"Gosset's variance estimator SD_G =\", np.sqrt(s2b), \"was BIASED!\"\n",
    "s2w = np.var(diffs, ddof = 0)\n",
    "print \"Gosset's variance estimate SD_W =\", np.sqrt(s2w), \"was WRONG!\"\n",
    "s2 = np.var(diffs, ddof = 1)\n",
    "print \"We use the unbiased (and correctly calculated) estimator, SD_U =\", np.sqrt(s2), \"\\n\"\n",
    "df = n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0638511726714\n",
      "[1.8242898814947432, 0.06385117267142619]\n",
      "[1.8242898814947432, 0.06385117267142619]\n"
     ]
    }
   ],
   "source": [
    "# V0 -- fully manual \n",
    "print 1 - stats.t.cdf((xbar - H0_mu0)/np.sqrt(s2/n), df = df) # one-sided test\n",
    "\n",
    "# V1 -- manual differences then single sample test\n",
    "one_sided_diffs_test = list(stats.ttest_1samp(diffs, H0_mu0))\n",
    "one_sided_diffs_test[1] = one_sided_diffs_test[1]/2\n",
    "print one_sided_diffs_test\n",
    "\n",
    "# V2 -- automated two related samples test\n",
    "paired_sample_test = list(stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "paired_sample_test[1] = paired_sample_test[1]/2\n",
    "print paired_sample_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{What if we again had $paired$ samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}f(\\theta_Y)$$ \n",
    "\n",
    "$\\text{such that $X_i$ and $Y_i$ shared some dependencey for $i = 1, \\cdots n$, and were again interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{but we } \\textit{did not } \\text{take the difference $Z_i = X_i - Y_i$}$\n",
    "\n",
    "$\\text{and instead directly estimated the means and difference therein? That is, we examined}$\n",
    "\n",
    "$$\\bar X - \\bar Y$$\n",
    "\n",
    "$\\text{Depending on $n$ and/or the distributions of $X_i$ and $Y_i$ ($how?$),}$\n",
    "$\\text{and after estimating $s_X^2$ and $s_Y^2$ in the usual way, we have either}$\n",
    "\n",
    "$$ \\bar X \\overset{\\tiny approx}{\\sim} N\\left(\\text{E}[X], \\frac{s_X^2}{n}\\right) \\text{ or }\n",
    "\\frac{\\bar X-\\text{E}[X]}{s_X^2/n} \\sim t_{n-1} \n",
    "\\overset{n\\rightarrow\\infty}{\\longrightarrow}\n",
    "N\\left(0, 1\\right)  \n",
    "$$\n",
    "\n",
    "$\\text{and the analogous situation holds for $\\bar Y$ as well. Subsequently, we have}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Var}[\\bar X - \\bar Y] &=& \\text{Var}[\\bar X] + \\text{Var}[\\bar Y] - 2\\text{Cov}[\\bar X, \\bar Y]\\\\\n",
    "&\\overset{?}{=}& \\text{Var}[\\bar X] + \\text{Var}[\\bar Y] \\\\\n",
    "&=& \\frac{\\text{Var}[X]}{n} + \\frac{\\text{Var}[Y]}{n} \\\\\n",
    "&=& \\frac{\\text{Var}[X] + \\text{Var}[Y]}{n} \\\\\n",
    "&\\overset{?}{=}& \\frac{\\text{Var}[X] + \\text{Var}[Y] −2Cov[X,Y]}{n} \\\\\n",
    "&=& \\frac{\\text{Var}[Z]}{n} \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$\\text{So if there's } \\textit{positive (negative) } \\text{dependency between $X_i$ and $Y_i$ estimating variance as }$\n",
    "\n",
    "$$\\text{Var}[\\bar X - \\bar Y] = \\text{Var}[\\bar X] + \\text{Var}[\\bar Y]$$\n",
    "\n",
    "$\\text{is wrong and } \\textit{overly optimistic (less efficient) } \\text{; on the other hand, testing arbitrarily paired independent variables}$ \n",
    "\n",
    "$$\\frac{\\text{Var}[Z]}{n} = \\frac{\\text{Var}[X] + \\text{Var}[Y] −2Cov[X,Y]}{n}$$\n",
    "\n",
    "$\\text{could produce chance covariation in $X_i$ and $Y_i$ which spuriously effected variance estimates}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If data is paired, paired testing is more powerful and efficient in terms of significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly treated as dependent samples: [1.8242898814947432, 0.06385117267142619]\n",
      "Incorrectly treated as independent samples: [0.35151413390161973, 0.36624828316469688]\n"
     ]
    }
   ],
   "source": [
    "paired_sample_test = list(stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "paired_sample_test[1] = paired_sample_test[1]/2\n",
    "print \"Correctly treated as dependent samples:\", paired_sample_test\n",
    "\n",
    "two_independent_samples_test = list(stats.ttest_ind(soft_corn_yield, hard_corn_yield, H0_mu0))\n",
    "two_independent_samples_test[1] = two_independent_samples_test[1]/2\n",
    "print \"Incorrectly treated as independent samples:\", two_independent_samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Making this concrete, suppose that we have two independent samples}$ \n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}Bern(p_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}Bern(p_Y)$$ \n",
    "\n",
    "$\\text{for $i = 1, \\cdots n$ and $j = 1, \\cdots m$ and we are again interested in testing}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{We will test this with $\\bar X - \\bar Y$ which we notate as\n",
    " $\\hat p_X - \\hat p_Y$}$\n",
    "\n",
    "$\\text{Suppose $H_0$ is true and $\\text{E}[X_i] = \\text{E}[Y_j] = p$}$\n",
    "$\\text{Then, under $H_0$ we have } \\textit{common variances}$\n",
    "\n",
    "$$\\text{Var}[X_i] = \\text{Var}[Y_i] = p(1-p)$$\n",
    "\n",
    "$\\text{And we estimate $p$ with $\\hat p = \\frac{\\sum X_i + \\sum Y_i}{n+m}$ and $\\text{Var}\\left[\\hat p_X - \\hat p_Y\\right] = \\frac{\\hat p(1- \\hat p)}{n} + \\frac{\\hat p(1- \\hat p)}{m}$}$\n",
    "\n",
    "$\\text{For large $n$ and $m$ then, under $H_0$ the CLT and normality of added normal variables imply}$ \n",
    "\n",
    "$$ \\hat p_X - \\hat p_Y \\overset{\\tiny approx}{\\sim} N\\left(0, \\frac{\\hat p(1- \\hat p)}{n} + \\frac{\\hat p(1- \\hat p)}{m}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We did this in the morning, plus you'll be doing a bit of this type of stuff in your sprint so I'll leave it for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Consider now the case of the two independent normally distributed samples}$\n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}N(\\mu_X,\\sigma^2) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}N(\\mu_Y,\\sigma^2) $$ \n",
    "\n",
    "$\\text{ for } i = 1, \\cdots n \\text{ and } j = 1, \\cdots m$\n",
    "\n",
    "$\\text{where we explicitly consider the case of a shared } \\textit{common variance } \\sigma^2$ \n",
    "\n",
    "$\\text{and where we are again interested in}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{E}[X_i] = \\text{E}[Y_j] \\\\\n",
    "H_a: \\text{E}[X_i] \\not = \\text{E}[Y_j]\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{If $H_0$ is true we can estimate the shared variance in an unbiased manner as}$\n",
    "\n",
    "\n",
    "$$ s^2_{XY} = \\frac{\\sum (X_i - \\bar X)^2 + \\sum (Y_i - \\bar Y)^2}{n + m - 2} $$\n",
    "\n",
    "$\\text{with the associated degrees of freedom $n + m - 2$ where}$\n",
    "\n",
    "\n",
    "$$\\text{Var}[\\bar X - \\bar Y] = \\frac{s^2_{XY}}{n} + \\frac{s^2_{XY}}{m}$$\n",
    "\n",
    "$\\text{and}$\n",
    "\n",
    "$$\\frac{\\bar X - \\bar Y}{\\sqrt{\\frac{s^2_{XY}}{n} + \\frac{s^2_{XY}}{m}}} \\sim t_{n+m-2}$$\n",
    "\n",
    "\n",
    "\n",
    "$\\textit{When the variances are not assumed to be equal, and CLT normality has not yet kicked in,}$\n",
    "$\\textit{the degrees of freedom for the t-distirubiton is given by the Welch–Satterthwaite equation}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In 1897, Thomas B. Case -- Guinness’s first scientific brewer -- and a cooperating scientist named Briant were examining the amount of soft and hard resins found in 50 gram samples of several seasons of American and Kent hops. For one lot of Kent hops Case had examined 11 samples and Briant had examined 14 and they observed soft resin averages of 4.05 grams and 4.2 grams, respectively. They observed even greater differences in their two samples of “American, 1895” at 0.35 grams and .5 grams for soft resins and hard resins, respectively.  Case wrote that “We could not... support the conclusion that there are no differences between pockets of the same lot” but he had no basis for evaluating whether observed differences represented random error from the samples or actual differences in the population.  It was 1899 when Case hired Gosset, but it would not be until 1908 that Gosset would solve this problem with his small sample inference theory.  \n",
    "\n",
    "http://fmmh.ycdsb.ca/teachers/fmmh_mcmanaman/pages/tok_ziliak1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.3726581884886247, pvalue=0.71281495261807737) \n",
      "(with common variances)\n"
     ]
    }
   ],
   "source": [
    "soft_resin_Case = np.array([3.33,3.39,3.85,4.16,3.59,4.25,4.94,4.06,4.06,4.55,4.37])\n",
    "soft_resin_Briant = np.array([4.11,5.48,2.94,1.52,3.87,5.64,4.85,4.68,5.11,4.13,5.44,2.34,3.50,5.19])\n",
    "\n",
    "print stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = True), \"\\n(with common variances)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.40860533169891627, pvalue=0.68774244874263024) \n",
      "(with unique variances)\n"
     ]
    }
   ],
   "source": [
    "print stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = False), \"\\n(with unique variances)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed variance for soft_resin_Case was 0.24024\n",
      "Observed variance for soft_resin_Briant was 1.58093846154\n",
      "\n",
      "Observed standard deviation for soft_resin_Case was 0.490142836324\n",
      "Observed standard deviation for soft_resin_Briant was 1.25735375354\n"
     ]
    }
   ],
   "source": [
    "print \"Observed variance for soft_resin_Case was\", np.var(soft_resin_Case, ddof = 1)\n",
    "print \"Observed variance for soft_resin_Briant was\", np.var(soft_resin_Briant, ddof = 1)\n",
    "\n",
    "print \"\\nObserved standard deviation for soft_resin_Case was\", \n",
    "print np.var(soft_resin_Case, ddof = 1)**.5\n",
    "print \"Observed standard deviation for soft_resin_Briant was\", \n",
    "print np.var(soft_resin_Briant, ddof = 1)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unequal length arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b820b714818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttest_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoft_resin_Case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoft_resin_Briant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/schwarls37/anaconda/lib/python2.7/site-packages/scipy/stats/stats.pyc\u001b[0m in \u001b[0;36mttest_rel\u001b[0;34m(a, b, axis, nan_policy)\u001b[0m\n\u001b[1;32m   3957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3959\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unequal length arrays'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3961\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unequal length arrays"
     ]
    }
   ],
   "source": [
    "stats.ttest_rel(soft_resin_Case, soft_resin_Briant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Guinness really does taste better in Ireland\n",
    "\n",
    "https://www.theguardian.com/science/blog/2011/may/27/barack-obama-guinness-taste-ireland\n",
    "http://blog.minitab.com/blog/michelle-paret/guinness-t-tests-and-proving-a-pint-really-does-taste-better-in-ireland\n",
    "\n",
    "$\\begin{array}{c|ccc}\n",
    "       & n & \\bar X & s_x \\\\\\hline\n",
    "Ireland &42 & 74 & 7.4 \\\\\n",
    "Elsewhere & 61 & 57 & 7.1 \\\\\n",
    "\\end{array}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbar1 = 74.\n",
    "s21 = 7.4**2\n",
    "n1 = 42\n",
    "\n",
    "xbar2 = 57.\n",
    "s22 = 7.1**2\n",
    "n2 = 61\n",
    "\n",
    "# Prove (xbar1-xbar2)/np.sqrt(s21/n1 + s22/n2) ~ N(0,1)...\n",
    "1 - stats.norm.cdf((xbar1-xbar2)/np.sqrt(s21/n1 + s22/n2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{If $n$ is small enough that the CLT does not yet provide a good approximation, and}$\n",
    "\n",
    "$$X_i \\sim f(\\theta) \\not = N\\left(\\mu,\\sigma^2\\right)$$\n",
    "\n",
    "$\\text{for $i = 1, \\cdots n,$ then according to scikit-learn you need to get more data.}$\n",
    "\n",
    "<table align=\"center\">\n",
    "<tr><td>\n",
    "<img src=\"stuff/ml_map.png\" width=\"750px\" align=\"center\">\n",
    "</tr></td>\n",
    "</table>\n",
    "\n",
    "\n",
    "$\\text{However, one thing you } \\textit{could } \\text{do is test the median, i.e.,}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{Median}(X_i) = m \\\\\n",
    "H_a: \\text{Median}(X_i) \\not = m\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{since if $H_0$ holds}$\n",
    "\n",
    "$$\\sum 1_{[X_i>m]} \\sim Binom(0.5, n)$$\n",
    "\n",
    "$\\text{This approach belongs to a class of tests known as } \\textit{nonparametric tests}$ \n",
    "\n",
    "$\\textit{Nonparametric tests } \\text{do not rely on any distributional assumptions about the data}$\n",
    "\n",
    "$- \\; \\text{They therefore are much more generally applicable than their parametric counterparts}$\n",
    "\n",
    "$- \\; \\text{And they are } \\textit{still } \\text{often nearly as powerful as their parametric counterparts}$\n",
    "\n",
    "$- \\; \\text{One then wonders why these more general purpose tools are not more heavily emphesized... }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sample degrees saccharine test p-values\n",
      "Parametric:  Ttest_1sampResult(statistic=2.4664308272896882, pvalue=0.090350065675395053)\n",
      "Nonparametric:  0.125\n"
     ]
    }
   ],
   "source": [
    "H0_mu0 = 133\n",
    "\n",
    "print \"Single Sample degrees saccharine test p-values\"\n",
    "print \"Parametric: \", stats.ttest_1samp(degrees_saccharine, H0_mu0)\n",
    "print \"Nonparametric: \", 2*stats.binom.pmf(np.sum((degrees_saccharine-H0_mu0)>0),4,0.5)\n",
    "# Show that this is the correct p-value based on the binomial idea in the last block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Some very useful nonparametric tests are }$\n",
    "\n",
    "$\\quad - \\; \\textbf{The Wilcoxon signed-rank test} \\text{, which addresses paired samples like the paired t-test}$\n",
    "\n",
    "$\\quad - \\; \\textbf{Mann–Whitney U} \\text{, which addresses independent samples, like the two-sample t-test}$\n",
    "\n",
    "$\\quad - \\; \\textbf{Fisher's exact test} \\text{, which addresses independence in (often, $2\\times2$) contingency tables } \\textit{(we will return to this soon)}$\n",
    "\n",
    "$\\quad - \\; \\textbf{Kolmogorov-Smirnov test} \\text{, which addresses distributional assumptions } \\textit{(we will return to this later)}$\n",
    "\n",
    "$\\quad - \\; \\text{And many many more... sign test, median test, Wilcoxon rank sum test, etc....} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired Soft/Hard corn yield test p-values\n",
      "Parametric:  Ttest_relResult(statistic=1.8242898814947432, pvalue=0.12770234534285238)\n",
      "Nonparametric:  WilcoxonResult(statistic=2.0, pvalue=0.093492483689835704)\n",
      "\n",
      "Case/Briant Independent soft resin test p-values\n",
      "Parametric:  Ttest_indResult(statistic=-0.40860533169891627, pvalue=0.68774244874263024)\n",
      "Nonparametric:  MannwhitneyuResult(statistic=61.0, pvalue=0.39604322238744261)\n"
     ]
    }
   ],
   "source": [
    "diffs = soft_corn_yield - hard_corn_yield\n",
    "H0_mu0 = 0\n",
    "\n",
    "print \"Paired Soft/Hard corn yield test p-values\"\n",
    "print \"Parametric: \", stats.ttest_rel(soft_corn_yield, hard_corn_yield, H0_mu0)\n",
    "print \"Nonparametric: \", stats.wilcoxon(soft_corn_yield, hard_corn_yield, correction = True) \n",
    "\n",
    "print \"\\nCase/Briant Independent soft resin test p-values\"\n",
    "print \"Parametric: \", stats.ttest_ind(soft_resin_Case, soft_resin_Briant, equal_var = False)\n",
    "print \"Nonparametric: \", stats.mannwhitneyu(soft_resin_Case, soft_resin_Briant,alternative = 'two-sided') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dr. Muriel Bristol, an acquaintance of Fisher was a British algologist who took tea with milk and (as was the usual practice of high class British aristocrats) prefered that the milk be poured into the cup before the tea. One day Fisher was having afternoon tea with Muriel and William Roach (who would later marry Muriel) but Muriel politely declined tea because (on this particular occasion) the milk had not been poured first. To this Fisher responded \"Nonsense. Surely it makes no difference,\" but Muriel insisted that it did, to which William proposed, \"Let's test her!\"  On their next afternoon tea engagement, Fisher brought back a newly invented test for the task at hand, a nonparametric test now known as \"Fisher's Exact Test\".  \n",
    "\n",
    "Together with William's help, Fisher tested tested Muriel using a sample of eight cups of tea. In a random order, William prepared four cups of tea with the milk poured first, and four cups of tea with the milk poured second -- all of which Muriel was able to correctly identify the correct milk/tea order for. \n",
    "\n",
    "$\\begin{array}{c|cc}\n",
    "&\\text{William poured milk first} & \\text{William poured milk second} \\\\\\hline\n",
    "\\text{Muriel said, \"Milk poured first\"} & 4 & 0\\\\\n",
    "\\text{Muriel said, \"Milk poured second\"} & 0 & 4 \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(inf, 0.014285714285714268)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.fisher_exact([[4,0],[0,4]], alternative = 'greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{As with Fisher's Exact test, interest in contingency tables often lies in the independence of the marginal variables }$\n",
    "\n",
    "$\\text{That is, for discrete variables }\n",
    "X_i \\overset{i.i.d.}{\\sim}f(\\theta_X) \\text{ and } Y_j \\overset{i.i.d.}{\\sim}f(\\theta_Y) \n",
    "\\text{ for } i = 1, \\cdots n \\text{ and } j = 1, \\cdots m$\n",
    "\n",
    "$\\text{we are often interested in }$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{$X_i$ and $Y_i$ are }independent \\\\\n",
    "H_a: \\text{$X_i$ and $Y_i$ are }dependent\n",
    "\\end{array} \\right. $\n",
    "\n",
    "\n",
    "\n",
    "$\\text{A } \\textit{chi-squared } \\text{test based on a large sample distributional approximation can be used to evaluate $H_0$ }$\n",
    "\n",
    "$\\text{The approximation is (conservatively) recommended for use } \\textit{only when }$\n",
    "\n",
    "$\\quad - \\; \\text{The number of counts in each cell is $\\geq 5$, or}$\n",
    "\n",
    "$\\quad - \\; \\text{The number of counts in each cell is $\\geq 10$ if the degrees of freedom are 1}$\n",
    "\n",
    "\n",
    "$\\text{The chi-squared test statistic itself is}$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "x^2 &=& \\sum_{r=1}^R \\sum_{c=1}^C \\frac{(O_{rc} - E_{rc})^2}{E_{rc}} \\text{ where }\\\\\n",
    "E_{rc} &=& \\frac{\\left(\\sum_i O_{ic}\\right)\\left(\\sum_j O_{rj}\\right)}{\\sum O_{ij}} \\text{ and} \\\\\\\\\n",
    "x^2 &\\overset{\\tiny approx}{\\sim}& \\chi^2_{df} \\;\\; where \\; df = (R-1)(C-1)\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Does grunting help McEnroe serve more aces?\n",
    "\n",
    "* McEnroe lost only one Collegiate match, which occured at Trinity University in San Antonio, TX\n",
    "\n",
    "\n",
    "$\\begin{array}{c|ccc}\n",
    "&\\text{McEnroe serves an ace} & \\text{McEnroe faults} & \\text{McEnroe's serve is retruned} \\\\\\hline\n",
    "\\text{McEnroe grunts on serve} & 61 & 32 & 144\\\\\n",
    "\\text{McEnroe silent on serve} & 35 & 8 & 53 \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed:\n",
      "[[61, 32, 144], [35, 8, 53]] \n",
      "\n",
      "Expected (if independent):\n",
      "[[  68.32432432   28.46846847  140.20720721]\n",
      " [  27.67567568   11.53153153   56.79279279]] \n",
      "\n",
      "Test Statistic: 4.5990483614\n",
      "p-value: 0.100306560167\n",
      "Degrees of Freedom: 2\n"
     ]
    }
   ],
   "source": [
    "chi2, p, ddof, expected = stats.chi2_contingency([[61,32,144],[35,8,53]])\n",
    "\n",
    "print \"Observed:\\n\", [[61,32,144],[35,8,53]],\"\\n\"\n",
    "print \"Expected (if independent):\\n\", expected,\"\\n\"\n",
    "\n",
    "msg = \"Test Statistic: {}\\np-value: {}\\nDegrees of Freedom: {}\"\n",
    "print msg.format( chi2, p, ddof ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{A } \\textit{chi-squared test } \\text{can also be used to examine distributional assumptions }in \\; toto$\n",
    "\n",
    "$\\text{For any binned random variable $X_i$ for $i = 1, \\cdots, n$ we can test}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: X_i \\sim f(\\theta) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{where $\\Pr(X_i = k) = \\theta_k$ }$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "x^2 &=& \\sum_{k=1}^K \\frac{(O_{k} - E_{k})^2}{E_{k}} \\text{ where }\\\\\n",
    "O_{k} &=& \\sum^n_{i=1} 1_{[X_i=k]}  \\\\\n",
    "E_{k} &=& n \\cdot \\theta \\text{ and} \\\\\n",
    "x^2 &\\overset{approx.}{\\sim}& \\chi^2_{df} \\;\\; where \\; df = n - K\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No example here... this is type of test is a total relic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{A } \\textit{chi-squared test } \\text{can also test variance in normally distributed populations }$\n",
    "\n",
    "$$X_i \\overset{i.i.d.}{\\sim}N(\\mu_X,\\sigma_X^2), \\; i = 1, \\cdots n$$\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\sigma_X^2 = \\sigma_0^2 \\\\\n",
    "H_a: \\sigma_X^2 \\not = \\sigma_0^2\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{since, if $H_0$ is True, then}$\n",
    "$$ \\frac{\\sum (X_i - \\bar X)^2}{\\sigma_0^2} \\sim \\chi_{n-1}^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance 0.2184\n",
      "p-value of H_0: variance=1 test 0.0155541196625\n",
      "\n",
      "Estimated variance 1.46801428571\n",
      "p-value of H_0: variance=1 test 0.164541843695\n"
     ]
    }
   ],
   "source": [
    "n = len(soft_resin_Case)\n",
    "print \"Estimated variance \" + str(np.var(soft_resin_Case, ddof = 0))\n",
    "print \"p-value of H_0: variance=1 test \" + str(2*stats.chi2.cdf((np.var(soft_resin_Case, ddof = 0)*n)/1, df = n - 1)) + \"\\n\"\n",
    "\n",
    "n = len(soft_resin_Briant)\n",
    "print \"Estimated variance \" + str(np.var(soft_resin_Briant, ddof = 0))\n",
    "print \"p-value of H_0: variance=1 test \" + str(2*(1-stats.chi2.cdf((np.var(soft_resin_Briant, ddof = 0)*n)/1, df = n - 1))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Further, the variances of two samples can be compared using an } \\textit{F-test:}$\n",
    "\n",
    "$\\text{If $\\chi^2_1$ and $\\chi^2_2$ are chi-squared random variables with $v$ and $w$ degrees of freedom}$\n",
    "\n",
    "$\\text{then $F = \\frac{\\chi^2_1}{v}\\div\\frac{\\chi^2_2}{w}$ is distributed as an } \\textit{F distribution } \\text{with degrees of freedom $v$ and $w$}$ \n",
    "\n",
    "$\\text{Therefore, for $Y_j \\overset{i.i.d.}{\\sim} N(\\mu_Y,\\sigma_Y^2), \\; j = 1, \\cdots m$, we can test}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\sigma_X^2 = \\sigma_Y^2 \\\\\n",
    "H_a: \\sigma_X^2 \\not = \\sigma_Y^2\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$ \\frac{\\sum (X_i - \\bar X)^2\\big/(n-1)}{\\sum (Y_i - \\bar Y)^2\\big/(m-1)} \\sim F_{n-1,m-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026270003435959755"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - stats.f.cdf(np.var(soft_resin_Briant, ddof = 1)/np.var(soft_resin_Case, ddof = 1),\n",
    "               len(soft_resin_Briant)-1, len(soft_resin_Case)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{The F-test is most often encountered in model selection contexts, and in particular in the } \\textit{multiple regression } \\text{context}$\n",
    "\n",
    "$\\text{Here, it is used to test if the $k$ features (covariates) } \\textbf{$X$}_i \\text{ have any explanatory power with respect to outcome $Y_i$}$\n",
    "\n",
    "$\\text{Under the usual multiple regression assumptions, we can evaluate}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: E[Y_i] = \\beta_0 \\\\\n",
    "H_a: E[Y_i] \\not = \\beta_0\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$ \\frac{\\sum (Y_i - \\beta_0)^2\\big/(n - 1 -1)}{\\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\big/(n-k-1)} \n",
    "=\n",
    "\\frac{RSS_{2}\\big/(n - 2)}{RSS_{k + 1}\\big/(n - k - 1)} \n",
    "\\sim F_{n-2,n-k-1} \n",
    "$$\n",
    "\n",
    "$\\text{ or, using the preferred method }$\n",
    "\n",
    "$$\\frac{\\left(\\sum (X_i - \\beta_0)^2 - \\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\right)\\big/(k - 1)}{\\sum (Y_i - \\textbf{$X$}_i\\textbf{$\\beta$})^2\\big/(n-k-1)} \n",
    "= \n",
    "\\frac{\\left(RSS_{2}- RSS_{k + 1}\\right)\\big/(k - 1)}{RSS_{k + 1}\\big/(n - k - 1)} \n",
    "\\sim F_{k-1,n-k-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will note this again in another lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{An interesting result we will revisit with } \\textit{logistic regression }\\text{is that } \\textit{deviance } \\text{for a $true$ $model$ $M$ with $k$ parameters}$\n",
    "\n",
    "$$D_M \\overset{\\tiny approx}{\\sim} \\chi^2_{n-k}$$\n",
    "\n",
    "$\\text{where for true model $M$ and saturated model $Y$}$\n",
    "\n",
    "$$D_M = -2\\left(\\log f\\left(Y|\\hat \\theta^M\\right) - \\log f\\left(Y|\\hat \\theta^Y\\right)\\right)$$\n",
    "\n",
    "$\\text{Hence for Model $R$ with $m-k$ parameters nested within Model $F$ with $m$ parameters we can evaluate}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: \\text{Models $R$ fits the data fairly well}  \\\\\n",
    "H_a: H_0 \\text{ is False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{using}$\n",
    "\n",
    "$$D_R - D_F \\overset{\\tiny approx}{\\sim} \\chi^2_{k}$$\n",
    "\n",
    "$\\text{since the sum (i.e., difference) of two $\\chi^2$ random variables remains $\\chi^2$ and if model $R$ does not fit the data $D_R$ will be large}$\n",
    "\n",
    "$\\text{Becaues there is no residual sums of squares ($RSS$) in GLM contexts and only the deviance approximation is available,}$ \n",
    "$\\text{in GLM contexts (such as logistic regression) this result plays the same role as the $F$ test in multiple linear regression}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will note this again in another lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{The } \\textit{Kolmogorov-Smirnov (K-S) test } \\text{provides another test of distributional assumptions}$\n",
    "\n",
    "$\\text{Interestingly, while the test tests distributional (often, parametric) assumptions,\n",
    "the test itself is } \\textit{nonparametric}$\n",
    "\n",
    "$\\text{The K-S test can examine distributional assumptions of a single sample}$\n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: X_i \\sim f(\\theta) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{of or compare the distributions of two samples} $ \n",
    "\n",
    "$ \\left\\{ \\begin{array}{l}\n",
    "H_0: f_X(\\theta_X) = f_Y(\\theta_Y) \\\\\n",
    "H_a: H_0 \\text{ False}\n",
    "\\end{array} \\right. $\n",
    "\n",
    "$\\text{Like all other tests, the K-S test has a test statistic (traditionally noted as $D_n$), and }$\n",
    "$\\text{is based on a null distribution (called the $Kolmogorov \\; distribution$ for the K-S test)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table align=\"center\">\n",
    "<tr>\n",
    "<td><img src=\"stuff/ks1.png\" width=\"300px\" align=\"center\"></td>\n",
    "<td><img src=\"stuff/ks2.png\" width=\"300px\" align=\"center\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ks_2sampResult(statistic=0.40909090909090906, pvalue=0.19174209011435731)\n",
      "KstestResult(statistic=0.020681760165127239, pvalue=0.78576856011520635)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print stats.ks_2samp(soft_resin_Case, soft_resin_Briant)\n",
    "\n",
    "print stats.kstest(stats.t.rvs(df=30,size=1000),'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parametric Versus Nonparametric\n",
    "\n",
    "#### We might characterize an analysis framework as parametric if\n",
    "\n",
    "### Results are buttressed or bolstered by modeling assumptions \n",
    "* E.g., leveraging the structure of normality via a t-test increases power but comes at a cost of loss of robustness compared to nonparametric tests free of distributional assumptions\n",
    "\n",
    "### Predicted values are based on \"parameters'' \n",
    "* E.g., the $\\beta$ coefficients in linear regression\n",
    "\n",
    "### Parameter estimation  determines the specific instance of a model within a \"model class'' defined by those parameters\n",
    "* E.g., the CLT is based on a normal distribution which is determined by estimating $\\mu$ and $\\sigma^2$\n",
    "\n",
    "###  The complexity of the model does grows as data size $n$ grows\n",
    "* E.g., trees grow in complexity as data becomes richer while a normal distribution is defined by $\\mu$ and $\\sigma^2$ regardless of $n$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
