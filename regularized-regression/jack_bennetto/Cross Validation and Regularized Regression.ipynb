{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Regularized Regression\n",
    "\n",
    "### Jack Bennetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of the day you should be able to\n",
    "\n",
    " * Describe the three kinds of model error.\n",
    " * State the purpose of cross validation\n",
    " * Explain k-fold cross validation\n",
    " * Explain the training, validation, testing data sets\n",
    " * State the purpose of Lasso and Ridge regression\n",
    " * Choose the regularization hyperparameter with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "This afternoon we will talk about\n",
    "\n",
    "* Bias and Variance\n",
    "* Train-test split\n",
    "* K-fold cross validation\n",
    "* Ridge\n",
    "* LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Let's plot some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs=8;npts=6;b0=2;b1=0.5;x=scs.uniform(0,10).rvs(npts,random_state=rs);y=b0+b1*x+scs.norm(0,1).rvs(npts, random_state=rs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'bo', label='some data')\n",
    "ax.set_ylim((0, 9))\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_col = x[:, None] # convert to column vector, to fit with sklearn\n",
    "xpts = np.linspace(0,10)[:, None] # points for plotting\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(x_col, y)\n",
    "yhat = model1.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'r:', label=\"linear fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know, that looks ok I guess, but it kind of looks quadratic. We can create a pipeline with `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(2)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model2.fit(x_col, y)\n",
    "yhat = model2.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'g:', label=\"quadratic fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but let's try a higher-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model3 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(4)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model3.fit(x_col, y)\n",
    "yhat = model3.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'k:', label='quartic fit')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! We did a great job!\n",
    "\n",
    "So how did we actually generate these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_actual = b0 + b1 * xpts\n",
    "ax.plot(xpts, y_actual, 'b:', label='actual function used to generate data')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we went off the rails there.\n",
    "\n",
    "First, what we did is called **overfitting**, when we fit the specific available data in a way that doesn't generalize to over data. This happens pretty often, whenever we have a very complicated model with many independent parameters. Making out model too complicated is bad.\n",
    "\n",
    "The opposite, called **underfitting**, is bad too. Suppose we'd just used the mean of the $y$ values to estimate $\\hat y$ for all the points. That's too simple of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "\n",
    "We'll come back to these again and again and again through the course. The error of a model can be divided into three components.\n",
    "\n",
    "1. **Irreducable error** is the error inherent in any value. Even if we had all possible data and could build a perfect model, we can't predict values exactly because there's error at each data point.\n",
    "2. **Bias** is due to the failure of the model to match our training sample. It's easy to get rid of bias with a complicated model that predicts all the data in our sample exactly.\n",
    "3. **Variance** is the error from the differences of our training sample and the larger population. If we had access to entire population of data, we would have no variance.\n",
    "\n",
    "In general, there is a tradeoff between bias and variance. A complex model might have very low bias, but will be highly dependent on the sample taken so wil have high variance. A simple model might have higher bias, because it underfits, but lower variance, predicting other data nearly as well as the training sample.\n",
    "\n",
    "Some models have **hyperparameters** that can be tuned. Most represent that tradeoff: moving them in one direction will lower the bias and raise the variance; moving them in the other will do the opposite.\n",
    "\n",
    "Ok, so how do we tell which model is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION!\n",
    "\n",
    "The basic concept behind cross validation is that we the data on which we train our model can't accurately access its effectiveness. That's due to overfitting, that no matter how hard we try to generalize the model, its always based more on the data we used than the data we didn't.\n",
    "\n",
    "Cross validation really has two separate purposes.\n",
    "\n",
    "First, it is used for **model comparison**. Over the coming week we'll learn a bunch of different models, and we need to evaluate which will do best for our data. In addition, many of these models have hyperparameters, and we need cross validation to choose the appropriate values. \n",
    "\n",
    "Second, it's used to **evaluate your model**. Part of the CRISP-DM is evaluation; you (usually) need to know how well your model will predict real-world results. There are many ways to measure that, like AUC/ROC or F-score some compination of precision and recall or sensitivity and specificity, based on your specific business case, but in the end the key thing is that you can't measure it on your training data.*\n",
    "\n",
    "\n",
    "**You can measure on your training data in some circumstances, either because your statistical measure allows some estimation of the error, or you have an ensemble model where different submodels see different data (out-of-bag error). But those aren't as general.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The train-test split\n",
    "\n",
    "The simplest approach we can use is the train-test split. You probably shouldn't call this cross validation, just say \"train-test split\" or \"hold-out validation.\"\n",
    "\n",
    "Let's start with the mtcars dataset that you've seen before in the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rows without data for horsepower. We're just going to throw those away for now without worrying to much if that's ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower.astype('float128')\n",
    "cars.mpg = cars.mpg.astype('float128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(cars.mpg, cars.horsepower, '.')\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cars[['mpg']]\n",
    "y = cars.horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(model, X, y):\n",
    "    return np.mean((model.predict(X) - y) **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we did a bit better on the training data, as expected...or did we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently it's pretty sensative to the random split. Let's explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for _ in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(model.score(X_train, y_train))\n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "                   \n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(train_score, test_score, '.', alpha=0.2)\n",
    "ax.plot([0, 1], [0, 1], ':')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('train $R^2$')\n",
    "ax.set_ylabel('test $R^2$')\n",
    "ax.set_xlim((.4,.8))\n",
    "ax.set_ylim((.4,.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we usually do better with the training set. Usually.\n",
    "\n",
    "Let's see if we can reproduce that train-test-split graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for degree in range(1, 11):\n",
    "    model = Pipeline([\n",
    "        ('pf', PolynomialFeatures(degree)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "    test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "    #train_score.append(-model.score(X_train, y_train))\n",
    "    #test_score.append(-model.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1, 11), train_score, '.-', label=\"train set\")\n",
    "ax.plot(range(1, 11), test_score, '.-', label=\"test set\")\n",
    "ax.set_xlabel('complexity')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for degree in range(1, 11):\n",
    "        model = Pipeline([\n",
    "            ('pf', PolynomialFeatures(degree)),\n",
    "            ('lr', LinearRegression())\n",
    "            ])\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "        test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'b.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'y.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'b.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'y.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('degree of polynomial')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")\n",
    "ax.plot(X_train, y_train, 'b.')\n",
    "ax.plot(X_test, y_test, 'y.')\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('pf', PolynomialFeatures(11)),\n",
    "    ('lr', LinearRegression())\n",
    "    ])\n",
    "model.fit(X_train, y_train)\n",
    "xpts = np.linspace(9, 47, 100).reshape(-1, 1)\n",
    "ax.plot(xpts, model.predict(xpts), 'b-')\n",
    "ax.set_ylim(40, 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this isn't all that consistant. We need something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "\n",
    "With Cross Validation, we randomly partition the data into $k$ groups, $D_1$, $D_2$, ..., $D_k$. For each $i \\in [1..k]$ we:\n",
    "\n",
    " * Build a model using $D_{j \\ne i}$ as a training data\n",
    " * Calculate the error of the model on $D_i$\n",
    " \n",
    "We average all these errors to compute the overall error of the model, and compare those across different models to choose the best model.\n",
    "\n",
    "There isn't a clear \"best\" value for $k$, but 5 is commonly choosen. The extreme version of k-fold cross validation, when $k=n$, is called leave-one-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "scores = []\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X.values[train], y.values[train])\n",
    "    scores.append(model.score(X.values[test], y.values[test]))\n",
    "    \n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many `sklearn` models include \"CV\" versions that use cross validation to calculate hyperparameters automatically.\n",
    "\n",
    "**Stratified cross validation** is a variation in which the partitions are choosen to have similar values for features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the testing data\n",
    "\n",
    "There's a problem with all this. Because the model and hyperparameters are choosen based on the training and testing data, the errors of the model aren't an accurate representaton of how it would behave on outside data. If we want to know how it will behave in general, we need to hold out additional data. In this case we have\n",
    "\n",
    " * **Training data** are used to fit the model.\n",
    " * **Vaidation data** are used to choose the model and hyperparameters. Once these are determined, these are combined with the training data to re-fit the model.\n",
    " * **Testing data** are used to evaluate the final accuracy of the model.\n",
    " \n",
    "Each of these can be used either with simple hold-out validation or with k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the past couple days we've talked about linear regression, in which we find the coefficients $\\beta_0$, $\\beta_1$, ..., $\\beta_p$ to minimize\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left( y_i = \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regession** we find the values to minimize\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left( y_i = \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right) + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Effectively we've penalizing extreme values of $\\beta$\n",
    "Note that we aren't including $\\beta_0$. The value $\\lambda$ is a hyperparameter of the model.\n",
    "\n",
    "Question: how should we decide the appropriate value for $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **LASSO Regession** (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) we find the values to minimize\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left( y_i = \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right) + \\lambda \\sum_{j=1}^{p} | \\beta_j |$$\n",
    "\n",
    "In many ways this is similar to Ridge:\n",
    "  * We're penalizing large values of $\\beta$.\n",
    "  * We aren't including $\\beta_0$\n",
    "  * We have a hyperparameter $\\lambda$\n",
    "\n",
    "The difference is the exponent. Ridge is sometimes known as **L2 regularization**, while LASSO is **L! regularization**. We'll talk more about this in a bit.\n",
    "\n",
    "For $\\lambda = 0$, these both reduce to a standard linear model.\n",
    "\n",
    "Questions:\n",
    "\n",
    "What does it mean if $\\lambda = 0$\n",
    "\n",
    "What does in mean if $\\lambda \\to \\infty$\n",
    "\n",
    "How does this relate to the bias-variance trade-off? If $\\lambda$ increasies, what happens to the bias? What happens to the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example\n",
    "\n",
    "Let's make up some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npts = 100\n",
    "nfeatures = 6\n",
    "x = np.zeros((npts, nfeatures))\n",
    "x[:, 0] = scs.uniform(-10, 20).rvs(npts)\n",
    "x[:, 1] = scs.uniform(-10, 20).rvs(npts)\n",
    "x[:, 2] = scs.uniform(-10, 20).rvs(npts)  + 0.2*x[:, 0]\n",
    "x[:, 3] = scs.uniform(-10, 20).rvs(npts)  + 0.4*x[:, 1]\n",
    "x[:, 4] = scs.uniform(-10, 20).rvs(npts)  + 0.6*x[:, 2] - 1.4*x[:, 0]\n",
    "x[:, 5] = scs.uniform(-10, 20).rvs(npts)  + 1.8*x[:, 3]\n",
    "\n",
    "beta = np.array([0.4, 0.3, 0.1, 1.2, 0.7, 0.2])\n",
    "\n",
    "y = np.sum(x * beta, axis=1) + scs.norm(0, 5).rvs(npts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the how the coefficients depend on various values of $\\lambda$.\n",
    "\n",
    "N.B.: the coefficient in `sklearn` is called $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = -2\n",
    "max_alpha_exp = 2\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(x, y)\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{}$\".format(feature))\n",
    "    plt.plot([10**min_alpha_exp, 10**max_alpha_exp], [beta[feature], beta[feature]],\n",
    "             ls=':',\n",
    "             color=color,\n",
    "             alpha=0.5)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for LASSO regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = 0\n",
    "max_alpha_exp = 6\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(x, y)\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{}$\".format(feature))\n",
    "    plt.plot([10**min_alpha_exp, 10**max_alpha_exp], [beta[feature], beta[feature]],\n",
    "             ls=':',\n",
    "             color=color,\n",
    "             alpha=0.5)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for Ridge regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: what's going on? How does this differ from LASSO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and regularization\n",
    "\n",
    "Over the next couple weeks we'll talk about a variety of predictive models, and various ways in which they are different. One of those is whether it is necessary to standardize/normalize the features before fitting. First, some definitions:\n",
    "\n",
    "**Standarization** (in this context) is the proccess of subtracting the mean from eah feature, and then dividing my the standard deviation, so each feature has a mean of 0 and standard deviation of 1.\n",
    "\n",
    "**Normalization** (again, in this context) is the process of subtracting the minimum value from each feature, and then dividing my the maximum, so each feature ranges from 0 to 1.\n",
    "\n",
    "Both accomplish the same purpose, of having all features on the same scale.\n",
    "\n",
    "Ok, so for some models you need to standardize (or normalize) the features before fitting the model, at least if they have significantly different ranges. It's not that hard to write code to do that, but the transformers in `sklearn` make that a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Lasso())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline model will take care of the transformations in fitting and prediction automatically. You can normalize using `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "In usual linear regression without regularization, scaling **does not matter**. If you multiply change the scale of a feature, it will change the cooresponding coefficient, but the predictions will be exactly the same.\n",
    "\n",
    "This changes when we add regularization. Since we include a term that is proportional to the $\\beta$, that actual predictions will change it we rescale the values.\n",
    "\n",
    "As a rule of thumb, if rescaling the values will change the predictions of a model, you need to standardize (or normalize) the values.\n",
    "\n",
    "Discussion: is standardizing or normalizing better? When should you do one or the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
