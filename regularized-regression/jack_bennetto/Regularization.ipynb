{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Regularized Regression\n",
    "\n",
    "### Jack Bennetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of the day you should be able to\n",
    "\n",
    " * State the purpose of Lasso and Ridge regression\n",
    " * Choose the regularization hyperparameter with cross validation\n",
    " * Perform standardization and normalization, and explain why it is needed with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent days we've talked about linear regression, in which we find the coefficients $\\beta_0$, $\\beta_1$, ..., $\\beta_p$ to minimize\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regession** we find the values to minimize\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right) \\right)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Effectively we've penalizing extreme values of $\\beta$\n",
    "Note that we aren't including $\\beta_0$. The value $\\alpha$ is a hyperparameter of the model.\n",
    "\n",
    "Question: how should we decide the appropriate value for $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **LASSO Regession** (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) we find the values to minimize\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right)\\right)^2 + \\alpha \\sum_{j=1}^{p} | \\beta_j |$$\n",
    "\n",
    "In many ways this is similar to Ridge:\n",
    "  * We're penalizing large values of $\\beta$.\n",
    "  * We aren't including $\\beta_0$.\n",
    "  * We have a hyperparameter $\\alpha$.\n",
    "\n",
    "The difference is the exponent. Ridge is sometimes known as **L2 regularization**, while LASSO is **L1 regularization**. We'll talk more about this in a bit.\n",
    "\n",
    "Questions:\n",
    "\n",
    "What does this mean if $\\alpha = 0$?\n",
    "\n",
    "What does this mean if $\\alpha \\to \\infty$\n",
    "\n",
    "How does this relate to the bias-variance trade-off? If $\\alpha$ increases, what happens to the bias? What happens to the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example\n",
    "\n",
    "Let's use the cars dataset to investigate, predicting the mpg from the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower = cars.horsepower.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model']\n",
    "x = cars[columns]\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the how the coefficients depend on various values of $\\lambda$.\n",
    "\n",
    "N.B.: the coefficient in `sklearn` is called $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = -3\n",
    "max_alpha_exp = 1.5\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    #model = Pipeline([('standardize', StandardScaler()),\n",
    "    #                  ('lasso', Lasso(alpha=alpha))])\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(x, y)\n",
    "    #coefs[i] = model.steps[1][1].coef_\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{{{}}}$\".format(columns[feature]))\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for LASSO regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = 0\n",
    "max_alpha_exp = 6\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(x, y)\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{{{}}}$\".format(columns[feature]))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for Ridge regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: what's going on? How does this differ from LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_circle(ax, radius, color, x=0, y=0):\n",
    "    ax.add_artist(plt.Circle((x, y),\n",
    "                             radius,\n",
    "                             color=color,\n",
    "                             fill=False))\n",
    "def draw_diamond(ax, radius, color, x=0, y=0):\n",
    "    ax.add_artist(plt.Polygon([(x, radius+y),\n",
    "                               (radius+x, y),\n",
    "                               (x, -radius+y),\n",
    "                               (-radius+x, y)],\n",
    "                              color=color,\n",
    "                              fill=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help understand this, consider the contour plots of the lost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.axhline(0, color='k')\n",
    "ax.axvline(0, color='k')\n",
    "\n",
    "for r in [1.0, 2.0, 2.5, 2.8, 3.0]:\n",
    "    draw_circle(ax, r, 'b', 3, 4)\n",
    "\n",
    "#for r in [1.0, 2.0, 2.5, 2.8, 3.0]:\n",
    "#    draw_circle(ax, r, 'b', 1.3, 4)\n",
    "\n",
    "#for r in [1.0, 1.4, 1.7, 2.0]:\n",
    "#    draw_circle(ax, r, 'g')\n",
    "\n",
    "#for r in [0.7, 1.3, 2, 2.7]:\n",
    "#    draw_diamond(ax, r, 'g')\n",
    "\n",
    "    \n",
    "for r in [0.7, 1.3]:\n",
    "    draw_diamond(ax, r, 'g')\n",
    "\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and regularization\n",
    "\n",
    "Over the next couple weeks we'll talk about a variety of predictive models, and various ways in which they are different. One of those is whether it is necessary to standardize/normalize the features before fitting. First, some definitions:\n",
    "\n",
    "**Standarization** (in this context) is the proccess of subtracting the mean from eah feature, and then dividing my the standard deviation, so each feature has a mean of 0 and standard deviation of 1.\n",
    "\n",
    "**Normalization** (again, in this context) is the process of subtracting the minimum value from each feature, and then dividing my the maximum, so each feature ranges from 0 to 1.\n",
    "\n",
    "Both accomplish the same purpose, of having all features on the same scale.\n",
    "\n",
    "Ok, so for some models you need to standardize (or normalize) the features before fitting the model, at least if they have significantly different ranges. It's not that hard to write code to do that, but the transformers in `sklearn` make that a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Lasso())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline model will take care of the transformations in fitting and prediction automatically. You can normalize using `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "In usual linear regression without regularization, scaling **does not matter**. If you multiply change the scale of a feature, it will change the cooresponding coefficient, but the predictions will be exactly the same.\n",
    "\n",
    "This changes when we add regularization. Since we include a term that is proportional to the $\\beta$, that actual predictions will change it we rescale the values.\n",
    "\n",
    "As a rule of thumb, if rescaling the values will change the predictions of a model, you need to standardize (or normalize) the values.\n",
    "\n",
    "Discussion: is standardizing or normalizing better? When should you do one or the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
