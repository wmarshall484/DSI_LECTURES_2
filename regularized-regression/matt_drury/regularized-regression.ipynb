{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from basis_expansions.basis_expansions import NaturalCubicSpline\n",
    "from regression_tools.dftransformers import (\n",
    "    ColumnSelector, Identity,\n",
    "    FeatureUnion, MapFeature,\n",
    "    StandardScaler)\n",
    "\n",
    "from regression_tools.plotting_tools import (\n",
    "    plot_partial_depenence,\n",
    "    plot_partial_dependences,\n",
    "    predicteds_vs_actuals)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "np.random.seed(154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression: Ridge and Lasso\n",
    "\n",
    "Regularized regression is a powerful technique used to control the complexity of predictive models.\n",
    "\n",
    "## Case Study: The Balance Data Set\n",
    "\n",
    "To explore how regularized regression works, we will revisit the `balance` dataset from our linear regression case study.  We will use regularization to control the complexity of our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balance = pd.read_csv(\"balance_non_zero.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be dealing with both continuous and catagorical features in this model.  Our treatments will be familiar at this point:\n",
    "\n",
    "  - We will use cubic splines to capture potential non-linear relationships between continuous varaibles and the response.\n",
    "  - We will use indicator encodings to include catagorical varaibles in the model.\n",
    "  \n",
    "To keep the code organized, we've written a couple of functions to make creating spline and indicator pipelines easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_spline_specification(name, knots):\n",
    "    select_name = \"{}_select\".format(name)\n",
    "    spline_name = \"{}_spline\".format(name)\n",
    "    return Pipeline([\n",
    "        (select_name, ColumnSelector(name=name)),\n",
    "        (spline_name, NaturalCubicSpline(knots=knots))\n",
    "    ])\n",
    "\n",
    "def simple_indicator_specification(var_name, levels):\n",
    "    select_name = \"{}_select\".format(var_name)\n",
    "    map_features = []\n",
    "    if not isinstance(levels, list):\n",
    "        levels = [levels]\n",
    "    for level in levels:\n",
    "        indicator_name = \"{}_{}_indicator\".format(var_name, level)\n",
    "        map_features.append(\n",
    "            (indicator_name, MapFeature(lambda var: var == level, indicator_name))\n",
    "        )\n",
    "    return Pipeline([\n",
    "        (select_name, ColumnSelector(name=var_name)),\n",
    "        (\"indicator_features\", FeatureUnion(map_features))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can very quickly write down the treatments for all our varaiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_spec = simple_spline_specification(\"Income\", [25, 50, 75, 100, 125])\n",
    "limit_spec = simple_spline_specification(\"Limit\", [3000, 4000, 5000, 6000, 7000, 8000])\n",
    "rating_spec = simple_spline_specification(\"Rating\", [200, 300, 400, 500, 600])\n",
    "age_spec = simple_spline_specification(\"Age\", [30, 40, 50, 60, 70])\n",
    "education_spec = simple_spline_specification(\"Education\", [8, 10, 12, 14, 16])\n",
    "\n",
    "cards_spec = simple_indicator_specification(\"Cards\", [0, 1, 2, 3])\n",
    "is_female = simple_indicator_specification(\"Gender\", \"Female\")\n",
    "student_spec = simple_indicator_specification(\"Student\", \"Yes\")\n",
    "married_spec = simple_indicator_specification(\"Married\", \"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a new, novel feature of our pipeline: when using regularized regression all the predictiors need to be on the same scale.  To achieve this, we have split the pipeline into two parts:\n",
    "\n",
    "  - The first path through the pipeline prepares continuous features for modeling.\n",
    "  - The second path through the pipeline prepares the catagorical features for modeling.\n",
    "  \n",
    "We have added a new transformer, `StandardScaler`, into our continous features pipeline.  This **standardizes** all of the continuous features.\n",
    "\n",
    "```\n",
    "standardized_feature = (raw_feature - mean(raw_feature)) / standard_deviation(raw_feature)\n",
    "```\n",
    "\n",
    "After standardization, each continuous feature has mean zero and variance one.  It will be clear soon why this adjustment is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features_scaled = Pipeline([\n",
    "    ('continuous_features', FeatureUnion([\n",
    "        ('income_fit', income_spec),\n",
    "        ('limit_fit', limit_spec),\n",
    "        ('rating_fit', rating_spec),\n",
    "        ('age_fit', age_spec),\n",
    "        ('education_fit', education_spec)])),\n",
    "    ('standardizer', StandardScaler())\n",
    "])\n",
    "    \n",
    "indicator_features = FeatureUnion([\n",
    "    ('cards_fit', cards_spec),\n",
    "    ('education_fit', education_spec),\n",
    "    ('student_fit', student_spec),\n",
    "    ('married_fit', married_spec)\n",
    "])\n",
    "\n",
    "balance_pipeline = FeatureUnion([\n",
    "    ('continuous_features', continuous_features_scaled),\n",
    "    ('indicator_features', indicator_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test\n",
    "\n",
    "To evaluate our models, we will split the data into training and testing sets.\n",
    "\n",
    "Since we **learn** the mean and variance of the features in our data before fitting any models, we need to take some care here.  Anything we learn needs to come from **only** the training data.  When transforming the test data, we need to use the transformers fit using only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw, test_raw = train_test_split(balance, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_pipeline.fit(train_raw)\n",
    "balance_train = balance_pipeline.transform(train_raw)\n",
    "balance_test = balance_pipeline.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it more clear to keep the response variable out of the featureization pipeline, though this is a debatable point.  To this end, I've written a small function to standadize the response variables in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_y(y_train, y_test):\n",
    "    y_mean, y_std = np.mean(y_train), np.std(y_test)\n",
    "    y_train_std = (y_train - y_mean) / y_std\n",
    "    y_test_std = (y_test - y_mean) / y_std\n",
    "    return y_train_std, y_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = standardize_y(train_raw[\"Balance\"], test_raw[\"Balance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization)** is a refinement of the classic linear regression model which adds a new parameter that can be used to control the complexity of the model structure.\n",
    "\n",
    "Recall that classical linear regression can be viewed as an optimization problem: we find the parameter estiamtes that minimize the mean squared error between the true and predicted values of the response.\n",
    "\n",
    "$$ \\hat \\beta_{lr} = argimin_{\\beta} \\frac{1}{n} \\sum_i (y_i - \\hat y_i)^2 $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\hat y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} $$\n",
    "\n",
    "Ridge regression adds a term to the loss function loss function to penalize complex models:\n",
    "\n",
    "$$ \\hat \\beta_{ridge} = argimin_{\\beta} \\frac{1}{n} \\sum_i (y_i - \\hat y_i)^2 + \\alpha \\sum_{j \\geq 1} \\beta_j^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new parameter $\\alpha$ is called the **regularization strength**.\n",
    "\n",
    "  - The regularization strenght must be a non-negative number, otherwise the ridge loss function can not be minimized.\n",
    "  - The ridge penalty does not apply to the intercept term, only the parameters associated with predictors.\n",
    "  - When $\\alpha = 0$ ridge regression reduces to standard linear regression.\n",
    "  - When $\\alpha = \\infty$, the ridge parameter estimates **except the intercept** are all zero.  We call this an **intercept only** model, and it always predicts the mean of the training data.\n",
    "\n",
    "So we have essentially created a slider that we can use to control the complexity of our model.\n",
    "\n",
    "  - When $\\alpha = \\infty$ our model is most simple: all predictions are the same number, all feature parameter estimates are zero.\n",
    "  - When $\\alpha = 0$ our model is most complex, it is free to choose any parameters, and it's only goal is to adapt to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Regularization Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to know which value of $\\alpha$ is best for any given problem, so we need an algorithm for choosing it.  Our main strategy is evaluate the predictive power of our model for different values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we create a grid of various values of $\\alpha$ and fit a ridge regression for each choice.\n",
    "\n",
    "It is traditional to choose the values of $\\alpha$ along a *logarithmic* grid, the numpy function `logspace` is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_regularization_strengths = np.logspace(np.log10(0.000001), np.log10(10000), num=100)\n",
    "\n",
    "ridge_regressions = []\n",
    "for alpha in ridge_regularization_strengths:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(balance_train, y_train)\n",
    "    ridge_regressions.append(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a entire bundle of ridge regressions, one for each value of $\\alpha$ in our grid.  Each ridge regression has it's own set of parameter estimates, and we can plot these parameter estimates as a function of $\\alpha$ to get a feel for how ridge regression behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_solution_paths(ax, regressions):\n",
    "    alphas = [np.log10(ridge.alpha) for ridge in regressions]\n",
    "    coeffs = np.concatenate([ridge.coef_.reshape(1, -1) \n",
    "                             for ridge in regressions])\n",
    "    for idx in range(coeffs.shape[1]):\n",
    "        ax.plot(alphas, coeffs[:, idx])\n",
    "    ax.set_xlabel(r\"$\\log_{10}(\\alpha)$\")\n",
    "    ax.set_ylabel(\"Estiamted Coefficient\")\n",
    "    ax.set_title(\"Coefficient Paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plot_solution_paths(ax, ridge_regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each curve here is the path of **one** estiamted parameter in the model.\n",
    "\n",
    "  - At the right hand side of the plot, $\\alpha$ is very large.  As you can see, the estimated parameters in ridge regression all converge towards zero as the regularization strength goes to zero.\n",
    "  - At the left hand side of the plot, $\\alpha$ is very small.  The coefficient paths disperse in this direction, and settle into the linear regression parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(balance_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "plot_solution_paths(ax, ridge_regressions)\n",
    "ax.scatter(np.repeat(-6, len(lr.coef_)), lr.coef_, color=\"black\", s=100,\n",
    "           label=\"Linear Regression Estimates\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a good sense of what ridge regression is up to, but we would like to know which $\\alpha$ is **best** for a given problem.  To do that, we evaluate the performance of each of our models using a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    n = X.shape[0]\n",
    "    return np.sum((y - preds)**2) / n\n",
    "\n",
    "def train_and_test_error(regressions, X_train, y_train, X_test, y_test):\n",
    "    alphas = [ridge.alpha for ridge in regressions]\n",
    "    train_scores = [rss(reg, X_train, y_train) for reg in regressions]\n",
    "    test_scores = [rss(reg, X_test, y_test) for reg in regressions]\n",
    "    return pd.DataFrame({\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "    }, index=alphas)\n",
    "\n",
    "def get_optimal_alpha(train_and_test_errors):\n",
    "    test_errors = train_and_test_errors[\"test_scores\"]\n",
    "    optimal_idx = np.argmin(test_errors.values)\n",
    "    return train_and_test_errors.index[optimal_idx]\n",
    "\n",
    "def plot_train_and_test_error(ax, train_and_test_errors, alpha=1.0, linewidth=2, legend=True):\n",
    "    alphas = train_and_test_errors.index\n",
    "    optimal_alpha = get_optimal_alpha(train_and_test_errors)\n",
    "    ax.plot(np.log10(alphas), train_and_test_errors.train_scores, label=\"Train MSE\",\n",
    "            color=\"blue\", linewidth=linewidth, alpha=alpha)\n",
    "    ax.plot(np.log10(alphas), train_and_test_errors.test_scores, label=\"Test MSE\",\n",
    "            color=\"red\", linewidth=linewidth, alpha=alpha)\n",
    "    ax.axvline(x=np.log10(optimal_alpha), color=\"grey\", alpha=alpha)\n",
    "    ax.set_xlabel(r\"$\\log_{10}(\\alpha)$\")\n",
    "    ax.set_ylabel(\"Mean Squared Error\")\n",
    "    ax.set_title(\"Mean Squared Error vs. Regularization Strength\")\n",
    "    if legend:\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_errors = train_and_test_error(\n",
    "    ridge_regressions, balance_train, y_train, balance_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "plot_train_and_test_error(ax, train_and_test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common, and expected picture.\n",
    "\n",
    "The **traning error** is monotonic with respect to the regularization parameter.  Linear regression gives the smallest training error, and increasing the regularization strength always increases the training error rate.\n",
    "\n",
    "The **testing error** behaves differently.\n",
    "\n",
    "  - Under regularizing results in an **overfit** model.  The model bias is at its lowest, but the model variance is high.\n",
    "  - Over regularizing results in an **underfit** model.  The model bais is very high (we are essentially just predicting a constant), but the model variance is very low (that constant is relatively insensitive to the training data).\n",
    "  \n",
    "In the middle, we find a good compromize between bias and variance by finding the value of $\\alpha$ that minimizes the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_optimal_alpha = get_optimal_alpha(train_and_test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Final Model\n",
    "\n",
    "Once you have determined the best regularization parameter for your problem, it is best practice to then combine all your data and refit the model using this value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_pipeline.fit(balance)\n",
    "final_train = balance_pipeline.transform(balance)\n",
    "y_final = (balance[\"Balance\"] - balance[\"Balance\"].mean()) / balance[\"Balance\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ridge_model = Ridge(alpha=ridge_optimal_alpha)\n",
    "final_ridge_model.fit(final_train, y_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this ridge regression to make predictions on our production data, comfortable that we have found a good balance between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression was the first regularization method discovered and studied.  Ridge is simple, and is often a very good tool for increasing the predictive power of linear models.\n",
    "\n",
    "None the less, there are situations that ridge regression does not address.  In some situations we suspect that but a very few predictors of many may be driving a phenomina.  For example, in detecting which genes drive a disease or syndrome, we do not suspect that \"all genes contribute a litte bit\", but that \"most contribute nothing, a few contribute a lot\".\n",
    "\n",
    "Since ridge regression leaves **all** predictors in the model, only shrinking thier parameter estiamtes, it is innapropriate for these situations.\n",
    "\n",
    "**[Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)** was introduced by Robert Tibshirani in 1996 to address this situation, it is much like ridge, but instead of shrinking estimates asymptotically to zero, it shrinks estimates exactly **to zero**, removing them from contributing to the model.\n",
    "\n",
    "### The Optimization Problem\n",
    "\n",
    "Lasso makes a simple change to the ridge loss function, but one that drastically changes its behaviour.\n",
    "\n",
    "$$ \\hat \\beta_{lasso} = argimin_{\\beta} \\frac{1}{n} \\sum_i (y_i - \\hat y_i)^2 + \\alpha \\sum_{j \\geq 1} \\| \\beta_j \\| $$\n",
    "\n",
    "The regularization term now penalizes the absolute value of the coefficients, not thier square.  This has a drastic effect on the solution path for the parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_regularization_strengths = np.logspace(np.log10(0.00001), np.log10(10), num=100)\n",
    "\n",
    "lasso_regressions = []\n",
    "for alpha in lasso_regularization_strengths:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(balance_train, y_train)\n",
    "    lasso_regressions.append(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plot_solution_paths(ax, lasso_regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that coefficients are set **exactly to zero**, instead of gradually shrinking towards zero but never getting there.  This means that lasso models tend to have a small number of non-zero parameters, while ridge models have many non-zero parameters.\n",
    "\n",
    "Evaluating a lasso regression is accomplished in the same way as with ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test_errors = train_and_test_error(\n",
    "    lasso_regressions, balance_train, y_train, balance_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "plot_train_and_test_error(ax, train_and_test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ridge and Lasso with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not often a good idea to use a single training and testing set when evaluating the performance of regularized regressions, too much risk is bundled up in the splitting of your data.\n",
    "\n",
    "As is usual, much better is to use cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "\n",
    "cv_models = []\n",
    "errors = []\n",
    "splitter = KFold(n_splits=n_folds)\n",
    "for train_idxs, test_idxs in splitter.split(balance):\n",
    "    # Split the raw data into train and test\n",
    "    train_raw, test_raw = balance.iloc[train_idxs], balance.iloc[test_idxs]\n",
    "    \n",
    "    # Fit and transform the raw data.\n",
    "    # All training of the transformers must only touch tfhe training data!\n",
    "    balance_pipeline.fit(train_raw)\n",
    "    balance_train_cv = balance_pipeline.transform(train_raw)\n",
    "    balance_test_cv = balance_pipeline.transform(test_raw)\n",
    "    y_train_cv, y_test_cv = standardize_y(train_raw[\"Balance\"], test_raw[\"Balance\"])\n",
    "    \n",
    "    # Fit all the models at different regularization strengths\n",
    "    ridge_regressions = []\n",
    "    for alpha in ridge_regularization_strengths:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(balance_train_cv, y_train_cv)\n",
    "        ridge_regressions.append(ridge)\n",
    "    cv_models.append(ridge_regressions)\n",
    "    \n",
    "    # Calculate the error curves for each CV fold, for each regularization strength\n",
    "    train_and_test_errors = train_and_test_error(\n",
    "        ridge_regressions, balance_train_cv, y_train_cv, balance_test_cv, y_test_cv)\n",
    "    errors.append(train_and_test_errors)\n",
    "    \n",
    "    # Calculate the mean errors across all CV folds, for each regularization strength\n",
    "    train_errors = np.empty(shape=(n_folds, len(ridge_regularization_strengths)))\n",
    "    for idx, tte in enumerate(errors):\n",
    "        te = tte['train_scores']\n",
    "        train_errors[idx, :] = te\n",
    "    mean_train_errors = np.mean(train_errors, axis=0)\n",
    "    \n",
    "    test_errors = np.empty(shape=(n_folds, len(ridge_regularization_strengths)))\n",
    "    for idx, tte in enumerate(errors):\n",
    "        te = tte['test_scores']\n",
    "        test_errors[idx, :] = te\n",
    "    mean_test_errors = np.mean(test_errors, axis=0)\n",
    "    \n",
    "    mean_errors = pd.DataFrame({\n",
    "        'train_scores': mean_train_errors,\n",
    "        'test_scores': mean_test_errors,\n",
    "    }, index=ridge_regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illistrate the variance that different splits into training and testing sets can have, here are the coefficient paths for all of the 10 models fit through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(16, 12))\n",
    "\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    plot_solution_paths(ax, cv_models[idx])\n",
    "    ax.set_title(\"Coefficient Paths, Fold {}\".format(idx))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the various training and testing error curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(16, 12), sharey=True)\n",
    "\n",
    "for idx, (ax, ttes) in enumerate(zip(axs.flatten(), errors)):\n",
    "    plot_train_and_test_error(ax, ttes, legend=False)\n",
    "    ax.set_title(\"Train vs. Test Error, Fold {}\".format(idx))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see here that some folds result in \"easy\" testing sets, and some folds result in \"hard\" test sets.\n",
    "\n",
    "To get a final value of $\\alpha$ to use, we average these training and testing eror curves, and then take the minimum average testing error.  This leads to a more stable estimate of $\\alpha$, and is a best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "for ttes in errors:\n",
    "    plot_train_and_test_error(ax, ttes, alpha=0.15, legend=False)\n",
    "plot_train_and_test_error(ax, mean_errors, linewidth=4, legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
