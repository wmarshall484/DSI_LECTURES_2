{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calculate TF-IDF using functions and broadcast variables.</h1>\n",
    "<p>TF-IDF or text frequency, inverse document frequency, is a way of rating the importance of a word in a particular document.  It is used in search engines, text vectorizers, just to name a few.  As an FYI, this is built into the Spark ETL package, and detailed here: http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf <br/><br/>\n",
    "\n",
    "To do so, we'll revisit the text sorting example from before, but we'll introduce a few new concepts like leveraging functions, and using broadcast variables.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "rdd = sc.parallelize(fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes')).data)\n",
    "\n",
    "n_docs = rdd.count()\n",
    "print(n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Find the document frequency</h2>\n",
    "<p>Here, were going to do a modified document frequency.  To do so, we'll use a function that will get the unique document tokens for us.  We'll be introducing two concepts, using functions for custom maps/filters, and the idea of a broadcast variable.<br/><br/>\n",
    "Using functions for maps is super helpful if you want to do a transformation the same way in multiple places, or if the transformation is more complex than something you want to write in a single line.<br/><br/>\n",
    "Broadcast variables are a little bit different.  If you are opperating on a distributed system, the remote cores don't know about the local instances.  You can either pass a local variable to a function, but that variable pay need to be passed hundreds of times to each node depending on the size of the data.  To avoid this needless use of bandwidth, you can create a broadcast variable.  What this does is it sends the variable to all the nodes a single time, so that they can use them whenever you call them.  The only way they differ from the variable, is that you'll need to call the `value` object (you can see it in the 1st line of 'unique_doc_words'\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk #-> if you need to download the stopword corpa, here's how to call the interface\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_doc_words(doc, stop_words):\n",
    "    l_words = filter(lambda x: x not in stop_words.value and x != '',\n",
    "                        map(lambda word: word.strip(\".,-;?!\\\"\").lower(), doc.split()))\n",
    "    return list(set(l_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['out', 'we', 'was', 'how', 'myself']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "print(list(stop)[:5])\n",
    "\n",
    "bc_stop = sc.broadcast(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842254"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_term_docs = rdd.flatMap(lambda doc: unique_doc_words(doc, bc_stop)).cache()\n",
    "rdd_term_docs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now for the document frequency</h2>\n",
    "<p>The rest of the document frequency will look very familiar. We will add a second function so that we are left with a dictionary that will give the IDF score in the form:\n",
    "$$IDF(t,N_D) = log( (N_D + 1)/ (DF(t)+1))$$\n",
    "<br/>\n",
    "Where ``N_D`` is the total number of documents, ``t`` is the term, and ``DF(t)`` is the document frequence for a given term.  Note that since we use a log, if a term exists in every document, it's ``IDF`` value will be 0.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "#Remember to create broadcast variables!\n",
    "bc_n_docs = sc.broadcast(n_docs)\n",
    "\n",
    "def idf(doc_freq, n_docs):\n",
    "    return log10((1+n_docs.value)/(1+doc_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pro-israeli', 3.752624562626766), ('alt.atheist.hard', 3.752624562626766), ('3c$1r$)jjt`b+33%!admicj!#!!!4a3!!!!!!!!b9!!!!!,ap!!!:', 3.752624562626766), (\"$m1vrbjcx!nr)3frchypk(r1chjjij#hk%'j84pq+#+$a2&r&bz,ff1v,-kg6qg9\", 3.752624562626766), ('a$lkh*q6-qhh2milc*q2iq$p([gesp(ejn!\"bhmdhll$&qh\\'lr`e26c2(qbqsrmm', 3.752624562626766)]\n"
     ]
    }
   ],
   "source": [
    "df = rdd_term_docs.map(lambda word: (word, 1))\\\n",
    "            .reduceByKey(lambda a, b: a+b)\\\n",
    "            .map(lambda x: (x[0], idf(x[1], bc_n_docs)))\\\n",
    "            .collectAsMap()\n",
    "            \n",
    "print(list(sorted(df.items(), key=lambda x: x[1], reverse=True))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Woo hoo!</h2>\n",
    "<p>If you've gotten through these three notebooks, I hope I've piqued your interest.  Spark is a huge library, and with a large open sourced community, it's expanding and getting better all the time.  I will try to build up a library of examples, but if you have a specific topic that you want to look into.  If you'd like to contribute, let me know and I'll have a look at what you would like to add.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
