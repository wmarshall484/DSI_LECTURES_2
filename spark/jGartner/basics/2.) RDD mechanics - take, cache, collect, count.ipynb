{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RDD Mechanics</h1>\n",
    "<p>The first example did a ton of things, so I'd like to proceed a bit more slowly to get a sense of what's going on.  Here, I'll simplify the code a bit so we can have a sense of what's happening at a more grainular level.  Let's begin by creating a small dataset manually, and turn it into an RDD.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark is great for big data.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"Spark is great for big data.\", \n",
    "        \"Big data cannot fit on one computer.\",\n",
    "        \"Spark can be installed on a cluster of computers.\"\n",
    "       ]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What is an RDD?</h2>\n",
    "<p>The most important thing to understand about Spark programing is what an RDD is.  An RDD is not a collection of data or variables, but rather, it is a map from a set of variables to a manipulated state of those variables.  Spark employs what is called 'lazy evaluation', which means that it will only compute up to the point that it needs to satisfy the current prompt.  To illustrate this point, I will create an rdd map without collecting, which should run very quickly.  However, when I try to aquire a sample, the process will then get kicked off.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def timeit(method):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "        ts = time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time()\n",
    "\n",
    "        print \"{} {}\".format(method.__name__, te-ts)\n",
    "        return result\n",
    "\n",
    "    return timed\n",
    "\n",
    "@timeit\n",
    "def col_rdd(rdd):\n",
    "    print rdd.collect()\n",
    "\n",
    "@timeit\n",
    "def sort_rdd(rdd):\n",
    "    rdd.sortBy(lambda x: x[0]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for completion, step 1: 0.000345945358276\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "rdd_fm = rdd.flatMap(lambda text: text.split())\n",
    "rdd_tup = rdd_fm.map(lambda word: (word.strip(\".,-;?\").lower(),1))\n",
    "print \"Time for completion, step 1:\", time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'is', 'great', 'for', 'big', 'data.', 'Big', 'data', 'cannot', 'fit', 'on', 'one', 'computer.', 'Spark', 'can', 'be', 'installed', 'on', 'a', 'cluster', 'of', 'computers.']\n",
      "col_rdd 0.159617900848\n",
      "sort_rdd 0.529456853867\n"
     ]
    }
   ],
   "source": [
    "col_rdd(rdd_fm)\n",
    "sort_rdd(rdd_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Caching progress</h2>\n",
    "<p>Here, we can see that the step of creating the RDD happens almost instantly, where the process of simply printing out the objects that are created in that step take much longer.  Again, this is due to the fact that the RDD is simply creating a map to a particular state.  This is exaserbated when you try to do multiple operation on the same RDD, because not only do the new operations need to be performed, but the new ones do as well.<br/><br/>\n",
    "In order to avoid this behavior, you can tell spark to cache results at various points along the way.  Caching will store the values of an RDD in memory (if you have enough) so that you can avoid uneeded calculations.<br/><br/>\n",
    "The other side of the coin is that the graphs that spark build are efficient, so unless you need to reference back to data, generally you are well served simply executing all at once.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('on', 2), ('big', 2), ('spark', 2), ('a', 1), ('be', 1), ('cluster', 1), ('can', 1), ('of', 1), ('great', 1), ('fit', 1), ('is', 1), ('for', 1), ('computers', 1), ('cannot', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd_tup.cache()\n",
    "terms = rdd_tup.reduceByKey(lambda a, b: a+b)\\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\\\n",
    "            .take(15)\n",
    "            \n",
    "print terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
