{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark in Python\n",
    "\n",
    "### Goals\n",
    "* Learn how to create a Spark Context in Python through pyspark\n",
    "* Learn how to create and use RDDs in Python\n",
    "* Learn how to use MLlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformations and Actions\n",
    "===========================\n",
    "\n",
    "Common RDD Constructors\n",
    "-----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`sc.parallelize(iterable)`               |Create RDD of elements of some iterable\n",
    "`sc.textFile(path)`                      |Create RDD of lines from file\n",
    "\n",
    "Common Transformations\n",
    "----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`filter(boolean condition)`              |Returns for where some boolean condition is True\n",
    "`map(some function)`                     |Applies some function\n",
    "`flatMap(some function)`                 |Apply some function that returns an iterator and flatten the entire output\n",
    "`sample(withReplacement=True, ratio)`    |Sample the data by some ratio\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(key function, ascending=True)`   |Sort elements by key defined in function in designated order\n",
    "`randomSplit([ratio1, ratio2], seed)`    |Splits your data into two depening on ratio array\n",
    "\n",
    "Common Actions\n",
    "--------------\n",
    "\n",
    "Expression                             |Meaning\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(n)`                              |First n elements of RDD \n",
    "`top(n)`                               |Top n elements of RDD\n",
    "`takeSample(withReplacement=True, n)`  |Create sample of n elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)\n",
    "`takeOrdered(n, function)`             |Returns n ordered elements as sorted by the value returned by the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Find all prime numbers between 1 and 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First lets import pyspark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_prime(n):\n",
    "    \"\"\"\n",
    "    Checks if a number is prime\n",
    "    \"\"\"\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for i in xrange(3, int(math.sqrt(n)) + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize a Spark Context using pyspark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Constrct a RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parallize creates a RDD using an iterator\n",
    "numbers_rdd = sc.parallelize(xrange(2, 101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use a Transformation RDD to filter for primes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primes_rdd = numbers_rdd.filter(check_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use an action RDD to show the primes we filtered for.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primes_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In practice, we can just do all these in a single entry.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize(xrange(2, 101)).filter(check_prime) \\\n",
    "                              .collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on (Key, Value) RDDs\n",
    "\n",
    "Common Pair RDD Transformations\n",
    "----------------------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`groupByKey(key value rdd)`              |Collapse a key value RDD by the key, and keeps the values in a iterable\n",
    "`reduceByKey(some function)`             |Collapse a key value RDD by the key, and combines the values by some function\n",
    "`mapValues(some function)`               |Apply some function to the values of some key value RDD\n",
    "`flatMapValues(some function)`           |Apply some function that returns an iterator the the values of some key value RDD, and create a key value for each iterates\n",
    "`keys()`                                 |Returns the keys of a key value RDD\n",
    "`values()`                               |Returns the values of a key value RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Using the below sales data, lets find the total dollars sold for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00\n",
    "107    11/20/2014     700     OR     329        400.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will construct a RDD using textFile.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_rdd = sc.textFile('sales.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we look at the top two lines of the file, we can see that the column gets imported.....*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_rdd.map(lambda x: x.split()) \\\n",
    "         .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*..... so lets get rid of it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_rdd.map(lambda x: x.split()) \\\n",
    "         .filter(lambda x: not x[0].startswith('#')) \\\n",
    "         .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can do this by grouping then summing the values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_rdd.map(lambda x: x.split()) \\\n",
    "         .filter(lambda x: not x[0].startswith('#')) \\\n",
    "         .map(lambda x: (x[4], float(x[5]))) \\\n",
    "         .groupByKey() \\\n",
    "         .map(lambda (k, v): (k, sum(v))) \\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Or we can do this by simply reducing the values by their keys*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_rdd.map(lambda x: x.split()) \\\n",
    "         .filter(lambda x: not x[0].startswith('#')) \\\n",
    "         .map(lambda x: (x[4], float(x[5]))) \\\n",
    "         .reduceByKey(lambda v1, v2: v1 + v2) \\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on Multiple RDDs \n",
    "\n",
    "Common Multiple RDD Transformations\n",
    "----------------------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`union(another rdd)`                     |Append another RDD to current RDD\n",
    "`join(another rdd)`                      |Join another RDD to current RDD by matching keys\n",
    "`leftOuterJoin(another rdd)`             |Join another RDD to current RDD where another RDD has matching keys\n",
    "`rightOuterJoin(another rdd)`            |Join current RDD to other RDD where current RDD has matching keys\n",
    "`zip(another rdd)`                       |Combines two RDD to form a key value pair RDD\n",
    "\n",
    "### Example 3:  Use the customer data below with the sales data to find average sold per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile customers.txt\n",
    "#Store   Customers\n",
    "100      50\n",
    "700      14\n",
    "203      25\n",
    "202      30\n",
    "101      10\n",
    "202      40\n",
    "700      20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_rdd = sc.textFile('customers.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First, lets calculate the total customers for each store.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_cust_rdd = customer_rdd.map(lambda x: x.split()) \\\n",
    "                             .filter(lambda x: not x[0].startswith('#')) \\\n",
    "                             .map(lambda x: (x[0], float(x[1]))) \\\n",
    "                             .reduceByKey(lambda v1, v2: v1 + v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next, lets calculate the total amount purchased for each store.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sales_rdd = sales_rdd.map(lambda x: x.split()) \\\n",
    "                           .filter(lambda x: not x[0].startswith('#')) \\\n",
    "                           .map(lambda x: (x[2], float(x[5]))) \\\n",
    "                           .reduceByKey(lambda v1, v2: v1 + v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can join the two tables and take the average.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sales_rdd.join(total_cust_rdd) \\\n",
    "               .mapValues(lambda (x, y): x / y) \\\n",
    "               .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz: Which state has the highest sales per customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cache RDDs you expect to use a lot to speed up your applications.  Do so simply by doing .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cached_rdd = customer_rdd.map(lambda x: x.split()) \\\n",
    "                         .filter(lambda x: not x[0].startswith('#')) \\\n",
    "                         .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Spark with MLlib\n",
    "\n",
    "Spark has implementation of most of the common/popular machine learning algorithms, such as:\n",
    "\n",
    "* Statistical Tests\n",
    "* Classification and regression\n",
    " * Linear models (SVMs, logistic regression, linear regression)\n",
    " * Naive Bayes\n",
    " * Decision Trees\n",
    " * Ensembles of Trees (Random Forests and Gradient-Boosted Trees)\n",
    "* Collaborative filtering\n",
    " * Alternating Least Squares (ALS or set to non-negative for NMF)\n",
    "* Clustering\n",
    " * K-means\n",
    "* Dimensionality reduction\n",
    " * Singular Value Decomposition (SVD)\n",
    " * Principal Component Analysis (PCA)\n",
    "\n",
    "For a full list of implementations, please reference the documentation at http://spark.apache.org/docs/latest/mllib-guide.html.\n",
    "\n",
    "## Vectors and Matrices\n",
    "\n",
    "You can create dense or sparse Vectors and Matrices in Spark, but they are types and not actually RDDs.  As a result, if you you create a RDD of matrices, it is essentially an Array of Matrices.  In most cases, you will only be using Vectors.\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.linalg import Matrices, Vectors\n",
    "\n",
    "Vectors.dense([1, 2, 4]) # Creates [1, 2, 4]\n",
    "Vectors.sparse(3, [0, 2], [1, 4]) # Creates [1, 0, 4]\n",
    "Matrices.dense(2, 2, np.array([1, 2, 3, 4])) # Creates [[1, 2], [3, 4]]\n",
    "Matrices.sparse(2, 2, [0, 1, 2], [0, 1], [1, 1]) # Creates [[1, 0], [0, 1]]\n",
    "```\n",
    "\n",
    "\n",
    "## Supervised Models and LabeledPoint\n",
    "\n",
    "Supervised models in Spark requires a LabeledPoint RDD, where each observation is a label with a feature vector.  LabeledPoints can be instantiated by:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "some_data_rdd.map(lambda x: LabeledPoint(x[{y index}], x[{feature(s) index}]))\n",
    "```\n",
    "\n",
    "To extract the labels and features, use map on each point.\n",
    "\n",
    "```python\n",
    "labels = some_labelpoints_rdd.map(lambda x: x.label)\n",
    "features = some_labelpoints_rdd.map(lambda x: x.features)\n",
    "```\n",
    "\n",
    "## StandardScaler\n",
    "\n",
    "Several models in Spark can be very sensitive to different feature scaling.  We will often need to use StandardScaler to resolve this issue.  StandardScaler requires a feature RDD, and can be utilized as such:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "scaler.transform(features)\n",
    "```\n",
    "\n",
    "## Train Test Split\n",
    "\n",
    "There is no built in train test split function in Spark, but we do have transformation RDDs that specializes in random sampling or splits such as randomSplit.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Spark has an evaluation package that allows you to calculate prediction errors.  Some options available are:\n",
    "\n",
    "* BinaryClassificationMetrics\n",
    "* MulticlassMetrics\n",
    "* RegressionMetrics\n",
    "* RankingMetrics\n",
    "* More at https://spark.apache.org/docs/1.5.2/mllib-evaluation-metrics.html\n",
    "\n",
    "Within each of the available Metrics module, there's a selection of suitable metrics such as mean squared error for regression and precision for classification.  All of these modules requires a key value or tuple RDD of label and prediction.  For example:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print metrics.meanSquaredError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Using the cars data, lets do a regression to predict MPG.\n",
    "\n",
    "*First, lets import all the relevant packages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionModel, LinearRegressionWithSGD\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets create the data RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars_rdd = sc.textFile('../data/cars_scrubbed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now lets transform our data into LabeledPoint RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars_labeledpoints = cars_rdd.map(lambda x: x.split(',')) \\\n",
    "                             .filter(lambda x: not x[1].startswith('m')) \\\n",
    "                             .map(lambda x: [float(y) for y in x]) \\\n",
    "                             .map(lambda x: LabeledPoint(x[1], x[2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We need to scale our data, this will require us to extract the feature label, scale, and recreate our LabeledPoint RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = cars_labeledpoints.map(lambda x: x.label)\n",
    "features = cars_labeledpoints.map(lambda x: x.features)\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "cars_scaled = labels.zip(scaler.transform(features))\n",
    "cars_scaled = cars_scaled.map(lambda (y, x): LabeledPoint(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets create a train test split.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = cars_scaled.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can train a Linear Regression Model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionWithSGD.train(training, iterations=2000, step=0.1, intercept=True, regType='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets Verify that our model is predicting fine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.map(lambda x: (x.label, model.predict(x.features))).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I want to see mean squared error for both test and training, so lets build a wrapper function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mse(rdd):\n",
    "    valuesAndPreds = rdd.map(lambda x: (x.label, float(model.predict(x.features))))\n",
    "    metrics = RegressionMetrics(valuesAndPreds)\n",
    "    return metrics.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training Error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_mse(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing Error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_mse(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz:  Build a Random Forest model to predict the origin of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
