{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Spark with MLlib\n",
    "\n",
    "Spark has implementation of most of the common/popular machine learning algorithms, such as:\n",
    "\n",
    "* Statistical Tests\n",
    "* Classification and regression\n",
    " * Linear models (SVMs, logistic regression, linear regression)\n",
    " * Naive Bayes\n",
    " * Decision Trees\n",
    " * Ensembles of Trees (Random Forests and Gradient-Boosted Trees)\n",
    "* Collaborative filtering\n",
    " * Alternating Least Squares (set to non-negative for NMF)\n",
    "* Clustering\n",
    " * K-means\n",
    "* Dimensionality reduction\n",
    " * Singular Value Decomposition (SVD)\n",
    " * Principal Component Analysis (PCA)\n",
    "\n",
    "For a full list of implementations, please reference the documentation at http://spark.apache.org/docs/latest/mllib-guide.html.\n",
    "\n",
    "\n",
    "## Supervised Models and LabeledPoint\n",
    "\n",
    "Supervised models in Spark requires a LabeledPoint RDD, where each observation is a label with a feature vector.  LabeledPoints can be instantiated by:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "some_data_rdd.map(lambda x: LabeledPoint(x[{y index}], x[{feature(s) index}]))\n",
    "```\n",
    "\n",
    "To extract the labels and features, use map on each point.\n",
    "\n",
    "```python\n",
    "labels = some_labelpoints_rdd.map(lambda x: x.label)\n",
    "features = some_labelpoints_rdd.map(lambda x: x.features)\n",
    "```\n",
    "\n",
    "## StandardScaler\n",
    "\n",
    "Several models in Spark can be very sensitive to different feature scaling.  We will often need to use StandardScaler to resolve this issue.  StandardScaler requires a feature RDD, and can be utilized as such:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "scaler.transform(features)\n",
    "```\n",
    "\n",
    "## Train Test Split\n",
    "\n",
    "There is no built in train test split function in Spark, but we do have transformation RDDs that specializes in random sampling or splits such as randomSplit.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "Spark has an evaluation package that allows you to calculate prediction errors.  Some options available are:\n",
    "\n",
    "* BinaryClassificationMetrics\n",
    "* MulticlassMetrics\n",
    "* RegressionMetrics\n",
    "* RankingMetrics\n",
    "* More at https://spark.apache.org/docs/1.5.2/mllib-evaluation-metrics.html\n",
    "\n",
    "Within each of the available Metrics module, there's a selection of suitable metrics such as mean squared error for regression and precision for classification.  All of these modules requires a key value or tuple RDD of label and prediction.  For example:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "print metrics.meanSquaredError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: setup churn rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "sc = ps.SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_rdd = sc.textFile('churn.csv')\n",
    "churn_rdd = churn_rdd.map(lambda x: x.split(','))\n",
    "header = churn_rdd.first()\n",
    "churn_rdd = churn_rdd.filter(lambda x: x != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: clean our churn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_churn_rdd = churn_rdd.map(lambda x: [x[1]] + [0 if z == 'no' else 1 for z in x[4:6]] + x[6:-1] + [0 if x[-1] == 'False.' else 1]) \\\n",
    "                             .map(lambda x: [float(z) for z in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: create a LabeledPoint object (because we're building a supervised model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run a Random Forest model to predict churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Lets make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: In order to look at our performance, we need to combine out prediction with actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Lets take a look at accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: How about the Area under ROC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #1: Build a logistic model to predict churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #2: Build a SVM to predict churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
