{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: DataFrames in Spark\n",
    "\n",
    "This course has been tested with Spark 2.2 (April 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and **puts a schema on them**.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as **RDDs with schema**.\n",
    "\n",
    "What are **schemas**?\n",
    "- Schemas are metadata about your data.\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using **column names** instead of column positions\n",
    "- Schemas enable **queries** using SQL and DataFrame syntax\n",
    "- Schemas make your data more **structured**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operational DataFrames in Python\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviromnent to run Spark SQL from python\n",
    "2. create DataFrames from RDDs or from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Initializing a `SparkSession` (`SparkContext` and `SqlContext`) in Python\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:55:18.963021",
     "start_time": "2018-04-13T13:55:18.945057"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. From an RDD (specifying schema)\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module. Here's a list of the most useful ones (subjective criteria).\n",
    "\n",
    "| Types | Python-like type |\n",
    "| - | - |\n",
    "| StringType | string |\n",
    "| IntegerType | int |\n",
    "| FloatType | float |\n",
    "| ArrayType\\* | array or list |\n",
    "| MapType | dict |\n",
    "\n",
    "\\* see later UDF functions on how to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:55:22.429303",
     "start_time": "2018-04-13T13:55:22.296699"
    }
   },
   "outputs": [],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:55:27.965437",
     "start_time": "2018-04-13T13:55:26.633628"
    }
   },
   "outputs": [],
   "source": [
    "# remember that csv file ?\n",
    "def casting_function(row):\n",
    "    trans_id, date, store, state, product, amount = row\n",
    "    return((int(trans_id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:55:43.313611",
     "start_time": "2018-04-13T13:55:40.656620"
    }
   },
   "outputs": [],
   "source": [
    "# import the many data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# create a schema of your own\n",
    "schema = StructType( [\n",
    "    StructField('id', IntegerType(),True),\n",
    "    StructField('date', StringType(),True),\n",
    "    StructField('store', IntegerType(),True),\n",
    "    StructField('state', StringType(),True),\n",
    "    StructField('product', IntegerType(),True),\n",
    "    StructField('amount', FloatType(),True) ] )\n",
    "\n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_sales, schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Reading from files (infering schema)\n",
    "\n",
    "Use [`sqlContext.read.csv`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv) to load a CSV into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:08.742751",
     "start_time": "2018-04-13T13:56:07.463500"
    }
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`sqlContext.read.json`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to load a JSON file into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:12.737255",
     "start_time": "2018-04-13T13:56:12.447048"
    }
   },
   "outputs": [],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales.json')\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:15.475372",
     "start_time": "2018-04-13T13:56:15.319387"
    }
   },
   "outputs": [],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales2.json.gz')\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Actions : turning your DataFrame into a local object\n",
    "\n",
    "Some actions just remain the same, you won't have to learn Spark all over again.\n",
    "\n",
    "Some new actions give you the possibility to describe and show the content in a more fashionable manner.\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | DF vs RDD? | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) | identical | Return a list that contains all of the elements as Rows. |\n",
    "| [`.count()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) | identical | Return the number of elements. |\n",
    "| [`.take(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) | identical | Take the first `n` elements. |\n",
    "| [`.top(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.top) | identical | Get the top `n` elements. |\n",
    "| [`.first()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) | identical | Return the first element. |\n",
    "| [`.show(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) | <span style=\"color:green\">new</span> | Show the DataFrame in table format (`n=20` by default) |\n",
    "| [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) | <span style=\"color:green\">new</span> | Convert the DF into a Pandas DF. |\n",
    "| [`.printSchema(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)\\* | <span style=\"color:green\">new</span> | Display the schema. This is not an action, it doesn't launch the DAG, but it fits better in this category. |\n",
    "| [`.describe(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) | <span style=\"color:green\">new</span> | Compute statistics for this column. |\n",
    "| [`.sum(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.mean(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.mean) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.min(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.max(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:18.394524",
     "start_time": "2018-04-13T13:56:18.238709"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:19.863771",
     "start_time": "2018-04-13T13:56:19.749383"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:20.315926",
     "start_time": "2018-04-13T13:56:20.020179"
    }
   },
   "outputs": [],
   "source": [
    "df_sales.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how `.collect()` returns things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:22.820913",
     "start_time": "2018-04-13T13:56:22.670151"
    }
   },
   "outputs": [],
   "source": [
    "df_sales.collect()[0][\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:24.383781",
     "start_time": "2018-04-13T13:56:23.654260"
    }
   },
   "outputs": [],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df_sales.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df_sales.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df_sales.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "You got that... DataFrames are just RDDs with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.withColumn(label,func)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) | transformation | mapping | Returns a new DataFrame by adding a column or replacing the existing column that has the same name. |\n",
    "| [`.filter(condition)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) | transformation | reduction |  Filters rows using the given condition. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) | transformation | reduction | Return a sampled subset of this DataFrame. |\n",
    "| [`.sampleBy(col)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sampleBy) | transformation | reduction | Returns a stratified sample without replacement based on the fraction given on each stratum. |\n",
    "| [`.select(cols)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select) | transformation | reduction | Projects a set of expressions and returns a new DataFrame. |\n",
    "| [`.join(dfB)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) | transformation | operations | Joins with another DataFrame, using the given join expression. |\n",
    "| [`.groupBy(col)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) | transformation | operations | Groups the DataFrame using the specified columns, so we can run aggregation on them.  |\n",
    "| [`.sort(cols)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sort) | transformation | sorting |  Returns a new DataFrame sorted by the specified column(s). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:28.514220",
     "start_time": "2018-04-13T13:56:28.285274"
    }
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:29.627283",
     "start_time": "2018-04-13T13:56:29.485237"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_out = df_aapl.withColumn(\"blabla\", lit(\"echo\"))\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:30.604870",
     "start_time": "2018-04-13T13:56:30.397792"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#df_out = df_aapl.withColumn(\"diff\", df_aapl.High - df_aapl.Low)\n",
    "df_out = df_aapl.withColumn(\"diff\", df_aapl[\"High\"] - df_aapl[\"Low\"])\n",
    "df_out.show(5)\n",
    "\n",
    "df_out = df_aapl.withColumn(\"diff\", df_aapl.High - df_aapl.Low)\n",
    "df_out.show(5)\n",
    "\n",
    "# below is the PREFERED METHOD for referencing columns\n",
    "df_out = df_aapl.withColumn(\"diff\", col(\"High\") - col(\"Low\"))\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:31.720330",
     "start_time": "2018-04-13T13:56:31.533463"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def my_specialfunc(h,l,o,c):\n",
    "    return ((h-l)*(o-c))\n",
    "\n",
    "my_specialfunc_udf = udf(lambda h,l,o,c : my_specialfunc(h,l,o,c), DoubleType())\n",
    "\n",
    "df_out = df_aapl.withColumn(\"special\",\n",
    "                            my_specialfunc_udf(col('High'),\n",
    "                                               col('Low'),\n",
    "                                               col('Open'),\n",
    "                                               col('Close')))\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .filter(condition) : filtering rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:33.158752",
     "start_time": "2018-04-13T13:56:32.911619"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_out = df_aapl.filter(\"High > 120\")\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:33.536301",
     "start_time": "2018-04-13T13:56:33.429411"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_out = df_aapl.filter(col('High') > 120)\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.select(*cols)` : selecting specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:34.729329",
     "start_time": "2018-04-13T13:56:34.651355"
    }
   },
   "outputs": [],
   "source": [
    "df_out = df_aapl.select([\"Open\", \"Close\"])\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupBy()`: aggregating in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:36.502471",
     "start_time": "2018-04-13T13:56:35.844735"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_out = df_sales.groupBy(col(\"State\")).agg(F.sum(col(\"Amount\")),F.mean(\"Amount\"))\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.orderBy()` : sorting by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:37.400332",
     "start_time": "2018-04-13T13:56:36.963426"
    }
   },
   "outputs": [],
   "source": [
    "df_out = df_sales.groupBy(col(\"State\"))\\\n",
    "            .agg(F.sum(col(\"Amount\")).alias(\"sumAmount\"))\\\n",
    "            .orderBy(col(\"sumAmount\"), ascending=False)\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Execute SQL statements\n",
    "\n",
    "#### .createOrReplaceTempView(name) : registering your dataframe as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:38.383174",
     "start_time": "2018-04-13T13:56:38.197006"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# Now create an SQL table and issue SQL queries against it without\n",
    "# using the sqlContext but through the SparkSession object.\n",
    "# Creates a temporary view of the DataFrame\n",
    "df_sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:39.457213",
     "start_time": "2018-04-13T13:56:39.078506"
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, AVG(amount) as avg_amount\n",
    "    FROM sales\n",
    "    GROUP BY state\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark.udf.register(name, func) : register a user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:40.123801",
     "start_time": "2018-04-13T13:56:39.997078"
    }
   },
   "outputs": [],
   "source": [
    "def myfun(x):\n",
    "    return x**2\n",
    "\n",
    "spark.udf.register('my_sql_fun', myfun)\n",
    "\n",
    "result = spark.sql('''\n",
    "    SELECT state, my_sql_fun(amount) as square_amount\n",
    "    FROM sales\n",
    "    ''')\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Let's design chains of transformations together ! (reloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:41.653573",
     "start_time": "2018-04-13T13:56:41.476605"
    }
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain an ~~RDD~~ DataFrame of the states sorted by their decreasing cumulated sales.\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:42.703474",
     "start_time": "2018-04-13T13:56:42.644776"
    }
   },
   "outputs": [],
   "source": [
    "df_out = df_sales # code your transformations here\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "df_out = df_sales.groupBy(col('State'))\\\n",
    "                 .agg(F.sum(col('Amount')).alias('Money'))\\\n",
    "                 .orderBy(\"Money\", ascending=False)\n",
    "\n",
    "df_out.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:43.710106",
     "start_time": "2018-04-13T13:56:43.548315"
    }
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "\n",
    "1. ~~filter out headers and last line~~\n",
    "2. ~~split each line based on comma~~\n",
    "3. keep only fields for Date ~~(col 0)~~ and Close ~~(col 4)~~\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T13:56:44.206624",
     "start_time": "2018-04-13T13:56:44.139856"
    }
   },
   "outputs": [],
   "source": [
    "df_out = df_aapl  # put your transformation here...\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "df_out.select(\"Close\", \"Date\").orderBy(df_aapl.Close, ascending=False).show(5)```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
