{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames in Spark (Part 1)\n",
    "\n",
    "This course has been designed with Spark 2.0.1 in mind (Oct 2016), and was updated with some niceties and new style guide for Spark 2.1.0 in June 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "## 1.1. RDDs versus DataFrames\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using column names instead of column positions\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "- Schemas make your data more structured.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as RDDs with schema.\n",
    "\n",
    "What is a schema?\n",
    "- Schemas are metadata about your data.\n",
    "- Schemas define table names, column names, and column types over your data.\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs, instead of using column positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operational DataFrames in Python\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviromnent to run Spark SQL from python\n",
    "2. create DataFrames from RDDs or from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Initializing a `SparkContext` and `SqlContext` in Python\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get pyspark, spark\n",
    "import findspark\n",
    "findspark.init('/home/sparkles/spark-2.1.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparkles/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: SparkContext already exists in this scope\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a `SQLContext` using our `SparkContext` as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. From an RDD (specifying schema)\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module. Here's a list of the most useful ones (subjective criteria).\n",
    "\n",
    "| Types | Python-like type |\n",
    "| - | - |\n",
    "| StringType | string |\n",
    "| IntegerType | int |\n",
    "| FloatType | float |\n",
    "| ArrayType\\* | array or list |\n",
    "| MapType | dict |\n",
    "\n",
    "\\* see later UDF functions on how to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that csv file ?\n",
    "def casting_function(row):\n",
    "    _id, date, store, state, product, amount = row\n",
    "    return (int(_id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.csv')\\\n",
    "        .map(lambda rowstr : rowstr.split(\",\"))\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[332] at collect at <ipython-input-94-2cc849822204>:9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_sales1 = rdd_sales.map(lambda x : x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[333] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- product: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the many data types\n",
    "from pyspark.sql.types import (StructType, \n",
    "    StructField, IntegerType, StringType, FloatType)\n",
    "\n",
    "# create a schema of your own\n",
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )\n",
    "\n",
    "# feed that into a DataFrame\n",
    "df = sqlContext.createDataFrame(rdd_sales,schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Reading from files (infering schema)\n",
    "\n",
    "Use [`sqlContext.read.csv`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv) to load a CSV into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = sqlContext.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`sqlContext.read.json`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to load a JSON file into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = sqlContext.read.json('data/sales.json')\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = sqlContext.read.json('data/sales2.json.gz')\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Niceties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read JSON\n",
    "df = sqlContext.read.json('data/sales.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'date', 'id', 'product', 'state', 'store']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, amount: string, date: string, id: string, product: string, state: string, store: string]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "|summary|            amount|      date|                id|           product|state|             store|\n",
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "|  count|                 6|         6|                 6|                 6|    6|                 6|\n",
      "|   mean| 371.6666666666667|      null|             103.5| 334.3333333333333| null|251.33333333333334|\n",
      "| stddev|207.40459654179958|      null|1.8708286933869716|19.500427345744672| null|225.39180700874346|\n",
      "|    min|             200.0|11/13/2014|               101|               321|   CA|               100|\n",
      "|    max|             750.0|11/19/2014|               106|               373|   WA|               700|\n",
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferred Schema (refresher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_csv = sqlContext.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df_csv.printSchema()\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Schema, with latest recommended style as of 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: float (nullable = true)\n",
      "\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Latest recommended flow as of 2.1.0\n",
    "import findspark\n",
    "findspark.init('/home/sparkles/spark-2.1.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Lecture').getOrCreate()  # called 'spark' by convention\n",
    "\n",
    "from pyspark.sql.types import (StructType, \n",
    "    StructField, IntegerType, StringType, FloatType)\n",
    "\n",
    "data_schema = [StructField('#ID', IntegerType(), True),\n",
    "              StructField('Date', StringType(), True),\n",
    "              StructField('Store', IntegerType(), True),\n",
    "              StructField('State', StringType(), True),\n",
    "              StructField('Product', IntegerType(), True),\n",
    "              StructField('Amount', FloatType(), True)]\n",
    "\n",
    "schema = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.csv('data/sales.csv', \n",
    "                    header=True, \n",
    "                    quote='\"', \n",
    "                    sep=\",\",\n",
    "                    schema=schema)\n",
    "\n",
    "# compare to:\n",
    "# read CSV\n",
    "# df_csv = sqlContext.read.csv('data/sales.csv',\n",
    "#                          header=True,       # use headers or not\n",
    "#                          quote='\"',         # char for quotes\n",
    "#                          sep=\",\",           # char for separation\n",
    "#                          inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0),\n",
       " Row(#ID=104, Date='11/18/2014', Store=700, State='OR', Product=329, Amount=450.0),\n",
       " Row(#ID=102, Date='11/15/2014', Store=203, State='CA', Product=321, Amount=200.0),\n",
       " Row(#ID=106, Date='11/19/2014', Store=202, State='CA', Product=331, Amount=330.0),\n",
       " Row(#ID=103, Date='11/17/2014', Store=101, State='WA', Product=373, Amount=750.0),\n",
       " Row(#ID=105, Date='11/19/2014', Store=202, State='CA', Product=321, Amount=200.0)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[State: string]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|State|\n",
      "+-----+\n",
      "|   WA|\n",
      "|   OR|\n",
      "|   CA|\n",
      "|   CA|\n",
      "|   WA|\n",
      "|   CA|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0),\n",
       " Row(#ID=104, Date='11/18/2014', Store=700, State='OR', Product=329, Amount=450.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|State|Amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['State', 'Amount']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+--------+\n",
      "|#ID|      Date|Store|State|Product|Amount|newState|\n",
      "+---+----------+-----+-----+-------+------+--------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|      WA|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|      OR|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|      CA|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|      CA|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|      WA|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|      CA|\n",
      "+---+----------+-----+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('newState', df['State']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------+-------+------+\n",
      "|#ID|      Date|Store|newState|Product|Amount|\n",
      "+---+----------+-----+--------+-------+------+\n",
      "|101|11/13/2014|  100|      WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|      OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|      CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|      CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|      WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|      CA|    321| 200.0|\n",
      "+---+----------+-----+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('State', 'newState').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hear you like SQL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('sales')  # 'sales' is the name of the 'table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = spark.sql(\"SELECT * FROM sales WHERE Product=331\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could I do that using DataFrame syntax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = df.filter(df['Product'] == 331)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = df.filter(df['Product'] == 331).collect()\n",
    "row = results[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#ID': 101,\n",
       " 'Amount': 300.0,\n",
       " 'Date': '11/13/2014',\n",
       " 'Product': 331,\n",
       " 'State': 'WA',\n",
       " 'Store': 100}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WA'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'#ID': 106,\n",
       "  'Amount': 330.0,\n",
       "  'Date': '11/19/2014',\n",
       "  'Product': 331,\n",
       "  'State': 'CA',\n",
       "  'Store': 202}]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = df.filter( (df['Product'] == 331) & ~(df['State'] == 'WA') ).collect()\n",
    "[r.asDict() for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Actions : turning your DataFrame into a local object\n",
    "\n",
    "Some actions just remain the same, you won't have to learn Spark all over again.\n",
    "\n",
    "Some new actions give you the possibility to describe and show the content in a more fashionable manner.\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | DF vs RDD? | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) | identical | Return a list that contains all of the elements as Rows. |\n",
    "| [`.count()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) | identical | Return the number of elements. |\n",
    "| [`.take(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) | identical | Take the first `n` elements. |\n",
    "| [`.top(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.top) | identical | Get the top `n` elements. |\n",
    "| [`.first()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) | identical | Return the first element. |\n",
    "| [`.show(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) | <span style=\"color:green\">new</span> | Show the DataFrame in table format (`n=20` by default) |\n",
    "| [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) | <span style=\"color:green\">new</span> | Convert the DF into a Pandas DF. |\n",
    "| [`.printSchema(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)\\* | <span style=\"color:green\">new</span> | Display the schema. This is not an action, it doesn't launch the DAG, but it fits better in this category. |\n",
    "| [`.describe(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) | <span style=\"color:green\">new</span> | Compute statistics for this column. |\n",
    "| [`.sum(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.mean(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.mean) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.min(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.max(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = sqlContext.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "      <th>Product</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>100</td>\n",
       "      <td>WA</td>\n",
       "      <td>331</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>700</td>\n",
       "      <td>OR</td>\n",
       "      <td>329</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>203</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>331</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>WA</td>\n",
       "      <td>373</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #ID        Date  Store State  Product  Amount\n",
       "0  101  11/13/2014    100    WA      331   300.0\n",
       "1  104  11/18/2014    700    OR      329   450.0\n",
       "2  102  11/15/2014    203    CA      321   200.0\n",
       "3  106  11/19/2014    202    CA      331   330.0\n",
       "4  103  11/17/2014    101    WA      373   750.0\n",
       "5  105  11/19/2014    202    CA      321   200.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how `.collect()` returns things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0),\n",
       " Row(#ID=104, Date='11/18/2014', Store=700, State='OR', Product=329, Amount=450.0),\n",
       " Row(#ID=102, Date='11/15/2014', Store=203, State='CA', Product=321, Amount=200.0),\n",
       " Row(#ID=106, Date='11/19/2014', Store=202, State='CA', Product=331, Amount=330.0),\n",
       " Row(#ID=103, Date='11/17/2014', Store=101, State='WA', Product=373, Amount=750.0),\n",
       " Row(#ID=105, Date='11/19/2014', Store=202, State='CA', Product=321, Amount=200.0)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|summary|               #ID|      Date|             Store|State|           Product|            Amount|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|  count|                 6|         6|                 6|    6|                 6|                 6|\n",
      "|   mean|             103.5|      null|251.33333333333334| null| 334.3333333333333| 371.6666666666667|\n",
      "| stddev|1.8708286933869716|      null|225.39180700874346| null|19.500427345744672|207.40459654179958|\n",
      "|    min|               101|11/13/2014|               100|   CA|               321|             200.0|\n",
      "|    max|               106|11/19/2014|               700|   WA|               373|             750.0|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+------------------+\n",
      "|summary|            Amount|\n",
      "+-------+------------------+\n",
      "|  count|                 6|\n",
      "|   mean| 371.6666666666667|\n",
      "| stddev|207.40459654179958|\n",
      "|    min|             200.0|\n",
      "|    max|             750.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df_sales.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df_sales.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df_sales.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "You got that... DataFrames are just RDDs with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. `.withColumn()`: adding column using operations or functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|blabla|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|  echo|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|  echo|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|  echo|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|  echo|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|  echo|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_out = df_aapl.withColumn(\"blabla\", lit(\"echo\"))\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+------------------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|              diff|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------------------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|1.0500030000000038|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|0.7399979999999999|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998| 0.630004999999997|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|1.0499950000000098|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|3.9599989999999963|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.withColumn(\"diff\", df_aapl.High - df_aapl.Low)\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+--------------------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|             special|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+--------------------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25| -0.3150040500090051|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002| -0.4070018599920009|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998| 0.13230104999999545|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|-0.20999585001499796|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|  0.5147879900030115|\n",
      "|2016-10-18 00:00:...|    118.18|118.209999|117.449997|117.470001|24553500|117.470001|   0.539600659998008|\n",
      "|2016-10-17 00:00:...|117.330002|117.839996|116.779999|117.550003|23624900|117.550003|-0.23320039999701023|\n",
      "|2016-10-14 00:00:...|117.879997|118.169998|117.129997|117.629997|35652200|117.629997| 0.26000025000000093|\n",
      "|2016-10-13 00:00:...|116.790001|117.440002|115.720001|116.980003|35192400|116.980003| -0.3268036300019894|\n",
      "|2016-10-12 00:00:...|117.349998|117.980003|    116.75|117.339996|37586800|117.339996|0.012302490006000045|\n",
      "|2016-10-11 00:00:...|117.699997|118.690002|116.199997|116.300003|64041000|116.300003|   3.485992059969996|\n",
      "|2016-10-10 00:00:...|115.019997|    116.75|114.720001|116.050003|36236000|116.050003|  -2.090911149994004|\n",
      "|2016-10-07 00:00:...|114.309998|114.559998|113.510002|114.059998|24358400|114.059998| 0.26249899999999826|\n",
      "|2016-10-06 00:00:...|113.699997|114.339996|113.129997|113.889999|28779300|113.889999|-0.22990222999800763|\n",
      "|2016-10-05 00:00:...|113.400002|113.660004|112.690002|113.050003|21453100|113.050003| 0.33949972999799477|\n",
      "|2016-10-04 00:00:...|113.059998|114.309998|112.629997|     113.0|29736800|     113.0| 0.10079669999798783|\n",
      "|2016-10-03 00:00:...|112.709999|113.050003|112.279999|112.519997|21701800|112.519997| 0.14630230000799438|\n",
      "|2016-09-30 00:00:...|112.459999|113.370003|111.800003|113.050003|36379100|113.050003| -0.9263062800000078|\n",
      "|2016-09-29 00:00:...|113.160004|113.800003|111.800003|    112.18|35887000|    112.18|  1.9600079999999878|\n",
      "|2016-09-28 00:00:...|113.690002|114.639999|    113.43|113.949997|29641100|113.949997| -0.3145936900049861|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def my_specialfunc(h,l,o,c):\n",
    "    return ((h-l)*(o-c))\n",
    "\n",
    "my_specialfunc_udf = udf(lambda h,l,o,c : my_specialfunc(h,l,o,c), DoubleType())\n",
    "\n",
    "df_out = df_aapl.withColumn(\"special\", my_specialfunc_udf(df_aapl.High, df_aapl.Low, df_aapl.Open, df_aapl.Close))\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.select(*cols)` : selecting specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.select([\"Open\", \"Close\"])\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupBy()`: aggregating in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   OR|      450.0|\n",
      "|   CA|      730.0|\n",
      "|   WA|     1050.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_out = df_sales.groupBy(\"State\").agg(F.sum(\"Amount\"))\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.orderBy()` : sorting by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   WA|     1050.0|\n",
      "|   CA|      730.0|\n",
      "|   OR|      450.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_sales.groupBy(\"State\").agg(F.sum(\"Amount\")).orderBy(\"sum(Amount)\", ascending=False)\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Let's design chains of transformations together ! (reloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_sales = sqlContext.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a sorted ~~RDD~~ DataFrame of the states in which you have most sales done (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_sales\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df_out = df_sales.groupBy(df_sales.State)\\<br/>\n",
    "                 .agg(F.sum(df_sales.Amount).alias('Money'))\\<br/>\n",
    "                 .orderBy(\"Money\", ascending=False)<br/>\n",
    "<br/>\n",
    "df_out.show()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revealed solution here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "\n",
    "1. ~~filter out headers and last line~~\n",
    "2. ~~split each line based on comma~~\n",
    "3. keep only fields for Date ~~(col 0)~~ and Close ~~(col 4)~~\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl # apply transformation here...\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "df_out.select(\"Close\", \"Date\").orderBy(df_aapl.Close, ascending=False).show(5)<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revealed solution here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning on DataFrames\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|    features|      scaledfeatures|\n",
      "+------------+--------------------+\n",
      "|    [118.25]| [0.865963404782699]|\n",
      "|[117.650002]|[0.8473472730564975]|\n",
      "|[116.599998]|[0.8147688098332226]|\n",
      "|[117.059998]|[0.8290412250646944]|\n",
      "|[117.120003]|[0.8309029995776607]|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "scaledData.select(\"features\", \"scaledfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
