{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "This notebook was designed with Spark 2.0.1 in October 2016 and parts were updated with 2.1.0 in mind in July 2017.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the relationship between Hadoop, MapReduce, and Spark\n",
    "    - Spark as a higher-level alternative to MapReduce for distributed computation\n",
    "    - Spark as a happy friend of HadoopDFS, S3, etc. for distributed data\n",
    "    - Describe the advantages/disadvantages of Spark compared to Hadoop MapReduce\n",
    "- RDD\n",
    "    - Define what an RDD is, by its properties and operations\n",
    "    - Explain the different operation types on an RDD\n",
    "        - transformations\n",
    "        - actions\n",
    "    - Apply different transformations through use cases\n",
    "    - Describe what persisting/caching an RDD means, and situations where this is useful\n",
    "- DataFrame\n",
    "    - Define what a DataFrame is, by its properties and operations\n",
    "        - Schema, Syntax\n",
    "    - Compare DataFrames to RDDs\n",
    "        - Operation Types (same)\n",
    "        - Cacheability (same)\n",
    "        - Syntax (different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Context\n",
    "### What is Big Data?\n",
    "\n",
    "Too big for local.  What do we do?\n",
    "- Use a SQL or NoSQL DB\n",
    "    - Sometimes these databases themselves are distributed\n",
    "- Use a distributed system\n",
    "\n",
    "### Hadoop, MapReduce, Spark\n",
    "### Local versus Distributed Systems\n",
    "\n",
    "A local system uses the resources of a single machine. \n",
    "\n",
    "A distributed system uses the resources of multiple machines.\n",
    "- After a certain point, it's easier to scale/add resources to a distributed system than it is to add them to a single machine system.\n",
    "Distributed systems include fault tolerance.  If a single task or even a whole machine has a failure, the whole system can still go on by re-running that task or running it on a different machine.\n",
    "\n",
    "### Overview of Hadoop Ecosystem\n",
    "\n",
    "Storage and Computation\n",
    "\n",
    "**Hadoop** for *storage and replication* and the Hadoop Distributed Filesystem, HDFS\n",
    "\n",
    "**MapReduce** for *computations* across the distributed dataset stored in HDFS\n",
    "\n",
    " - Job tracker sends code to run on the Task tracker\n",
    " - Task trackers then allocate resources (CPU, RAM) for the tasks on the worker nodes, monitors the tasks, reruns if necessary...\n",
    "\n",
    "### Overview of Spark\n",
    "\n",
    "**Spark** improves on the *compute* side of things.\n",
    "\n",
    "Spark doesn't compete with Hadoop; in fact, Spark can use data in Hadoop.\n",
    "\n",
    "People sometimes talk about \"Hadoop MapReduce\", and contrast that to \"Spark\".  Sometimes they shorten the former to just \"Hadoop\" and compare \"Hadoop versus Spark\".  But really they mean to contrast \"Hadoop & MapReduce\" with \"Possibly-Hadoop & Spark\".  The contrast is between MapReduce and Spark, as ways to do computation over distributed datasets.\n",
    "\n",
    "#### What does Spark do that's so great?\n",
    "\n",
    "- Speed\n",
    "    - Spark can perform operations up to 100x faster than MapReduce. How?\n",
    "        - MapReduce writes data to disk after each map and reduce op. That's slow.\n",
    "        - Spark keeps most of the data in memory after each transformation, spilling over to disk if necessary. As RAM has gotten cheaper, it makes sense to keep things in it more.\n",
    "        \n",
    "- RDDs\n",
    "    - Immutable\n",
    "    - Lazily Evaluated\n",
    "    - Cacheable\n",
    "- Two types of operations, for working with large datasets\n",
    "    - Transformations\n",
    "        - Recipe to follow (\"describe it\")\n",
    "    - Actions\n",
    "        - Call to action; follow the recipe (\"do it\")\n",
    "- Syntax\n",
    "    - RDD\n",
    "    - DataFrame\n",
    "        - Now the standard for Spark's ML capabilities; RDD support is in 'maintenance mode' now, will be deprecated, then removed\n",
    "- Framework for dealing with large data\n",
    "    - Not itself a language\n",
    "        - Written in Scala, which itself is written in Java\n",
    "    - Clients in Scala, Java, Python, R...\n",
    "        - Scala and Java are sort of \"first class\" clients\n",
    "    - PySpark is how we work with Spark from Python\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Key Concepts\n",
    "\n",
    "## 1.1. MapReduce vs Spark\n",
    "\n",
    "<img src=\"images/apache_hadoop_ecosystem.jpg\" width=\"500\">\n",
    "\n",
    "**Hadoop MapReduce limits:**\n",
    "- your job has to fit the `<key,value>` paradigm\n",
    "- no interactions (except by programming)\n",
    "- each job read from disk: problem with iterative algorithms (machine learning)\n",
    "\n",
    "**How Spark answers this:**\n",
    "- Spark proposes other processing workflows than MapReduce\n",
    "- highly efficient distributed operations\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk.\n",
    "- Spark keeps everything in memory when possible, uses lots of it.\n",
    "\n",
    "<img src=\"images/spark_ecosystem.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.2. Resilient Distributed Datasets (RDD)\n",
    "\n",
    "<img src=\"images/rdd_on_cluster.png\" width=\"200\" align=\"right\">\n",
    "\\[[Image Source](http://horicky.blogspot.com/2015/02/big-data-processing-in-spark.html)\\]\n",
    "\n",
    "- created from HDFS, S3, HBase, JSON, text, local... or transformed from another RDD\n",
    "- distributed accross the cluster, partitioned (atomic chunks of data)\n",
    "- can recover from errors (node failure, slow process)\n",
    "- traceability of each partition, can re-run the processing\n",
    "- **immutable** : you cannot *modify* an RDD in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.3. A \"functional programming paradigm\" and DAGs\n",
    "\n",
    "RDDs are **immutable** ! You can only **transform** an existing RDD into another one.\n",
    "\n",
    "Spark provides many transformations functions. By programming these functions, you construct a **Directed Acyclic Graph** (DAG).\n",
    "\n",
    "<img src=\"images/dag.png\">\n",
    "\\[[Image Source]()\\]\n",
    "\n",
    "When you use them, these functions are passed from the **client** to the **master**, who then distributes them to workers, who apply them accross their partitions of the RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.4. Spark architecture : from your coding hands to the cluster\n",
    "\n",
    "<img src=\"images/from_rdd_to_cluster.png\">\n",
    "\\[[Image Source]()\\]\n",
    "\n",
    "You construct your sequence of transformations in python.\n",
    "Spark functional programming interface builds up a **DAG**\n",
    "This DAG is sent by the **driver** for execution to the **cluster manager**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.5. Jargon\n",
    "\n",
    "Excerpt taken from \\[[Arush Kharbanda](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work) on Quora\\]\n",
    "\n",
    "**Job**: A piece of code which reads some input  from HDFS or local, performs some computation on the data and writes some output data.\n",
    "\n",
    "**Stages**: Jobs are divided into stages. Stages are classified as a Map or reduce stages(Its easier to understand if you have worked on Hadoop and want to correlate). Stages are divided based on computational boundaries, all computations(operators) cannot be Updated in a single Stage. It happens over many stages.\n",
    "\n",
    "**Tasks**: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor(machine).\n",
    "\n",
    "**DAG**: DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\n",
    "\n",
    "**Executor**: The process responsible for executing a task.\n",
    "\n",
    "**Driver**: The program/process responsible for running the Job over the Spark Engine\n",
    "\n",
    "**Master**: The machine on which the Driver program runs\n",
    "\n",
    "**Slave/Worker**: The machine on which the Executor program runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Operational Spark in Python\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviromnent to run spark from python\n",
    "2. extract RDDs from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)\n",
    "\n",
    "**Brainstorming**: So, let's suppose you have this thing called an RDD, which is just basically a dataset made of rows and values. What are all the operations you'd like to do to that RDD ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your ideas here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1. Initializing a `SparkContext` in Python\n",
    "\n",
    "IPython / IPython notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkContext` that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"images/spark_driver_etc.png\"/>\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparkles/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: SparkContext already exists in this scope\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/sparkles/spark-2.1.0-bin-hadoop2.7')\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning\n",
    "\n",
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2. Creating an RDD (from files)\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from an external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| [`sc.parallelize(array)`]() | Create an RDD from a python array or list |\n",
    "| [`sc.textFile(path)`]() | Create an RDD from a text file |\n",
    "| [`sc.pickleFile(path)`]() | Create an RDD from a pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2.1. Creating RDDs from local files\n",
    "#### `sc.parallelize()` : create an RDD from a python iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/toy_data.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('data/toy_data.txt')\n",
    "\n",
    "# to output the content in python [irl, use collect() with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">`sc.pickeFile()` : from a HDFS pickle file\n",
    "\n",
    "The import will give you an rdd composed of whatever table was stored into that file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32maapl.csv\u001b[0m*         \u001b[01;32minput.txt\u001b[0m*       \u001b[01;32msales.csv\u001b[0m*   \u001b[01;32msales.txt\u001b[0m*      \u001b[01;34mtoy_data.pkl\u001b[0m/\r\n",
      "\u001b[01;32mcookie_data.txt\u001b[0m*  \u001b[01;32msales2.json.gz\u001b[0m*  \u001b[01;32msales.json\u001b[0m*  \u001b[01;32mtoy_dataB.txt\u001b[0m*  \u001b[01;32mtoy_data.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emilie,23', 'yunjin,42', 'matthew,4', 'jorge,8', 'josh,15', 'evangeline,16']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.pickleFile('data/toy_data.pkl')\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Creating RDDs from S3\n",
    "\n",
    "These two functions above can perform loading from an s3 repository too ! Effortless.\n",
    "\n",
    "<span style=\"color:red\">Warning: don't .collect() that, or you'll break the internet !</span>\n",
    "\n",
    "**Note**: in order to do that, you need to have launched jupyter with the `--packages` options for aws and hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# obtaining your credentials from your environment variables\n",
    "ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# link to the S3 repository\n",
    "link = 's3n://mortar-example-data/airline-data'\n",
    "\n",
    "# creating an RDD...\n",
    "rdd = sc.textFile(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requires s3n packages to be configured #todo\n",
    "# rdd.getNumPartitions()\n",
    "# rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform an RDD into another RDD because RDD are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "\\[[Image Source](http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html)\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "In the sequence below, we will in one sequence:\n",
    "1. read an RDD from a text file\n",
    "2. transform by applying `split`\n",
    "3. transform by filtering\n",
    "4. transform by casting some columns to their corresponding type.\n",
    "5. use an action to output the results\n",
    "\n",
    "Each transformation is a method of an RDD, and returns another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID    Date           Store   State  Product    Amount\n",
      "101    11/13/2014     100     WA     331        300.00\n",
      "104    11/18/2014     700     OR     329        450.00\n",
      "102    11/15/2014     203     CA     321        200.00\n",
      "106    11/19/2014     202     CA     331        330.00\n",
      "103    11/17/2014     101     WA     373        750.00\n",
      "105    11/19/2014     202     CA     321        200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/sales.txt', 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall: Input functions, reading RDDs from files, \n",
    "# are functions of the SparkContext.\n",
    "\n",
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd1.collect()  # beware collect() in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount'],\n",
       " ['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies split() to each row\n",
    "rdd2 = rdd1.map(lambda rowstr : rowstr.split())\n",
    "\n",
    "rdd2.collect()  # beware collect() in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters rows\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "# rdd3.collect()  # beware collect() in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd1.map(lambda rowstr : rowstr.split()).filter(lambda row: not row[0].startswith('#'))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    _id, date, store, state, product, amount = row\n",
    "    return (int(_id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "# applies casting_function to rows\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "\n",
    "# shows the result\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's see the canonical way to write that in Python...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v0\n",
    "rdd_sales = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount'],\n",
       " ['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v1\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())   # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))    # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v3\n",
    "def casting_function(row):\n",
    "    _id, date, store, state, product, amount = row\n",
    "    return (int(_id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)   # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice.  We did that all at once!**\n",
    "Just so we can have a fresh plate, let's create our two RDDs again for more exploration of RDD functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a names rdd again...\n",
    "\n",
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's make a sales rdd again...\n",
    "\n",
    "def casting_function(row):\n",
    "    _id, date, store, state, product, amount = row\n",
    "    return (int(_id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Mapping\n",
    "#### `.map(func)` : applying a function on every row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [7, 5, 4, 10, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda x : len(x[0]))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How readable was that?\n",
    "\n",
    "When using lambda functions, we used to be able to use **argument unpacking** to provide a more readable transformation.\n",
    "\n",
    "... but after fierce debate, it was taken away with Python 3.  See: https://www.python.org/dev/peps/pep-3113/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-162-9c790a899f8d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-162-9c790a899f8d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    rddout = rdd_names.map(lambda (name,number) : len(name))  # no-no in Python 3, see PEP 3113\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda (name,number) : len(name))  # no-no in Python 3, see PEP 3113\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.flatMap(func)` : applying a function on every row and flattening the resulting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [4, 6, 11, 8, 10, 13, 15, 17, 19, 16, 18, 26, 23, 25, 29, 42, 44, 48]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.flatMap(lambda x : [x[1], x[1]+2, x[1]+len(x[0])])\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Row reduction\n",
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, '11/13/2014', 100, 'WA', 331, 300.0), (104, '11/18/2014', 700, 'OR', 329, 450.0), (102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (103, '11/17/2014', 101, 'WA', 373, 750.0), (105, '11/19/2014', 202, 'CA', 321, 200.0)]\n",
      "after: [(102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (105, '11/19/2014', 202, 'CA', 321, 200.0)]\n"
     ]
    }
   ],
   "source": [
    "# filtering an rdd\n",
    "rddout = rdd_sales.filter(lambda x: (x[3] == 'CA'))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, '11/13/2014', 100, 'WA', 331, 300.0), (104, '11/18/2014', 700, 'OR', 329, 450.0), (102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (103, '11/17/2014', 101, 'WA', 373, 750.0), (105, '11/19/2014', 202, 'CA', 321, 200.0)]\n",
      "after: [(102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (103, '11/17/2014', 101, 'WA', 373, 750.0)]\n"
     ]
    }
   ],
   "source": [
    "# sampling an rdd\n",
    "rddout = rdd_sales.sample(True, 0.4)\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, '11/13/2014', 100, 'WA', 331, 300.0), (104, '11/18/2014', 700, 'OR', 329, 450.0), (102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (103, '11/17/2014', 101, 'WA', 373, 750.0), (105, '11/19/2014', 202, 'CA', 321, 200.0)]\n",
      "after: ['CA', 'WA', 'OR']\n"
     ]
    }
   ],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "rddout = rdd_sales.map(lambda x: x[3]).distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Methods with a `<k,v>` paradigm\n",
    "#### `.values()`: returns the values of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [4, 8, 15, 16, 23, 42]\n"
     ]
    }
   ],
   "source": [
    "rddout = rdd_names.values()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: ['matthew', 'jorge', 'josh', 'evangeline', 'emilie', 'yunjin']\n"
     ]
    }
   ],
   "source": [
    "rddout = rdd_names.keys()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 300.0),\n",
       " ('OR', 450.0),\n",
       " ('CA', 200.0),\n",
       " ('CA', 330.0),\n",
       " ('WA', 750.0),\n",
       " ('CA', 200.0)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_salesperstate = rdd_sales.map(lambda x: (x[3],x[5]))\n",
    "\n",
    "rdd_salesperstate.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', (200.0, 'matthew')),\n",
       " ('CA', (330.0, 'matthew')),\n",
       " ('CA', (200.0, 'matthew')),\n",
       " ('OR', (450.0, 'jorge')),\n",
       " ('WA', (300.0, 'matthew')),\n",
       " ('WA', (750.0, 'matthew'))]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list of managers for each state\n",
    "data_array = [['CA', 'matthew'],\n",
    "              ['OR', 'jorge'],\n",
    "              ['WA','matthew'],\n",
    "              ['TX', 'emilie']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_managers = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_salesperstate.join(rdd_managers).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### `.reduceByKey(func)`: reduce `v`s by their `k` by applying func (what ?)\n",
    "\n",
    "The `func` here needs to be associative and commutative... can you guess why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1], ['WA', 1], ['CA', 2], ['OR', 1], ['CA', 5], ['OR', 1]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 8), ('WA', 1), ('OR', 2)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda v1,v2 : v1+v2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### `.groupByKey(func)`: reduce `v`s by their `k` by applying func (again ?)\n",
    "\n",
    "This can use any function non-commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1], ['WA', 1], ['CA', 2], ['OR', 1], ['CA', 5], ['OR', 1]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 2.6666666666666665), ('WA', 1.0), ('OR', 1.0)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(iterator):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return total / count\n",
    "\n",
    "rdd.groupByKey()\\\n",
    "    .map(lambda x: (x[0], mean(x[1])))\\\n",
    "    .collect()\n",
    "    \n",
    "# rdd2 = rdd.groupByKey().map(lambda x: (x[0], mean(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Sorting methods\n",
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[['josh', 15], ['evangeline', 16], ['jorge', 8], ['matthew', 4], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    }
   ],
   "source": [
    "# sorting by any function (because why not?)\n",
    "rddout = rdd_names.sortBy(lambda x : (13-x[1])**2, ascending=True)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[('yunjin', 42), ('matthew', 4), ('josh', 15), ('jorge', 8), ('evangeline', 16), ('emilie', 23)]\n"
     ]
    }
   ],
   "source": [
    "# sorting k,v pairs by key\n",
    "rddout = rdd_names.sortByKey(ascending=False)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4. Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (a python object, or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in a RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDD’s elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDD’s elements. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : returning the *full* content of an RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic, cause `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd: <class 'pyspark.rdd.RDD'>\n",
      "type of rdd_collected: <class 'list'>\n",
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "collected = rdd_names.collect()\n",
    "\n",
    "# let's check the type of RDD\n",
    "print(\"type of rdd: {}\".format(type(rdd_names)))\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_collected: {}\".format(type(collected)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : returning (any) n lines of an RDD\n",
    "\n",
    "Returns `n` the rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd_taken: <class 'list'>\n",
      "[['matthew', 4], ['jorge', 8]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "taken = rdd_names.take(2)\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_taken: {}\".format(type(taken)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : returning the first line of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matthew', 4]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Actions that compute some statistics\n",
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: you get that right ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3153021346\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's design chains of transformations together !\n",
    "## 3.1. Computing sales per state\n",
    "### Input RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    _id, date, store, state, product, amount = row\n",
    "    return (int(_id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a sorted RDD of the states in which you have most sales done (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_sales # apply transformation here...\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rddout = rdd_sales.map(lambda x: (x[3],x[5]))\\<br/>\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\<br/>\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False)<br/>\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revealed solution here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2. Word count (again)\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "another line\n",
      "yet another line\n",
      "yet another another line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/input.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "\n",
    "rdd = sc.textFile('data/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world', 'another line', 'yet another line', 'yet another another line']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd # apply transformation here...\n",
    "\n",
    "# collect the result\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "rddout = rdd.flatMap(lambda str : str.split())\\<br/>\n",
    "            .map(lambda word: (word,1))\\<br/>\n",
    "            .reduceByKey(lambda v1,v2: v1+v2)<br/>\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revealed solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines in file: 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_aapl_raw = sc.textFile('data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(rdd_aapl_raw.count()))\n",
    "\n",
    "rdd_aapl_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_aapl_raw # apply transformation here...\n",
    "\n",
    "rddout.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "rddout = rdd_aapl_raw.filter(lambda line: not line.startswith(\"Date\"))\\<br/>\n",
    ".map(lambda line: line.split(\",\"))\\<br/>\n",
    ".map(lambda fields: (float(fields[4]),fields[0]))\\<br/>\n",
    ".sortBy(lambda (close, date): close, ascending=False)\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revealed solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD: one use case ? oh I don't know maybe... **MACHINE LEARNING !!!**\n",
    "\n",
    "## 4.1. Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in range(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 603 ms\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 406 ms\n",
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 388 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 384 ms\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 43.7 ms\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 68.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parts of this doc were created for use with Spark 2.1.0 in June 2017\n",
    "# Spark 2.1 Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get pyspark, spark\n",
    "import findspark\n",
    "findspark.init('/home/sparkles/spark-2.1.0-bin-hadoop2.7') # your spark dir\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Lecture').getOrCreate()  # called 'spark' by convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Some Data from the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Activity Period,Operating Airline,Operating Airline IATA Code,Published Airline,Published Airline IATA Code,GEO Summary,GEO Region,Activity Type Code,Price Category Code,Terminal,Boarding Area,Passenger Count\\n,,,,,,,,,,,\\n,,,,,,,,,,,\\n,Operating Airline,Operating Airline IATA Code,Published Airline,Published Airline IATA Code,GEO Summary,GEO Region,Activity Type Code,Price Category Code,Terminal,Boarding Area,\\n200507,ATA Airlines,TZ,ATA Airlines,TZ,Domestic,US,Deplaned,Low Fare,Terminal 1,B,27271\\n200507,ATA Airlines,TZ,ATA Airlines,TZ,Domestic,US,Enplaned,Low Fare,Terminal 1,B,29131\\n200507,ATA Airlines,TZ,ATA Airlines,TZ,Domestic,US,Thru / Transit,Low Fare,Terminal 1,B,5415\\n200507,Air Canada,AC,Air Canada,AC,International,Canada,Deplaned,Other,Terminal 1,B,35156\\n200507,Air Canada,AC,Air Canada,AC,International,Canada,Enplaned,Other,Terminal 1,B,34090\\n200507,Air China,CA,Air China,CA,International,Asia,Deplaned,Other,International,G,6263\\n200507,Air China,CA,Air China,CA,International,Asia'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab some data from the web, save it to disk as csv\n",
    "# Air Traffic Passenger Statistics for San Francisco: \n",
    "# Data as CSV: https://data.sfgov.org/api/views/rkru-6vcg/rows.csv\n",
    "# Source: https://catalog.data.gov/dataset/air-traffic-passenger-statistics\n",
    "import requests\n",
    "import contextlib\n",
    "import csv\n",
    "\n",
    "csv_path = 'https://data.sfgov.org/api/views/rkru-6vcg/rows.csv'\n",
    "response = requests.get(csv_path)\n",
    "with open('rawdata.csv', 'w') as f:\n",
    "    f.write(response.text)\n",
    "    \n",
    "# Let's look at the beginning of the data:\n",
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internet Bleach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity Period</th>\n",
       "      <th>Operating Airline</th>\n",
       "      <th>Operating Airline IATA Code</th>\n",
       "      <th>Published Airline</th>\n",
       "      <th>Published Airline IATA Code</th>\n",
       "      <th>GEO Summary</th>\n",
       "      <th>GEO Region</th>\n",
       "      <th>Activity Type Code</th>\n",
       "      <th>Price Category Code</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>Boarding Area</th>\n",
       "      <th>Passenger Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>B</td>\n",
       "      <td>27271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>B</td>\n",
       "      <td>29131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>ATA Airlines</td>\n",
       "      <td>TZ</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Thru / Transit</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>B</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air Canada</td>\n",
       "      <td>AC</td>\n",
       "      <td>Air Canada</td>\n",
       "      <td>AC</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>B</td>\n",
       "      <td>35156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air Canada</td>\n",
       "      <td>AC</td>\n",
       "      <td>Air Canada</td>\n",
       "      <td>AC</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>B</td>\n",
       "      <td>34090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air China</td>\n",
       "      <td>CA</td>\n",
       "      <td>Air China</td>\n",
       "      <td>CA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>6263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air China</td>\n",
       "      <td>CA</td>\n",
       "      <td>Air China</td>\n",
       "      <td>CA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air France</td>\n",
       "      <td>AF</td>\n",
       "      <td>Air France</td>\n",
       "      <td>AF</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>12050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air France</td>\n",
       "      <td>AF</td>\n",
       "      <td>Air France</td>\n",
       "      <td>AF</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>11638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>NZ</td>\n",
       "      <td>International</td>\n",
       "      <td>Australia / Oceania</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>4998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>NZ</td>\n",
       "      <td>International</td>\n",
       "      <td>Australia / Oceania</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>4962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>AirTran Airways</td>\n",
       "      <td>FL</td>\n",
       "      <td>AirTran Airways</td>\n",
       "      <td>FL</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>8055.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>AirTran Airways</td>\n",
       "      <td>FL</td>\n",
       "      <td>AirTran Airways</td>\n",
       "      <td>FL</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>7984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>36641.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>39379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Thru / Transit</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>3678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>7977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>8837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>6969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>10046.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>Alaska Airlines</td>\n",
       "      <td>AS</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Thru / Transit</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>2266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>All Nippon Airways</td>\n",
       "      <td>NH</td>\n",
       "      <td>All Nippon Airways</td>\n",
       "      <td>NH</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>6545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>All Nippon Airways</td>\n",
       "      <td>NH</td>\n",
       "      <td>All Nippon Airways</td>\n",
       "      <td>NH</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>6094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>166577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>160890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>American Eagle Airlines</td>\n",
       "      <td>MQ</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>5493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>American Eagle Airlines</td>\n",
       "      <td>MQ</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>AA</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>5213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Asiana Airlines</td>\n",
       "      <td>OZ</td>\n",
       "      <td>Asiana Airlines</td>\n",
       "      <td>OZ</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>5041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Asiana Airlines</td>\n",
       "      <td>OZ</td>\n",
       "      <td>Asiana Airlines</td>\n",
       "      <td>OZ</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>4744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>200507.0</td>\n",
       "      <td>Atlantic Southeast Airlines</td>\n",
       "      <td>EV</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>DL</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 1</td>\n",
       "      <td>C</td>\n",
       "      <td>1552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Thru / Transit</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16190</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>82634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16191</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>81945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16192</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16193</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>F</td>\n",
       "      <td>638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16194</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Australia / Oceania</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>14817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16195</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Australia / Oceania</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>14497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16196</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16197</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>4449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16198</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>1199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>F</td>\n",
       "      <td>5799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>F</td>\n",
       "      <td>8960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>21355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16202</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>21364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>30756.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16204</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>6311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16205</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>E</td>\n",
       "      <td>1380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16206</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UA</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>Terminal 3</td>\n",
       "      <td>F</td>\n",
       "      <td>29861.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16207</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 2</td>\n",
       "      <td>D</td>\n",
       "      <td>194472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16208</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Domestic</td>\n",
       "      <td>US</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 2</td>\n",
       "      <td>D</td>\n",
       "      <td>203477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16209</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>4198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16210</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>VX</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>Terminal 2</td>\n",
       "      <td>D</td>\n",
       "      <td>5118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin Atlantic</td>\n",
       "      <td>VS</td>\n",
       "      <td>Virgin Atlantic</td>\n",
       "      <td>VS</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>9863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Virgin Atlantic</td>\n",
       "      <td>VS</td>\n",
       "      <td>Virgin Atlantic</td>\n",
       "      <td>VS</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>10575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Volaris Airlines</td>\n",
       "      <td>Y4</td>\n",
       "      <td>Volaris Airlines</td>\n",
       "      <td>Y4</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>4647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>Volaris Airlines</td>\n",
       "      <td>Y4</td>\n",
       "      <td>Volaris Airlines</td>\n",
       "      <td>Y4</td>\n",
       "      <td>International</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Low Fare</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>5883.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>WOW Air</td>\n",
       "      <td>WW</td>\n",
       "      <td>WOW Air</td>\n",
       "      <td>WW</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Enplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>A</td>\n",
       "      <td>5843.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>201612.0</td>\n",
       "      <td>WOW Air</td>\n",
       "      <td>WW</td>\n",
       "      <td>WOW Air</td>\n",
       "      <td>WW</td>\n",
       "      <td>International</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Deplaned</td>\n",
       "      <td>Other</td>\n",
       "      <td>International</td>\n",
       "      <td>G</td>\n",
       "      <td>5031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16217</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480481935.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16218</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- 1 -</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1:32:36 PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16219 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Activity Period            Operating Airline  \\\n",
       "0             200507.0                 ATA Airlines   \n",
       "1             200507.0                 ATA Airlines   \n",
       "2             200507.0                 ATA Airlines   \n",
       "3             200507.0                   Air Canada   \n",
       "4             200507.0                   Air Canada   \n",
       "5             200507.0                    Air China   \n",
       "6             200507.0                    Air China   \n",
       "7             200507.0                   Air France   \n",
       "8             200507.0                   Air France   \n",
       "9             200507.0              Air New Zealand   \n",
       "10            200507.0              Air New Zealand   \n",
       "11            200507.0              AirTran Airways   \n",
       "12            200507.0              AirTran Airways   \n",
       "13            200507.0              Alaska Airlines   \n",
       "14            200507.0              Alaska Airlines   \n",
       "15            200507.0              Alaska Airlines   \n",
       "16            200507.0              Alaska Airlines   \n",
       "17            200507.0              Alaska Airlines   \n",
       "18            200507.0              Alaska Airlines   \n",
       "19            200507.0              Alaska Airlines   \n",
       "20            200507.0              Alaska Airlines   \n",
       "21            200507.0           All Nippon Airways   \n",
       "22            200507.0           All Nippon Airways   \n",
       "23            200507.0            American Airlines   \n",
       "24            200507.0            American Airlines   \n",
       "25            200507.0      American Eagle Airlines   \n",
       "26            200507.0      American Eagle Airlines   \n",
       "27            200507.0              Asiana Airlines   \n",
       "28            200507.0              Asiana Airlines   \n",
       "29            200507.0  Atlantic Southeast Airlines   \n",
       "...                ...                          ...   \n",
       "16189         201612.0              United Airlines   \n",
       "16190         201612.0              United Airlines   \n",
       "16191         201612.0              United Airlines   \n",
       "16192         201612.0              United Airlines   \n",
       "16193         201612.0              United Airlines   \n",
       "16194         201612.0              United Airlines   \n",
       "16195         201612.0              United Airlines   \n",
       "16196         201612.0              United Airlines   \n",
       "16197         201612.0              United Airlines   \n",
       "16198         201612.0              United Airlines   \n",
       "16199         201612.0              United Airlines   \n",
       "16200         201612.0              United Airlines   \n",
       "16201         201612.0              United Airlines   \n",
       "16202         201612.0              United Airlines   \n",
       "16203         201612.0              United Airlines   \n",
       "16204         201612.0              United Airlines   \n",
       "16205         201612.0              United Airlines   \n",
       "16206         201612.0              United Airlines   \n",
       "16207         201612.0               Virgin America   \n",
       "16208         201612.0               Virgin America   \n",
       "16209         201612.0               Virgin America   \n",
       "16210         201612.0               Virgin America   \n",
       "16211         201612.0              Virgin Atlantic   \n",
       "16212         201612.0              Virgin Atlantic   \n",
       "16213         201612.0             Volaris Airlines   \n",
       "16214         201612.0             Volaris Airlines   \n",
       "16215         201612.0                      WOW Air   \n",
       "16216         201612.0                      WOW Air   \n",
       "16217              NaN                          NaN   \n",
       "16218              NaN                          NaN   \n",
       "\n",
       "      Operating Airline IATA Code   Published Airline  \\\n",
       "0                              TZ        ATA Airlines   \n",
       "1                              TZ        ATA Airlines   \n",
       "2                              TZ        ATA Airlines   \n",
       "3                              AC          Air Canada   \n",
       "4                              AC          Air Canada   \n",
       "5                              CA           Air China   \n",
       "6                              CA           Air China   \n",
       "7                              AF          Air France   \n",
       "8                              AF          Air France   \n",
       "9                              NZ     Air New Zealand   \n",
       "10                             NZ     Air New Zealand   \n",
       "11                             FL     AirTran Airways   \n",
       "12                             FL     AirTran Airways   \n",
       "13                             AS     Alaska Airlines   \n",
       "14                             AS     Alaska Airlines   \n",
       "15                             AS     Alaska Airlines   \n",
       "16                             AS     Alaska Airlines   \n",
       "17                             AS     Alaska Airlines   \n",
       "18                             AS     Alaska Airlines   \n",
       "19                             AS     Alaska Airlines   \n",
       "20                             AS     Alaska Airlines   \n",
       "21                             NH  All Nippon Airways   \n",
       "22                             NH  All Nippon Airways   \n",
       "23                             AA   American Airlines   \n",
       "24                             AA   American Airlines   \n",
       "25                             MQ   American Airlines   \n",
       "26                             MQ   American Airlines   \n",
       "27                             OZ     Asiana Airlines   \n",
       "28                             OZ     Asiana Airlines   \n",
       "29                             EV     Delta Air Lines   \n",
       "...                           ...                 ...   \n",
       "16189                          UA     United Airlines   \n",
       "16190                          UA     United Airlines   \n",
       "16191                          UA     United Airlines   \n",
       "16192                          UA     United Airlines   \n",
       "16193                          UA     United Airlines   \n",
       "16194                          UA     United Airlines   \n",
       "16195                          UA     United Airlines   \n",
       "16196                          UA     United Airlines   \n",
       "16197                          UA     United Airlines   \n",
       "16198                          UA     United Airlines   \n",
       "16199                          UA     United Airlines   \n",
       "16200                          UA     United Airlines   \n",
       "16201                          UA     United Airlines   \n",
       "16202                          UA     United Airlines   \n",
       "16203                          UA     United Airlines   \n",
       "16204                          UA     United Airlines   \n",
       "16205                          UA     United Airlines   \n",
       "16206                          UA     United Airlines   \n",
       "16207                          VX      Virgin America   \n",
       "16208                          VX      Virgin America   \n",
       "16209                          VX      Virgin America   \n",
       "16210                          VX      Virgin America   \n",
       "16211                          VS     Virgin Atlantic   \n",
       "16212                          VS     Virgin Atlantic   \n",
       "16213                          Y4    Volaris Airlines   \n",
       "16214                          Y4    Volaris Airlines   \n",
       "16215                          WW             WOW Air   \n",
       "16216                          WW             WOW Air   \n",
       "16217                         NaN                 NaN   \n",
       "16218                         NaN                 NaN   \n",
       "\n",
       "      Published Airline IATA Code    GEO Summary           GEO Region  \\\n",
       "0                              TZ       Domestic                   US   \n",
       "1                              TZ       Domestic                   US   \n",
       "2                              TZ       Domestic                   US   \n",
       "3                              AC  International               Canada   \n",
       "4                              AC  International               Canada   \n",
       "5                              CA  International                 Asia   \n",
       "6                              CA  International                 Asia   \n",
       "7                              AF  International               Europe   \n",
       "8                              AF  International               Europe   \n",
       "9                              NZ  International  Australia / Oceania   \n",
       "10                             NZ  International  Australia / Oceania   \n",
       "11                             FL       Domestic                   US   \n",
       "12                             FL       Domestic                   US   \n",
       "13                             AS       Domestic                   US   \n",
       "14                             AS       Domestic                   US   \n",
       "15                             AS       Domestic                   US   \n",
       "16                             AS  International               Canada   \n",
       "17                             AS  International               Canada   \n",
       "18                             AS  International               Mexico   \n",
       "19                             AS  International               Mexico   \n",
       "20                             AS  International               Mexico   \n",
       "21                             NH  International                 Asia   \n",
       "22                             NH  International                 Asia   \n",
       "23                             AA       Domestic                   US   \n",
       "24                             AA       Domestic                   US   \n",
       "25                             AA       Domestic                   US   \n",
       "26                             AA       Domestic                   US   \n",
       "27                             OZ  International                 Asia   \n",
       "28                             OZ  International                 Asia   \n",
       "29                             DL       Domestic                   US   \n",
       "...                           ...            ...                  ...   \n",
       "16189                          UA       Domestic                   US   \n",
       "16190                          UA  International                 Asia   \n",
       "16191                          UA  International                 Asia   \n",
       "16192                          UA  International                 Asia   \n",
       "16193                          UA  International                 Asia   \n",
       "16194                          UA  International  Australia / Oceania   \n",
       "16195                          UA  International  Australia / Oceania   \n",
       "16196                          UA  International               Canada   \n",
       "16197                          UA  International               Canada   \n",
       "16198                          UA  International               Canada   \n",
       "16199                          UA  International               Canada   \n",
       "16200                          UA  International               Canada   \n",
       "16201                          UA  International               Europe   \n",
       "16202                          UA  International               Europe   \n",
       "16203                          UA  International               Mexico   \n",
       "16204                          UA  International               Mexico   \n",
       "16205                          UA  International               Mexico   \n",
       "16206                          UA  International               Mexico   \n",
       "16207                          VX       Domestic                   US   \n",
       "16208                          VX       Domestic                   US   \n",
       "16209                          VX  International               Mexico   \n",
       "16210                          VX  International               Mexico   \n",
       "16211                          VS  International               Europe   \n",
       "16212                          VS  International               Europe   \n",
       "16213                          Y4  International               Mexico   \n",
       "16214                          Y4  International               Mexico   \n",
       "16215                          WW  International               Europe   \n",
       "16216                          WW  International               Europe   \n",
       "16217                         NaN            NaN                  NaN   \n",
       "16218                      - 1 -             NaN                  NaN   \n",
       "\n",
       "      Activity Type Code Price Category Code       Terminal Boarding Area  \\\n",
       "0               Deplaned            Low Fare     Terminal 1             B   \n",
       "1               Enplaned            Low Fare     Terminal 1             B   \n",
       "2         Thru / Transit            Low Fare     Terminal 1             B   \n",
       "3               Deplaned               Other     Terminal 1             B   \n",
       "4               Enplaned               Other     Terminal 1             B   \n",
       "5               Deplaned               Other  International             G   \n",
       "6               Enplaned               Other  International             G   \n",
       "7               Deplaned               Other  International             A   \n",
       "8               Enplaned               Other  International             A   \n",
       "9               Deplaned               Other  International             G   \n",
       "10              Enplaned               Other  International             G   \n",
       "11              Deplaned            Low Fare  International             A   \n",
       "12              Enplaned            Low Fare  International             A   \n",
       "13              Deplaned               Other  International             A   \n",
       "14              Enplaned               Other  International             A   \n",
       "15        Thru / Transit               Other  International             A   \n",
       "16              Deplaned               Other  International             A   \n",
       "17              Enplaned               Other  International             A   \n",
       "18              Deplaned               Other  International             A   \n",
       "19              Enplaned               Other  International             A   \n",
       "20        Thru / Transit               Other  International             A   \n",
       "21              Deplaned               Other  International             G   \n",
       "22              Enplaned               Other  International             G   \n",
       "23              Deplaned               Other     Terminal 3             E   \n",
       "24              Enplaned               Other     Terminal 3             E   \n",
       "25              Deplaned               Other     Terminal 3             E   \n",
       "26              Enplaned               Other     Terminal 3             E   \n",
       "27              Deplaned               Other  International             A   \n",
       "28              Enplaned               Other  International             A   \n",
       "29              Deplaned               Other     Terminal 1             C   \n",
       "...                  ...                 ...            ...           ...   \n",
       "16189     Thru / Transit               Other     Terminal 3             F   \n",
       "16190           Deplaned               Other  International             G   \n",
       "16191           Enplaned               Other  International             G   \n",
       "16192           Enplaned               Other     Terminal 3             E   \n",
       "16193           Enplaned               Other     Terminal 3             F   \n",
       "16194           Deplaned               Other  International             G   \n",
       "16195           Enplaned               Other  International             G   \n",
       "16196           Enplaned               Other  International             G   \n",
       "16197           Deplaned               Other     Terminal 3             E   \n",
       "16198           Enplaned               Other     Terminal 3             E   \n",
       "16199           Deplaned               Other     Terminal 3             F   \n",
       "16200           Enplaned               Other     Terminal 3             F   \n",
       "16201           Deplaned               Other  International             G   \n",
       "16202           Enplaned               Other  International             G   \n",
       "16203           Deplaned               Other  International             G   \n",
       "16204           Enplaned               Other  International             G   \n",
       "16205           Enplaned               Other     Terminal 3             E   \n",
       "16206           Enplaned               Other     Terminal 3             F   \n",
       "16207           Deplaned            Low Fare     Terminal 2             D   \n",
       "16208           Enplaned            Low Fare     Terminal 2             D   \n",
       "16209           Deplaned            Low Fare  International             A   \n",
       "16210           Enplaned            Low Fare     Terminal 2             D   \n",
       "16211           Deplaned               Other  International             A   \n",
       "16212           Enplaned               Other  International             A   \n",
       "16213           Deplaned            Low Fare  International             A   \n",
       "16214           Enplaned            Low Fare  International             A   \n",
       "16215           Enplaned               Other  International             A   \n",
       "16216           Deplaned               Other  International             G   \n",
       "16217                NaN                 NaN            NaN           NaN   \n",
       "16218                NaN          1:32:36 PM            NaN           NaN   \n",
       "\n",
       "       Passenger Count  \n",
       "0              27271.0  \n",
       "1              29131.0  \n",
       "2               5415.0  \n",
       "3              35156.0  \n",
       "4              34090.0  \n",
       "5               6263.0  \n",
       "6               5500.0  \n",
       "7              12050.0  \n",
       "8              11638.0  \n",
       "9               4998.0  \n",
       "10              4962.0  \n",
       "11              8055.0  \n",
       "12              7984.0  \n",
       "13             36641.0  \n",
       "14             39379.0  \n",
       "15              3678.0  \n",
       "16              7977.0  \n",
       "17              8837.0  \n",
       "18              6969.0  \n",
       "19             10046.0  \n",
       "20              2266.0  \n",
       "21              6545.0  \n",
       "22              6094.0  \n",
       "23            166577.0  \n",
       "24            160890.0  \n",
       "25              5493.0  \n",
       "26              5213.0  \n",
       "27              5041.0  \n",
       "28              4744.0  \n",
       "29              1552.0  \n",
       "...                ...  \n",
       "16189             15.0  \n",
       "16190          82634.0  \n",
       "16191          81945.0  \n",
       "16192            141.0  \n",
       "16193            638.0  \n",
       "16194          14817.0  \n",
       "16195          14497.0  \n",
       "16196            404.0  \n",
       "16197           4449.0  \n",
       "16198           1199.0  \n",
       "16199           5799.0  \n",
       "16200           8960.0  \n",
       "16201          21355.0  \n",
       "16202          21364.0  \n",
       "16203          30756.0  \n",
       "16204           6311.0  \n",
       "16205           1380.0  \n",
       "16206          29861.0  \n",
       "16207         194472.0  \n",
       "16208         203477.0  \n",
       "16209           4198.0  \n",
       "16210           5118.0  \n",
       "16211           9863.0  \n",
       "16212          10575.0  \n",
       "16213           4647.0  \n",
       "16214           5883.0  \n",
       "16215           5843.0  \n",
       "16216           5031.0  \n",
       "16217      480481935.0  \n",
       "16218              NaN  \n",
       "\n",
       "[16219 rows x 12 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(\"rawdata.csv\", header=0, skiprows=[1,2,3])\n",
    "# pandas_df = pd.read_csv(\"rawdata.csv\")\n",
    "pandas_df\n",
    "# pandas_df.to_csv('data.csv')\n",
    "# pandas_df = pd.read_csv('data.csv')\n",
    "# pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------------+---------------------------+-----------------+---------------------------+-------------+-------------------+------------------+-------------------+-------------+-------------+---------------+\n",
      "|_c0|Activity Period|Operating Airline|Operating Airline IATA Code|Published Airline|Published Airline IATA Code|  GEO Summary|         GEO Region|Activity Type Code|Price Category Code|     Terminal|Boarding Area|Passenger Count|\n",
      "+---+---------------+-----------------+---------------------------+-----------------+---------------------------+-------------+-------------------+------------------+-------------------+-------------+-------------+---------------+\n",
      "|  0|       200507.0|     ATA Airlines|                         TZ|     ATA Airlines|                         TZ|     Domestic|                 US|          Deplaned|           Low Fare|   Terminal 1|            B|        27271.0|\n",
      "|  1|       200507.0|     ATA Airlines|                         TZ|     ATA Airlines|                         TZ|     Domestic|                 US|          Enplaned|           Low Fare|   Terminal 1|            B|        29131.0|\n",
      "|  2|       200507.0|     ATA Airlines|                         TZ|     ATA Airlines|                         TZ|     Domestic|                 US|    Thru / Transit|           Low Fare|   Terminal 1|            B|         5415.0|\n",
      "|  3|       200507.0|       Air Canada|                         AC|       Air Canada|                         AC|International|             Canada|          Deplaned|              Other|   Terminal 1|            B|        35156.0|\n",
      "|  4|       200507.0|       Air Canada|                         AC|       Air Canada|                         AC|International|             Canada|          Enplaned|              Other|   Terminal 1|            B|        34090.0|\n",
      "|  5|       200507.0|        Air China|                         CA|        Air China|                         CA|International|               Asia|          Deplaned|              Other|International|            G|         6263.0|\n",
      "|  6|       200507.0|        Air China|                         CA|        Air China|                         CA|International|               Asia|          Enplaned|              Other|International|            G|         5500.0|\n",
      "|  7|       200507.0|       Air France|                         AF|       Air France|                         AF|International|             Europe|          Deplaned|              Other|International|            A|        12050.0|\n",
      "|  8|       200507.0|       Air France|                         AF|       Air France|                         AF|International|             Europe|          Enplaned|              Other|International|            A|        11638.0|\n",
      "|  9|       200507.0|  Air New Zealand|                         NZ|  Air New Zealand|                         NZ|International|Australia / Oceania|          Deplaned|              Other|International|            G|         4998.0|\n",
      "| 10|       200507.0|  Air New Zealand|                         NZ|  Air New Zealand|                         NZ|International|Australia / Oceania|          Enplaned|              Other|International|            G|         4962.0|\n",
      "| 11|       200507.0|  AirTran Airways|                         FL|  AirTran Airways|                         FL|     Domestic|                 US|          Deplaned|           Low Fare|International|            A|         8055.0|\n",
      "| 12|       200507.0|  AirTran Airways|                         FL|  AirTran Airways|                         FL|     Domestic|                 US|          Enplaned|           Low Fare|International|            A|         7984.0|\n",
      "| 13|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|     Domestic|                 US|          Deplaned|              Other|International|            A|        36641.0|\n",
      "| 14|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|     Domestic|                 US|          Enplaned|              Other|International|            A|        39379.0|\n",
      "| 15|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|     Domestic|                 US|    Thru / Transit|              Other|International|            A|         3678.0|\n",
      "| 16|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|International|             Canada|          Deplaned|              Other|International|            A|         7977.0|\n",
      "| 17|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|International|             Canada|          Enplaned|              Other|International|            A|         8837.0|\n",
      "| 18|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|International|             Mexico|          Deplaned|              Other|International|            A|         6969.0|\n",
      "| 19|       200507.0|  Alaska Airlines|                         AS|  Alaska Airlines|                         AS|International|             Mexico|          Enplaned|              Other|International|            A|        10046.0|\n",
      "+---+---------------+-----------------+---------------------------+-----------------+---------------------------+-------------+-------------------+------------------+-------------------+-------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"data.csv\")\n",
    "# rdd = sc.textFile(\"/FileStore/tables/2esy8tnj1455052720017/\")\n",
    "# diamonds = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Activity Period: string (nullable = true)\n",
      " |-- Operating Airline: string (nullable = true)\n",
      " |-- Operating Airline IATA Code: string (nullable = true)\n",
      " |-- Published Airline: string (nullable = true)\n",
      " |-- Published Airline IATA Code: string (nullable = true)\n",
      " |-- GEO Summary: string (nullable = true)\n",
      " |-- GEO Region: string (nullable = true)\n",
      " |-- Activity Type Code: string (nullable = true)\n",
      " |-- Price Category Code: string (nullable = true)\n",
      " |-- Terminal: string (nullable = true)\n",
      " |-- Boarding Area: string (nullable = true)\n",
      " |-- Passenger Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, Activity Period: string, Operating Airline: string, Operating Airline IATA Code: string, Published Airline: string, Published Airline IATA Code: string, GEO Summary: string, GEO Region: string, Activity Type Code: string, Price Category Code: string, Terminal: string, Boarding Area: string, Passenger Count: string]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+---------------------------+-----------------+---------------------------+-------------+----------+------------------+-------------------+-------------+-------------+-----------------+\n",
      "|summary|              _c0|   Activity Period|Operating Airline|Operating Airline IATA Code|Published Airline|Published Airline IATA Code|  GEO Summary|GEO Region|Activity Type Code|Price Category Code|     Terminal|Boarding Area|  Passenger Count|\n",
      "+-------+-----------------+------------------+-----------------+---------------------------+-----------------+---------------------------+-------------+----------+------------------+-------------------+-------------+-------------+-----------------+\n",
      "|  count|            16219|             16217|            16217|                      16159|            16217|                      16160|        16217|     16217|             16217|              16218|        16217|        16217|            16218|\n",
      "|   mean|           8109.0| 201087.0767712894|             null|                       null|             null|                       null|         null|      null|              null|               null|         null|         null|59252.92082870884|\n",
      "| stddev|4682.166343335814|335.76330796754615|             null|                       null|             null|                       null|         null|      null|              null|               null|         null|         null|3773158.085088906|\n",
      "|    min|                0|          200507.0|     ATA Airlines|                         4T|     ATA Airlines|                     - 1 - |     Domestic|      Asia|          Deplaned|         1:32:36 PM|International|            A|              1.0|\n",
      "|    max|             9999|          201612.0|     Xtra Airways|                         YX|     Xtra Airways|                         YX|International|        US|    Thru / Transit|              Other|   Terminal 3|        Other|           9999.0|\n",
      "+-------+-----------------+------------------+-----------------+---------------------------+-----------------+---------------------------+-------------+----------+------------------+-------------------+-------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't super useful, yet, was it?  Next up, properly setting up our schemas and working with the super-awesome DataFrame syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
