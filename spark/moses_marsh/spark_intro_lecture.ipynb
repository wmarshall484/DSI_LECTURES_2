{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- Describe the advantages/disadvantages of Spark compared to Hadoop MapReduce\n",
    "- Define what an RDD is, by its properties and operations\n",
    "- Explain the different between transformations and actions on an RDD\n",
    "- Implement the different transformations through use cases\n",
    "- Describe what persisting/caching an RDD means, and situations where this is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Key Concepts\n",
    "\n",
    "## 1.1. MapReduce vs Spark\n",
    "\n",
    "<img src=\"images/apache_hadoop_ecosystem.jpg\" width=\"500\">\n",
    "\n",
    "**Hadoop MapReduce limits:**\n",
    "- your job has to fit the `<key,value>` paradigm\n",
    "- no interactions (except by programming)\n",
    "- each job read from disk: problem with iterative algorithms (machine learning)\n",
    "\n",
    "**How Spark answers this:**\n",
    "- Spark proposes other processing workflows than MapReduce\n",
    "- highly efficient distributed operations\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk.\n",
    "- Spark keeps everything in memory when possible, uses lots of it.\n",
    "\n",
    "<img src=\"images/spark_ecosystem.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Resilient Distributed Datasets (RDD)\n",
    "\n",
    "<img src=\"images/rdd_on_cluster.png\" width=\"200\" align=\"right\">\n",
    "\\[[Image Source](http://horicky.blogspot.com/2015/02/big-data-processing-in-spark.html)\\]\n",
    "\n",
    "- created from HDFS, S3, HBase, JSON, text, local... or transformed from another RDD\n",
    "- distributed accross the cluster, partitioned (atomic chunks of data)\n",
    "- can recover from errors (node failure, slow process)\n",
    "- traceability of each partition, can re-run the processing\n",
    "- **immutable** : you cannot *modify* an RDD in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. A \"functional programming paradigm\" and DAGs\n",
    "\n",
    "RDDs are **immutable** ! You can only **transform** an existing RDD into another one.\n",
    "\n",
    "Spark provides many transformations functions. By programming these functions, you construct a **Directed Acyclic Graph** (DAG).\n",
    "\n",
    "<img src=\"images/dag.png\">\n",
    "\n",
    "When you use them, these functions are passed from the **client** to the **master**, who then distributes them to workers, who apply them accross their partitions of the RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Spark architecture : from your coding hands to the cluster\n",
    "\n",
    "<img src=\"images/from_rdd_to_cluster.png\">\n",
    "\n",
    "You construct your sequence of transformations in python.\n",
    "Spark functional programming interface builds up a **DAG**\n",
    "This DAG is sent by the **driver** for execution to the **cluster manager**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Jargon\n",
    "\n",
    "Excerpt taken from \\[[Arush Kharbanda](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work) on Quora\\]\n",
    "\n",
    "**Job**: A piece of code which reads some input  from HDFS or local, performs some computation on the data and writes some output data.\n",
    "\n",
    "**Stages**: Jobs are divided into stages. Stages are classified as a Map or reduce stages(Its easier to understand if you have worked on Hadoop and want to correlate). Stages are divided based on computational boundaries, all computations(operators) cannot be Updated in a single Stage. It happens over many stages.\n",
    "\n",
    "**Tasks**: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor(machine).\n",
    "\n",
    "**DAG**: DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\n",
    "\n",
    "**Executor**: The process responsible for executing a task.\n",
    "\n",
    "**Driver**: The program/process responsible for running the Job over the Spark Engine\n",
    "\n",
    "**Master**: The machine on which the Driver program runs\n",
    "\n",
    "**Slave/Worker**: The machine on which the Executor program runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operational Spark in Python\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviromnent to run spark from python\n",
    "2. extract RDDs from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)\n",
    "\n",
    "**Brainstorming**: So, let's suppose you have this thing called an RDD, which is just basically a dataset made of rows and values. What are all the operations you'd like to do to that RDD ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your ideas here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Initializing a `SparkContext` in Python\n",
    "\n",
    "IPython / IPython notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkContext` that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"images/spark_driver_etc.png\"/>\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Creating an RDD (from files)\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from an external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| [`sc.parallelize(array)`]() | Create an RDD from a python array or list |\n",
    "| [`sc.textFile(path)`]() | Create an RDD from a text file |\n",
    "| [`sc.pickleFile(path)`]() | Create an RDD from a pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Creating RDDs from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.parallelize()` : create an RDD from a python iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/toy_data.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('data/toy_data.txt')\n",
    "\n",
    "# to output the content in python [in practice, use collect() with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">`sc.pickeFile()` : from a HDFS pickle file\n",
    "\n",
    "The import will give you an rdd composed of whatever table was stored into that file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl.csv         input.txt       sales.csv   sales.txt      \u001b[0m\u001b[01;34mtoy_data.pkl\u001b[0m/\r\n",
      "cookie_data.txt  \u001b[01;31msales2.json.gz\u001b[0m  sales.json  toy_dataB.txt  toy_data.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emilie,23', 'yunjin,42', 'matthew,4', 'jorge,8', 'josh,15', 'evangeline,16']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.pickleFile('data/toy_data.pkl')\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Creating RDDs from S3\n",
    "\n",
    "These two functions above can perform loading from an s3 repository too ! Effortless.\n",
    "\n",
    "<span style=\"color:red\">Warning: don't .collect() that, or you'll break the internet !</span>\n",
    "\n",
    "**Note**: in order to do that, you need to have launched jupyter with the `--packages` options for aws and hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# link to the S3 repository\n",
    "link = 's3a://mortar-example-data/airline-data'\n",
    "\n",
    "# creating an RDD...\n",
    "rdd = sc.textFile(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this repository has like 5 million rows, but this was so fast ! right ?\n",
    "\n",
    "Not really, Spark is just **lazy**: it only executes the operations when necessary. For instance, when we call for an Action (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28.partitions.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7125aec39760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find out how many partitions there are...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \"\"\"\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.partitions.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "# find out how many partitions there are...\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform an RDD into another RDD because RDD are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "\\[[Image Source](http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html)\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "In the sequence below, we will in one sequence:\n",
    "1. read an RDD from a text file\n",
    "2. transform by applying `split`\n",
    "3. transform by filtering\n",
    "4. transform by casting some columns to their corresponding type.\n",
    "5. use an action to output the results\n",
    "\n",
    "Each transformation is a method of an RDD, and returns another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID    Date           Store   State  Product    Amount\n",
      "101    11/13/2014     100     WA     331        300.00\n",
      "104    11/18/2014     700     OR     329        450.00\n",
      "102    11/15/2014     203     CA     321        200.00\n",
      "106    11/19/2014     202     CA     331        330.00\n",
      "103    11/17/2014     101     WA     373        750.00\n",
      "105    11/19/2014     202     CA     321        200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/sales.txt', 'r') as fin:\n",
    "    print fin.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall: Input functions, reading RDDs from files, are functions of the SparkContext.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#ID    Date           Store   State  Product    Amount',\n",
       " u'101    11/13/2014     100     WA     331        300.00',\n",
       " u'104    11/18/2014     700     OR     329        450.00',\n",
       " u'102    11/15/2014     203     CA     321        200.00',\n",
       " u'106    11/19/2014     202     CA     331        330.00',\n",
       " u'103    11/17/2014     101     WA     373        750.00',\n",
       " u'105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#ID', u'Date', u'Store', u'State', u'Product', u'Amount'],\n",
       " [u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00'],\n",
       " [u'102', u'11/15/2014', u'203', u'CA', u'321', u'200.00'],\n",
       " [u'106', u'11/19/2014', u'202', u'CA', u'331', u'330.00'],\n",
       " [u'103', u'11/17/2014', u'101', u'WA', u'373', u'750.00'],\n",
       " [u'105', u'11/19/2014', u'202', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies split() to each row\n",
    "rdd2 = rdd1.map(lambda rowstr : rowstr.split())\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00'],\n",
       " [u'102', u'11/15/2014', u'203', u'CA', u'321', u'200.00'],\n",
       " [u'106', u'11/19/2014', u'202', u'CA', u'331', u'330.00'],\n",
       " [u'103', u'11/17/2014', u'101', u'WA', u'373', u'750.00'],\n",
       " [u'105', u'11/19/2014', u'202', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters rows\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "# applies casting_function to rows\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "\n",
    "# shows the result\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's see the canonical way to write that in Python...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#ID    Date           Store   State  Product    Amount',\n",
       " u'101    11/13/2014     100     WA     331        300.00',\n",
       " u'104    11/18/2014     700     OR     329        450.00',\n",
       " u'102    11/15/2014     203     CA     321        200.00',\n",
       " u'106    11/19/2014     202     CA     331        330.00',\n",
       " u'103    11/17/2014     101     WA     373        750.00',\n",
       " u'105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales = sc.textFile('data/sales.txt')\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#ID', u'Date', u'Store', u'State', u'Product', u'Amount'],\n",
       " [u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00'],\n",
       " [u'102', u'11/15/2014', u'203', u'CA', u'321', u'200.00'],\n",
       " [u'106', u'11/19/2014', u'202', u'CA', u'331', u'330.00'],\n",
       " [u'103', u'11/17/2014', u'101', u'WA', u'373', u'750.00'],\n",
       " [u'105', u'11/19/2014', u'202', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())   # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00'],\n",
       " [u'102', u'11/15/2014', u'203', u'CA', u'321', u'200.00'],\n",
       " [u'106', u'11/19/2014', u'202', u'CA', u'331', u'330.00'],\n",
       " [u'103', u'11/17/2014', u'101', u'WA', u'373', u'750.00'],\n",
       " [u'105', u'11/19/2014', u'202', u'CA', u'321', u'200.00']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))    # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)   # <= JUST ADDED THIS HERE\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">FROM NOW ON WE'LL RELY ON THESE TWO RDDs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map(func)` : applying a function on every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [7, 5, 4, 10, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda x : len(x[0]))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using lambda functions, use **argument unpacking** to provide a more readable transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [7, 5, 4, 10, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda (name,number) : len(name))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.flatMap(func)` : applying a function on every row and flattening the resulting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.flatMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [4, 6, 11, 8, 10, 13, 15, 17, 19, 16, 18, 26, 23, 25, 29, 42, 44, 48]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.flatMap(lambda (name,number) : [number, number+2, number+len(name)])\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Row reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, u'11/13/2014', 100, u'WA', 331, 300.0), (104, u'11/18/2014', 700, u'OR', 329, 450.0), (102, u'11/15/2014', 203, u'CA', 321, 200.0), (106, u'11/19/2014', 202, u'CA', 331, 330.0), (103, u'11/17/2014', 101, u'WA', 373, 750.0), (105, u'11/19/2014', 202, u'CA', 321, 200.0)]\n",
      "after: [(102, u'11/15/2014', 203, u'CA', 321, 200.0), (106, u'11/19/2014', 202, u'CA', 331, 330.0), (105, u'11/19/2014', 202, u'CA', 321, 200.0)]\n"
     ]
    }
   ],
   "source": [
    "# filtering an rdd\n",
    "rddout = rdd_sales.filter(lambda (i, date, store, state, pdt, amnt): (state == 'CA'))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, u'11/13/2014', 100, u'WA', 331, 300.0), (104, u'11/18/2014', 700, u'OR', 329, 450.0), (102, u'11/15/2014', 203, u'CA', 321, 200.0), (106, u'11/19/2014', 202, u'CA', 331, 330.0), (103, u'11/17/2014', 101, u'WA', 373, 750.0), (105, u'11/19/2014', 202, u'CA', 321, 200.0)]\n",
      "after: [(101, u'11/13/2014', 100, u'WA', 331, 300.0), (102, u'11/15/2014', 203, u'CA', 321, 200.0), (103, u'11/17/2014', 101, u'WA', 373, 750.0), (103, u'11/17/2014', 101, u'WA', 373, 750.0)]\n"
     ]
    }
   ],
   "source": [
    "# sampling an rdd\n",
    "rddout = rdd_sales.sample(True, 0.4)\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [(101, u'11/13/2014', 100, u'WA', 331, 300.0), (104, u'11/18/2014', 700, u'OR', 329, 450.0), (102, u'11/15/2014', 203, u'CA', 321, 200.0), (106, u'11/19/2014', 202, u'CA', 331, 330.0), (103, u'11/17/2014', 101, u'WA', 373, 750.0), (105, u'11/19/2014', 202, u'CA', 321, 200.0)]\n",
      "after: [u'CA', u'WA', u'OR']\n"
     ]
    }
   ],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "rddout = rdd_sales.map(lambda (i, date, store, state, pdt, amnt): state)\\\n",
    "                    .distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_sales.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Methods with a `<k,v>` paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.values()`: returns the values of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [4, 8, 15, 16, 23, 42]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.values()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: ['matthew', 'jorge', 'josh', 'evangeline', 'emilie', 'yunjin']\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.keys()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 300.0),\n",
       " (u'OR', 450.0),\n",
       " (u'CA', 200.0),\n",
       " (u'CA', 330.0),\n",
       " (u'WA', 750.0),\n",
       " (u'CA', 200.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_salesperstate = rdd_sales.map(lambda (i, date, store, state, pdt, amnt): (state,amnt))\n",
    "\n",
    "rdd_salesperstate.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', (300.0, 'matthew')),\n",
       " (u'WA', (750.0, 'matthew')),\n",
       " (u'CA', (200.0, 'matthew')),\n",
       " (u'CA', (330.0, 'matthew')),\n",
       " (u'CA', (200.0, 'matthew')),\n",
       " (u'OR', (450.0, 'jorge'))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list of managers for each state\n",
    "data_array = [['CA', 'matthew'],\n",
    "              ['OR', 'jorge'],\n",
    "              ['WA','matthew'],\n",
    "              ['TX', 'emilie']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_managers = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_salesperstate.join(rdd_managers).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.reduceByKey(func)`: reduce `v`s by their `k` by applying func (what ?)\n",
    "\n",
    "The `func` here needs to be associative and commutative... can you guess why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1], ['WA', 1], ['CA', 2], ['OR', 1], ['CA', 5], ['OR', 1]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OR', 2), ('CA', 8), ('WA', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda v1,v2 : v1+v2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupByKey(func)`: reduce `v`s by their `k` by applying func (again ?)\n",
    "\n",
    "This can use any function non-commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1], ['WA', 1], ['CA', 2], ['OR', 1], ['CA', 5], ['OR', 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OR', 1.0), ('CA', 2.6666666666666665), ('WA', 1.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(iterator):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return total / count\n",
    "\n",
    "rdd.groupByKey()\\\n",
    "    .map(lambda (state, iterator): (state, mean(iterator)))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Sorting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[['josh', 15], ['evangeline', 16], ['jorge', 8], ['matthew', 4], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    }
   ],
   "source": [
    "# sorting by any function (because why not?)\n",
    "rddout = rdd_names.sortBy(lambda (name,number) : (13-number)**2, ascending=True)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[('yunjin', 42), ('matthew', 4), ('josh', 15), ('jorge', 8), ('evangeline', 16), ('emilie', 23)]\n"
     ]
    }
   ],
   "source": [
    "# sorting k,v pairs by key\n",
    "rddout = rdd_names.sortByKey(ascending=False)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (a python object, or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small, as all the data is loaded into the drivers memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in a RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDDs elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDDs elements. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.top?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : returning the *full* content of an RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic, cause `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd: <class 'pyspark.rdd.RDD'>\n",
      "type of rdd_collected: <type 'list'>\n",
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "collected = rdd_names.collect()\n",
    "\n",
    "# let's check the type of RDD\n",
    "print(\"type of rdd: {}\".format(type(rdd_names)))\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_collected: {}\".format(type(collected)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : returning (any) n lines of an RDD\n",
    "\n",
    "Returns `n` the rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd_taken: <type 'list'>\n",
      "[['matthew', 4], ['jorge', 8]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "taken = rdd_names.take(2)\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_taken: {}\".format(type(taken)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : returning the first line of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matthew', 4]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Actions that compute some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: you get that right ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3153021346\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Let's design chains of transformations together !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function((id, date, store, state, product, amount)):\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a sorted RDD of the states in which you have most sales done (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_sales # apply transformation here...\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rddout = rdd_sales.map(lambda x: (x[3],x[5]))\\<br/>\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\<br/>\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False)<br/>\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Word count (again)\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "another line\n",
      "yet another line\n",
      "yet another another line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/input.txt', 'r') as fin:\n",
    "    print fin.read()\n",
    "\n",
    "# reading the file using SparkContext\n",
    "\n",
    "rdd = sc.textFile('data/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello world',\n",
       " u'another line',\n",
       " u'yet another line',\n",
       " u'yet another another line']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd # apply transformation here...\n",
    "\n",
    "# collect the result\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "rddout = rdd.flatMap(lambda str : str.split())\\<br/>\n",
    "            .map(lambda word: (word,1))\\<br/>\n",
    "            .reduceByKey(lambda v1,v2: v1+v2)<br/>\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines in file: 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " u'2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " u'2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " u'2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " u'2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_aapl_raw = sc.textFile('data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(rdd_aapl_raw.count()))\n",
    "\n",
    "rdd_aapl_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " u'2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " u'2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " u'2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " u'2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_aapl_raw # apply transformation here...\n",
    "\n",
    "rddout.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "rddout = rdd_aapl_raw.filter(lambda line: not line.startswith(\"Date\"))\\<br/>\n",
    ".map(lambda line: line.split(\",\"))\\<br/>\n",
    ".map(lambda fields: (float(fields[4]),fields[0]))\\<br/>\n",
    ".sortBy(lambda (close, date): close, ascending=False)\n",
    "<br/>\n",
    "rddout.collect()<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD: one use case ? oh I don't know maybe... **MACHINE LEARNING !!!**\n",
    "\n",
    "## 4.1. Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in xrange(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile('s2n://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 ms, sys: 8.04 ms, total: 36.9 ms\n",
      "Wall time: 3.07 s\n",
      "CPU times: user 20.7 ms, sys: 3.74 ms, total: 24.4 ms\n",
      "Wall time: 1.35 s\n",
      "CPU times: user 22.5 ms, sys: 3.67 ms, total: 26.2 ms\n",
      "Wall time: 1.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 4.37 ms, total: 25.8 ms\n",
      "Wall time: 1.58 s\n",
      "CPU times: user 22.1 ms, sys: 4.23 ms, total: 26.3 ms\n",
      "Wall time: 160 ms\n",
      "CPU times: user 22.5 ms, sys: 4.11 ms, total: 26.7 ms\n",
      "Wall time: 177 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
