{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "## 1.1. RDDs versus DataFrames\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using column names instead of column positions\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "- Schemas make your data more structured.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as RDDs with schema.\n",
    "\n",
    "What is a schema?\n",
    "- Schemas are metadata about your data.\n",
    "- Schemas define table names, column names, and column types over your data.\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs, instead of using column positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operational DataFrames in Python\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the enviroment to run Spark SQL from python\n",
    "2. create DataFrames from RDDs or from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Initializing a `SparkContext` and `SqlContext` in Python\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5bb6fb7630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spark` session object serves as our SQL context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old (Spark 1.x) way of making a SQL Context: sqlContext = ps.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. From an RDD (specifying schema)\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspark.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module. Here's a list of the most useful ones (subjective criteria).\n",
    "\n",
    "| Types | Python-like type |\n",
    "| - | - |\n",
    "| StringType | string |\n",
    "| IntegerType | int |\n",
    "| FloatType | float |\n",
    "| ArrayType\\* | array or list |\n",
    "| MapType | dict |\n",
    "\n",
    "\\* see later UDF functions on how to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that csv file ?\n",
    "def casting_function(row):\n",
    "    (id, date, store, state, product, amount) = row\n",
    "    return (int(id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "rdd_sales = (\n",
    "    sc.textFile('data/sales.csv')\n",
    "        .map(lambda rowstr : rowstr.split(\",\"))\n",
    "        .filter(lambda row: not row[0].startswith('#'))\n",
    "        .map(casting_function)\n",
    "            )\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- product: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the many data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# create a schema of your own\n",
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )\n",
    "\n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_sales,schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Reading from files (infering schema)\n",
    "\n",
    "Use [`sqlContext.read.csv`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv) to load a CSV into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`spark.read.json`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to load a JSON file into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales.json')\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales2.json.gz')\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date='11/13/2014', id=101, product=331, state='WA', store=100),\n",
       " Row(amount=450.0, date='11/18/2014', id=104, product=329, state='OR', store=700),\n",
       " Row(amount=200.0, date='11/15/2014', id=102, product=321, state='CA', store=203),\n",
       " Row(amount=330.0, date='11/19/2014', id=106, product=331, state='CA', store=202),\n",
       " Row(amount=750.0, date='11/17/2014', id=103, product=373, state='WA', store=101),\n",
       " Row(amount=200.0, date='11/19/2014', id=105, product=321, state='CA', store=202)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Actions : turning your DataFrame into a local object\n",
    "\n",
    "Some actions just remain the same, you won't have to learn Spark all over again.\n",
    "\n",
    "Some new actions give you the possibility to describe and show the content in a more fashionable manner.\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | DF vs RDD? | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) | identical | Return a list that contains all of the elements as Rows. |\n",
    "| [`.count()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) | identical | Return the number of elements. |\n",
    "| [`.take(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) | identical | Take the first `n` elements. |\n",
    "| [`.top(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.top) | identical | Get the top `n` elements. |\n",
    "| [`.first()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) | identical | Return the first element. |\n",
    "| [`.show(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) | <span style=\"color:green\">new</span> | Show the DataFrame in table format (`n=20` by default) |\n",
    "| [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) | <span style=\"color:green\">new</span> | Convert the DF into a Pandas DF. |\n",
    "| [`.printSchema(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)\\* | <span style=\"color:green\">new</span> | Display the schema. This is not an action, it doesn't launch the DAG, but it fits better in this category. |\n",
    "| [`.describe(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) | <span style=\"color:green\">new</span> | Compute statistics for this column. |\n",
    "| [`.sum(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.mean(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.mean) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.min(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.max(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|amount|\n",
      "+----------+------+\n",
      "|11/13/2014| 300.0|\n",
      "|11/18/2014| 450.0|\n",
      "|11/15/2014| 200.0|\n",
      "|11/19/2014| 330.0|\n",
      "|11/17/2014| 750.0|\n",
      "|11/19/2014| 200.0|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['date','amount']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "      <th>Product</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>100</td>\n",
       "      <td>WA</td>\n",
       "      <td>331</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>700</td>\n",
       "      <td>OR</td>\n",
       "      <td>329</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>203</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>331</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>WA</td>\n",
       "      <td>373</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #ID        Date  Store State  Product  Amount\n",
       "0  101  11/13/2014    100    WA      331   300.0\n",
       "1  104  11/18/2014    700    OR      329   450.0\n",
       "2  102  11/15/2014    203    CA      321   200.0\n",
       "3  106  11/19/2014    202    CA      331   330.0\n",
       "4  103  11/17/2014    101    WA      373   750.0\n",
       "5  105  11/19/2014    202    CA      321   200.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how `.collect()` returns things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0),\n",
       " Row(#ID=104, Date='11/18/2014', Store=700, State='OR', Product=329, Amount=450.0),\n",
       " Row(#ID=102, Date='11/15/2014', Store=203, State='CA', Product=321, Amount=200.0),\n",
       " Row(#ID=106, Date='11/19/2014', Store=202, State='CA', Product=331, Amount=330.0),\n",
       " Row(#ID=103, Date='11/17/2014', Store=101, State='WA', Product=373, Amount=750.0),\n",
       " Row(#ID=105, Date='11/19/2014', Store=202, State='CA', Product=321, Amount=200.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|summary|               #ID|      Date|             Store|State|           Product|            Amount|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|  count|                 6|         6|                 6|    6|                 6|                 6|\n",
      "|   mean|             103.5|      null|251.33333333333334| null| 334.3333333333333| 371.6666666666667|\n",
      "| stddev|1.8708286933869716|      null|225.39180700874346| null|19.500427345744672|207.40459654179958|\n",
      "|    min|               101|11/13/2014|               100|   CA|               321|             200.0|\n",
      "|    max|               106|11/19/2014|               700|   WA|               373|             750.0|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+------------------+\n",
      "|summary|            Amount|\n",
      "+-------+------------------+\n",
      "|  count|                 6|\n",
      "|   mean| 371.6666666666667|\n",
      "| stddev|207.40459654179958|\n",
      "|    min|             200.0|\n",
      "|    max|             750.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df_sales.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df_sales.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df_sales.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "You got that... DataFrames are just RDDs with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. `.withColumn()`: adding column using operations or functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.select(*cols)` : selecting specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.select(\"Open\", \"Close\")\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl[[\"Open\",\"Close\"]].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|      Open|      High|blabla|\n",
      "+----------+----------+------+\n",
      "|117.949997|118.360001|  true|\n",
      "|117.099998|117.739998|  true|\n",
      "|116.809998|116.910004|  true|\n",
      "|116.860001|117.379997|  true|\n",
      "|    117.25|117.760002|  true|\n",
      "+----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_out = df_aapl.withColumn(\"blabla\", lit(True))\n",
    "\n",
    "df_out[['Open','High','blabla']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+\n",
      "|               Date|      High|       Low|              diff|\n",
      "+-------------------+----------+----------+------------------+\n",
      "|2016-10-25 00:00:00|118.360001|117.309998|1.0500030000000038|\n",
      "|2016-10-24 00:00:00|117.739998|     117.0|0.7399979999999999|\n",
      "|2016-10-21 00:00:00|116.910004|116.279999| 0.630004999999997|\n",
      "|2016-10-20 00:00:00|117.379997|116.330002|1.0499950000000098|\n",
      "|2016-10-19 00:00:00|117.760002|113.800003|3.9599989999999963|\n",
      "+-------------------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.withColumn(\"diff\", df_aapl['High'] - df_aapl['Low']).select('Date', 'High', 'Low', 'diff')\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_thing = df_aapl['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_thing.getField?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+-----------+\n",
      "|      High|       Low|      Open|     Close|    special|\n",
      "+----------+----------+----------+----------+-----------+\n",
      "|118.360001|117.309998|117.949997|    118.25|-0.31500405|\n",
      "|117.739998|     117.0|117.099998|117.650002|-0.40700185|\n",
      "|116.910004|116.279999|116.809998|116.599998| 0.13230105|\n",
      "|117.379997|116.330002|116.860001|117.059998|-0.20999585|\n",
      "|117.760002|113.800003|    117.25|117.120003|   0.514788|\n",
      "|118.209999|117.449997|    118.18|117.470001|  0.5396007|\n",
      "|117.839996|116.779999|117.330002|117.550003| -0.2332004|\n",
      "|118.169998|117.129997|117.879997|117.629997| 0.26000026|\n",
      "|117.440002|115.720001|116.790001|116.980003|-0.32680362|\n",
      "|117.980003|    116.75|117.349998|117.339996| 0.01230249|\n",
      "|118.690002|116.199997|117.699997|116.300003|   3.485992|\n",
      "|    116.75|114.720001|115.019997|116.050003| -2.0909111|\n",
      "|114.559998|113.510002|114.309998|114.059998|   0.262499|\n",
      "|114.339996|113.129997|113.699997|113.889999|-0.22990222|\n",
      "|113.660004|112.690002|113.400002|113.050003| 0.33949974|\n",
      "|114.309998|112.629997|113.059998|     113.0|  0.1007967|\n",
      "|113.050003|112.279999|112.709999|112.519997|  0.1463023|\n",
      "|113.370003|111.800003|112.459999|113.050003| -0.9263063|\n",
      "|113.800003|111.800003|113.160004|    112.18|   1.960008|\n",
      "|114.639999|    113.43|113.690002|113.949997| -0.3145937|\n",
      "+----------+----------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "def my_specialfunc(h,l,o,c):\n",
    "    return ((h-l)*(o-c))\n",
    "\n",
    "my_specialfunc_udf = udf(my_specialfunc, FloatType())\n",
    "\n",
    "df_out = df_aapl.withColumn(\"special\", my_specialfunc_udf(df_aapl['High'], \n",
    "                                                          df_aapl['Low'], \n",
    "                                                          df_aapl['Open'], \n",
    "                                                          df_aapl['Close']))\n",
    "\n",
    "df_out.select('High', 'Low', 'Open', 'Close', 'special').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupBy()`: aggregating in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------------+\n",
      "|State|sum(Amount)|     avg(Product)|\n",
      "+-----+-----------+-----------------+\n",
      "|   OR|      450.0|            329.0|\n",
      "|   CA|      730.0|324.3333333333333|\n",
      "|   WA|     1050.0|            352.0|\n",
      "+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_out = df_sales.groupBy(\"State\").agg(F.sum(\"Amount\"), F.avg('Product'))\n",
    "#g = df_sales.groupBy(\"State\").sum(\"Amount\")\n",
    "#g.show()\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.orderBy()` : sorting by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   WA|     1050.0|\n",
      "|   CA|      730.0|\n",
      "|   OR|      450.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_sales.groupBy(\"State\").agg(F.sum(\"Amount\")).orderBy(\"sum(Amount)\", ascending=False)\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Let's design chains of transformations together ! (reloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_sales = sqlContext.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a sorted ~~RDD~~ DataFrame of the states in which you have most money from sales (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_sales\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|max(Amount)|\n",
      "+-----+-----------+\n",
      "|   OR|      450.0|\n",
      "|   CA|      330.0|\n",
      "|   WA|      750.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out.groupBy('State').max('Amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|   uh|\n",
      "+-----+-----+\n",
      "|   CA|330.0|\n",
      "|   OR|450.0|\n",
      "|   WA|750.0|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out.groupBy('State').agg(F.max('Amount').alias('uh')).orderBy('uh').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (use your mouse to uncover)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "df_out = df_sales.groupBy(df_sales.State)\\<br/>\n",
    "                 .agg(F.sum(df_sales.Amount).alias('Money'))\\<br/>\n",
    "                 .orderBy(\"Money\", ascending=False)<br/>\n",
    "<br/>\n",
    "df_out.show()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "\n",
    "1. ~~filter out headers and last line~~\n",
    "2. ~~split each line based on comma~~\n",
    "3. keep only fields for Date ~~(col 0)~~ and Close ~~(col 4)~~\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               Date|     Close|\n",
      "+-------------------+----------+\n",
      "|2015-11-03 00:00:00|    122.57|\n",
      "|2015-11-04 00:00:00|     122.0|\n",
      "|2015-11-02 00:00:00|    121.18|\n",
      "|2015-11-06 00:00:00|121.059998|\n",
      "|2015-11-05 00:00:00|120.919998|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.select('Date', 'Close').orderBy('Close', ascending=False)\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "df_out.select(\"Close\", \"Date\").orderBy(df_aapl.Close, ascending=False).show(5)<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The SQL Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know you missed it. Let's run some SQL queries on these tables!\n",
    "\n",
    "First we tell spark to create \"SQL namespace\" and assign a name to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl.registerTempTable('aapl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write queries using `spark.sql`, and it will have access to any table we have registered like above. The output of the query is another spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|      Open|     Close|                diff|\n",
      "+----------+----------+--------------------+\n",
      "|117.949997|    118.25|  0.3000030000000038|\n",
      "|117.099998|117.650002|  0.5500040000000013|\n",
      "|116.809998|116.599998|-0.20999999999999375|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT Open, Close, Close - Open as diff FROM aapl LIMIT 3\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "|2016-10-18 00:00:00|    118.18|118.209999|117.449997|117.470001|24553500|117.470001|\n",
      "|2016-10-17 00:00:00|117.330002|117.839996|116.779999|117.550003|23624900|117.550003|\n",
      "|2016-10-14 00:00:00|117.879997|118.169998|117.129997|117.629997|35652200|117.629997|\n",
      "|2016-10-13 00:00:00|116.790001|117.440002|115.720001|116.980003|35192400|116.980003|\n",
      "|2016-10-12 00:00:00|117.349998|117.980003|    116.75|117.339996|37586800|117.339996|\n",
      "|2016-10-11 00:00:00|117.699997|118.690002|116.199997|116.300003|64041000|116.300003|\n",
      "|2016-10-10 00:00:00|115.019997|    116.75|114.720001|116.050003|36236000|116.050003|\n",
      "|2016-10-07 00:00:00|114.309998|114.559998|113.510002|114.059998|24358400|114.059998|\n",
      "|2016-10-06 00:00:00|113.699997|114.339996|113.129997|113.889999|28779300|113.889999|\n",
      "|2016-10-05 00:00:00|113.400002|113.660004|112.690002|113.050003|21453100|113.050003|\n",
      "|2016-10-04 00:00:00|113.059998|114.309998|112.629997|     113.0|29736800|     113.0|\n",
      "|2016-10-03 00:00:00|112.709999|113.050003|112.279999|112.519997|21701800|112.519997|\n",
      "|2016-09-30 00:00:00|112.459999|113.370003|111.800003|113.050003|36379100|113.050003|\n",
      "|2016-09-29 00:00:00|113.160004|113.800003|111.800003|    112.18|35887000|    112.18|\n",
      "|2016-09-28 00:00:00|113.690002|114.639999|    113.43|113.949997|29641100|113.949997|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state| total|\n",
      "+-----+------+\n",
      "|   WA|1050.0|\n",
      "|   CA| 730.0|\n",
      "|   OR| 450.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT state, SUM(Amount) as total \n",
    "            FROM sales \n",
    "            GROUP BY State \n",
    "            ORDER BY total DESC'''\n",
    "\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning on DataFrames\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = sqlContext.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|    features|      scaledfeatures|\n",
      "+------------+--------------------+\n",
      "|    [118.25]| [0.865963404782699]|\n",
      "|[117.650002]|[0.8473472730564975]|\n",
      "|[116.599998]|[0.8147688098332226]|\n",
      "|[117.059998]|[0.8290412250646944]|\n",
      "|[117.120003]|[0.8309029995776607]|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "scaledData.select(\"features\", \"scaledfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
