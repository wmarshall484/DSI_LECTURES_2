{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "### or: the power of laziness\n",
    "\n",
    "Brandon Martin-Anderson, March 2019, with great debt to Moses Marsh &c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperative programming: all rules and no mercy\n",
    "\n",
    "How does normal computer programming work?\n",
    "\n",
    "1. Think of something you want done.\n",
    "2. Describe **precisely** how it is accomplished.\n",
    "3. The computer executes **precisely** your commands, quickly and mercilessly.\n",
    "\n",
    "This is called **imperative programming**, marked by a **tight coupling** of intention and implementation.\n",
    "\n",
    "This has a number of advantages\n",
    "* Even simple languages can execute any algorithm (\"Turing complete\")\n",
    "* Very efficient implementations possible\n",
    "\n",
    "The disadvantages, of course:\n",
    "* Difficult to learn\n",
    "* Debugging is time-consuming\n",
    "* **Jumps first without looking**\n",
    " * executing a O(n!) traveling salesman algorithm\n",
    " * opening a 300-million-line CSV\n",
    "\n",
    "### Declarative programming: don't make me think\n",
    "\n",
    "On the other hand, a **declarative language** allows you to specify **what** you want done. The implementation is left to the machine.\n",
    "\n",
    "A familiar example:\n",
    "\n",
    "```SQL\n",
    "SELECT name, balance \n",
    "FROM accounts \n",
    "ORDER BY balance DESC \n",
    "LIMIT 10\n",
    "```\n",
    "\n",
    "An SQL query allows a **loose coupling** of intention and implementation. How is a database implemented? Who knows! Who cares! SQL lets you use a **chain of command**. You say what you want, and the machine employee figures it out.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Easier to learn\n",
    "* Less debugging\n",
    "* **May respond appropriately to scale**\n",
    " * Might refuse to overload memory\n",
    " * Might pursue its own indexing/caching strategy\n",
    " * Might distribute jobs to subcontractors\n",
    "   * Potentially even **on different machines**\n",
    "   \n",
    "This strategy enables operations on **extremely large** datasets; potentially involving thousands of machines.\n",
    " \n",
    "## General Purpose Declarative Programming\n",
    "\n",
    "It turns out that in order to make use of a chain of command, you need to express your algorithm in alignment with two important principles: laziness, and immutability.\n",
    "\n",
    "### Laziness\n",
    "\n",
    "Any given operation (and all operations that depend on it) can be deferred to a later time. This allows the chain of command to schedule operations in whatever order it determines is most efficient.\n",
    "\n",
    "### Immutability\n",
    "\n",
    "All operations generate new data and do not modify their input. This enables automatic analysis of optimal order of operations without risking corrupting data or introducting bugs.\n",
    "\n",
    "## DAG\n",
    "\n",
    "Lazy operations on immutable data are joined into a **directed acyclic graph (DAG)** of operations. Tasks expressed in this form are the basis of a broad class of delarative programming environments broadly known as **MapReduce**. \n",
    "\n",
    "MapReduce-style systems like Hadoop, Spark, Amazon EMR etc are characterized by **high parallelism** and **portability**. If you go to the trouble of expressing your problem in terms of a lazy DAG, it's an easy additional step to run your analysis on a **thousand computers in an hour**, instead of **one computer for a thousand hours**.\n",
    "\n",
    "Today we're going to learn specifically about Spark, but the principles extend to similar systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting into it: Spark\n",
    "\n",
    "The Spark execution environment is divided into three different programs: the driver, the master, and worker.\n",
    "\n",
    "### Driver\n",
    "\n",
    "AKA the \"client\"; typically this is a short computer programm written by the end user. It's similar in length and purpose to an SQL query; expressing the intent of the program. Today, our driver is **this** Jupyter notebook.\n",
    "\n",
    "### Master\n",
    "\n",
    "AKA the \"scheduler\", \"cluster manager\". This is a program that the Driver interacts with, usually through a specific protocol, like an SQL client.\n",
    "\n",
    "### Workers\n",
    "\n",
    "AKA \"worker node\". Performs the bulk of operations. Workers are usually mediated by the Master, which performs the distribution and collection of work on the Driver's behalf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark cluster with four workers, all running on the current machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"spark context\" is our communication channel to the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bafbb000fb76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=lecture>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperative operation: squares\n",
    "\n",
    "Specify specifically how to perform operation in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = list(range(100))\n",
    "for i in range(100):\n",
    "    nums[i] = nums[i]**2\n",
    "nums[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark operation: squares\n",
    "\n",
    "```\n",
    "--(immutable input)--->[lazy operation]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# immutable, distributable input, created by the master via `sc`\n",
    "inp = sc.parallelize(range(1000000000000000000))\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lazy operation - not yet executed\n",
    "outp = inp.map( lambda x: x**2 )\n",
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking to see the output forces the lazy DAG to actually execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation chaining, `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_square(x):\n",
    "    return x**2\n",
    "\n",
    "inp = sc.parallelize(range(100))\n",
    "\n",
    "outp = (\n",
    "inp.map(my_square)\n",
    ".map(lambda x: -x)\n",
    ".filter(lambda x: x<-10)\n",
    ")\n",
    "\n",
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-16, -25, -36, -49, -64, -81, -100, -121, -144, -169]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sc.parallelize` creates an \"Resiliant Distributed Dataset\" (RDD).\n",
    "- `map` and `filter` are *transformations*.\n",
    "  - They create new RDDs from existing RDDs.\n",
    "- `count`, `take`, and `collect` are *actions* that bring the data from the RDDs back to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "Won't spend long on this; RDD is mostly depricated as a way to interact with Spark.\n",
    "\n",
    "- Common RDD Constructors\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`sc.parallelize(list)` | Create RDD of elements of list\n",
    "`sc.textFile(path)` | Create RDD of lines from file\n",
    "\n",
    "- Common Transformations\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`filter(lambda x: x % 2 == 0)` | Discard non-even elements\n",
    "`map(lambda x: x * 2)` | Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())` | Split each string into words\n",
    "`flatMap(lambda x: x.split())` | Split each string into words and flatten sequence\n",
    "`sample(withReplacement = True, 0.25)` | Create sample of 25% of elements with replacement\n",
    "`union(rdd)` | Append `rdd` to existing RDD\n",
    "`distinct()` | Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending = False)` | Sort elements in descending order\n",
    "\n",
    "- Common Actions\n",
    "\n",
    "Expression | Meaning\n",
    "--- | ---\n",
    "`collect()` | Convert RDD to in-memory list \n",
    "`take(3)` | First 3 elements of RDD \n",
    "`takeSample(withReplacement = True, 3)` | Create sample of 3 elements with replacement\n",
    "`sum()` | Find element sum (assumes numeric elements)\n",
    "`mean()` | Find element mean (assumes numeric elements)\n",
    "`stdev()` | Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = sc.textFile( \"data/sales.txt\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = inp.map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function(row):\n",
    "    (id,date,store,state,product,amount) = row\n",
    "    return (int(id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "rdd3 = rdd2.filter( lambda row: not row[0][0]==\"#\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.map( casting_function )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5 = rdd4.map( lambda x: x[5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.filter(lambda x:x[0]!=\"#\").map(lambda x:x.split()).map(lambda x:float(x[-1])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complicated operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/sales.txt MapPartitionsRDD[13] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.sample(False, 0.5)\n",
    "rdd3 = rdd2.map(lambda x : x.split())\n",
    "rdd4 = rdd3.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "def casting_function(row):\n",
    "    id_, date, store, state, product, amount = row\n",
    "    return (int(id_), date, int(store), state, int(product), float(amount))\n",
    "rdd5 = rdd4.map(casting_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Lazy bones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the date on which AAPL's stock price was the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_aapl_raw = sc.textFile('data/aapl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/aapl.csv MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_aapl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_aapl_raw.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_aapl_raw # apply transformation here...\n",
    "\n",
    "rddout.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD: one use case ? oh I don't know maybe... **MACHINE LEARNING !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in range(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
      "Wall time: 5.09 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 979 ms\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 1.49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.5 s\n",
      "CPU times: user 10 ms, sys: 10 ms, total: 20 ms\n",
      "Wall time: 192 ms\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 272 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames in Spark\n",
    "\n",
    "# Overview\n",
    "\n",
    "## RDDs versus DataFrames\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as RDDs with schema.\n",
    "\n",
    "What is a schema?\n",
    "- Schemas are metadata about your data.\n",
    "- Schemas define table names, column names, and column types over your data.\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs, instead of using column positions.\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What is a schema again?\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using column names instead of column positions\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "- Schemas make your data more structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operational DataFrames in Python\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the environment to run Spark / Spark SQL from python\n",
    "2. create DataFrames from RDDs or from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In RDD land, `sc` is the connection to the cluster.\n",
    "* In DF land, `spark` is the connection to the cluster.\n",
    "\n",
    "```¯\\_(ツ)_/¯```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bafbb000fb76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2bd0cfa6a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bafbb000fb76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=lecture>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From an RDD (specifying schema)\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspark.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module. Here's a list of the most useful ones (subjective criteria).\n",
    "\n",
    "| Types | Python-like type |\n",
    "| - | - |\n",
    "| StringType | string |\n",
    "| IntegerType | int |\n",
    "| FloatType | float |\n",
    "| ArrayType\\* | array or list |\n",
    "| MapType | dict |\n",
    "\n",
    "\\* see later UDF functions on how to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID,Date,Store,State,Product,Amount\n",
      "101,11/13/2014,100,WA,331,300.00\n",
      "104,11/18/2014,700,OR,329,450.00\n",
      "102,11/15/2014,203,CA,321,200.00\n",
      "106,11/19/2014,202,CA,331,330.00\n",
      "103,11/17/2014,101,WA,373,750.00\n",
      "105,11/19/2014,202,CA,321,200.00\n"
     ]
    }
   ],
   "source": [
    "!head data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    (id, date, store, state, product, amount) = row\n",
    "    return (int(id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "rdd_sales = (\n",
    "    sc.textFile('data/sales.csv')\n",
    "        .map(lambda rowstr : rowstr.split(\",\"))\n",
    "        .filter(lambda row: not row[0].startswith('#'))\n",
    "        .map(casting_function)\n",
    "            )\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- product: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the many data types\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "# create a schema of your own\n",
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )\n",
    "\n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_sales,schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from files (inferring schema)\n",
    "\n",
    "Use [`sqlContext.read.csv`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv) to load a CSV into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[#ID: int, Date: string, Store: int, State: string, Product: int, Amount: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`spark.read.json`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to load a JSON file into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales.json')\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date='11/13/2014', id=101, product=331, state='WA', store=100),\n",
       " Row(amount=450.0, date='11/18/2014', id=104, product=329, state='OR', store=700),\n",
       " Row(amount=200.0, date='11/15/2014', id=102, product=321, state='CA', store=203),\n",
       " Row(amount=330.0, date='11/19/2014', id=106, product=331, state='CA', store=202),\n",
       " Row(amount=750.0, date='11/17/2014', id=103, product=373, state='WA', store=101),\n",
       " Row(amount=200.0, date='11/19/2014', id=105, product=321, state='CA', store=202)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions : turning your DataFrame into a local object\n",
    "\n",
    "Some actions just remain the same, you won't have to learn Spark all over again.\n",
    "\n",
    "Some new actions give you the possibility to describe and show the content in a more fashionable manner.\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | DF vs RDD? | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) | identical | Return a list that contains all of the elements as Rows. |\n",
    "| [`.count()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) | identical | Return the number of elements. |\n",
    "| [`.take(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) | identical | Take the first `n` elements. |\n",
    "| [`.first()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) | identical | Return the first element. |\n",
    "| [`.show(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) | <span style=\"color:green\">new</span> | Show the DataFrame in table format (`n=20` by default) |\n",
    "| [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) | <span style=\"color:green\">new</span> | Convert the DF into a Pandas DF. |\n",
    "| [`.printSchema(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)\\* | <span style=\"color:green\">new</span> | Display the schema. This is not an action, it doesn't launch the DAG, but it fits better in this category. |\n",
    "| [`.describe(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) | <span style=\"color:green\">new</span> | Compute statistics for this column. |\n",
    "| [`.sum(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.mean(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.mean) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.min(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.max(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really handy to go from DAG -> Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>product</th>\n",
       "      <th>state</th>\n",
       "      <th>store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>331</td>\n",
       "      <td>WA</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450.0</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>104</td>\n",
       "      <td>329</td>\n",
       "      <td>OR</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>102</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>330.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>106</td>\n",
       "      <td>331</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750.0</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>103</td>\n",
       "      <td>373</td>\n",
       "      <td>WA</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>105</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount        date   id  product state  store\n",
       "0   300.0  11/13/2014  101      331    WA    100\n",
       "1   450.0  11/18/2014  104      329    OR    700\n",
       "2   200.0  11/15/2014  102      321    CA    203\n",
       "3   330.0  11/19/2014  106      331    CA    202\n",
       "4   750.0  11/17/2014  103      373    WA    101\n",
       "5   200.0  11/19/2014  105      321    CA    202"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas() #heck that's useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how `.collect()` returns things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date='11/13/2014', id=101, product=331, state='WA', store=100),\n",
       " Row(amount=450.0, date='11/18/2014', id=104, product=329, state='OR', store=700),\n",
       " Row(amount=200.0, date='11/15/2014', id=102, product=321, state='CA', store=203),\n",
       " Row(amount=330.0, date='11/19/2014', id=106, product=331, state='CA', store=202),\n",
       " Row(amount=750.0, date='11/17/2014', id=103, product=373, state='WA', store=101),\n",
       " Row(amount=200.0, date='11/19/2014', id=105, product=321, state='CA', store=202)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "|summary|            amount|      date|                id|           product|state|             store|\n",
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "|  count|                 6|         6|                 6|                 6|    6|                 6|\n",
      "|   mean| 371.6666666666667|      null|             103.5| 334.3333333333333| null|251.33333333333334|\n",
      "| stddev|207.40459654179958|      null|1.8708286933869716|19.500427345744672| null|225.39180700874346|\n",
      "|    min|             200.0|11/13/2014|               101|               321|   CA|               100|\n",
      "|    max|             750.0|11/19/2014|               106|               373|   WA|               700|\n",
      "+-------+------------------+----------+------------------+------------------+-----+------------------+\n",
      "\n",
      "--- describe(amount)\n",
      "+-------+------------------+\n",
      "|summary|            amount|\n",
      "+-------+------------------+\n",
      "|  count|                 6|\n",
      "|   mean| 371.6666666666667|\n",
      "| stddev|207.40459654179958|\n",
      "|    min|             200.0|\n",
      "|    max|             750.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(amount)\")\n",
    "df.describe(\"amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "You got that... DataFrames are just RDDs with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting and adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.select(\"Open\", \"Close\")\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl[[\"Open\",\"Close\"]].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|      Open|      High|blabla|\n",
      "+----------+----------+------+\n",
      "|117.949997|118.360001|    34|\n",
      "|117.099998|117.739998|    34|\n",
      "|116.809998|116.910004|    34|\n",
      "|116.860001|117.379997|    34|\n",
      "|    117.25|117.760002|    34|\n",
      "+----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_out = df_aapl.withColumn(\"blabla\", lit(34))\n",
    "\n",
    "df_out[['Open','High','blabla']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+\n",
      "|               Date|      High|       Low|              diff|\n",
      "+-------------------+----------+----------+------------------+\n",
      "|2016-10-25 00:00:00|118.360001|117.309998|1.0500030000000038|\n",
      "|2016-10-24 00:00:00|117.739998|     117.0|0.7399979999999999|\n",
      "|2016-10-21 00:00:00|116.910004|116.279999| 0.630004999999997|\n",
      "|2016-10-20 00:00:00|117.379997|116.330002|1.0499950000000098|\n",
      "|2016-10-19 00:00:00|117.760002|113.800003|3.9599989999999963|\n",
      "+-------------------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = (df_aapl\n",
    "            .withColumn(\"diff\", \n",
    "                         df_aapl['High'] - df_aapl['Low'])\n",
    "            .select('Date', 'High', 'Low', 'diff')\n",
    "         )\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.withColumn(\"label\", func)` : user defined function\n",
    "`udf()` turns a normal python function into something Spark can parallelized across its distributed data. `udf()` requires two arguments: a function, and the data type the function will return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|      High|       Low|      Open|     Close|   special|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|118.360001|117.309998|117.949997|    118.25|0.77785903|\n",
      "|117.739998|     117.0|117.099998|117.650002|   0.42694|\n",
      "|116.910004|116.279999|116.809998|116.599998|0.77722335|\n",
      "|117.379997|116.330002|116.860001|117.059998|0.85966575|\n",
      "|117.760002|113.800003|    117.25|117.120003| 4.5097456|\n",
      "|118.209999|117.449997|    118.18|117.470001| 1.5458359|\n",
      "|117.839996|116.779999|117.330002|117.550003|0.85066664|\n",
      "|118.169998|117.129997|117.879997|117.629997| 1.3353877|\n",
      "|117.440002|115.720001|116.790001|116.980003| 1.4223677|\n",
      "|117.980003|    116.75|117.349998|117.339996| 1.2423673|\n",
      "|118.690002|116.199997|117.699997|116.300003| 10.097407|\n",
      "|    116.75|114.720001|115.019997|116.050003| 0.7247194|\n",
      "|114.559998|113.510002|114.309998|114.059998| 1.3482215|\n",
      "|114.339996|113.129997|113.699997|113.889999| 1.0006177|\n",
      "|113.660004|112.690002|113.400002|113.050003|  1.376497|\n",
      "|114.309998|112.629997|113.059998|     113.0| 1.7838829|\n",
      "|113.050003|112.279999|112.709999|112.519997| 0.9311289|\n",
      "|113.370003|111.800003|112.459999|113.050003|0.87029034|\n",
      "|113.800003|111.800003|113.160004|    112.18| 5.3289337|\n",
      "|114.639999|    113.43|113.690002|113.949997| 0.9329763|\n",
      "+----------+----------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "def my_specialfunc(h,l,o,c):\n",
    "    return ((h-l)*(math.exp(o-c)))\n",
    "\n",
    "my_specialfunc_udf = udf(my_specialfunc, FloatType())\n",
    "\n",
    "df_out = df_aapl.withColumn(\"special\", my_specialfunc_udf(df_aapl['High'], \n",
    "                                                          df_aapl['Low'], \n",
    "                                                          df_aapl['Open'], \n",
    "                                                          df_aapl['Close']))\n",
    "\n",
    "df_out.select('High', 'Low', 'Open', 'Close', 'special').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregating and sorting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------------+\n",
      "|State|sum(Amount)|     avg(Product)|\n",
      "+-----+-----------+-----------------+\n",
      "|   OR|      450.0|            329.0|\n",
      "|   CA|      730.0|324.3333333333333|\n",
      "|   WA|     1050.0|            352.0|\n",
      "+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_out = df.groupBy(\"State\").agg(F.sum(\"Amount\"), F.avg('Product'))\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.orderBy()` : sorting by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   WA|     1050.0|\n",
      "|   CA|      730.0|\n",
      "|   OR|      450.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = (df.groupBy(\"State\")\n",
    "                  .agg(F.sum(\"Amount\"))\n",
    "                  .orderBy(\"sum(Amount)\", ascending=False)\n",
    "         )\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "\n",
    "1. keep only fields for Date and Close \n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-late lecture reminder\n",
    "\n",
    "* Why are we doing this?\n",
    "* When should we use this?\n",
    "* When shuuld we **not** use this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SQL Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know you missed it. Let's run some SQL queries on these tables!\n",
    "\n",
    "First we tell spark to create \"SQL namespace\" and assign a name to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a table **on the cluster**\n",
    "df_aapl.createOrReplaceTempView('aapl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write queries using `spark.sql`, and it will have access to any table we have registered like above. The output of the query is another spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|      Open|     Close|                diff|\n",
      "+----------+----------+--------------------+\n",
      "|117.949997|    118.25|  0.3000030000000038|\n",
      "|117.099998|117.650002|  0.5500040000000013|\n",
      "|116.809998|116.599998|-0.20999999999999375|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT Open, Close, Close - Open as diff FROM aapl LIMIT 3\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "|2016-10-18 00:00:00|    118.18|118.209999|117.449997|117.470001|24553500|117.470001|\n",
      "|2016-10-17 00:00:00|117.330002|117.839996|116.779999|117.550003|23624900|117.550003|\n",
      "|2016-10-14 00:00:00|117.879997|118.169998|117.129997|117.629997|35652200|117.629997|\n",
      "|2016-10-13 00:00:00|116.790001|117.440002|115.720001|116.980003|35192400|116.980003|\n",
      "|2016-10-12 00:00:00|117.349998|117.980003|    116.75|117.339996|37586800|117.339996|\n",
      "|2016-10-11 00:00:00|117.699997|118.690002|116.199997|116.300003|64041000|116.300003|\n",
      "|2016-10-10 00:00:00|115.019997|    116.75|114.720001|116.050003|36236000|116.050003|\n",
      "|2016-10-07 00:00:00|114.309998|114.559998|113.510002|114.059998|24358400|114.059998|\n",
      "|2016-10-06 00:00:00|113.699997|114.339996|113.129997|113.889999|28779300|113.889999|\n",
      "|2016-10-05 00:00:00|113.400002|113.660004|112.690002|113.050003|21453100|113.050003|\n",
      "|2016-10-04 00:00:00|113.059998|114.309998|112.629997|     113.0|29736800|     113.0|\n",
      "|2016-10-03 00:00:00|112.709999|113.050003|112.279999|112.519997|21701800|112.519997|\n",
      "|2016-09-30 00:00:00|112.459999|113.370003|111.800003|113.050003|36379100|113.050003|\n",
      "|2016-09-29 00:00:00|113.160004|113.800003|111.800003|    112.18|35887000|    112.18|\n",
      "|2016-09-28 00:00:00|113.690002|114.639999|    113.43|113.949997|29641100|113.949997|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state| total|\n",
      "+-----+------+\n",
      "|   WA|1050.0|\n",
      "|   CA| 730.0|\n",
      "|   OR| 450.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''SELECT state, SUM(Amount) as total \n",
    "            FROM sales \n",
    "            GROUP BY State \n",
    "            ORDER BY total DESC'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
