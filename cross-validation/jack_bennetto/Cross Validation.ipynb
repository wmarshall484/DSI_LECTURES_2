{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "### Jack Bennetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of the day you should be able to\n",
    "\n",
    " * Describe the three kinds of model error\n",
    " * State the two ways cross validation is used\n",
    " * Explain k-fold cross validation\n",
    " * Explain the training, validation, testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "In this lesson we will talk about\n",
    "\n",
    "* A simple example\n",
    "* Bias and Variance\n",
    "* Train-test split\n",
    "* K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Let's plot some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs=8;npts=6;b0=2;b1=0.5;\n",
    "x=stats.uniform(0,10).rvs(npts,random_state=rs);\n",
    "y=b0+b1*x+stats.norm(0,1).rvs(npts, random_state=rs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'bo', label='some data')\n",
    "ax.set_ylim((0, 9))\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = x[:, None] # convert to column vector, to fit with sklearn\n",
    "xpts = np.linspace(0,10)[:, None] # points for plotting\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(x_col, y)\n",
    "yhat = model1.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'r:', label=\"linear fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know, that looks ok I guess, but it kind of looks quadratic. We can create a pipeline with `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(2)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model2.fit(x_col, y)\n",
    "yhat = model2.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'g:', label=\"quadratic fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but let's try a higher-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(4)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model3.fit(x_col, y)\n",
    "yhat = model3.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'k:', label='quartic fit')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! We did a great job!\n",
    "\n",
    "So how did we actually generate these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = b0 + b1 * xpts\n",
    "ax.plot(xpts, y_actual, 'b:', label='actual function used to generate data')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we went off the rails there.\n",
    "\n",
    "First, what we did is called **overfitting**, when we fit the specific available data in a way that doesn't generalize to other data. This happens pretty often, whenever we have a very complicated model with many independent parameters. Making our model too complicated is bad.\n",
    "\n",
    "The opposite, called **underfitting**, is bad too. Suppose we'd just used the mean of the $y$ values to estimate $\\hat y$ for all the points. That's too simple of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "\n",
    "We'll come back to these again and again and again through the course. The error of a model can be divided into three components.\n",
    "\n",
    "1. **Irreducible error** is the error inherent in any value. Even if we had all possible data and could build a perfect model, we can't predict values exactly because there's error at each data point.\n",
    "2. **Bias** is due to the failure of the model to match our training sample. It's easy to get rid of bias with a complicated model that predicts all the data in our sample exactly.\n",
    "3. **Variance** is the error from the differences of our training sample and the larger population. If we had access to entire population of data, we would have no variance.\n",
    "\n",
    "In general, there is a trade-off between bias and variance. A complex model might have very low bias, but will be highly dependent on the sample taken so will have high variance. A simple model might have higher bias, because it underfits, but lower variance, predicting other data nearly as well as the training sample.\n",
    "\n",
    "Some models have **hyperparameters** that can be tuned. Most represent that trade-off: moving them in one direction will lower the bias and raise the variance; moving them in the other will do the opposite.\n",
    "\n",
    "Okay, so how do we tell which model is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION!\n",
    "\n",
    "The basic concept behind cross validation is that we the data on which we train our model can't accurately access its effectiveness. That's due to overfitting, that no matter how hard we try to generalize the model, its always based more on the data we used than the data we didn't.\n",
    "\n",
    "Cross validation really has two separate purposes.\n",
    "\n",
    "First, it is used for **model comparison**. Over the coming week we'll learn a bunch of different models, and we need to evaluate which will do best for our data. In addition, many of these models have hyperparameters, and we need cross validation to choose the appropriate values. \n",
    "\n",
    "Second, it's used to **evaluate your model**. Part of the CRISP-DM is evaluation; you (usually) need to know how well your model will predict real-world results. There are many ways to measure that, like AUC/ROC or F-score some combination of precision and recall or sensitivity and specificity, based on your specific business case, but in the end the key thing is that you can't measure it on your training data.*\n",
    "\n",
    "\n",
    "Note: You can measure on your training data in some circumstances, either because your statistical measure allows some estimation of the error, or you have an ensemble model where different submodels see different data (out-of-bag error). But those aren't as general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The train-test split\n",
    "\n",
    "The simplest approach we can use is the train-test split. You probably shouldn't call this cross validation, just say \"train-test split\" or \"hold-out validation.\"\n",
    "\n",
    "Let's start with the mtcars dataset that you may have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rows without data for horsepower, which is why it shows up as an object. We're just going to throw those away for now without worrying too much if that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower.astype('float128')\n",
    "cars.mpg = cars.mpg.astype('float128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(cars.mpg, cars.horsepower, '.')\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[['mpg']]\n",
    "y = cars.horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we did a bit better on the training data, as expected...or did we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently it's pretty sensitive to the random split. Let's explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for _ in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(model.score(X_train, y_train))\n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "                   \n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(train_score, test_score, '.', alpha=0.2)\n",
    "ax.plot([0, 1], [0, 1], ':')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('train $R^2$')\n",
    "ax.set_ylabel('test $R^2$')\n",
    "ax.set_xlim((.4,.8))\n",
    "ax.set_ylim((.4,.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we usually do better with the training set. Usually.\n",
    "\n",
    "Let's see if we can reproduce that train-test-split graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(model, X, y):\n",
    "    return np.mean((model.predict(X) - y) **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for degree in range(1, 11):\n",
    "    model = Pipeline([\n",
    "        ('pf', PolynomialFeatures(degree)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "    test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "    #train_score.append(-model.score(X_train, y_train))\n",
    "    #test_score.append(-model.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1, 11), train_score, '.-', label=\"train set\")\n",
    "ax.plot(range(1, 11), test_score, '.-', label=\"test set\")\n",
    "ax.set_xlabel('complexity')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for degree in range(1, 11):\n",
    "        model = Pipeline([\n",
    "            ('pf', PolynomialFeatures(degree)),\n",
    "            ('lr', LinearRegression())\n",
    "            ])\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "        test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'b.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'y.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'b.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'y.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('degree of polynomial')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")\n",
    "ax.plot(X_train, y_train, 'b.')\n",
    "ax.plot(X_test, y_test, 'y.')\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('pf', PolynomialFeatures(11)),\n",
    "    ('lr', LinearRegression())\n",
    "    ])\n",
    "model.fit(X_train, y_train)\n",
    "xpts = np.linspace(9, 47, 100).reshape(-1, 1)\n",
    "ax.plot(xpts, model.predict(xpts), 'b-')\n",
    "ax.set_ylim(40, 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this isn't all that consistent. We need something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "\n",
    "With Cross Validation, we randomly partition the data into $k$ groups, $D_1$, $D_2$, ..., $D_k$. For each $i \\in [1..k]$ we:\n",
    "\n",
    " * Build a model using $D_{j \\ne i}$ as a training data\n",
    " * Calculate the error of the model on $D_i$\n",
    " \n",
    "We average all these errors to compute the overall error of the model. We can either compare those across different models to choose the best model or use that number to report the actual error of our model.\n",
    "\n",
    "There isn't a clear \"best\" value for $k$. The extreme version of k-fold cross validation, when $k=n$, is called leave-one-out cross validation. That's generally not so good, and $k=2$ is not so good, but experience has shown that other choices are fine. I like $k=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One confusing thing about the `KFold` object in `sklearn` is that it returns indices, not the data themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)  # almost always use shuffle=True\n",
    "scores = []\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X.values[train], y.values[train])\n",
    "    scores.append(model.score(X.values[test], y.values[test]))\n",
    "    \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many `sklearn` models include \"CV\" versions that use cross validation to calculate hyperparameters automatically.\n",
    "\n",
    "**Stratified cross validation** is a variation in which the partitions are chosen to have similar values for features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the testing data\n",
    "\n",
    "There's a problem with all this. Because the model and hyperparameters are chosen based on the training and testing data, the errors of the model aren't an accurate representation of how it would behave on outside data. If we want to know how it will behave in general, we need to hold out additional data. In this case we have\n",
    "\n",
    " * **Training data** are used to fit the model.\n",
    " * **Validation data** are used to choose the model and hyperparameters. Once these are determined, these are combined with the training data to re-fit the model.\n",
    " * **Testing data** are used to evaluate the final accuracy of the model.\n",
    " \n",
    "For each of these we can use either simple hold-out validation or k-fold cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
