{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "### Jack Bennetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of the day you should be able to\n",
    "\n",
    " * Describe the three kinds of model error\n",
    " * State the two ways cross validation is used\n",
    " * Explain k-fold cross validation\n",
    " * Explain the training, validation, testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "In this lesson we will talk about\n",
    "\n",
    "* A simple example\n",
    "* Bias and Variance\n",
    "* Train-test split\n",
    "* K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Let's plot some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs=8;npts=6;b0=2;b1=0.5;\n",
    "x=stats.uniform(0,10).rvs(npts,random_state=rs);\n",
    "y=b0+b1*x+stats.norm(0,1).rvs(npts, random_state=rs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'bo', label='some data')\n",
    "ax.set_ylim((0, 9))\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = x[:, None] # convert to column vector, to fit with sklearn\n",
    "xpts = np.linspace(0,10)[:, None] # points for plotting\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(x_col, y)\n",
    "yhat = model1.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'r:', label=\"linear fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know, that looks okay I guess, but it kind of looks quadratic. We can create a pipeline with `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(2)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model2.fit(x_col, y)\n",
    "yhat = model2.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'g:', label=\"quadratic fit\")\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but let's try a higher-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Pipeline([\n",
    "        ('pf', PolynomialFeatures(4)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "model3.fit(x_col, y)\n",
    "yhat = model3.predict(xpts)\n",
    "ax.plot(xpts, yhat, 'k:', label='quartic fit')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! We did a great job!\n",
    "\n",
    "So how did we actually generate these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = b0 + b1 * xpts\n",
    "ax.plot(xpts, y_actual, 'b:', label='actual function used to generate data')\n",
    "ax.legend(loc='upper left')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we went off the rails there.\n",
    "\n",
    "First, what we did is called **overfitting**, when we fit the specific available data in a way that doesn't generalize to other data. This happens pretty often, whenever we have a very complicated model with many independent parameters. Making our model too complicated is bad.\n",
    "\n",
    "The opposite, called **underfitting**, is bad too. Suppose we'd just used the mean of the $y$ values to estimate $\\hat y$ for all the points. That's too simple of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "\n",
    "We'll come back to these again and again and again through the course. The mean-squared error of a model can be decomposed into three components.\n",
    "\n",
    "Note that we don't every actually know the values of these components, but they are useful in understanding how we make the best possible prediction.\n",
    "\n",
    "1. The **irreducible error** is the error inherent in any value. Even if we had all possible data (the \"population\") and could build the best possible model, we can't predict values exactly.\n",
    "2. The **bias** is the difference between the expected value of $y$ (averaging out the irreducible error) and the expected prediction of the model (that is, we had a large number of training dataset and we built a model using the same procedure on each one, and then averaged the predictions). If we predict the training set well we will generally have low bias.\n",
    "3. The **variance** of the model over all possible training sets, which is a measure of how far the training-set model is from what it would be if trained on all the data. If we had access to entire population of data, we would have no variance.\n",
    "\n",
    "Again, if we can't know these values, why do we care?\n",
    "\n",
    "In general, there is a trade-off between bias and variance. A complex model might have very low bias, but will be highly dependent on the sample taken so will have high variance. A simple model might have higher bias, because it underfits, but lower variance, predicting other data nearly as well as the training sample.\n",
    "\n",
    "Some models have **hyperparameters** that can be tuned. Most represent that trade-off: moving them in one direction will lower the bias and raise the variance; moving them in the other will do the opposite.\n",
    "\n",
    "Every time you learn about a hyperparameter you should ask yourself if it represents a bias-variance trade-off, and which direction increases bias and decreases variance, and which direction does the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, with math\n",
    "$\\DeclareMathOperator{\\E}{\\mathbb{E}}$\n",
    "$\\DeclareMathOperator{\\Bias}{Bias}$\n",
    "$\\DeclareMathOperator{\\Var}{Var}$\n",
    "\n",
    "Suppose there is some underlying true function $f(x)$, such that $y = f(x) + \\epsilon$, where the noise $\\epsilon$ has a mean of zero and standard deviation of $\\sigma$. We've created some function $\\hat f(x)$ to estimate $y$, and want $|y - \\hat f(x)|$ to be as small as possible. We can decompose this into three parts: the  square of the **bias**, the **variance**, and the **irreducible error**.\n",
    "\n",
    "$$\\E\\left[(y - \\hat f(x))^2\\right] = \\left(\\Bias[\\hat f(x)]\\right)^2 + \\Var[\\hat f(x)] + \\sigma^2$$\n",
    "\n",
    "where\n",
    "$$\\Bias[\\hat f(x)] = \\E[\\hat f(x)] - f(x)$$\n",
    "and\n",
    "$$\\text{Var}[\\hat f(x)] = \\E[\\hat f(x)^2] - \\E[\\hat f(x)]^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to go through the derivation; you can find in elsewhere (e.g. [wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation)). Instead I'll cover we'll look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance example\n",
    "\n",
    "Let's see what that looks like with some fake data. Our true function $f$ will be a cosine curve; we'll add some noise to give values for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def y(x):\n",
    "    return f(x) + stats.norm(0, sigma).rvs(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (0, 10)\n",
    "n_pts = 100\n",
    "xpts = np.linspace(*xlim, n_pts)\n",
    "ypts = f(xpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(xpts, ypts, 'b-', label='$f(x)$')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll choose a training sample and fit a model to those data. We'll use a kNN regressor, since that's a simple model that we've already learned. We'll start with `k=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "sample = stats.uniform(xlim[0], xlim[1]-xlim[0]).rvs(sample_size)\n",
    "y_sample = y(sample)\n",
    "model = KNeighborsRegressor(3)\n",
    "model.fit(sample[:, None], y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ax.plot(sample, y_sample, 'ro', label='$y$')\n",
    "ax.plot(xpts, model.predict(xpts[:, None]), 'r-', lw=0.5, label='$\\hat{f}(x)$')\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0].remove()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to make sense of bias and variance, we need to consider a large number of training samples, and build a model for each. In reality we only have our one training sample, but we're doing this to try to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(18,10), sharey='row', sharex='all')\n",
    "\n",
    "n_trials = 100\n",
    "sample_size = 50\n",
    "for i, k in enumerate((1, 3, 11, 25)):\n",
    "    predictions = np.zeros((n_trials, n_pts))\n",
    "    for t in range(n_trials):\n",
    "        sample = stats.uniform(xlim[0], xlim[1]-xlim[0]).rvs(sample_size)\n",
    "        y_sample = y(sample)\n",
    "        model = KNeighborsRegressor(k)\n",
    "        model.fit(sample[:, None], y_sample)\n",
    "        predictions[t, :] = model.predict(xpts[:, None])\n",
    "        ax[0, i].plot(xpts, predictions[t,:], 'r-', lw=0.1)\n",
    "    ax[0, i].plot(xpts, ypts, 'b-', label='$f(x)$')\n",
    "    ax[0, i].plot(xpts, predictions.mean(axis=0), color='#AA0000', lw=3, label=\"$E[\\hat f(x)]$\")\n",
    "\n",
    "    ax[0, i].set_ylim((-2, 2))\n",
    "    ax[0, i].set_title(f\"k = {k}\")\n",
    "    ax[0, i].legend()\n",
    "    #ax[1].plot(xpts, predictions.mean(axis=0) - ypts, 'k', label='bias')\n",
    "    bias = predictions.mean(axis=0) - ypts\n",
    "    variance = predictions.var(axis=0)\n",
    "    ax[1, i].plot(xpts, bias**2, 'k', label='bias^2')\n",
    "    ax[1, i].plot(xpts, variance, 'r', label='variance')\n",
    "    ax[1, i].plot(xpts, bias**2 + variance, 'g', label='total')\n",
    "    ax[1, i].plot(xpts, np.zeros_like(xpts), 'k', lw=0.5)\n",
    "\n",
    "    ax[1, i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so how do we tell which model is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION!\n",
    "\n",
    "The basic concept behind cross validation is that we the data on which we train our model can't accurately access its effectiveness. That's due to overfitting, that no matter how hard we try to generalize the model, its always based more on the data we used than the data we didn't.\n",
    "\n",
    "Cross validation really has two separate purposes.\n",
    "\n",
    "First, it is used for **model comparison**. Over the coming week we'll learn a bunch of different models, and we need to evaluate which will do best for our data. In addition, many of these models have hyperparameters, and we need cross validation to choose the appropriate values. \n",
    "\n",
    "Second, it's used to **evaluate your model**. Part of the CRISP-DM is evaluation; you (usually) need to know how well your model will predict real-world results. There are many ways to measure that, like mean-square-error or mean-absolute-error for regression models, or log loss or brier score or AUC/ROC or F-score or some other combination of precision and recall or sensitivity and specificity, based on your specific business case, but in the end the key thing is that **you can't measure it on your training data**.\n",
    "\n",
    "Note: Actually you can measure on your training data in some circumstances, either because your statistical measure allows some estimation of the error, or you have an ensemble model where different submodels see different data (out-of-bag error). But those aren't as general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The train-test split\n",
    "\n",
    "The simplest approach we can use is the train-test split. You shouldn't call this cross validation, just say \"train-test split\" or \"hold-out validation.\"\n",
    "\n",
    "Let's start with the mtcars dataset that you may have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rows without data for horsepower, which is why it shows up as an object. We're just going to throw those away for now without worrying too much if that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower.astype('float128')\n",
    "cars.mpg = cars.mpg.astype('float128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(cars.mpg, cars.horsepower, '.')\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[['mpg']]\n",
    "y = cars.horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we did a bit better on the training data, as expected...or did we? Let's try a different random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently it's pretty sensitive to the random split. Let's explore more, considering many different splits and plotting the test score against the training score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for _ in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(model.score(X_train, y_train))\n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "                   \n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(train_score, test_score, '.', alpha=0.2)\n",
    "ax.plot([0, 1], [0, 1], ':')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('train $R^2$')\n",
    "ax.set_ylabel('test $R^2$')\n",
    "ax.set_xlim((.4,.8))\n",
    "ax.set_ylim((.4,.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we usually do better with the training set. Usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting with a higher-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"mpg\")\n",
    "ax.set_ylabel(\"horsepower\")\n",
    "ax.plot(X_train, y_train, 'r.')\n",
    "ax.plot(X_test, y_test, 'b.')\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('pf', PolynomialFeatures(11)),\n",
    "    ('lr', LinearRegression())\n",
    "    ])\n",
    "model.fit(X_train, y_train)\n",
    "xpts = np.linspace(9, 47, 100).reshape(-1, 1)\n",
    "ax.plot(xpts, model.predict(xpts), 'k-')\n",
    "ax.set_ylim(40, 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a graph you've probably seen showing the error in the training and test sets for different complexities. Let's see if we can reproduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(model, X, y):\n",
    "    return np.mean((model.predict(X) - y) **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for degree in range(1, 11):\n",
    "    model = Pipeline([\n",
    "        ('pf', PolynomialFeatures(degree)),\n",
    "        ('lr', LinearRegression())\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "    test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "    #train_score.append(-model.score(X_train, y_train))\n",
    "    #test_score.append(-model.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1, 11), train_score, '.-r', label=\"train set\")\n",
    "ax.plot(range(1, 11), test_score, '.-b', label=\"test set\")\n",
    "ax.set_xlabel('complexity (degree of polynomial)')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! So we do better with the training set than the test set for any complexity. We have a minimum value of around 3-5. Let's try some many more splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for degree in range(1, 11):\n",
    "        model = Pipeline([\n",
    "            ('pf', PolynomialFeatures(degree)),\n",
    "            ('lr', LinearRegression())\n",
    "            ])\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "        test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('complexity (degree of polynomial')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.set_ylim(200, 1000)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this isn't all that consistent. We need something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "\n",
    "With Cross Validation, we randomly partition the data into $k$ groups, $D_1$, $D_2$, ..., $D_k$. For each $i \\in [1..k]$ we:\n",
    "\n",
    " * Build a model using $D_{j \\ne i}$ as a training data\n",
    " * Calculate the error of the model on $D_i$\n",
    " \n",
    "We average all these errors to compute the overall error of the model. We can either compare those across different models to choose the best model or use that number to report the actual error of our model.\n",
    "\n",
    "There isn't a clear \"best\" value for $k$. The extreme version of k-fold cross validation, when $k=n$, is called leave-one-out cross validation. That's generally not so good, and $k=2$ is not so good, but experience has shown that other choices are fine. I like $k=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So suppose we have fifteen data points (labeled a...o) and we're doing 5-fold cross validation. We'll put the 1st three in the first fold, the next three in the next fold, and so on.\n",
    "\n",
    "We would build 5 different models (one for each split column, below) using the \"train\" data points, test each on the \"TEST\" data points, and average the results.\n",
    "\n",
    "data point | fold | 1st split  | 2nd split | 3rd split | 4th split | 5th split\n",
    "---|---|---|---|---|---|---\n",
    " a | 1 | **test** | train | train | train | train | \n",
    " b | 1 | **test** | train | train | train | train | \n",
    " c | 1 | **test** | train | train | train | train | \n",
    " d | 2 | train | **test** | train | train | train | \n",
    " e | 2 | train | **test** | train | train | train | \n",
    " f | 2 | train | **test** | train | train | train | \n",
    " g | 3 | train | train | **test** | train | train | \n",
    " h | 3 | train | train | **test** | train | train | \n",
    " i | 3 | train | train | **test** | train | train | \n",
    " j | 4 | train | train | train | **test** | train | \n",
    " k | 4 | train | train | train | **test** | train | \n",
    " l | 4 | train | train | train | **test** | train | \n",
    " m | 5 | train | train | train | train | **test** | \n",
    " n | 5 | train | train | train | train | **test** | \n",
    " o | 5 | train | train | train | train | **test** | \n",
    "\n",
    "**Question:** Generally we shuffle the data points first. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write code to do this, but `sklearn` has already done that. One confusing thing about the `KFold` object in `sklearn` is that it returns indices, not the data themselves. Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)  # almost always use shuffle=True\n",
    "fold_scores = []\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X.values[train], y.values[train])\n",
    "    fold_scores.append(model.score(X.values[test], y.values[test]))\n",
    "    \n",
    "print(np.mean(fold_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this give more consistent results than the train-test split? Let's do the same error-vs-complexity graph using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for degree in range(1, 11):\n",
    "        train_fold_scores = []\n",
    "        test_fold_scores = []\n",
    "\n",
    "        for train, test in kf.split(X):\n",
    "            model = Pipeline([\n",
    "                ('pf', PolynomialFeatures(degree)),\n",
    "                ('lr', LinearRegression())\n",
    "                ])\n",
    "            model.fit(X.values[train], y.values[train])\n",
    "            train_fold_scores.append(mean_squared_error(model, X.values[train], y.values[train]))\n",
    "            test_fold_scores.append(mean_squared_error(model, X.values[test], y.values[test]))\n",
    "\n",
    "        train_score.append(np.mean(train_fold_scores))\n",
    "        test_score.append(np.mean(test_fold_scores))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('complexity (degree of polynomial')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.set_ylim(200, 1000)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple other notes.\n",
    "\n",
    "Many `sklearn` models include \"CV\" versions that use cross validation to calculate hyperparameters automatically.\n",
    "\n",
    "**Stratified cross validation** is a variation in which the partitions are chosen to have similar distributions of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the testing data\n",
    "\n",
    "There's a problem with all this. Because the model and hyperparameters are chosen based on the training and testing data, the errors of the model aren't an accurate representation of how it would behave on outside data. If we want to know how it will behave in general, we need to hold out additional data. In this case we have\n",
    "\n",
    " * **Training data** are used to fit the model.\n",
    " * **Validation data** are used to choose the model and hyperparameters. Once these are determined, these are combined with the training data to re-fit the model.\n",
    " * **Testing data** are used to evaluate the final accuracy of the model.\n",
    " \n",
    "For each of these we can use either simple hold-out validation or k-fold cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
