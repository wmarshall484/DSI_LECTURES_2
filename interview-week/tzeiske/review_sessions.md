We will do 5 sessions according to the sections below.

* Sections 1 and 2 will be covered in two blocks of 1.5 hours on Monday.
* Sections 3 and 4 will be covered in two blocks of 1.5 hours on Tuesday.
* Section 5 and any leftover subjects will be covered on Thursday before graduation.

Every subject should be covered by one student for about 5 minutes with discussion afterwards.

Subjects will be chosen by students on a first come first serve basis, but students are strongly urged to choose stubjects they feel weaker in.

**Probability/Statistics/(A/B)Testing/Bayesian Analysis:**

1. **(Dan 2)** Compare and contrast three common continuous distributions, and three common discrete distributions.
1. **(Madhan 2)** Explain the difference between sample statistics and population parameters, and give a few examples.
1. **(Tyler)** Explain the difference between a joint distribution, and conditional distribution and a marginal distribution, and their respective roles in Bayes' theorem.
1. **(Madhan)** Explain the steps to perform a hypothesis test.
1. **(Chad 2)** Explain the steps to perform a power calculation.
1. **(Matt)** Compare and contrast CLT and bootstrapping  confidence intervals.
1. **(Chad)** Expound upon the Bayesian philosophy relative to the frequentist approach based on p-values.
1. **(Dan)** Show what the multi-armed bandit is, how it's implemented, and why it works.



**Parametric/Non-parametric/Ensemble/Overfitting:**

1. **(Chad)** Give examples of two parametric models and two non-parametric models, and compare and contrast the strengths of each.
1. **(Dan)** Compare and contrast ensemble methods.
1. **(Tyler 2)** Compare and contrast linear and logistic regression models.
1. **(Matt)** Explain stepwise selection in a linear regression model context.
1. **(Madhan)** Describe the issue of multicollinearity with particular attention to linear regression model contexts.
1. **(Tyler)** Contrast model bias, model variance and intrinsic variance; and discuss their relationship to underfitting and overfitting.
1. **(Matt 2)** Discuss three ways to combat high model variance.
1. **(Dan 2)** Discuss the issues and context of imbalanced classes classification.


**SVMs/NNs/Profit Curves:**

1. **(Chad)** What are support vectors and what is the margin in SVM contexts?
1. **(Dan)** What is the kernel trick?
1. **(Matt 2)** In a NN, how do we initialize the weights and how do we train the weights?
1. **(Matt)** Discuss activation and softmax functions in the NN context.
1. **(Tyler)** Describe the mechanics and role of convolutional kernel and max pooling layers in CNN contexts.
1. **(Dan 2)** Distinguish a confusion matrix from a cost-benefit matrix.
1. **(Madhan)** Compare a profit curve to an ROC curve.
1. **(Tyler 2)** Discuss how multi-cass (>2 class) classification is done.


**NLP/Clustering/Web:**

1. **(Chad 2)** What is text normalization?
1. **(Chad)** Compare bag-of-words, binary bag-of-words and TF-IDF.
1. **(Dan)** Why is Cosine Similarity (usually) a better choice of a distance metric for NLP instead of Euclidean Distance?
1. **(Matt)** Comare and contrast k-means with hierarchical clustering.
1. **(Madhan 2)** Explain two ways to choose k.
1. **(Madhan)** Explain how Naive Bayes predicts classifications in NLP contexts.
1. **(Tyler)** Tell us the most useful things to know about in order to do web scrapping.
1. **(Dan 2)** Show off your Rest API, describing how it works.


**Model Fitting/Recommenders/easter eggs:**

1. **(Madhan)** Compare Maximum Likelihood and Method of Moments.
1. **(Dan)** Explain loss and cost functions and model fitting.
1. **(Matt)** Compare and contrast gradient decent and gradient boosting.
1. **(Tyler 2)** Explain why gradient boosting can outperform random forests.
1. **(Madhan 2)** Explain how PCA can be used for dimensionality reduction
1. **(Chad)** Compare PCA/SVD, NMF, and UV decomposition.
1. **(Chad 2)** Discuss how to evaluate recommender system performance.
1. **(Tyler)** Tell us what a RNN is!
~
