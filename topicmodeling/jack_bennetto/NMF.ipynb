{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling/NMF\n",
    "### Jack Bennetto\n",
    "#### February 14, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    " * Write down and explain NMF equation\n",
    " * Compare and contrast NMF, SVD, PCA and K-means\n",
    " * Implement Alternating-Least-Squares algorithm\n",
    " * Use NMF to find and interpret latent topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    " * Discussion of topic modeling\n",
    " * Problems with SVD for topic analysis\n",
    " * Introduce NMF\n",
    " * Review solving linear equations\n",
    " * Alternating Least Squares\n",
    " * NMF for topic analysis example\n",
    " * Pair exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "### Motivation and connections\n",
    "\n",
    "For most of this program we've talked about $X$ being a matrix of samples and features which are very different things. With SVD yesterday, NMF today, and recommender systems over the next few days, the rows and columns look a lot more like each other. When we figure out how they fit together, we don't just care about clustering the rows, but clustering the columns as well.  These 'topics' (a.k.a. 'concepts', 'latent features' or 'archetype') apply to both axis.\n",
    "\n",
    "Examples:\n",
    " * Recommender systems mapping items to users\n",
    " * NLP mapping words to documents\n",
    " * Image processing mapping pixels to pictures\n",
    "\n",
    "With SVD the relation to a topic is purely numeric, but with NMF the mapping we require all entries are non-negative, so we can see the rows (or columns!) as being part of a topic or not (if zero). That said, it's still a 'soft clustering', as rows (or columns) can be part of more than one topic.\n",
    "\n",
    "Requiring entries to be non-negative provides more interpretable mappings and leads to a parts-based representation of the whole (https://github.com/zipfian/DSI_Lectures/blob/master/topicmodeling/isaac_laughlin/nmf_nature.pdf).\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = ['Matrix','Alien','StarWars','Casablanca','Titanic']\n",
    "users = ['Alice','Bob','Cindy','Dan','Emily','Frank','Greg']\n",
    "M = pd.DataFrame([[1, 2, 2, 0, 0],\n",
    "                  [3, 5, 5, 0, 0],\n",
    "                  [4, 4, 4, 0, 0],\n",
    "                  [5, 5, 5, 0, 0],\n",
    "                  [0, 2, 0, 4, 4],\n",
    "                  [0, 0, 0, 5, 5],\n",
    "                  [0, 1, 0, 2, 2]],\n",
    "                 index=users, columns=movies)\n",
    "display(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should able to group together these movies and find topics with math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "k = 2\n",
    "\n",
    "# Compute SVD\n",
    "U, sigma, VT = svd(M)\n",
    "\n",
    "U = pd.DataFrame(U, index=users)\n",
    "VT = pd.DataFrame(VT, columns=movies)\n",
    "\n",
    "# Keep top two concepts\n",
    "U = U.iloc[:,:k]\n",
    "sigma = sigma[:k]\n",
    "VT = VT.iloc[:k,:]\n",
    "\n",
    "# Make pretty\n",
    "U, sigma, VT = (np.around(x,2) for x in (U,sigma,VT))\n",
    "\n",
    "display(U)\n",
    "display(pd.DataFrame(np.diag(sigma)))\n",
    "display(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. What do the concepts mean?\n",
    "2. To which concept(s) does each user/document belong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with SVD for topic analysis\n",
    "\n",
    "**Recall:** $M = U S V^T$\n",
    "\n",
    "1. Values in $U$ and $V^T$ can be negative, which is weird and hard to interpret. For example, suppose a latent feature is the genre 'Science fiction'. This feature can be positive (makes sense), zero (makes sense), or negative (what does that mean?).\n",
    "\n",
    "2. The number of columns in $U$ can differ from the number of rows in $V^T$. I.e. The number of latent features differs in $U$ and $V^T$, which is weird.\n",
    "\n",
    "3. SVD forces us to fill in missing values, then SVD models those missing values, which is bad.\n",
    "\n",
    "How can we avoid these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a matrix $V \\in \\mathbb{R}^{m \\times n}$. With NMF, we try to write it as the product of two smaller matrices, $W \\in \\mathbb{R}^{m \\times r}$ and $H \\in \\mathbb{R}^{r \\times n}$, \n",
    "\n",
    "$$ V = W H$$\n",
    "\n",
    "or, to be more graphical,\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    v_{11}       & v_{12} & v_{13} & \\dots & v_{1n} \\\\\n",
    "    v_{21}       & v_{22} & v_{23} & \\dots & v_{2n} \\\\\n",
    "    v_{31}       & v_{32} & v_{33} & \\dots & v_{3n} \\\\\n",
    "    v_{41}       & v_{42} & v_{43} & \\dots & v_{4n} \\\\\n",
    "    \\vdots       & \\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "    v_{m1}       & v_{m2} & v_{m3} & \\dots & v_{mn}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    w_{11}       & \\dots & w_{1r} \\\\\n",
    "    w_{21}       & \\dots & w_{2r} \\\\\n",
    "    w_{31}       & \\dots & w_{3r} \\\\\n",
    "    w_{41}       & \\dots & w_{4r} \\\\\n",
    "    \\vdots       & \\ddots& \\vdots \\\\\n",
    "    w_{m1}       & \\dots & w_{mr}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    h_{11}       & h_{12} & h_{13} & \\dots & h_{1n} \\\\\n",
    "    \\vdots       & \\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "    h_{r1}       & h_{r2} & h_{r3} & \\dots & h_{rn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with the constraint that all $w_{ik} \\ge 0$ and $h_{kj} \\ge 0$.\n",
    "\n",
    "In general, this isn't possible, but we'll try to do the best we can, minimizing\n",
    "\n",
    "$$||V - WH||^2 = \\sum_{ij} (V_{ij} - \\sum_k W_{ik} H_{kj})^2$$\n",
    "\n",
    "again with the constraint that the components are greater than zero.\n",
    "\n",
    "Note that the number of topics $r$ is a hyperparameter; we can choose how many topics we want.\n",
    "\n",
    "**Question** What would the value of $r$ have to do with the bias-variance tradeoff?\n",
    "\n",
    "**Question** What would we lose compared to SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving NMF\n",
    "\n",
    "There are a couple approaches to solving NMF.\n",
    "\n",
    "**Alternating Least Squares** involves solving first for one matrix while holding the other constant, that the other, back and forth until in converges.\n",
    "\n",
    "**Stocastic Gradient Descent** minimizes the components of the matrices using the previously discussed algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares\n",
    "\n",
    "First, some review.\n",
    "\n",
    "### Exact solution for a system of linear equations\n",
    "$$ Ax = b$$\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 2 \\\\ -3 & 4 \\end{bmatrix} \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right] = \\left[ \\begin{array}{cc} 7 \\\\ -9 \\end{array} \\right] $$\n",
    "\n",
    "There are two unknowns ($x_1$ and $x_2$) and two equations ($x_1 + 2x_2 = 7$ and $-3x_1 + 4x_2 = -9$) so (usually) there is one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [-3, 4]])\n",
    "b = np.array([7, -9])\n",
    "\n",
    "print np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares solver\n",
    "\n",
    "What if we have an overdetermined system of linear equations? E.g.\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 2 \\\\ -3 & 4 \\\\ 1 & -4 \\end{bmatrix} \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right] = \\left[ \\begin{array}{cc} 7 \\\\ -9 \\\\ 17 \\end{array} \\right] $$\n",
    "\n",
    "An exact solution is not guaranteed, so we must do something else. Least Squares dictates that we find the $x$ that minimizes the residual sum of squares (RSS).\n",
    "\n",
    "(Note: This is the solver we use when doing Linear Regression!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [-3, 4], [1, -4]])\n",
    "b = np.array([7, -9, 17])\n",
    "\n",
    "print np.linalg.lstsq(A, b)[0]\n",
    "print \"Residual sum of squares (error): {}\".format(np.linalg.lstsq(A, b)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative least-squares solver\n",
    "\n",
    "What if you want to constrain the solution to be non-negative?\n",
    "\n",
    "We have optimizers for that too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import nnls\n",
    "\n",
    "A = np.array([[1, 2], [-3, 4], [1, -4]])\n",
    "b = np.array([7, -9, 17])\n",
    "\n",
    "print nnls(A, b)[0]\n",
    "print \"Residual sum of squares (error): {}\".format(nnls(A, b)[1] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Alternating Least Squares\n",
    "\n",
    "**Question** Given a matrices $A$ and $B$, least squares and non-negative least squares find the solution $X$ that minimizes the error (RSS) in $A \\cdot X = B$. So can you guess what alternating least squares is, and how may we apply it to NMF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement the first two steps of alternating least squares\n",
    "\n",
    "# Set up our matrix V we want to decompose\n",
    "V = np.random.rand(10,15)\n",
    "\n",
    "# Initialize a random matrix W\n",
    "W = np.random.rand(10,5)\n",
    "\n",
    "# Solve for H using a least squares solver\n",
    "H = np.linalg.lstsq(W, V)[0]\n",
    "\n",
    "# Clip H so there are no negative values\n",
    "H[H < 0] = 0\n",
    "\n",
    "# Print the current error. Why did the error go up?\n",
    "print np.linalg.norm(V - np.dot(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve for W using H\n",
    "W = np.linalg.lstsq(H, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang that blew up... Let's do some math to figure out why and what we could have done\n",
    "\n",
    "**Question** np.linalg.lstsq(W, V) solves $W \\cdot H = V$. To solve for W we need to solve $H \\cdot W = V$ which is invalid due to the dimensions of H and W. What can we do to our matrices to make this fix this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Using the answer provided, go ahead and solve for W and print out the new error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With ALS, we continue this, H in terms of W and W in terms of H until\n",
    "we are \"satisfied\" with your result (low enough error) or reach some maximum number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General vs. non-negative least squares solver\n",
    "\n",
    "Non-negative least squares solver:\n",
    "    \n",
    " * Returns result with least squares error given non-negativity constraint\n",
    " * While alternating, converges to a local minimum\n",
    " * Orders of magnitude slower than general least squares solver\n",
    "\n",
    "General least squares solver:\n",
    "    \n",
    " * Returns result with least squares error with no constraints\n",
    " * While alternating, converges to a stationary point (saddle point or minimum)\n",
    " * Much much faster\n",
    " * Have to clip the matrix at every iteration to ensure non-negativity\n",
    "   \n",
    "In industry the general least squares solver is commonly used. The tradeoff between speed and strong convergence seems to be worth it. For more information check out: http://users.wfu.edu/plemmons/papers/BBLPP-rev.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF for topic analysis\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we'll try to find the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def fit_nmf(r):\n",
    "    nmf = NMF(n_components=r)\n",
    "    nmf.fit(M)\n",
    "    W = nmf.transform(M)\n",
    "    H = nmf.components_\n",
    "    return nmf.reconstruction_err_\n",
    "\n",
    "error = [fit_nmf(i) for i in range(1,6)]\n",
    "plt.plot(range(1,6), error)\n",
    "plt.xticks(range(1, 6))\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('Reconstruction Errror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What might be the optimal r (number of topics) value and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit using 2 hidden concepts\n",
    "nmf = NMF(n_components=2)\n",
    "nmf.fit(M)\n",
    "W = nmf.transform(M)\n",
    "H = nmf.components_\n",
    "print 'RSS = %.2f' % nmf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make interpretable\n",
    "W, H = (np.around(x,2) for x in (W,H))\n",
    "W = pd.DataFrame(W,index=users)\n",
    "H = pd.DataFrame(H,columns=movies)\n",
    "\n",
    "display(W) \n",
    "display(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Discussion**\n",
    "1. What do the concepts (clusters) mean?\n",
    "2. To which concept(s) does each user/document belong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify reconstruction\n",
    "display(np.around(W.dot(H),2))\n",
    "display(pd.DataFrame(M, index=users, columns=movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is concept 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 2 movies in genre 0\n",
    "top_movies = H.iloc[0].sort_values(ascending=False).index[:3]\n",
    "top_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which users align with concept 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 2 users for genre 1\n",
    "top_users = W.iloc[:,0].sort_values(ascending=False).index[:2]\n",
    "top_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What concepts does Emily align with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W.loc['Emily']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are all the movies in each concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of movies in each concept\n",
    "thresh = .2  # movie is included if at least 20% of max weight\n",
    "for g in range(2):\n",
    "    all_movies = H.iloc[g,:]\n",
    "    included = H.columns[all_movies >= (thresh * all_movies.max())]\n",
    "    print \"Concept %i contains: %s\" % (g, ', '.join(included))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which users are associated with each concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Users in each concept\n",
    "thresh = .2  # user is included if at least 20% of max weight\n",
    "for g in range(2):\n",
    "    all_users = W.iloc[:,g]\n",
    "    included = W.index[all_users >= (thresh * all_users.max())]\n",
    "    print \"Concept %i contains: %s\" % (g, ', '.join(included))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes\n",
    "\n",
    "We can use **regularization** with NMF, adding a terms cost for large values in $W$ and $H$, with either an L1 or L2 penelty.\n",
    "\n",
    "Some implementations (not sklearn) allow ignoring **missing values** in the original matrix. In a recommender these would correspond to unrated items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A scatterplot\n",
    "\n",
    "Because I like them.\n",
    "\n",
    "This is meant to simulate ratings of five different items. The users come in two groups with diffferent (poisson) distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate cluster data\n",
    "#X, y = make_blobs(centers=2, n_features=2, center_box=(1,3),random_state=1)\n",
    "#X = np.exp(X)\n",
    "import scipy.stats as scs\n",
    "npts = 100\n",
    "X = np.zeros((npts*2, 5))\n",
    "y = np.zeros((npts*2,), dtype=int)\n",
    "X[:npts,0] = scs.poisson(2).rvs(npts)\n",
    "X[:npts,1] = scs.poisson(3).rvs(npts)\n",
    "X[:npts,2] = scs.poisson(0).rvs(npts)\n",
    "X[:npts,3] = scs.poisson(1).rvs(npts)\n",
    "X[:npts,4] = scs.poisson(1).rvs(npts)\n",
    "\n",
    "X[npts:,0] = scs.poisson(1).rvs(npts)\n",
    "X[npts:,1] = scs.poisson(1).rvs(npts)\n",
    "X[npts:,2] = scs.poisson(4).rvs(npts)\n",
    "X[npts:,3] = scs.poisson(3).rvs(npts)\n",
    "X[npts:,4] = scs.poisson(0).rvs(npts)\n",
    "y[npts:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=2)\n",
    "U = nmf_model.fit_transform(X)\n",
    "V = nmf_model.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(U[:,0],U[:,1], c=np.array(['r','b'])[y], s=100, alpha=0.3)\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_xlabel(\"topic 1\")\n",
    "ax.set_ylabel(\"topic 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra bonus question** what would be a better distribution for simulating ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
