{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality Reduction is exactly what it sounds like. These are techniques for reducing the dimensions.\n",
    "\n",
    "## Why do we want to reduce the dimensions?\n",
    "\n",
    "1. Remove multicolinearity\n",
    "2. Deal with the *curse of dimensionality*\n",
    "3. Remove redundant features\n",
    "4. Interpretation & visualization\n",
    "5. Make computations of algorithms easier\n",
    "6. Discover hidden topics\n",
    "\n",
    "## Feature Matrix\n",
    "Our feature matrix has dimension $n \\times p$:\n",
    "\n",
    "#$$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\cdots & x_{np} \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$n$ = the number of subjects  \n",
    "$p$ = the number of features\n",
    "\n",
    "## Standardize your dataset\n",
    "We should always start by standardizing the dataset. This means:\n",
    "\n",
    "1. Center the data for each feature by subtracting the mean\n",
    "2. Divide by the standard deviation\n",
    "\n",
    "    For each column feature vector($x$) the resulting transformed feature vector ($x^*$) will be:\n",
    "    ## $x_i^* = \\frac{x_i - \\bar{x}}{s}$\n",
    "\n",
    "## Covariance Matrix\n",
    "Recall that the covariance matrix for features that have been centered about the mean is given by:\n",
    "\n",
    "##$\\frac{1}{n}X^T X$\n",
    "\n",
    "Note: The covariance matrix of fully standardized variables gives us the correlation matrix.\n",
    "\n",
    "## Example\n",
    "Take the following feature matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " 10 & 3 \\\\\n",
    " 10 & 4 \\\\\n",
    " 40 & 7 \\\\\n",
    " 60 & 6 \\\\\n",
    " 70 & 9 \\\\\n",
    "100 & 7 \\\\\n",
    "100 & 8\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the feature matrix after we standardize it:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-1.306 & -1.660 \\\\\n",
    "-1.306 & -1.155 \\\\\n",
    "-0.449 &  0.361 \\\\\n",
    " 0.122 & -0.144 \\\\\n",
    " 0.408 &  1.371 \\\\\n",
    " 1.266 &  0.361 \\\\\n",
    " 1.266 &  0.866\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the resulting covariance matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1.0  & 0.801 \\\\\n",
    "0.801 & 1.0\n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "X:\n",
      "[[-1.30639453 -1.65988202]\n",
      " [-1.30639453 -1.15470054]\n",
      " [-0.44907312  0.36084392]\n",
      " [ 0.12247449 -0.14433757]\n",
      " [ 0.40824829  1.37120689]\n",
      " [ 1.2655697   0.36084392]\n",
      " [ 1.2655697   0.8660254 ]]\n",
      "\n",
      "covariance matrix:\n",
      "[[ 1.          0.80138769]\n",
      " [ 0.80138769  1.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAEhtJREFUeJzt3X+IZWd9x/H3p1Fh/JGGibIxydogRkxKKbGwLpriQKu7\n",
       "phC7oNVAcaugQRq2NNs2NQrZ/tXaMqGsQQlFJaWtUVoT1iZhdy2OTSmNjSYxxmxNwMAmjaswxqhJ\n",
       "MYnf/nHPhuvkzq977sz9cd4vuOz58cx5nsMz+9lnn3vufVJVSJK645fG3QBJ0vYy+CWpYwx+SeoY\n",
       "g1+SOsbgl6SOMfglqWNaBX+SnUm+kuSBJN9KcmCVcoeTPJTkviSXtKlTktTOi1r+/DPAH1fVvUle\n",
       "Dnw9yfGqevB0gSSXAa+rqguTvAn4FLC7Zb2SpCG1GvFX1feq6t5m+yfAg8C5K4pdDtzUlLkLOCvJ\n",
       "jjb1SpKGN7I5/iQXAJcAd604dR5wsm//UeD8UdUrSdqckQR/M83zz8AfNSP/FxRZse/3REjSmLSd\n",
       "4yfJi4F/Af6hqm4dUOQxYGff/vnNsZXX8R8DSRpCVa0cXK+pVfAnCfBp4NtV9berFDsCXAXcnGQ3\n",
       "8ERVnRpUcLONnyZJDlXVoXG3YyvM8r2B9zftOnB/mx40tx3xvwX4feCbSe5pjl0LvAagqm6sqtuT\n",
       "XJbkYeCnwPtb1ilJaqFV8FfVf7CB9wmq6qo29UiSRsdP7m6fpXE3YAstjbsBW2xp3A3YYkvjbsAW\n",
       "Wxp3AyZNJmUhliQ1y3P8krQVhslOR/yS1DEGvyR1jMEvSR1j8EtSxxj8ktQxBr8kdYzBL0kdY/BL\n",
       "UscY/JLUMQa/JHWMwS9JHWPwS1LHGPyS1DEGvyR1jMEvSR1j8EtSxxj8ktQxrYM/yWeSnEpy/yrn\n",
       "F5L8KMk9zetjbeuUJA2v1WLrjc8CnwD+fo0yX62qy0dQlySppdYj/qq6E/jhOsVcS1eSJsR2zPEX\n",
       "8OYk9yW5PcnF21CnJGkVo5jqWc83gJ1V9VSSdwC3Aq/fhnolSQNsefBX1Y/7tu9I8skk81W1vLJs\n",
       "kkN9u0tVtbTV7ZOkaZJkAVhodY2qGkVDLgC+VFW/NuDcDuD7VVVJdgFfqKoLBpSrqvK9AEnahGGy\n",
       "s/WIP8nngLcCr0xyErgOeDFAVd0IvAv4cJJngaeA97atU5I0vJGM+EfBEb8kbd4w2eknd6UZk2RP\n",
       "cvax3it7xt0eTR5H/NIM6QX9mbfA4bnekQNPw5P7quroeFumrTKWOX5Jk2T+IFw/B/tPH5iDqw8C\n",
       "Br+e51SPJHWMI35ppiwvwoFLgf6pnsWxNkkTxzl+acb05vnnD/b2lhed359tw2SnwS9JU8zHOSVJ\n",
       "6zL4JaljDH5J6hiDX5I6xuCXpI4x+CWpYwx+SeoYg1+SOsbgl6SOMfglqWMMfknqGINfkjqmdfAn\n",
       "+UySU0nuX6PM4SQPJbkvySVt65QkDW8UI/7PAntXO5nkMuB1VXUh8CHgUyOoU5I0pNbBX1V3Aj9c\n",
       "o8jlwE1N2buAs5LsaFuvJGk42zHHfx5wsm//UeD8bahXkjTAdi29uHKRgIGrvyQ51Le7VFVLW9Ug\n",
       "SZpGSRaAhTbX2I7gfwzY2bd/fnPsBarq0Da0R5KmVjMgXjq9n+S6zV5jO6Z6jgDvA0iyG3iiqk5t\n",
       "Q72SpAFaj/iTfA54K/DKJCeB64AXA1TVjVV1e5LLkjwM/BR4f9s6JUnDc7F1SZpiLrYuSWOUZE9y\n",
       "9rHeK3vG3Z7VOOKXpBHoBf2Zt8Dhud6RA0/Dk/uq6ugW17vp7NyuxzklacbNH4Tr52D/6QNzcPVB\n",
       "YEuDfxhO9UhSxzjil6SRWF6EA5cC/VM9i2Nt0iqc45ekEenN888f7O0tL271/H5T56az0+CXpCnm\n",
       "45ySpHUZ/JLUMQa/JHWMwS9JHWPwS1LHGPyS1DEGvyR1jMEvSR1j8EtSxxj8ktQxBr8kdYzBL0kd\n",
       "0zr4k+xNciLJQ0muGXB+IcmPktzTvD7Wtk5J0vBafR9/kjOAG4DfBh4D/jvJkap6cEXRr1bV5W3q\n",
       "kiSNRtsR/y7g4ap6pKqeAW4G3jmgnF+3LEkTom3wnwec7Nt/tDnWr4A3J7kvye1JLm5ZpySphbZL\n",
       "L25kFZdvADur6qkk7wBuBV4/qGCSQ327S1W11LJ9kjRTkiwAC62u0WYFriS7gUNVtbfZ/wjw86r6\n",
       "+Bo/813gN6pqecVxV+CSpE0axwpcdwMXJrkgyUuA9wBHVjRqR5I027vo/WOz/MJLSZK2Q6upnqp6\n",
       "NslVwFHgDODTVfVgkiub8zcC7wI+nORZ4CngvS3bLElqwcXWJWmKudi6JkKSPcnZx3qv7Bl3e6Tt\n",
       "Mi2/+474NVK9X/Yzb4HDc70jB56GJ/dV1dHxtkzaWuP63R8mO9s+zimtMH8Qrp+D/acPzMHVB+m9\n",
       "DyTNsOn53XeqR5I6xhG/Rmx5EQ5cCvT/d3dxrE2StsX0/O47x6+R6811zh/s7S0vOr+vrhjH7/4w\n",
       "2WnwS9IU83FOSdK6DH5J6hiDX5I6xuCXpI4x+CWpYwx+SeoYg1+SOsbgl6SOMfglqWMMfknqGINf\n",
       "kjrG4Jekjmkd/En2JjmR5KEk16xS5nBz/r4kl7StU5I0vFbBn+QM4AZgL3AxcEWSi1aUuQx4XVVd\n",
       "CHwI+FSbOiVJ7bQd8e8CHq6qR6rqGeBm4J0rylwO3ARQVXcBZyXZ0bJeSdKQ2gb/ecDJvv1Hm2Pr\n",
       "lTm/Zb2SpCG1XXpxo6u4rFwkYODPJTnUt7tUVUtDtEmSZlaSBWChzTXaBv9jwM6+/Z30RvRrlTm/\n",
       "OfYCVXWoZXskaaY1A+Kl0/tJrtvsNdpO9dwNXJjkgiQvAd4DHFlR5gjwvqaBu4EnqupUy3olSUNq\n",
       "NeKvqmeTXAUcBc4APl1VDya5sjl/Y1XdnuSyJA8DPwXe37rVkqShudi6JE0xF1uXJkCSPcnZx3qv\n",
       "7Bl3e6SVHPFLI9QL+jNvgcNzvSMHnoYn91XV0fG2TLNqmOxs+1SPpF8wfxCun4P9pw/MwdUH6b0P\n",
       "Jk0Ep3okqWMc8UsjtbwIBy4F+qd6FsfaJGkF5/ilEevN888f7O0tLzq/r600THYa/JI0xXycU5K0\n",
       "LoNfkjrG4JekjjH4JaljDH5J6hiDX5I6xuCXpI4x+CWpYwx+SeoYg1+SOsbgl6SOMfglqWOG/lrm\n",
       "JPPA54FfAR4Bfq+qnhhQ7hHgSeA54Jmq2jVsnZKk9tqM+P8cOF5Vrwf+rdkfpICFqrrE0Jek8WsT\n",
       "/JcDNzXbNwG/u0ZZv25ZkiZEm+DfUVWnmu1TwI5VyhXw5SR3J/lgi/okSSOw5hx/kuPAOQNOfbR/\n",
       "p6oqyWorurylqh5P8irgeJITVXXnKvUd6ttdqqqltdonSV2TZAFYaHWNYVfgSnKC3tz995K8GvhK\n",
       "Vb1hnZ+5DvhJVb1gDVJX4JKkzdvuFbiOAPub7f3ArQMa9NIkr2i2Xwa8Hbi/RZ2SpJbajPjngS8A\n",
       "r6Hvcc4k5wJ/V1W/k+S1wBebH3kR8I9V9ZerXM8RvyRtkoutS1LHuNi6JGldBr8kdYzBL0kdY/BL\n",
       "UscY/JLUMQa/JHWMwS9JHWPwS1LHGPyS1DEGvyR1jMEvSR1j8EtSxxj8ktQxBr8kdYzBL0kdY/BL\n",
       "UscY/JLUMQa/JHWMwS9JHTN08Cd5d5IHkjyX5I1rlNub5ESSh5JcM2x9kqTRaDPivx/YB/z7agWS\n",
       "nAHcAOwFLgauSHJRizolSS29aNgfrKoTAMmai7vvAh6uqkeasjcD7wQeHLZeSVI7Wz3Hfx5wsm//\n",
       "0eaYJGlM1hzxJzkOnDPg1LVV9aUNXL8205gkh/p2l6pqaTM/L0mzLskCsNDmGmsGf1W9rc3FgceA\n",
       "nX37O+mN+ler71DL+iRppjUD4qXT+0mu2+w1RjXVs9pE/93AhUkuSPIS4D3AkRHVKUkaQpvHOfcl\n",
       "OQnsBm5Lckdz/NwktwFU1bPAVcBR4NvA56vKN3YlaYxStalp+C2TpKpqzUeEJEm/aJjs9JO7ktQx\n",
       "Mxn8SfYkZx/rvbJn3O2RpEkyc1M9vaA/8xY4PNc7cuBpeHJfVR1te21JmjTDZOfQn9ydXPMH4fo5\n",
       "2H/6wBxcfZDeG8yS1HkzOdUjSVrdDI74lxfhwKVA/1TP4libJEkTZObm+Jtr7elN+QAsLzq/L2lW\n",
       "DZOdMxn8ktQVPscvSVqXwS9JHWPwS1LHGPyS1DEGvyR1jMEvSR1j8EtSxxj8ktQxBr8kdYzBL0kd\n",
       "02bN3XcneSDJc0neuEa5R5J8M8k9Sb42bH2SpNFo8+2c9wP7gBvXKVfAQlUtt6hLkjQiQwd/VZ0A\n",
       "SDb03UB++ZokTYjtmOMv4MtJ7k7ywW2oT5K0hjVH/EmOA+cMOHVtVX1pg3W8paoeT/Iq4HiSE1V1\n",
       "52YbKkkajTWDv6re1raCqnq8+fMHSW4BdgEDgz/Job7dpapaalu/JM2SJAvAQqtrtF2IJclXgD+p\n",
       "qq8POPdS4Iyq+nGSlwHHgL+oqmMDyroQiyRt0rYuxJJkX5KTwG7gtiR3NMfPTXJbU+wc4M4k9wJ3\n",
       "Af86KPQlSdvHpRclaYq59KIkaV0GvyR1zEwGf5I9ydnHeq/sGXd7JGmSzNwcfy/oz7wFDs/1jhx4\n",
       "Gp7cV1VH215bkibNMNnZ5rt6JtT8Qbh+DvafPjAHVx8EDH5JYkaneiRJq5vBEf/yIhy4FOif6lkc\n",
       "a5MkaYLM3Bx/c609vSkfgOVF5/clzaphsnMmg1+SusIPcEmS1mXwS1LHGPyS1DEGvyR1jMEvSR1j\n",
       "8EtSxxj8ktQxBr8kdYzBL0kdY/BLUse0WWz9b5I8mOS+JF9M8surlNub5ESSh5JcM3xTJUmj0GbE\n",
       "fwz41ar6deA7wEdWFkhyBnADsBe4GLgiyUUt6pxaSRbG3YatMsv3Bt7ftJv1+xvG0MFfVcer6ufN\n",
       "7l3A+QOK7QIerqpHquoZ4GbgncPWOeUWxt2ALbQw7gZssYVxN2CLLYy7AVtsYdwNmDSjmuP/AHD7\n",
       "gOPnASf79h9tjkmSxmTNhViSHAfOGXDq2qr6UlPmo8DPquqfBpSbjO98liQ9r9X38Sf5A+CDwG9V\n",
       "1f8NOL8bOFRVe5v9jwA/r6qPDyjrPxKSNIRtW2w9yV7gT4G3Dgr9xt3AhUkuAP4XeA9wxaCCLsIi\n",
       "SdujzRz/J4CXA8eT3JPkkwBJzk1yG0BVPQtcBRwFvg18vqoebNlmSVILE7P0oiRpe4ztk7tJ3p3k\n",
       "gSTPJXnjGuUeSfLN5n8VX9vONg5rE/c2lR9uSzKf5HiS7yQ5luSsVcpNVd9tpD+SHG7O35fkku1u\n",
       "Yxvr3V+ShSQ/avrrniQfG0c7h5HkM0lOJbl/jTLT3Hdr3t+m+66qxvIC3gC8HvgK8MY1yn0XmB9X\n",
       "O7fq3oAzgIeBC4AXA/cCF4277Ru8v78G/qzZvgb4q2nvu430B3AZcHuz/Sbgv8bd7hHf3wJwZNxt\n",
       "HfL+fhO4BLh/lfNT23cbvL9N9d3YRvxVdaKqvrPB4lP1xu8G722aP9x2OXBTs30T8LtrlJ2WvttI\n",
       "fzx/31V1F3BWkh3b28yhbfT3bVr66xdU1Z3AD9coMs19t5H7g0303TR8SVsBX05yd5IPjrsxIzTN\n",
       "H27bUVWnmu1TwGp/gaap7zbSH4PKDPrE+iTayP0V8OZmKuT2JBdvW+u23jT33UZsqu+GfpxzIzby\n",
       "AbANeEtVPZ7kVfSeIDrR/Os3ViO4t4l+V32N+/to/05V1RqfwZjIvlvFRvtj5ahqovuxz0ba+Q1g\n",
       "Z1U9leQdwK30pixnxbT23UZsqu+2NPir6m0juMbjzZ8/SHILvf+yjj08RnBvjwE7+/Z30huFTIS1\n",
       "7q95k+mcqvpeklcD31/lGhPZd6vYSH+sLHN+c2warHt/VfXjvu07knwyyXxVLW9TG7fSNPfdujbb\n",
       "d5My1TNwbirJS5O8otl+GfB2YNV37SfUavNuz3+4LclL6H247cj2NauVI8D+Zns/vdHFL5jCvttI\n",
       "fxwB3gfPfyr9ib4pr0m37v0l2ZEkzfYueo97z0Low3T33bo23XdjfJd6H705t6eB7wF3NMfPBW5r\n",
       "tl9L7+mDe4FvAR8Z97vro7q3Zv8dwP/Qe9piKu6tafc88GV6X8d9DDhrFvpuUH8AVwJX9pW5oTl/\n",
       "H2s8jTaJr/XuD/jDpq/uBf4T2D3uNm/i3j5H79sBftb83fvAjPXdmve32b7zA1yS1DGTMtUjSdom\n",
       "Br8kdYzBL0kdY/BLUscY/JLUMQa/JHWMwS9JHWPwS1LH/D9OOqEZhslkBgAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105fcd750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.array([[10., 3.], [10., 4.], [40., 7.], [60., 6.], [70., 9.], [100., 7.], [100., 8.]])\n",
    "n = len(data)\n",
    "X = StandardScaler().fit_transform(data)\n",
    "print \"X:\"\n",
    "print X\n",
    "print\n",
    "print \"covariance matrix:\"\n",
    "print 1.0/n * X.T.dot(X)\n",
    "plt.scatter(X[:,0], X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our features have been standardized, this tells us that the *correlation* between feature 1 and feature 2 is 0.801. This intuitively makes sense, since we can tell the two features are correlated by visual inspection.\n",
    "\n",
    "The computations are done below with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.80138769],\n",
       "       [ 0.80138769,  1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(X,rowvar=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "Usually we will get a covariance matrix with a lot of large values; however, our ideal covariance matrix would be one where all the off-diagonal values are 0. This would indicate that there is *no relationship between the features*. As luck would have it, we can transform the data to make this happen!\n",
    "\n",
    "An ideal covariance matrix would look something like this:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "10 & 0 & 0 \\\\\n",
    "0  & 8 & 0 \\\\\n",
    "0  & 0 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The idea is to find a new set of axes (i.e. a *basis*) that better fits the data and decorrelates the features.\n",
    "\n",
    "We choose the first principal component to be in the direction of the most variance. Here's a look at what we're doing:\n",
    "\n",
    "<img src=\"images/correlated_2d.png\" align=\"left\">\n",
    "<br clear=\"all\">\n",
    "\n",
    "The green line here is in the direction of maximum variance. We make this our first axis.\n",
    "\n",
    "We then choose the second dimension (the pink line) to be orthogonal (perpendicular) to the first. There is no longer any covariance between the two features (i.e. they've been decorrelated) after we rotate the data:\n",
    "\n",
    "<img src=\"images/uncorrelated_2d.png\" align=\"left\">\n",
    "<br clear=\"all\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Mathematically\n",
    "\n",
    "Our goal is to find a transformation matrix $V$ which when applied to $X$ gives us our ideal covariance matrix:\n",
    "\n",
    "$(XV)^T (XV) = V^TX^TXV =\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$V$ is the new basis so it should look like this:\n",
    "\n",
    "$V =\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that the $u_1, u_2, \\ldots, u_p$ is an *orthonormal* basis.\n",
    "\n",
    "An *orthonormal* basis means the vectors are:\n",
    "1. all unit vectors (dot product = 1).\n",
    "2. orthogonal to each other (dot product = 0)\n",
    "\n",
    "So the following is true:\n",
    "\n",
    "$\\begin{align*}\n",
    "V^TV \n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "- & u_1 & - \\\\\n",
    "- & u_2 & - \\\\\n",
    "- & u_3 & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & u_p & -\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "u_1 \\cdot u_1 & u_1 \\cdot u_2 & \\cdots & u_1 \\cdot u_p \\\\\n",
    "u_2 \\cdot u_1 & u_2 \\cdot u_2 & \\cdots & u_2 \\cdot u_p \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_p \\cdot u_1 & u_p \\cdot u_2 & \\cdots & u_p \\cdot u_p\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$\n",
    "\n",
    "So $V^TV$ is the identity matrix which implies $V^T = V^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the original equation of our ideal world:\n",
    "\n",
    "$\\begin{align*}\n",
    "V^TX^TXV &=\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "X^TXV &= V\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$\n",
    "\n",
    "Where $ \\lambda_1 > \\lambda_2 > \\cdots > \\lambda_p $\n",
    "\n",
    "$X^TXV = V \\Lambda$\n",
    "\n",
    "If we examine any one of these vectors, we get:\n",
    "\n",
    "$X^TXu_i = \\lambda_iu_i$\n",
    "\n",
    "So we are looking for the ***eigenvalues*** ($\\lambda_i$) and ***eigenvectors*** ($u_i$) of $X^TX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of PCA\n",
    "\n",
    "To get the transformation, we need to find the eigenvalues and eigenvectors of $X^TX$.\n",
    "\n",
    "The eigenvectors are the new basis. The eigenvalues are the variance in each of these dimensions.\n",
    "\n",
    "If we would like to reduce the number of dimensions to $m << p$, we can just get rid of the smallest of the lambdas. To determine how many to keep, we often look at the *scree plot* which is a plot of the variances (eigenvalues, also called *loadings*) in increasing order.\n",
    "\n",
    "### Example (MNIST)\n",
    "\n",
    "The MNIST dataset has digits stored as $28 \\times 28$ pixel images which means $28^2 = 784$ features. We want to use PCA to reduce the number of features needed to adequately capture the variance. The following is the scree plot:\n",
    "\n",
    "<img src=\"images/screeplot.png\" align=\"left\">\n",
    "<br clear=\"all\">\n",
    "\n",
    "You are generally looking for the elbow in the graph. Here it's around 25 that you stop gaining value from adding more features. You might even be able to get by with just 1 feature!\n",
    "\n",
    "We can get a visual understanding of how much information is kept with each principal component by looking at the visual representation of the first eigenvector. We also look here at the first 10, 50 and 250 eigenvectors.\n",
    "\n",
    "<img src=\"images/mnist_three.png\" align=\"left\" width=\"800\" height=\"200\">\n",
    "<br clear=\"all\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "It's not always easy to directly compute the eigenvalues and eigenvectors. However, we can use a technique called SVD for more efficient computation. SVD is also useful for discovering hidden topics or ***latent features***.\n",
    "\n",
    "Every matrix has a *unique* decomposition in the following form:\n",
    "\n",
    "$X = U \\Sigma V^T$\n",
    "\n",
    "where\n",
    "* $U$ is column orthogonal: $U^T U = I$\n",
    "* $V$ is column orthogonal: $V^T V = I$\n",
    "* $\\Sigma$ is a diagonal matrix of positive values, where the diagonal is ordered in decreasing order\n",
    "\n",
    "We can reduce the dimensions by sending the smaller of the diagonals to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between PCA and SVD\n",
    "\n",
    "Recall, in PCA, we have the following:\n",
    "\n",
    "$X^TXV = V\\Lambda$\n",
    "\n",
    "where $\\Lambda$ is the diagonal matrix of eigenvalues.\n",
    "\n",
    "According to SVD, $X = U \\Sigma V^T$:\n",
    "\n",
    "$X^TX = (U \\Sigma V^T)^T U \\Sigma V^T = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T$\n",
    "\n",
    "So $X^TXV = V \\Sigma^2$.\n",
    "\n",
    "This looks like the same form as PCA, with $\\Lambda = \\Sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for topic analysis\n",
    "\n",
    "We can use SVD to determine what we call ***latent features***, best demonstrated with an example.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie.\n",
    "\n",
    "|       | Matrix | Alien | Serenity | Casablanca | Amelie |\n",
    "| ----- | ------ | ----- | -------- | ---------- | ------ |\n",
    "| **Alice** |      1 |     2 |        2 |          0 |      0 |\n",
    "|   **Bob** |      3 |     5 |        5 |          0 |      0 |\n",
    "| **Cindy** |      4 |     4 |        4 |          0 |      0 |\n",
    "|   **Dan** |      5 |     5 |        5 |          0 |      0 |\n",
    "| **Emily** |      0 |     2 |        0 |          4 |      4 |\n",
    "| **Frank** |      0 |     0 |        0 |          5 |      5 |\n",
    "|  **Greg** |      0 |     1 |        0 |          2 |      2 |\n",
    "\n",
    "Note that the first three movies (Matrix, Alien, Serenity) are Sci-fi movies and the last two (Casablanca, Amelie) are Romance. We will be able to mathematically pull out these topics!\n",
    "\n",
    "Let's do the computation with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0     1     2     3     4     5     6\n",
      "Alice -0.21  0.02  0.31  0.26 -0.52 -0.70  0.17\n",
      "Bob   -0.55  0.06  0.53  0.46  0.26  0.35 -0.09\n",
      "Cindy -0.50  0.07 -0.31 -0.20  0.61 -0.48  0.12\n",
      "Dan   -0.62  0.08 -0.39 -0.24 -0.54  0.31 -0.08\n",
      "Emily -0.12 -0.60  0.40 -0.52 -0.00 -0.10 -0.43\n",
      "Frank -0.04 -0.73 -0.42  0.53  0.00 -0.00  0.00\n",
      "Greg  -0.06 -0.30  0.20 -0.26 -0.00  0.21  0.87\n",
      "\n",
      "[[ 13.84   0.     0.     0.     0.  ]\n",
      " [  0.     9.52   0.     0.     0.  ]\n",
      " [  0.     0.     1.69   0.     0.  ]\n",
      " [  0.     0.     0.     1.02   0.  ]\n",
      " [  0.     0.     0.     0.     0.  ]]\n",
      "\n",
      "   Matrix  Alien  Serenity  Casablanca  Amelie\n",
      "0   -0.50  -0.62     -0.60       -0.06   -0.06\n",
      "1    0.09  -0.05      0.11       -0.70   -0.70\n",
      "2   -0.78   0.62      0.03       -0.07   -0.07\n",
      "3   -0.36  -0.48      0.79        0.05    0.05\n",
      "4    0.00   0.00      0.00       -0.71    0.71\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array([[1, 2, 2, 0, 0],\n",
    "              [3, 5, 5, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [5, 5, 5, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 1, 0, 2, 2]])\n",
    "\n",
    "U, sigma, VT = svd(X)\n",
    "\n",
    "# Make interpretable\n",
    "movies = ['Matrix','Alien','Serenity','Casablanca','Amelie']\n",
    "users = ['Alice','Bob','Cindy','Dan','Emily','Frank','Greg']\n",
    "U, sigma, VT = (np.around(x,2) for x in (U,sigma,VT))\n",
    "\n",
    "U = pd.DataFrame(U, index=users)\n",
    "VT = pd.DataFrame(VT, columns=movies)\n",
    "\n",
    "print U\n",
    "print\n",
    "print np.diag(sigma)\n",
    "print\n",
    "print VT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last singular values is 0, so we can drop it and reconstruct our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0     1     2     3\n",
      "Alice -0.21  0.02  0.31  0.26\n",
      "Bob   -0.55  0.06  0.53  0.46\n",
      "Cindy -0.50  0.07 -0.31 -0.20\n",
      "Dan   -0.62  0.08 -0.39 -0.24\n",
      "Emily -0.12 -0.60  0.40 -0.52\n",
      "Frank -0.04 -0.73 -0.42  0.53\n",
      "Greg  -0.06 -0.30  0.20 -0.26\n",
      "\n",
      "[ 13.84   9.52   1.69   1.02]\n",
      "\n",
      "   Matrix  Alien  Serenity  Casablanca  Amelie\n",
      "0   -0.50  -0.62     -0.60       -0.06   -0.06\n",
      "1    0.09  -0.05      0.11       -0.70   -0.70\n",
      "2   -0.78   0.62      0.03       -0.07   -0.07\n",
      "3   -0.36  -0.48      0.79        0.05    0.05\n",
      "\n",
      "Our Matrix Approximation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matrix</th>\n",
       "      <th>Alien</th>\n",
       "      <th>Serenity</th>\n",
       "      <th>Casablanca</th>\n",
       "      <th>Amelie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td> 1</td>\n",
       "      <td> 2</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td> 3</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cindy</th>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td> 0</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frank</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greg</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Matrix  Alien  Serenity  Casablanca  Amelie\n",
       "Alice       1      2         2           0       0\n",
       "Bob         3      5         5           0       0\n",
       "Cindy       4      4         4           0       0\n",
       "Dan         5      5         5           0       0\n",
       "Emily       0      2         0           4       4\n",
       "Frank       0      0         0           5       5\n",
       "Greg        0      1         0           2       2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only top four concepts\n",
    "U = U.iloc[:,:4]\n",
    "sigma = sigma[:4]\n",
    "VT = VT.iloc[:4,:]\n",
    "\n",
    "print U\n",
    "print \n",
    "print sigma\n",
    "print \n",
    "print VT\n",
    "print \n",
    "print \"Our Matrix Approximation:\"\n",
    "np.abs(np.around(U.dot(np.diag(sigma)).dot(VT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66957013,  0.98637933,  0.99636316,  1.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Power\n",
    "# singular values are square roots of eigenvalues\n",
    "total_power = np.sum(sigma**2)\n",
    "total_power\n",
    "fraction_power = np.cumsum(sigma**2) / total_power\n",
    "fraction_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the two topics capture most of the power:\n",
    "\n",
    "$U$ is the ***user-to-topic*** matrix and $V$ is the ***movie-to-topic*** matrix.\n",
    "\n",
    "The third and fourth singular values are relatively small, explaining only 1% and 0.4% of the variance respectively, so we could also exclude them with only a small loss of information. Let's try doing that and reconstruct our matrix!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0     1\n",
      "Alice -0.21  0.02\n",
      "Bob   -0.55  0.06\n",
      "Cindy -0.50  0.07\n",
      "Dan   -0.62  0.08\n",
      "Emily -0.12 -0.60\n",
      "Frank -0.04 -0.73\n",
      "Greg  -0.06 -0.30\n",
      "[ 13.84   9.52]\n",
      "   Matrix  Alien  Serenity  Casablanca  Amelie\n",
      "0   -0.50  -0.62     -0.60       -0.06   -0.06\n",
      "1    0.09  -0.05      0.11       -0.70   -0.70\n"
     ]
    }
   ],
   "source": [
    "# Keep only top two concepts\n",
    "U = U.iloc[:,:2]\n",
    "sigma = sigma[:2]\n",
    "VT = VT.iloc[:2,:]\n",
    "\n",
    "print U\n",
    "print sigma\n",
    "print VT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine the two latent topics:\n",
    "\n",
    "1. Science Fiction\n",
    "    * First singular value (13.84)\n",
    "    * First column of the $U$ matrix (note that the first four users have relatively large values here)\n",
    "    * First row of the $V$ matrix (note that the first three movies have large values here)\n",
    "2. Romance\n",
    "    * Second singular value (9.52)\n",
    "    * Second column of the $U$ matrix (note that the last three users have large values here)\n",
    "    * Second row of the $V$ matrix (note that the last two movies have large values here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximation using only two singular values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matrix</th>\n",
       "      <th>Alien</th>\n",
       "      <th>Serenity</th>\n",
       "      <th>Casablanca</th>\n",
       "      <th>Amelie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td> 1</td>\n",
       "      <td> 2</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td> 4</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cindy</th>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td> 4</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 4</td>\n",
       "      <td> 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frank</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 5</td>\n",
       "      <td> 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greg</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Matrix  Alien  Serenity  Casablanca  Amelie\n",
       "Alice       1      2         2           0       0\n",
       "Bob         4      5         5           0       0\n",
       "Cindy       4      4         4           0       0\n",
       "Dan         4      5         5           0       0\n",
       "Emily       0      1         0           4       4\n",
       "Frank       0      1         0           5       5\n",
       "Greg        0      1         0           2       2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"The approximation using only two singular values\"\n",
    "np.abs(np.around(U.dot(np.diag(sigma)).dot(VT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix        0.000000\n",
       "Alien         0.033242\n",
       "Serenity      0.000005\n",
       "Casablanca    1.092455\n",
       "Amelie        1.092455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which movies are most similar to Matrix?\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "matrix = VT['Matrix']\n",
    "distances = [cosine(matrix, VT[col]) for col in VT]\n",
    "pd.Series(np.around(distances,6), index=movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.4311,  3.2101,  2.9149,  1.1518,  1.1518]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make recommendations for a new user\n",
    "my_ratings = np.array([[5, 0, 4, 0, 3]])\n",
    "\n",
    "# Translate to weighted concept space\n",
    "my_weighted_concept = my_ratings.dot(VT.T)\n",
    "my_weighted_concept\n",
    "\n",
    "\n",
    "# Translate back to rating space\n",
    "new_rating = my_weighted_concept.dot(VT)\n",
    "new_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best recommendation for a new movie for me to watch is Alien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Which user am I most similar to?\n",
    "Translate to user space by multiplying by $VΣ^{−1}$ on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alice    0.090329\n",
       "Bob      0.096108\n",
       "Cindy    0.109543\n",
       "Dan      0.104693\n",
       "Emily    0.493822\n",
       "Frank    0.621577\n",
       "Greg     0.493822\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_inv = np.diag(1/sigma)\n",
    "\n",
    "# Translate to concept space\n",
    "my_concept = my_ratings.dot(VT.T).dot(sigma_inv)\n",
    "my_concept\n",
    "\n",
    "# Find distance to other users\n",
    "distances = [cosine(my_concept, row) for name,row in U.iterrows()]\n",
    "pd.Series(distances, index=users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
