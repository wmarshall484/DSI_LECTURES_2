{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "# Objectives\n",
    "\n",
    "## PCA\n",
    "\n",
    "- Reduce data dimensions\n",
    "- Compress data to optimize ML speed and memory\n",
    "- Visualize data by reducing dimensions to 2 or 3\n",
    "\n",
    "## SVD\n",
    "\n",
    "- Decompose matrix into factors\n",
    "- Identify minimal underlying basis\n",
    "- Extract generic categories from sample\n",
    "\n",
    "# Curse of Dimensionality\n",
    "\n",
    "## Intro\n",
    "\n",
    "<br><details><summary>\n",
    "What is the curse of dimensionality?\n",
    "</summary>\n",
    "\n",
    "1. Higher dimensional spaces have weird geometry.<br>\n",
    "2. Average distance between data points increases.<br>\n",
    "3. Data points become sparser the more dimensions you have.<br>\n",
    "4. Classifier performance peaks then drops.<br>\n",
    "</details>\n",
    "\n",
    "## Distance\n",
    "\n",
    "<br><details><summary>\n",
    "Why does the average distance increase?\n",
    "</summary>\n",
    "\n",
    "1. Consider a unit ball in N-dimensions.<br>\n",
    "2. Distance is calculated by adding up the sum of squares of\n",
    "   difference in each dimension, and then taking a square root.<br>\n",
    "3. The more dimensions there are the more terms there are in this\n",
    "   sum.<br>\n",
    "4. The more terms there are the higher the value of the sum.<br>\n",
    "</details>\n",
    "\n",
    "## Sparsity\n",
    "\n",
    "<br><details><summary>\n",
    "Why does the sparsity increase?\n",
    "</summary>\n",
    "\n",
    "A sub-hypercube will be a smaller fraction of the hypercube as the<br>\n",
    "dimensions increase.<br>\n",
    "</details>\n",
    "\n",
    "## Classifier Performance\n",
    "\n",
    "<br><details><summary>\n",
    "How do you expect classifier performance to be affected?\n",
    "</summary>\n",
    "\n",
    "1. If you have too few dimensions the classifier is missing important information.<br>\n",
    "2. Past an optimal number of dimensions the information will mostly be noise.<br>\n",
    "</details>\n",
    "\n",
    "## Fixing Problem\n",
    "\n",
    "What if we could reduce the number of dimensions of the data and still\n",
    "retain the information in our data?\n",
    "\n",
    "## Sources of Dimensions\n",
    "\n",
    "Suppose these are the ages and weights 4 people represented as a\n",
    "matrix.\n",
    "\n",
    "$\n",
    "\\left[ \\begin{matrix}\n",
    "20 & 130 \\\\ \n",
    "30 & 150 \\\\ \n",
    "1 & 20 \\\\ \n",
    "10 & 60 \\\\\n",
    "\\end{matrix} \\right]\n",
    "$\n",
    "\n",
    "<br><details><summary>\n",
    "How many dimensions does this have? Are dimensions features or are\n",
    "they the number of samples?\n",
    "</summary>\n",
    "\n",
    "1. Dimensions are features.<br>\n",
    "2. There are 2 dimensions.<br>\n",
    "3. There are 4 samples.<br>\n",
    "</details>\n",
    "\n",
    "## Rank \n",
    "\n",
    "<br><details><summary>\n",
    "What is the rank of a matrix?\n",
    "</summary>\n",
    "\n",
    "1. Maximum number of linearly independent column vectors of D<br>\n",
    "2. Maximum number of linearly independent row vectors of D.<br>\n",
    "3. $rank(D) = r \\le \\min(n,d)$ where $n$ is the number of rows and $d$ is the number of columns.<br>\n",
    "</details>\n",
    "\n",
    "## Rank Quiz\n",
    "\n",
    "Consider this matrix.\n",
    "\n",
    "$\n",
    "\\left[ \\begin{matrix}\n",
    "20 & 130 \\\\ \n",
    "30 & 150 \\\\ \n",
    "1 & 20 \\\\ \n",
    "10 & 60 \\\\\n",
    "\\end{matrix} \\right]\n",
    "$\n",
    "\n",
    "<br><details><summary>\n",
    "What is its rank? \n",
    "</summary>\n",
    "\n",
    "Rank = 2<br>\n",
    "</details>\n",
    "\n",
    "## Rank Quiz\n",
    "\n",
    "Consider this matrix.\n",
    "\n",
    "$\n",
    "\\left[ \\begin{matrix}\n",
    "4 & 4 & 4 \\\\ \n",
    "5 & 5 & 5 \\\\ \n",
    "4 & 4 & 4 \\\\ \n",
    "3 & 3 & 3 \\\\\n",
    "\\end{matrix} \\right]\n",
    "$\n",
    "\n",
    "<br><details><summary>\n",
    "What is is rank?\n",
    "</summary>\n",
    "\n",
    "Rank = 1<br>\n",
    "</details>\n",
    "\n",
    "## Dimensionality of Data\n",
    "\n",
    "<br><details><summary>\n",
    "What is the dimensionality of data?\n",
    "</summary>\n",
    "\n",
    "1. The dimensionality of data is the number of linearly independent *basis vectors* needed to represent all the data vectors.<br>\n",
    "2. The data points live in a *d*-dimensional *attribute space*.<br>\n",
    "3. If $r \\le d$ then the data points live in a lower *r*-dimensional space.<br>\n",
    "4. Practically speaking, this will be the number of variables in the data.<br>\n",
    "</details>\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "<br><details><summary>\n",
    "What is Principal Component Analysis (PCA)?\n",
    "</summary>\n",
    "\n",
    "1. PCA transforms your data to a smaller basis.<br>\n",
    "2. The new smaller basis retains most of the variance of the original data set.<br>\n",
    "</details>\n",
    "\n",
    "## Eigenvectors\n",
    "\n",
    "<br><details><summary>\n",
    "What are *eigenvectors* and *eigenvalues*?\n",
    "</summary>\n",
    "\n",
    "1. Eigenvectors are fixed points of a matrix. <br>\n",
    "2. $\\mathbf{M}\\times\\mathbf{v} = \\lambda\\mathbf{v}$.<br>\n",
    "3. Here $\\mathbf{v}$ is an eigenvector, and $\\lambda$ is an eigenvalue.<br>\n",
    "4. Eigenvectors form a basis for $\\mathbf{M}$ column vectors.<br>\n",
    "5. Eigenvectors form a basis for $\\mathbf{M}^T$ row vectors.<br>\n",
    "</details>\n",
    "\n",
    "## Eigenvectors and PCA\n",
    "\n",
    "<br><details><summary>\n",
    "Why are eigenvectors useful for PCA?\n",
    "</summary>\n",
    "1. Eigenvectors represent a basis for $\\mathbf{M}$.<br>\n",
    "2. Eigenvalues are the variance captured by each dimension.<br>\n",
    "3. Eigenvectors and eigenvalues occur in pairs.<br>\n",
    "4. Using eigenvector decomposition you can pick the eigenvectors and eigenvalues that capture most of the variance.<br>\n",
    "</details>\n",
    "\n",
    "## Intuition\n",
    "\n",
    "<br><details><summary>\n",
    "What is PCA intuitively?\n",
    "</summary>\n",
    "\n",
    "1. Intuitively, PCA is fitting an n-dimensional ellipsoid to data.<br>\n",
    "2. Each axis of the ellipsoid represents a principal component.<br>\n",
    "3. The axis along which the ellipsoid is fattest is the first principal component.<br>\n",
    "4. The axes along which the ellipsoid is thinnest can be ignored.<br>\n",
    "5. The thinner axes are not capturing as much variance as the fatter ones.<br>\n",
    "</details>\n",
    "\n",
    "![](images/pca-ellipsoid.png)\n",
    "\n",
    "## PCA Process\n",
    "\n",
    "<br><details><summary>\n",
    "What is the process for finding the PCA?\n",
    "</summary>\n",
    "\n",
    "1. Center your variables to their mean.<br>\n",
    "2. Meaning, for each column vector of $\\mathbf{M}$, find its mean and subtract it from the column values.<br>\n",
    "3. Optionally, you can also scale the variables. For each column find its standard deviation, and divide each value in the column by the standard deviation.<br>\n",
    "4. Find covariance matrix of the variables.<br>\n",
    "5. Calculate eigenvalues and eigenvectors of covariance matrix.<br>\n",
    "6. Orthogonalize and normalize eigenvectors to unit vectors. <br>\n",
    "7. The mutually orthogonal unit eigenvectors form an axes of the ellipsoid fitted to the data. <br>\n",
    "8. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.<br>\n",
    "</details>\n",
    "\n",
    "## Scaling\n",
    "\n",
    "<br><details><summary>\n",
    "Should I scale the data (divide by standard deviation) or not? What\n",
    "are the pros and cons of scaling?\n",
    "</summary>\n",
    "\n",
    "1. The variable with the higher variance will dominate the principal components.<br>\n",
    "2. Argument for scaling: If variables are using different units the variable with\n",
    "   the bigger values will dominate.<br>\n",
    "3. Argument against scaling: If one variable is noisier it will\n",
    "   dominate the principal components.<br>\n",
    "4. There is no consensus on this; it is a judgement call.<br>\n",
    "</details>\n",
    "\n",
    "## Covariance vs Correlation\n",
    "\n",
    "![](images/cor-vs-cov.png)\n",
    "\n",
    "<br><details><summary>\n",
    "Should I use the covariance matrix or the correlation matrix?\n",
    "</summary>\n",
    "\n",
    "1. If you scale the variables then that is the same as using the\n",
    "   correlation matrix.<br>\n",
    "2. If you do not scale the variables then you are using the\n",
    "   covariance.<br>\n",
    "3. The pros and cons are identical to what we discussed above.<br>\n",
    "</details>\n",
    "\n",
    "## Why Covariance\n",
    "\n",
    "\n",
    "## Matrix Intuition\n",
    "\n",
    "Consider this matrix in the following discussion.\n",
    "\n",
    "$\n",
    "\\left[ \\begin{matrix}\n",
    "1 & 2 \\\\ \n",
    "3 & 4 \\\\ \n",
    "\\end{matrix} \\right]\n",
    "$\n",
    "\n",
    "<br><details><summary>\n",
    "What is a matrix? What does multiplying a matrix and a vector do?\n",
    "</summary>\n",
    "\n",
    "1. A matrix is a transformation.<br>\n",
    "2. When you multiply a matrix and a vector, that takes the vector, stretches it out, and reorients it.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How do we know what the exact stretching and reorienting effect of a\n",
    "matrix is?\n",
    "</summary>\n",
    "\n",
    "1. Consider its effect on the unit vector.<br>\n",
    "$\n",
    "\\left[ \\begin{matrix}\n",
    "1 & 2 \\\\ \n",
    "3 & 4 \\\\ \n",
    "\\end{matrix} \\right]\n",
    "\\times\n",
    "\\left[ \\begin{matrix}\n",
    "1 \\\\ \n",
    "0 \\\\ \n",
    "\\end{matrix} \\right]\n",
    "$\n",
    "<br>\n",
    "2. Each vector is made up of a mixture of the unit vectors.<br>\n",
    "3. The matrix's final effect on the vector is the sum of the effects\n",
    "   on its unit vectors.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Eigenvector Intuition\n",
    "\n",
    "An eigenvector is a vector that is stretched by a matrix but not\n",
    "reoriented. \n",
    "\n",
    "<br><details><summary>\n",
    "Intuitively, what is an eigenvector?\n",
    "</summary>\n",
    "\n",
    "1. Consider the ellipsoid of data points.<br>\n",
    "2. The covariance matrix for this is a matrix that takes a unit ball\n",
    "   and skews and stretches it into the ellipsoid.<br>\n",
    "3. The eigenvectors are the axes of the ellipsoid because they only\n",
    "   get stretched and not skewed.<br>\n",
    "4. The eigenvalues are the amount that the eigenvectors are stretched by.<br>\n",
    "</details>\n",
    "\n",
    "## Eigenvectors and PCA\n",
    "\n",
    "<br><details><summary>\n",
    "Why use eigenvectors for PCA?\n",
    "</summary>\n",
    "\n",
    "1. If we think of the matrix as a stretching and a reorientation, then\n",
    "   the matrix takes a unit ball and stretches and reorients it to an\n",
    "   ellipsoid.<br>\n",
    "2. The eigenvectors are the axes of this ellipsoid.<br>\n",
    "3. The eigenvalues are the amount the ellipsoid is stretched along a\n",
    "   particular eigen-direction.<br>\n",
    "</details>\n",
    "\n",
    "## Why Eigenvectors\n",
    "\n",
    "<br><details><summary>\n",
    "What does the matrix look like if we transform it by this new\n",
    "eigenvector basis?\n",
    "</summary>\n",
    "\n",
    "1. The matrix now only stretches the basis. <br>\n",
    "2. It does not reorient it. <br>\n",
    "3. This means the matrix is now a diagonal matrix.<br>\n",
    "4. Only its diagonal has non-zero values.<br>\n",
    "</details>\n",
    "\n",
    "## Eigenvalues And Variance\n",
    "\n",
    "<br><details><summary>\n",
    "Why are eigenvalues the variance along a principal component?\n",
    "</summary>\n",
    "\n",
    "1. Suppose we change the coordinates of a matrix to the basis formed\n",
    "   by its eigenvectors.<br>\n",
    "2. In this basis multiplying an eigenvector of the matrix by the\n",
    "   matrix only scales the vector.<br>\n",
    "3. The value of the diagonal, the eigenvalue, is how fat the ellipsoid\n",
    "   is along that eigenvector axis.<br>\n",
    "</details>\n",
    "\n",
    "## PCA vs Regression\n",
    "\n",
    "### Regression\n",
    "\n",
    "![](images/pca-vs-regression-1.png)\n",
    "\n",
    "### PCA\n",
    "\n",
    "![](images/pca-vs-regression-2.png)\n",
    "\n",
    "<br><details><summary>\n",
    "What is the difference between PCA and regression?\n",
    "</summary>\n",
    "\n",
    "1. In regression you have labeled data and you are trying to find a\n",
    "   line or a plane that predicts *y* based on *x*.<br>\n",
    "2. You are trying to minimize your prediction error.\n",
    "3. In PCA you do not have labeled data, there is no *y*.<br>\n",
    "4. Instead you are trying to find a smaller dimensional space that\n",
    "   approximates your features.<br>\n",
    "5. You are trying to minimize your approximation error.\n",
    "6. The similarity is only superficial.\n",
    "</details>\n",
    "\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "<br><details><summary>\n",
    "Should I always use PCA on my data?\n",
    "</summary>\n",
    "\n",
    "1. Not necessarily.<br>\n",
    "2. For example, because the data is unlabeled PCA might collapse a dimension that separates two labels.<br>\n",
    "3. Without scaling important but smaller valued dimensions might get flattened out.<br>\n",
    "4. Assumes linear projections.<br>\n",
    "</details>\n",
    "\n",
    "## Applications\n",
    "\n",
    "<br><details><summary>\n",
    "What are some applications of PCA?\n",
    "</summary>\n",
    "\n",
    "1. Reduce memory used by machine learning algorithms.<br>\n",
    "2. Improve performance of machine learning algorithms.<br>\n",
    "3. Visualization.<br>\n",
    "</details>\n",
    "\n",
    "## How To Pick K\n",
    "\n",
    "<br><details><summary>\n",
    "How can we decide what *k* to use?\n",
    "</summary>\n",
    "\n",
    "1. Sort the eigenvalues?<br>\n",
    "2. Divide them by the sum of the eigenvalues to get percentage\n",
    "   variances explained.<br>\n",
    "3. Add eigenvalues until you have 90% variance explained (or 95% or\n",
    "   99% or whatever your goal is).<br>\n",
    "4. Another approach is to create a Scree plot and look for the\n",
    "   elbow.<br>\n",
    "</details>\n",
    "\n",
    "## Scree Plots\n",
    "\n",
    "<br><details><summary>\n",
    "What is a Scree Plot?\n",
    "</summary>\n",
    "\n",
    "1. A scree plot displays the eigenvalues associated with a component\n",
    "   in descending order versus the number of the component.<br>\n",
    "2. You can use scree plots in PCA to visually assess which components\n",
    "   explain most of the variability in the data.<br>\n",
    "</details>\n",
    "\n",
    "## Job Performance Scree Plot\n",
    "\n",
    "![](images/scree-plot.png)\n",
    "\n",
    "## Eigen\n",
    "\n",
    "<br><details><summary>\n",
    "What does eigen mean?\n",
    "</summary>\n",
    "\n",
    "1. Eigen means *proper* or *characteristic* in German.<br>\n",
    "2. Eigenvectors and eigenvalues characterize a matrix viewed as a\n",
    "   transformation.<br>\n",
    "3. Why are we using a German word for them? Because they were named \n",
    "   by David Hilbert who wrote in German.<br>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
