### Dimensionality Reduction Lecture (based on Matt Drury)



Objectives
==========

### Morning: PCA

  - State three reasons one may want to perform dimension reduction.
  - Given a scatterplot, pen, and paper, draw the principal components of the data (approximately).
  - Define an eigenvector of a matrix.
  - Compute the projections of a data set onto its principal components to reduce dimension.
  - Relate the eigenvectors of `X^t X` to the principal components of `X`.
  - Critque the principal components regression procedure.

### Afternoon: SVD

  - State the Singular Value Decomposition of a matrix.
  - Define an orthogonal matrix.
  - Explain the relationship between PCA and SVD.
  - Use SVD to derive latent features in a dataset.
