{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd, eig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import load_iris, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# Always make it pretty.\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "### Morning: Principal Component Analysis\n",
    "- State reasons why one might perform diminsionality reduction.\n",
    "- Given a printed scatterplot and pen, draw the principal components of the data (approximately).\n",
    "- Define the eigenvector of a matrix.\n",
    "- Compute the projections of a dataset onto its principal components to reduce its dimensions.\n",
    "- Relate the eigenvectors of the covariance matrix of X to the principal components of X.\n",
    "\n",
    "\n",
    "### Afternoon: Singular Value Decomposition\n",
    "- State what makes up the Singular Value Decomposition of a matrix.\n",
    "- Define an orthogonal matrix.\n",
    "- Explain the relationship between PCA and SVD.\n",
    "- Use SVD to derive latent features in a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is the dimensionality of this data set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "names= ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car_name']\n",
    "cars = pd.read_csv('data/cars.tsv', delim_whitespace=True, header=None, names=names)\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cars, figsize = (7,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is the dimensionality of this data set?\n",
    "MNIST handwritten digits. Each grayscale image is 28x28\n",
    "<img src=\"images/mnist.png\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each pixel is a feature, so each image (data point) is a 784-dimensional feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Reduce Dimensions?\n",
    "- Combat the **Curse of Dimensionality**\n",
    "    - Remember how that ruined kNN and clustering?\n",
    "- Facilitate **Visualization**\n",
    "    - Most people only want to see 2 or 3 dimensions at a time\n",
    "- Combine many raw features into a few meaningful latent features\n",
    "    - More about this in the afternoon! And tomorrow!\n",
    "- Compress your data\n",
    "    - If you can represent your data in fewer dimensions without losing much information, you have saved some hard drive space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to Principal Component Analysis\n",
    "\n",
    "Let's think of our data $X$ as a point cloud in $p$-dimensional space, and ask the following question:\n",
    "\n",
    "**Fundamental Question:** How can we find a 1-dimensional representation of our data $X_1$ so that\n",
    "\n",
    "  - Going from $X$ to $X_1$ is a very simple operation.\n",
    "  - In some sense, $X_1$ is the *best* one dimensional reconstruction of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Discussion:** Consider \"Going from $X$ to $X_1$ is a very simple operation\".  What are some good candidates for *simple operations*.  The following animation may be suggestive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Pca Animation](images/pca.gif)\n",
    "\n",
    "Image source: http://stats.stackexchange.com/a/140579/74500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-Dimensional PCA\n",
    "\n",
    "We look for a **line** so that the *projection* of the data $X$ onto that line\n",
    "\n",
    "  - Results in points minimizing the total squared distance to $X$\n",
    "  - Results in points with *maximum variance* as a 1-dimensional data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "N_points = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def random_data_set(n=N_points):\n",
    "    theta = np.random.uniform(low=0, high=2*3.14159)\n",
    "    rotation = np.array([[np.cos(theta), np.sin(theta)], \n",
    "                         [-np.sin(theta), np.cos(theta)]])\n",
    "    data = np.column_stack([np.random.normal(size=n), 2*np.random.normal(size=n)])\n",
    "    rotated_data = np.dot(data, rotation)\n",
    "    return rotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_1dim_pca(ax):\n",
    "    \"\"\"Make a plot of:\n",
    "        - A point cloud rotated to an random angle (blue).\n",
    "        - The principal component line through the orgin (green).\n",
    "        - Plot the eigenvector representing the principal component (black).\n",
    "        - The point cloud projected onto the principal component line (green).\n",
    "    \"\"\"\n",
    "    X = random_data_set()\n",
    "    pcd = PCA(1).fit(X)\n",
    "    e = pcd.components_[0]\n",
    "    # Plot the data set.\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    # Plot a line for the principal component.\n",
    "    x = np.linspace(-10, 10, num=3)\n",
    "    ax.plot(e[0]*x, e[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    # Plot the projections of the data points onto the line.\n",
    "    X_proj = np.dot(X, e)\n",
    "    X_reconst = np.array([t*e for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot an arrow for the first principal direction.\n",
    "    ax.arrow(0, 0, e[0], e[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.set(adjustable='box-forced', aspect='equal')\n",
    "    ax.set_xlim((-5, 5))\n",
    "    ax.set_ylim((-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    plot_1dim_pca(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In these plots, we superimpose the first principal component onto a random scatter plot.  Notice that:\n",
    "\n",
    "> The green line is chosen so that the projections of the data points onto this line are **maximally spread out**.\n",
    "\n",
    "The direction of this line is called the **first principal component** of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### General Principal Components\n",
    "\n",
    "**In general** PCA applied to a dataset $X$ returns the best line, plane, 3-space, 4-space, ... so that when $X$ is projected into the subspace\n",
    "\n",
    "  - The total squared distance from the original data to the projections in minimized.\n",
    "  - The total variance (i.e. the sum of the variances in all posible orthogonal directions) of the projected dataset is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_2dim_pca(ax):\n",
    "    \"\"\"Make a plot of:\n",
    "        - A point cloud rotated to an random angle (blue).\n",
    "        - The principal component lines through the orgin (green).\n",
    "        - The eigenvectors representing the principal components (black).\n",
    "        - The point cloud projected onto the principal component lines (green).\n",
    "    \"\"\"\n",
    "    X = random_data_set()\n",
    "    pcd = PCA(2).fit(X)\n",
    "    e_1 = pcd.components_[0]\n",
    "    e_2 = pcd.components_[1]\n",
    "    # Plot the data set.\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    # Plot a lines for the principal components.\n",
    "    x = np.linspace(-10, 10, num=3)\n",
    "    ax.plot(e_1[0]*x, e_1[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    ax.plot(e_2[0]*x, e_2[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    # Plot the projections of the data points onto the first line.\n",
    "    X_proj = np.dot(X, e_1)\n",
    "    X_reconst = np.array([t*e_1 for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot the projections of the data points onto the second line.\n",
    "    X_proj = np.dot(X, e_2)\n",
    "    X_reconst = np.array([t*e_2 for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot an arrow for the first principal direction.\n",
    "    ax.arrow(0, 0, e_1[0], e_1[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, e_2[0], e_2[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.set(adjustable='box-forced', aspect='equal')\n",
    "    ax.set_xlim((-5, 5))\n",
    "    ax.set_ylim((-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    plot_2dim_pca(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In these plots, we superimpose the first and second principal components onto a random scatter plot.  Notice that:\n",
    "\n",
    "  - The **first principlal component** determines a green line that maximizes the variance of the data's projection.\n",
    "  - The **second principal component** is orthogonal to the first, and maximizes the projection of the \"leftover\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution to the problem of finding principal components involves, somewhat surprisingly, the *eigenvalues* and *eigenvectors* of the covariance matrix $X^t X$ of $X$.\n",
    "\n",
    "**Note:** X must be centered before computing the covariance in this manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Setup:\n",
    "\n",
    "$X$ is a dataset, which we represent as a $n \\times p$ matrix of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = random_data_set(n=50)\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 1: Center the Matrix.\n",
    "\n",
    "**Centering** the matrix is the process of subracting the column means from the columns themselves.  This results in a new matrix with column means zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_centered = X - np.mean(X, axis=0)\n",
    "print(np.mean(X_centered, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(X_centered[:, 0], X_centered[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 2: Compute the sample covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The sample covariance matrix is $M = \\frac{1}{n} X^t X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = np.dot(X_centered.T, X_centered) * (1/float(X_centered.shape[0]))\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 3: Compute the Eigenvectors and Eigenvalues of M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The eigenvectors, when ordered in order of decreasing eigenvalue, are the principal components of $X$.\n",
    "\n",
    "**Note:**\n",
    "  - Since $M$ is a symmetric, it has a full set of $p$ eigenvectors.\n",
    "  - Since $M$ is non-negative definite,  the eigenvalues are non-negative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "e_values, e_vectors = eig(M)\n",
    "print(\"The eigenvectors of M are:\")\n",
    "print(e_vectors)\n",
    "print(\"The eigenvlaues of M are {}\".format(e_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intermission: Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that the **eigenvectors** of $M$ are vectors $v$ that satisfy a relationship like:\n",
    "\n",
    "$$ M v = \\lambda v $$\n",
    "\n",
    "I.e., the matrix $M$ acts as a **scaling** on the vector $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"M dot v equals: {}\".format(np.dot(M, e_vectors[:, 0])))\n",
    "print(\"lambda times v equals: {}\".format(e_values[0] * e_vectors[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"M dot v equals: {}\".format(np.dot(M, e_vectors[:, 1])))\n",
    "print(\"lambda times v equals: {}\".format(e_values[1] * e_vectors[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 6),subplot_kw=dict(aspect='equal'))\n",
    "\n",
    "coord_vectors = np.array([[1, 0], [0, 1]])\n",
    "coord_image = np.dot(M, coord_vectors)\n",
    "e_image = np.dot(M, e_vectors)\n",
    "\n",
    "# Set coordinate ranges to the maximal possible arrow position\n",
    "max_coord = max(np.max(coord_vectors), np.max(coord_image), np.max(e_image))\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlim(-max_coord - 0.5, max_coord + 0.5)\n",
    "    ax.set_ylim(-max_coord - 0.5, max_coord + 0.5)\n",
    "\n",
    "# Plot the coordinate vectors and their images.\n",
    "axs[0, 0].arrow(0, 0, coord_vectors[0, 0],  coord_vectors[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[0, 0].arrow(0, 0, coord_vectors[0, 1], coord_vectors[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[0, 0].set_title(\"Coordinate Vectors\")\n",
    "axs[0, 1].arrow(0, 0, coord_image[0, 0],  coord_image[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[0, 1].arrow(0, 0, coord_image[0, 1], coord_image[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[0, 1].set_title(\"$M \\\\times$ Coordinate Vectors\")\n",
    "\n",
    "\n",
    "# Plot the eigenvectors and their images.\n",
    "axs[1, 0].arrow(0, 0, e_vectors[0, 0],  e_vectors[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[1, 0].arrow(0, 0, e_vectors[0, 1], e_vectors[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[1, 0].set_title(\"Eigenvectors\")\n",
    "axs[1, 1].arrow(0, 0, e_image[0, 0],  e_image[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[1, 1].arrow(0, 0, e_image[0, 1], e_image[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[1, 1].set_title(\"$M \\\\times$ Eigenvectors\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Above, we see that the effect of multiplying the matrix $M$ by coordinate vectors is difficult to understand, the coordinate vectors are rotated and scaled, each by a different amount.\n",
    "\n",
    "On the other hand, the eigenvectors of $M$ are much better behaved when multiplied by $M$, the directon of the eigenvectors are preserved, but they are streched or shrunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the eigenvectors of $M$ on top of the centered data set to see how they relate to the scatterplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 6), subplot_kw=dict(aspect='equal'))\n",
    "\n",
    "ax.scatter(X_centered[:, 0], X_centered[:, 1])\n",
    "ax.arrow(0, 0, e_vectors[0, 0], e_vectors[1, 0],\n",
    "         head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "ax.arrow(0, 0, e_vectors[0, 1], e_vectors[1, 1],\n",
    "         head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "_ = ax.set(adjustable='box-forced', aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary:\n",
    "\n",
    "  - The Principal Components algorithm computes the *eigenvectors* and *eigenvalues* of the matrix $X^t X$.\n",
    "  - Each eigenvector is called a *principal component* of $X$.\n",
    "  - Projecting onto the first $k$ principal components creates the **best k dimensional reconstruction of the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practice, we use `sklearn.decomposition.PCA` to find principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pcd = PCA(2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sklearn returns a `PCA` returns an object with a `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pcd.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `components_` attribute is an orthogonal matrix containing (as rows) the eigenvalues of the correlation matrix.  That is, the *principal components*.\n",
    "\n",
    "Let's call the matrix of principal components $E$.\n",
    "\n",
    "### Properties of Principal Components\n",
    "\n",
    "1. Taking the first $k$ rows of $E$ gives (a basis for) the \"best\" $k$ dimensional subspace.  We call this subset matrix $E_{k\\times p}$.\n",
    "\n",
    "2. \"Best\" above means: projecting the dataset onto this subspace preserves the **most variance in the data** out of all possible such projections.\n",
    "\n",
    "3. The matrix multiplication $XE_{k\\times p}^T$ gives the \"best\" reconstruction of $X$ in the basis $E_k$ as a $k$ dimensional object. That is, this reconstructs $X$ as a matrix with $k$ columns.\n",
    "\n",
    "4. The matrix multiplication $(X E_{k\\times p}^T) E_{k \\times p}$ is a $n \\times p$ matrix where the sum of squared distances between it and the *centered* $X$ matrix are minimized, *for a given $k$*.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, the `explained_variance_` attribute contains the total variance of the *projected* dataset.\n",
    "\n",
    "Each of the numbers in `explained_variance_` is an *eigenvalue* of the covariance matrix $\\frac{1}{n}X^t X$.\n",
    "\n",
    "5. The **eigenvalues** of the covariance matrix measure the variance of the projection of the data onto the associated eigenvector (principal component)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### What Principal Component Analysis IS For\n",
    "\n",
    "So, **the purpose of PCA is to approximately reconstruct data sets as a lower dimensional object.**\n",
    "\n",
    "Applications of this include:\n",
    "\n",
    "  - Visualization.\n",
    "  - Clustering.\n",
    "  - Data Compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: The Best Scatterplot of Iris\n",
    "\n",
    "In Fischer's classic iris dataset, we describe flowers by four measurements, so it is a four dimensional representation of the flowers.\n",
    "\n",
    "![Iris Measurements](images/iris-measurements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's construct a \"best\" scatterplot of the iris dataset using PCA to project\n",
    "\n",
    "```\n",
    "Four dimensional iris data => Best two dimensional reconstruction\n",
    "                           => Best two dimensional scatterplot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In terms of the above discussion we are plotting the *expression of $X$ in the principal component basis*.\n",
    "\n",
    "$$ X E_2^t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "iris_type = iris.target\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcd = PCA(2).fit(iris_data.values)\n",
    "iris_reduced = pcd.transform(iris_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Double check that sklearn is doing what we think it is with .transform()\n",
    "iris_centered = iris_data.values - iris_data.values.mean(axis=0)\n",
    "iris_reduced_manual = np.dot(iris_centered, pcd.components_.T)\n",
    "\n",
    "np.all(iris_reduced_manual == iris_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "\n",
    "ax.scatter(iris_reduced[:, 0], iris_reduced[:, 1], \n",
    "           color=np.array([\"red\", \"green\", \"blue\"])[iris_type])\n",
    "ax.set_title(\"Scatterplot of Iris Data in PCA 2-Plane\")\n",
    "ax.set_xlabel(\"First Principal Component\")\n",
    "ax.set_ylabel(\"Second Principal Component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Eigenfaces: Reconstructing Faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As an example of *very* high dimensional data, we introduce the `faces` data set.\n",
    "\n",
    "An extended version of this example is available in the sklearn docs here: http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=154)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# Global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# Local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2.0 * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_gallery(\"Centered Olivetti faces\", faces_centered[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each of these images is stored as a numpy array.\n",
    "\n",
    "Each entry in the array measures *one* pixel intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "first_face = faces_centered[0].reshape(image_shape)\n",
    "print(first_face)\n",
    "plt.imshow(first_face, cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each image is a $64 \\times 64$ array, and so is represented as a $4096$ dimensional object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's apply PCA to the faces dataset to lower the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "faces_pcd = PCA(100).fit(faces_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The columns in the $E^t$ matrix (i.e. `faces_pcd.components_.T`) are called **eigenfaces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "for ax, i in zip(axs.flatten(), range(6)):\n",
    "    eigenface = faces_pcd.components_[i, :].reshape(image_shape)\n",
    "    ax.imshow(eigenface, cmap=cm.gray)\n",
    "    ax.set_title(\"{}'th Eigenface\".format(i))\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** What facial features seem to be captured in the first few eigenfaces?  Why do you think this may be so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's do our reconstruction procedure with the face data.  This lets us create **smaller dimensional faces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_face_data(n_dim):\n",
    "    eigenvalues = faces_pcd.components_[:n_dim, :].T\n",
    "    faces_reduced = np.dot(np.dot(faces_centered, eigenvalues), eigenvalues.T)\n",
    "    return faces_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(13, 6))\n",
    "\n",
    "for ax, dim in zip(axs.flatten(), [1, 2, 4, 10, 25, 50]):\n",
    "    reduced_data = reduce_face_data(dim)\n",
    "    first_face = reduced_data[0].reshape(image_shape)\n",
    "    ax.imshow(first_face, cmap=cm.gray)\n",
    "    ax.set_title(\"Face Reconstructed with {} PCA\\nComponents\".format(dim))\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Discussion:** When applying PCA to reduce the dimensionality of a data set, we have to choose a *number of dimensions* to keep.  What kind of concerns should we consider when choosing this number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Principal Components Analysis is NOT For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is an unfortunately popular method which combines PCA with regression, with the intent of improving the generalization properties of the regression.  It is called **Principal Component Regression** and it goes like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Setup**: You have a matrix $X$ and a response $y$, and you want to fit a regression to predict $y$ from $X$.\n",
    "\n",
    "**Procedure**:\n",
    "1. Do PCA on $X$, let $E$ be the matrix of principal components.\n",
    "2. Discard some of the principal components, get the matrix $E_{k\\times p}$.\n",
    "3. Regress $y$ on $E_{k\\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Discussion:** Critique this procedure, pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For details on why **not** to use selection of principal components as a regularization strategy in regression [see this paper](http://www.uvm.edu/~rsingle/stat380/F04/possible/Hadi%2BLing-AmStat-1998_PCRegression.pdf) and [this paper](http://automatica.dei.unipd.it/public/Schenato/PSC/2010_2011/gruppo4-Building_termo_identification/IdentificazioneTermodinamica20072008/Biblio/Articoli/PCR%20vecchio%2082.pdf).\n",
    "\n",
    "See also [this example](https://gist.github.com/lemonlaug/976543b650e53db24ab2) from our own Isaac Laughlin\n",
    "\n",
    "[This question](http://stats.stackexchange.com/questions/101485/examples-of-pca-where-pcs-with-low-variance-are-useful) gives real life examples of data sets where PCR fails because $y$ is only related to the **low variance** principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition - Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we just derived PCA\n",
    "\n",
    "We found the set of eigenvectors $\\{\\vec{e_i}\\}$ of the covariance matrix $M = \\frac{1}{n}X^T X$\n",
    "\n",
    "These satisfy the equation $M \\vec{e_i} = \\lambda_i \\vec{e_i}$, where $\\lambda_i$ is the variance along the direction defined by $\\vec{e_i}$\n",
    "\n",
    "Concatenating all these eigenvectors into a matrix $E$ gives us the matrix representing a rotation in feature space.\n",
    "\n",
    "$X$ in our new coordinates is $X' = XE$.\n",
    "\n",
    "Our covariance matrix in these new coordinates is: \n",
    "\n",
    "$M' = \\frac{1}{n}X'^T X' = \\frac{1}{n}(XE)^T(XE) = \\frac{1}{n}E^TX^TXE = E^TME$.\n",
    "\n",
    "It is possible to rewrite the above equation due to the fact that the eigenvectors that make up $E$ are orthogonal:\n",
    "\n",
    "$E M' E^T = E E^T M E E^T = M = \\frac{1}{n}X^T X $\n",
    "\n",
    "\n",
    "$$X^T X = E \\frac{M'}{n} E^T$$\n",
    "\n",
    "Moreover this looks like the definition of the eigendecomposition of a matrix, and we can conclude that $M'$ is of the form:\n",
    "\n",
    "$M' = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 \t& 0 \t & \\cdots \t  & 0 \t\\\\\n",
    "0 \t& \\lambda_2 \t & \\cdots \t  & 0 \t\\\\\n",
    "\\vdots \t& \\vdots \t & \\ddots & 0 \t\\\\\n",
    "0 \t& 0 & 0 \t  & \\lambda_p\t\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    " \n",
    "$M$ is a symmetric matrix, which means its *diagonalizable* so it can be written as $M = E L E^T$, where $L$ is of the form:\n",
    "\n",
    "$L = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 \t& 0 \t & \\cdots \t  & 0 \t\\\\\n",
    "0 \t& \\lambda_2 \t & \\cdots \t  & 0 \t\\\\\n",
    "\\vdots \t& \\vdots \t & \\ddots & 0 \t\\\\\n",
    "0 \t& 0 & 0 \t  & \\lambda_p\t\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$X$ in our new coordinates is $X' = XE$\n",
    "\n",
    "Our covariance matrix in these new coordinates is \n",
    "\n",
    "$M' = \\frac{1}{n}X'^T X' = \\frac{1}{n}(XE)^T(XE) = \\frac{1}{n}E^TX^TXE = E^TME$ \n",
    "\n",
    "\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a matrix factorization closely related to PCA, the Singular Value Decomposition.\n",
    "\n",
    "**Any** $n \\times p$ matrix $X$ can be factored as follows\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "Where\n",
    "\n",
    "  - $U$ is a $n \\times n$ orthogonal matrix.\n",
    "  - $\\Sigma$ is a $n \\times p$ matrix with non-zero entries on the diagonal *only*.\n",
    "  - $V$ is a $p \\times p$ orthogonal matrix.\n",
    "  \n",
    "One proof of the uniqueness of the SVD is [here](https://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-fall-2004/lecture-notes/lecture3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVD decomposition](images/svd_diagram.png)\n",
    "\n",
    "Image Source: https://en.wikipedia.org/wiki/Singular_value_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\Sigma$ matrix has positive diagonal entries, these are called the **singular values** of $X$, they are closely related to eigenvalues.\n",
    "\n",
    "So, for a $4 \\times 2$ matrix $X$, the singular value matrix would look like:\n",
    "\n",
    "$$ \\Sigma =  \\left( \\begin{array}{cc} \n",
    "\\sigma_1 & 0  \\\\       \n",
    "0 & \\sigma_2 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array} \\right) $$\n",
    "\n",
    "and for a $6 \\times 3$ like:\n",
    "\n",
    "$$ \\Sigma =  \\left( \\begin{array}{cc} \n",
    "\\sigma_1 & 0 & 0  \\\\       \n",
    "0 & \\sigma_2 & 0 \\\\\n",
    "0 & 0 & \\sigma_3 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{array} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we factor $X$ into its singular value decomposition\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "We can plug this into $X^T X$ and derive a relationship between PCA and the SVD factorization.\n",
    "\n",
    "$$ X^T X = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T $$\n",
    "\n",
    "But $U$ is orthogonal, so the inner $U^T U$ is the identity matrix:\n",
    "\n",
    "$$ X^T X = V \\Sigma^T \\Sigma V^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two expressions for $X^t X$\n",
    "\n",
    "#### PCA\n",
    "\n",
    "$$ X^T X = E \\frac{M'}{n} E^T $$\n",
    "\n",
    "#### SVD\n",
    "\n",
    "$$ X^T X = V \\Sigma^T \\Sigma V^T $$\n",
    "\n",
    "We deduce the following\n",
    "\n",
    "#### Relationship between PCA and SVD\n",
    "\n",
    "$$ E^T = V $$\n",
    "$$ M' = \\Sigma^T \\Sigma $$\n",
    "\n",
    "So the **eigenvectors** from PCA are the **columns** of the matrix V.\n",
    "\n",
    "And, the **eigenvalues** from PCA are the **squares of the singular values**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency of PCA vs. SVD\n",
    "\n",
    "Suppose our data set $X$ contains grayscale image data:\n",
    "\n",
    "  - $500$ images.\n",
    "  - Each image is $200 \\times 200$ pixels.  So, $40,000$ total pixels.\n",
    "  \n",
    "Therefore, $X$ is a $500 \\times 40,000$ matrix.  Many, many more columns than rows.\n",
    "\n",
    "**Note:** This is commonly called the $p \\gg n$ situation.\n",
    "\n",
    "Now, **PCA** needs to compute the matrix $X^T X$.\n",
    "\n",
    "This is a $40,000 \\times 40,000$ matrix, so it has $1,600,000,000 = 1.6 \\times 10^9$ entries.\n",
    "\n",
    "One floating point number is:\n",
    "  - $64$ bits (on a modern computer)\n",
    "  - $8$ bytes (each byte is $8$ bits).\n",
    "  \n",
    "So, $X^T X$ takes\n",
    "\n",
    "$$ 8 \\times 1.6 \\times 10^9 = 12.8 \\times 10^9 $$\n",
    "\n",
    "bytes of data to store.  This is $12.8$ gigabytes.  **A huge amount of data**.\n",
    "\n",
    "SVD **does not have to compute the $X^T X$ matrix, it operates completely on $X$ itself**.  Much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for topic analysis\n",
    "\n",
    "We can use SVD to determine what we call ***latent features***. This will be best demonstrated with an example.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie.\n",
    "\n",
    "|       | Matrix | Alien | StarWars | Casablanca | Titanic |\n",
    "| ----- | ------ | ----- | -------- | ---------- | ------ |\n",
    "| **Alice** |      1 |     2 |        2 |          0 |      0 |\n",
    "|   **Bob** |      3 |     5 |        5 |          0 |      0 |\n",
    "| **Cindy** |      4 |     4 |        4 |          0 |      0 |\n",
    "|   **Dan** |      5 |     5 |        5 |          0 |      0 |\n",
    "| **Emily** |      0 |     2 |        0 |          4 |      4 |\n",
    "| **Frank** |      0 |     0 |        0 |          5 |      5 |\n",
    "|  **Greg** |      0 |     1 |        0 |          2 |      2 |\n",
    "\n",
    "Note that the first three movies (Matrix, Alien, StarWars) are Sci-fi movies and the last two (Casablanca, Titanic) are Romance. We will be able to mathematically pull out these topics!\n",
    "\n",
    "Let's do the computation with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating_matrix = np.array([\n",
    "              [1, 2, 2, 0, 0],\n",
    "              [3, 5, 5, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [5, 5, 5, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 1, 0, 2, 2]])\n",
    "\n",
    "movies = ['TheMatrix', 'Alien', 'StarWars', 'Casablanca', 'Titanic']\n",
    "users = ['Alice', 'Bob', 'Cindy', 'Dan', 'Emily', 'Frank', 'Greg']\n",
    "ratings_df = pd.DataFrame(rating_matrix, index=users, columns=movies)\n",
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy can compute the full SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, sigma, VT = svd(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, sigma, VT = (np.around(x, 2) for x in (U, sigma, VT))\n",
    "\n",
    "U_df = pd.DataFrame(U, index=users)\n",
    "VT_df = pd.DataFrame(VT, columns=movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(U_df.shape)\n",
    "print(sigma.shape)\n",
    "print(VT_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VT matrix has columns indexed by movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(VT_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the *rows* of this matrix as some latent features.  There is *something* about these movies that is\n",
    "\n",
    "  - Being measured in each row.\n",
    "  - Being inferred from how users rated the movies as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the **first row** seems to draw a strong distinction between science fiction and romantic movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U matrix has rows indexed by *users*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(U_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the rows in this matrix contains *weights*, they measure how important each *latent feature* is to each user's ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** that the last three columns of this matrix *do not matter* for reconstructing X.  These are the columns for which the associated rows of the singular value matrix are **completely zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to figure out an approximation of the user's rating of the movies using **only** the first two latent features, we can zero out all but the first two singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Zero out all but the first two singular values\n",
    "sigma_reduced = np.zeros(rating_matrix.shape)\n",
    "np.fill_diagonal(sigma_reduced, sigma)\n",
    "sigma_reduced[:, 2:] = 0\n",
    "\n",
    "# Reoconstruct the ratings matrix\n",
    "ratings_reconstructed = np.dot(np.dot(U, sigma_reduced), VT)\n",
    "\n",
    "ratings_reconstructed_df = pd.DataFrame(ratings_reconstructed, index=users, columns=movies)\n",
    "print(ratings_reconstructed_df), '\\n'\n",
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a pretty good reconstruction of the ratings matrix, even though we only kept two singular values.\n",
    "\n",
    "If we investigate, the first two singular values are by far the largest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeroing out some of our singular values to reduce dimensionality is known as formally Truncated SVD. \n",
    "\n",
    "One issue that is common to Truncated SVD is that we seem to have picked up some **negative** ratings, which is weird.  Tomorrow we will study a procedure to fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
