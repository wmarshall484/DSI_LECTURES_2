{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sampling\n",
    "\n",
    "Chris Overton  \n",
    "2016.09.20  \n",
    "Adapted from presentations most recently by Brian Mann and Darren Reger\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This afternoon you will\n",
    "\n",
    "* Discover the Central Limit Theorem\n",
    "* Choose a sampling strategy (e.g. simple, stratified, clustered...) for targeted understanding of a population\n",
    "* Apply the Central Limit Theorem to construct confidence intervals for the mean of a population\n",
    "* Use bootstrapping to construct confidence intervals for any population statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Central Limit Theorem (CLT)\n",
    "\n",
    "One of the most important results in classical statistical inference is the *Central Limit Theorem* which says that if $X_1, X_2, \\ldots, X_n$ are i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2$ then their mean $$\\bar{X} = \\frac{X_1 + \\cdots + X_n}{n}$$ is approximately normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$ $$\\bar{X} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})$$  \n",
    "\n",
    "This is a really cool result because:  \n",
    "- Averages of enough of almost any distribution start to look normal\n",
    "- To improve accuracy by a factor of n, all you have to do is sample $n^2$ as much data :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CLT: Danger!!!\n",
    "- Just because the averages appear normal doesn't mean underlying data are!\n",
    "- Convergence to multivariate normal can take much longer than you wish it did!\n",
    "\n",
    "So sometimes, a different underlying distribution is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - CLT\n",
    "\n",
    "Recall that $Binom(n, p)$ is the sum of $n$ independent Bernoulli trials with parameter $p$. This means that $$Binom(n, p) \\sim N\\left(np, \\sqrt{np(1-p)}\\right)$$\n",
    "\n",
    "Why??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CLT demo\n",
    "In class, we showed results of averages of n values for each of two dice, each of whose six sides had equal probability.    \n",
    "die_1 has sides [1,2,3,4,5,6], and so mean 3.5 and variance 2.9  \n",
    "die_2 has sides [1,1,1,1,2,15], and so again mean 3.5, but variance 26.6  \n",
    "\n",
    "Key demo findings:  \n",
    "* As n increases from 1 to 2 to low single digits:\n",
    "    - The histogram of die_1 sums goes from flat to triangular to approximating a normal distribution\n",
    "    - The histogram of die_2 is right-skewed, and splits into pieces, each of which is also right-skewed\n",
    "* Eventually, as n increases say to 100, even die_2 sums start to resemble a normal distribution, but compared to a normal distribution with the correct mean and standard deviation, the die_2 histogram is still much more 'wiggly.'\n",
    "* If average are used instead of sums, you see the 'width' of histograms decrease by about the square root of n.  \n",
    "* When plotted with mean, +/- 1 * std, and +/- 2 * std, as n increases, you eventually see about 68% of the pmf within 1 std of mean, and roughly 95% within 2 std of mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.583333333333332"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "die_1 = np.array([1,2,3,4,5,6])\n",
    "die_2 = np.array([1,1,1,1,2,15])\n",
    "np.var(die_2, ddof = 0)\n",
    "#... simulation shown was done in Mathematica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Discovery in General\n",
    "\n",
    "1. Ask a question\n",
    "2. Design an experiment\n",
    "3. Collect data (Sampling)\n",
    "4. Analyze data (Estimation/Inference)\n",
    "5. Repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Make sure you have good data!\n",
    "\n",
    "Your results are only as good as your data. Garbage in, garbage out.\n",
    "\n",
    "* Your data should be representative of the population\n",
    "* Important to make sure there is no bias when designing your experiment or \"randomly\" sampling\n",
    "* For example, if you want to estimate the average height of a person in the US, but all the people you measure are in the 90th percentile for weight, something is wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaches to sampling  \n",
    "The CLT shows that we can get 'arbitrarily close to' population statistics by taking appropriate samples.  \n",
    "\n",
    "In real life, one likely has to work from a subset of data:\n",
    "* This may be due to cost limitations\n",
    "* Even in the age of 'big data', where it is tempting to think one is working with 'all' data, at the very least, this is probably limited by time - often in rapidly changing environments.  \n",
    "\n",
    "It is important to understand possible biases in a given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaches to sampling \n",
    "Here, we focus on the mechanics of how to select a sample in the first place.  \n",
    "\n",
    "Four common approaches are:  \n",
    "- Simple random sample  \n",
    "- Stratified random sample  \n",
    "- Cluster sample\n",
    "- A hybrid approach sutomized to specific data and limitations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple random sampling\n",
    "\n",
    "The most common way to sample from a population is called *simple random sampling*\n",
    "\n",
    "* Each subject has an equal chance of being selected from the population\n",
    "* If your population is $x_1, \\ldots, x_n$, to sample choose a number uniformly at random from $1, \\ldots, n$ and select that observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stratified samples\n",
    "it is often important to have statistical power in each of several known categories, such as for patients with k particular diseases. If one just sampled randomly from the population, it might take a very large sample so that the subet with a given disease has a large enough $n_i$ so variance within that group is small.  \n",
    "\n",
    "In this case, one could insist on having say 50 patients each with no disease and with each of the k diseases.\n",
    "\n",
    "Stratified sampling can be performed over multiple variables, but then the number of strata (layers) can become too large to manage easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster samples\n",
    "\n",
    "Say one wants to know how different Galvanize cohorts have been doing.\n",
    "\n",
    "One possibility would be to release a survey to all students, and do analysis with whoever responds.\n",
    "\n",
    "Problem with this: nonresponder bias!\n",
    "\n",
    "To improve this, one can target certain subsets (e.g specific cohorts), and then work harder to try to obtain complete coverage of these.  \n",
    "\n",
    "Advantage:  \n",
    "- Might get better response rates, such as due to limitation in time/space\n",
    "\n",
    "Disadvantage:\n",
    "- A particular cohort could be nonrepresentative, say due to unusual recruiting or temporary circumstances\n",
    "\n",
    "In summary: cluster sampling samples with clusters to consider, but then tries to get complete (non-sampled) coverage of these entire clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence Intervals (1/2)\n",
    "\n",
    "A *confidence interval* is an interval estimate of the true parameter of your population\n",
    "\n",
    "* An $\\alpha$ confidence interval is an interval centered around estimated parameter which contains the true value of that parameter with *confidence* $\\alpha$ ($\\alpha$ is usually 99%, 95%, 90%, or 80%)\n",
    "* In other words, if you resample or rerun the experiment many times, $\\alpha$ percent of the time the true value will be in the computed confidence interval\n",
    "* It is *not* a statement that the true value of the parameter is contained in the interval with a certain probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence Intervals (2/2)\n",
    "\n",
    "For $n \\geq 30$ a 95% confidence interval for the mean is $$(\\bar{x} - 1.96\\frac{\\sigma}{\\sqrt{n}}, \\bar{x} + 1.96\\frac{\\sigma}{\\sqrt{n}})$$\n",
    "\n",
    "* Why??\n",
    "* Since we don't know $\\sigma$, use the sample standard deviation $s$ instead\n",
    "* If $n$ is small, the central limit theorem does not guarantee normality. We need a $t$-distribution instead $$\\bar{x} \\pm t_{(\\alpha/2, n-1)}\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "## Check for Mastery\n",
    "\n",
    "Using Python, sample $100$ times from a normal distribution. Compute the sample mean and a 95\\% confidence interval. Is the true mean in your interval?!? Rerun your code several time and see if you find an interval which doesn't contain the true mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping (1/2)\n",
    "\n",
    "Another way to generate confidence intervals for a population parameter is through a process called bootstrapping\n",
    "\n",
    "* Simple idea: sample from your observed data *with replacement* $B$ times\n",
    "* With these $B$ samples, compute the statistic (i.e. mean, median, variance, etc...) of interest and then estimate the sample variance\n",
    "* Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping (2/2)\n",
    "\n",
    "How to bootstrap:\n",
    "\n",
    "Start with $n$ i.i.d. samples $X_1, \\ldots, X_n$.\n",
    "\n",
    "For $i$ from $1$ to $B$:\n",
    "\n",
    "1. Sample $X_1^*, \\ldots, X_n^*$ with replacement from your data\n",
    "2. Compute your sample statistic $\\theta_i^* = g(X_1^*, \\ldots, X_n^*)$\n",
    "\n",
    "Then compute $$v_{boot} =\\frac{1}{B-1}\\sum_{b=1}^B \\left( \\theta_b^* - \\frac{1}{B} \\sum_{r=1}^B \\theta_r^*\\right)^2$$\n",
    "\n",
    "which is the sample variance of your statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap Confidence Intervals (The Normal Interval)\n",
    "\n",
    "There are a few different ways to build bootstrap confidence intervals that rely of differing assumptions. The first is the *normal interval*\n",
    "\n",
    "* If your parameter is approximately normally distributed (like the mean of a sample with $n > 30$) your interval will be $$\\theta_n \\pm z_{\\alpha/2} \\hat{se}_{boot}$$ where $\\theta_n = g(X_1, \\ldots, X_n)$ is your estimate of the parameter, $z$ is standard normal (e.g. for 95% it is 1.96), and $\\hat{se}_{boot} = \\sqrt{v_{boot}}$ is the bootstrap estimated standard error of your parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap Confidence Intervals (Percentile Method)\n",
    "\n",
    "Let $\\theta^*_{\\beta}$ be the $\\beta$ sample quantile of your bootstrap sample statistics $(\\theta_1^*, \\ldots \\theta_B^*)$. Then an $1-\\alpha$ bootstrap percentile interval is $$C_n = (\\theta^*_{\\alpha/2}, \\theta^*_{1-\\alpha/2})$$\n",
    "\n",
    "## Why Bootstrap?\n",
    "\n",
    "Why would we use bootstrapping over standard confidence intervals?\n",
    "\n",
    "* Small sample size\n",
    "* The distribution of the statistic is complicated or hard to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
