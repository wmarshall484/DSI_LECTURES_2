{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# $$\\textit{inference}$$\n",
    "$$\\text{Schwartz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### todos:\n",
    "- parameters/expectations versus statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is it $n-1$? \n",
    "### The proof for $E[S^2] = E\\left[\\sum_{i=1}^n \\frac{(X_i-\\bar X)^2}{n-1}\\right] = \\sigma^2$ \n",
    "\n",
    "<img src=\"stuff/inmunusone.png\" width=\"900px\" align=\"left\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "\n",
    "## Friday you did the \"forward problem\" of the discipline of statistcs:\n",
    "### Given distributions and parameters, you made some statements about data (i.e., random variables) \n",
    "\n",
    "\n",
    "## Today we do the \"inverse problem\" of the discipline of statistics:\n",
    "### Given data (i.e., random variables) we say something about distributions and their parameters \n",
    "\n",
    "\n",
    "# Objectives\n",
    "\n",
    "\n",
    "\n",
    "- _Populations_ (i.e., Distributions and their Parameters) versus _Samples_ (statistics) \n",
    "- Distribution Estimation (point estimation)\n",
    "    - Kernel Density Estimation\n",
    "    - Method of Moments\n",
    "    - Maximum Likelihood Estimation\n",
    "\n",
    "- Inference (uncertainty estimation)\n",
    "    - Central Limit Theorem\n",
    "    - Confidence Intervals\n",
    "    - Bootstrapping \n",
    "        - Bootstrapped Confidence Intervals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### don't write import scipy.stats as scs -- it's *stupid*\n",
    "# from scipy import stats\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### tab complete and ? and google\n",
    "stats.poisson.rvs(mu=10, size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more general code\n",
    "# sample = getattr(stats, \"poisson\").rvs(mu=10, size=30)\n",
    "pars = {'mu': 10, 'size': 30}\n",
    "sample = getattr(stats, \"poisson\").rvs(**pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sample(n=30, model=\"uniform\", pars={\"loc\":0, \"scale\":1}, population=None):\n",
    "    \"\"\"Generate a sample of size \"n\" [int] from any \"model\" [string] \n",
    "    specified by \"pars\" [dictionary of parameter values]; or, \n",
    "    specifiy a population of numbers to sample from [list]\"\"\"\n",
    "    \n",
    "    if population is not None:\n",
    "        return np.random.choice(population, n)\n",
    "    else:    \n",
    "        return getattr(stats,model).rvs(size=n,**pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print generate_sample()\n",
    "sample = generate_sample(30, \"poisson\", {\"mu\":10})\n",
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get used to these -- you'll use them for like *every* jupyter notebook evar\n",
    "### \"%matplotlib inline\" allows inline figures (and plt.show() isn't needed)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convenience so you don't have to call e.g., \"plt.figure(figsize=[8,4])\" every time\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Here is exactly what the data is\n",
    "fig = plt.figure(figsize = [12, 5])\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(sample, bins = 7)\n",
    "plt.xlim([0,30])\n",
    "\n",
    "### Histograms require bin specifications...\n",
    "from collections import Counter\n",
    "value, count = zip(*Counter(sample).items())\n",
    "value_proportion = np.array(count)/float(sum(count))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.vlines(value, 0, count)\n",
    "plt.title(\"Sample\")\n",
    "plt.xlabel(\"Observed Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim([0,30])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_sample(sample, support, smooth=None, width=20):\n",
    "    '''Arguments sample and support are required.\n",
    "    Raw data, histogram, and kernel density estimation representations are available.\n",
    "\n",
    "    Integer valued data points are treated as a sample from a discrete distribution.\n",
    "    Real valued data points are treated as a sample from a continuous distribution\n",
    "    Discrete data is ploted in think blue; continuous data is plotted in thin red.\n",
    "        \n",
    "    support: tuple (of min and max range to examine)\n",
    "    sample: list (of samples)\n",
    "    smooth: text (\"hist\" or \"kde\")\n",
    "    width: number (bin width for \"hist\", kernel sd for \"kde\")'''\n",
    "    \n",
    "    value, count = zip(*Counter(sample).items())\n",
    "    count = np.array(count)\n",
    "    value_proportion = count/float(sum(count))\n",
    "\n",
    "    if all(map(lambda x: issubclass(type(x), int), sample)):\n",
    "        linecolor = 'b'; linewidth = 5; support = range(*support)\n",
    "    else:\n",
    "        linecolor = 'r'; linewidth = 1; support = np.linspace(*support, num=500)\n",
    "        \n",
    "    if smooth is \"hist\":\n",
    "        plt.hist(sample, bins=int(np.ptp(support)/width), color=linecolor)\n",
    "    elif smooth is \"kde\":\n",
    "        #stats.norm scale parameter specifies the *standard deviation* \n",
    "        density=[sum(stats.norm.pdf(sample, loc=mu, scale=width)) for mu in support]\n",
    "        plt.plot(support,np.array(density)/float(len(sample)), color=linecolor, lw=linewidth)\n",
    "    else:\n",
    "        plt.vlines(value, 0, value_proportion, colors=linecolor, lw=linewidth, alpha=.5)     \n",
    "        \n",
    "    plt.xlim([min(support)-1,max(support)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(model, support, pars, colr = 'k', tf=lambda x: x):\n",
    "    '''model: text of distribution name\n",
    "    pars: dictionary of distribution parameters\n",
    "    support: tuple of limits of the range of the distribution to examine\n",
    "    tf: transforms densities by applying a function to the distribution value\n",
    "     -- e.g., scale or log transform the value\n",
    "    '''\n",
    "    if issubclass(type(getattr(stats,model)), stats.rv_continuous):\n",
    "        support = np.linspace(*support, num=200)\n",
    "        plt.plot(support, tf(getattr(stats,model).pdf(support, **pars)), color=colr, lw=1)\n",
    "    else:\n",
    "        support = np.arange(*support)\n",
    "        plt.vlines(support, 0, getattr(stats,model).pmf(support, **pars), colors=colr, lw=1)\n",
    "        plt.plot(support,getattr(stats,model).pmf(support, **pars), color=colr, ls='', ms=10, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_plot(xlab, ylab, title):\n",
    "    '''xlab, ylab, title, in that order'''\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"poisson\"\n",
    "pars = {\"mu\":10}\n",
    "n = 30\n",
    "sample = generate_sample(n, model, pars)\n",
    "support = [0, 30]\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_distribution(model, support, pars)\n",
    "label_plot(\"Random Variable Outcome\", \"Probability\", model + \" distribution \" + str(pars))\n",
    "    \n",
    "plt.subplot(122)\n",
    "plot_sample(sample, support)\n",
    "label_plot(\"Observed Value\", \"Proportion\",\"i.i.d. sample (n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = generate_sample(n, model, pars)\n",
    "plot_sample(sample, support)\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"Observed Value\")\n",
    "plt.title(\"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "\n",
    "model = \"poisson\"\n",
    "pars1 = {\"mu\": 9}\n",
    "plot_distribution(model, support, pars1)\n",
    "pars2 = {\"mu\": 10}\n",
    "plot_distribution(model, support, pars2, 'r')\n",
    "plt.legend([model+str(pars1),model+str(pars2)], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"expon\"\n",
    "pars = {\"scale\":10}\n",
    "n = 30\n",
    "sample = generate_sample(n,model,pars)\n",
    "support = [0, 30]\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_distribution(model, support, pars)\n",
    "label_plot(\"Random Variable Outcome\", \"Density\", model + \" distribution \" + str(pars))\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_sample(sample, support)\n",
    "label_plot(\"Observed Value\", \"Proportion\", \"i.i.d. sample (n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plot_distribution(model, support, pars)\n",
    "label_plot(\"Random Variable Outcome\", \"Density\", model + \" distribution \" + str(pars))\n",
    "\n",
    "plt.subplot(122)\n",
    "bw=3\n",
    "sample = generate_sample(n,model,pars)\n",
    "plot_sample(sample, support, smooth='hist', width=bw)\n",
    "label_plot(\"Observed Value\", \"Frequency\", \"i.i.d. sample (n = \" + str(n) + \", histogram bin width \" + str(bw) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plot_distribution(model, support, pars)\n",
    "label_plot(\"Random Variable Outcome\", \"Density\", model + \" distribution \" + str(pars))\n",
    "\n",
    "plt.subplot(122)\n",
    "bw=2\n",
    "sample = generate_sample(n,model,pars)\n",
    "plot_sample(sample, support, smooth = 'kde', width=bw)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \", kde kernel sd \" + str(bw) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demonstrate comment command\n",
    "plot_sample(sample, support)\n",
    "\n",
    "sd=2\n",
    "for x in sample:\n",
    "    plot_distribution(\"norm\", [-10, 50], {\"loc\":x, \"scale\":sd}, tf=lambda x: np.array(x)/float(len(sample)))\n",
    "    \n",
    "plot_sample(sample, support, smooth='kde', width=sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = generate_sample(n,model,pars)\n",
    "plot_sample(sample, support)\n",
    "\n",
    "plot_sample(sample, support, smooth='kde', width=2)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \", kde kernel sd \" + str(bw) + \")\")\n",
    "\n",
    "pars1 = {\"scale\":9}\n",
    "plot_distribution(model, support, pars1)\n",
    "pars2 = {\"scale\":15}\n",
    "plot_distribution(model, support, pars2,'b')\n",
    "plt.legend([\"data\", model+str(pars1),model+str(pars2)], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# For a _Poisson distributed_ random variable $X$\n",
    "\n",
    "# $\\begin{align} \n",
    "Pr(X=x|\\lambda) &= \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n",
    "\\text{ we have that } &\\\\\n",
    "E[X] &= \\sum_{x=0}^{\\infty}x \\cdot Pr(X=x|\\lambda) \\\\\n",
    "&= \\sum_{x=1}^{\\infty}x \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n",
    "&= \\lambda \\;& (= \\mu)\\\\\n",
    "{}\\\\\n",
    "Var[X] &=\\sum_{x=0}^{\\infty}(x-E[X])^2 \\cdot Pr(X=x|\\lambda) \\\\\n",
    "&= \\sum_{x=1}^{\\infty}(x-E[X])^2 \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n",
    "&= \\lambda \\; &(= \\sigma^2)\\\\\n",
    "\\end{align}$\n",
    "\n",
    "<br>\n",
    "\n",
    "### (so, interestingly, the expected value and variance of a Poisson random variable $X$ are the same)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# Consider these two estimators:\n",
    "# $$\\begin{align} \n",
    "\\bar X &= \\frac{1}{n}\\sum_{i=1}^n X_i \\\\\n",
    "S^2 &= \\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2 \n",
    "\\end{align}$$\n",
    "# Their expected values are:\n",
    "# $$\\begin{align} \n",
    "E[\\bar X] &= E[X] \\\\\n",
    "E[S^2] &= Var[X] \n",
    "\\end{align}$$\n",
    "## So $\\bar X$ and $S^2$ are, in some sense, good estimators of $E[X]$ and $Var[X]$, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"poisson\"\n",
    "pars = {\"mu\":10}\n",
    "n = 30\n",
    "sample = generate_sample(n,model,pars)\n",
    "support = [0, 30]\n",
    "\n",
    "plt.figure(figsize=[14,5])\n",
    "plt.subplot(131)\n",
    "plot_sample(sample, support)\n",
    "label_plot(\"Observed Value\", \"Proportion\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "pars1 = {\"mu\":round(np.mean(sample),2)}\n",
    "plot_distribution(model, support, pars1)\n",
    "plt.legend([model+\"\\n\"+str(pars1)+\"\\n(sample mean)\"], loc='best')\n",
    "\n",
    "plt.subplot(132)\n",
    "plot_sample(sample, support)\n",
    "label_plot(\"Observed Value\", \"Proportion\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "pars2 = {\"mu\":round(np.var(sample, ddof=1),2)}\n",
    "plot_distribution(model, support, pars2)\n",
    "plt.legend([model+\"\\n\"+str(pars2)+\"\\n(sample variance)\"], loc='best')\n",
    "\n",
    "plt.subplot(133)\n",
    "plot_sample(sample, support)\n",
    "label_plot(\"Observed Value\", \"Proportion\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "pars3 = {\"mu\": 10}\n",
    "plot_distribution(model, support, pars3)\n",
    "plt.legend([model+\"\\n\"+str(pars3)+\"\\n(arbitary choice)\"], loc='best')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For a _Exponentially distributed_ random variable $X$\n",
    "\n",
    "# $\\begin{align} \n",
    "f(X=x|\\lambda) &= \\lambda e^{-\\lambda x}\\\\\n",
    "\\text{ we have that } &\\\\\n",
    "E[X] &= \\int_{x=0}^{\\infty}x f(X=x|\\lambda) dx \\\\\n",
    " &= \\int_{x=0}^{\\infty}x \\lambda e^{-\\lambda x} dx \\\\\n",
    " &= \\lambda^{-1} \\; &(= \\mu) \\\\\n",
    "Var[X] &=\\int_{x=0}^{\\infty}(x-E[X])^2 \\cdot f(X=x|\\lambda) dx \\\\\n",
    "&= \\int_{x=1}^{\\infty}(x-E[X])^2 \\lambda e^{-\\lambda x} dx \\\\\n",
    "&= \\lambda^{-2} \\;&(= \\sigma^2)\\\\\n",
    "\\end{align}$\n",
    "\n",
    "### so the $E[X]$ and $Var[X]$ of an exponential random variable $X$ are simple functions of the parameter $\\lambda$ \n",
    "\n",
    "### [The exponential distribution can be parameterized using the ''rate'' $\\lambda$ or the ''scale'' $\\lambda^{-1}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"expon\"\n",
    "pars = {\"scale\":10}\n",
    "n = 30\n",
    "sample = generate_sample(n,model,pars)\n",
    "support = [0, 30]\n",
    "bw=3\n",
    "\n",
    "plt.subplot(131)\n",
    "pars1 = {\"scale\":round(np.mean(sample),2)}\n",
    "plot_distribution(model, support, pars1)\n",
    "plt.legend([model+\"\\n\"+str(pars1)], loc='best')\n",
    "plot_sample(sample, support)\n",
    "plot_sample(sample, support, \"kde\", bw)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "plt.ylim([0, .12])\n",
    "\n",
    "plt.subplot(132)\n",
    "pars2 = {\"scale\":round(np.sqrt(np.var(sample, ddof=1)),2)}\n",
    "plot_distribution(model, support, pars2)\n",
    "plt.legend([model+\"\\n\"+str(pars2)], loc='best')\n",
    "plot_sample(sample, support)\n",
    "plot_sample(sample, support, \"kde\", bw)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "plt.ylim([0, .12])\n",
    "\n",
    "plt.subplot(133)\n",
    "pars3 = {\"scale\":10}\n",
    "plot_distribution(model, support, pars3)\n",
    "plt.legend([model+\"\\n\"+str(pars3)], loc='best')\n",
    "plot_sample(sample, support)\n",
    "plot_sample(sample, support, \"kde\", bw)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")\n",
    "plt.ylim([0, .12])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Method of Moments_\n",
    "\n",
    "# 1. find a statistic estimating the parameter\n",
    "# 2. calculate the statistic from the sample \n",
    "# 3. \"plug and play\" in the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# And now for an *alternative* to *Method of Moments*: Maximum Likelihood\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poisson_pmf(x, lamda):\n",
    "    return (lamda**x) * np.exp(-lamda)/math.factorial(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# poisson_pmf(x=10, lamda=np.array([1,2,4]))\n",
    "poisson_pmf(x=10,lamda=np.array([8,9,10,11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exponential_pdf(x, lamda):\n",
    "    return lamda * np.exp(-lamda * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exponential_pdf(x=10, lamda=np.array([1,2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 30, 100)\n",
    "x1 = 10\n",
    "x2 = 6\n",
    "x3 = 14\n",
    "plt.plot(lambdas, poisson_pmf(x=x1, lamda=lambdas))\n",
    "plt.plot(lambdas, poisson_pmf(x=x2, lamda=lambdas))\n",
    "plt.plot(lambdas, poisson_pmf(x=x3, lamda=lambdas))\n",
    "label_plot(\"$\\lambda$\", \"Poisson Likelihood\", \"What $\\lambda$ is best for each $x$? And what $\\lambda$ is best overall?\")\n",
    "plt.legend([\"x=\"+str(x1), \"x=\"+str(x2), \"x=\"+str(x3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_over_lambdas = np.linspace(0,100,100)[1:]\n",
    "x1=10\n",
    "x2=20\n",
    "x3=5\n",
    "plt.plot(one_over_lambdas, exponential_pdf(x=10, lamda=1/one_over_lambdas))\n",
    "plt.plot(one_over_lambdas, exponential_pdf(x=20, lamda=1/one_over_lambdas))\n",
    "plt.plot(one_over_lambdas, exponential_pdf(x=5, lamda=1/one_over_lambdas))\n",
    "label_plot(\"$\\lambda$\", \"Exonential Likelihood\", \"What $\\lambda$ is best for each $x$? And what $\\lambda$ is best overall?\")\n",
    "plt.legend([\"x=\"+str(x1), \"x=\"+str(x2), \"x=\"+str(x3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# So how do we get the likelihood of *all the data* for the above plots?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# The Poisson distribution likelihood for parameter $\\lambda$ is\n",
    "\n",
    "# $$\\begin{align}\n",
    " &\\; Pr(X_1=x_1, X_2=x_2, \\cdots, X_n=x_n|\\lambda) \\\\\n",
    "=&\\; Pr(X_1=x_1|\\lambda)Pr(X_2=x_2|\\lambda)\\cdots Pr(X_n=x_n|\\lambda)\\\\\n",
    "=&\\; \\prod_{i=1}^n Pr(X_i=x_i|\\lambda)\\\\\n",
    "=&\\; \\prod_{i=1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\\\\n",
    "=&\\; \\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!}\\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!}\\cdots\\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!}\\\\\n",
    "=&\\; L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n)\n",
    "\\end{align}$$\n",
    "### since $ f(X=x|\\lambda) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}$ and we assume observations are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The exponential distribution likelihood for rate parameter $\\lambda$\n",
    "\n",
    "# $$\\begin{align}\n",
    "&\\; f(X_1=x_1, X_2=x_2, \\cdots, X_n=x_n|\\lambda) \\\\\n",
    "=&\\; f(X_1=x_1|\\lambda)f(X_2=x_2|\\lambda)\\cdots f(X_n=x_n|\\lambda)\\\\\n",
    "=&\\; \\prod_{i=1}^n f(X_i=x_i|\\lambda)\\\\\n",
    "=&\\; \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}\\\\\n",
    "=&\\; \\lambda e^{-\\lambda x_1}\\lambda e^{-\\lambda x_2}\\cdots\\lambda e^{-\\lambda x_n}\\\\\n",
    "=&\\; L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n)\n",
    "\\end{align}$$\n",
    "\n",
    "### since $ f(X=x|\\lambda) = \\lambda e^{-\\lambda x}$ and we assume observations are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Diversion: how can one find the *maximum* of a function?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plot_distribution(\"norm\", [-2.5, 2.5], {\"loc\":0, \"scale\":1})\n",
    "label_plot(\"\", \"\", \"'some function'\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_distribution(\"norm\", [-2.5, 2.5], {\"loc\":0, \"scale\":1}, tf=lambda x: np.log(np.array(x)))\n",
    "label_plot(\"\", \"\", \"Log of 'some function'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# $\\textit{\"The log of the product is the sum of the logs\", so}$\n",
    "\n",
    "# The Poisson distribution $log$ likelihood for parameter $\\lambda$ is\n",
    "\n",
    "# $$\\begin{align}\n",
    "&\\; log L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n) \\\\\n",
    "=&\\; log \\left(\\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!} \\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!} \\cdots \\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!}\\right)\\\\\n",
    "=&\\; log \\left(\\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!}\\right)+log\\left(\\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!}\\right)+\\cdots +log\\left(\\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!}\\right)\\\\\n",
    "=&\\; \\sum_{i=1}^n log \\left(\\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\right)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "### $\\text{since } L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n) = \\prod_{i=1}^n\\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} = \n",
    "\\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!}\\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!}\\cdots\\frac{\\lambda^{x_n} e^{-\\lambda}}{x_n!}$\n",
    "\n",
    "\n",
    "# The exponential distribution $log$ likelihood for rate $\\lambda^{-1}$ is\n",
    "\n",
    "# $$\\begin{align}\n",
    "&\\; log L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n)\\\\\n",
    "=&\\; log\\left(\\lambda e^{-\\lambda x_1} \\lambda e^{-\\lambda x_2} \\cdots \\lambda e^{-\\lambda x_n}\\right)\\\\\n",
    "=&\\; log\\left(\\lambda e^{-\\lambda x_1}\\right)+log\\left(\\lambda e^{-\\lambda x_2}\\right)+\\cdots+ log\\left(\\lambda e^{-\\lambda x_n}\\right)\\\\\n",
    "=&\\; \\sum_{i=1}^n log\\left(\\lambda e^{-\\lambda x_i}\\right)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "### $\\text{since } L(\\lambda|X_1=x_1, X_2=x_2, \\cdots, X_n=x_n) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda e^{-\\lambda x_1}\\lambda e^{-\\lambda x_2}\\cdots\\lambda e^{-\\lambda x_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglikelihood(sample, model, pars):\n",
    "    if issubclass(type(getattr(stats,model)), stats.rv_continuous):\n",
    "        ll = lambda x: np.log(getattr(stats,model).pdf(x,**pars))\n",
    "    else:\n",
    "        ll = lambda x: np.log(getattr(stats,model).pmf(x,**pars))\n",
    "    \n",
    "    log_likelihood = 0\n",
    "    for x in sample:\n",
    "            log_likelihood = log_likelihood + ll(x)\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = [1, 2, 4, 5, 6]\n",
    "model = \"poisson\"\n",
    "pars = {'mu': [3, 5, 6]}\n",
    "\n",
    "loglikelihood(sample, model, pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"poisson\"\n",
    "pars = {'mu': 10}\n",
    "n = 30\n",
    "sample = generate_sample(n, model, pars)\n",
    "\n",
    "print sample\n",
    "print \"mean: \"+str(round(np.mean(sample),2))\n",
    "\n",
    "support = np.linspace(0, 30, 300)[1:]\n",
    "pars = {'mu': support}          \n",
    "\n",
    "for x in sample:\n",
    "    plt.plot(support, np.exp(loglikelihood([x], model, pars)), ls='--')\n",
    "\n",
    "plt.plot(support, np.exp(loglikelihood(sample, model, pars))**(1./n), lw=5)\n",
    "label_plot(\"$\\lambda$\", \"Likelihood\", \"Poisson Likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in sample:\n",
    "    plt.plot(support, loglikelihood([x], model, pars), ls='--')\n",
    "\n",
    "plt.plot(support, loglikelihood(sample, model, pars)/len(sample), lw=5)\n",
    "label_plot(\"$\\lambda$\", \"Log Likelihood\", \"Poisson Log Likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"expon\"\n",
    "pars = {'scale': 10}\n",
    "n = 30\n",
    "sample = generate_sample(n, model, pars)\n",
    "\n",
    "print sample\n",
    "print \"mean: \"+str(round(np.mean(sample),2))\n",
    "\n",
    "support = np.linspace(5, 30, 300)[1:]\n",
    "pars = {'scale': support}          \n",
    "\n",
    "for x in sample:\n",
    "    plt.plot(support, np.exp(loglikelihood([x], model, pars)), ls='--')\n",
    "\n",
    "plt.plot(support, np.exp(loglikelihood(sample, model, pars))**(1./n), lw=5)\n",
    "label_plot(\"$\\lambda$\", \"Likelihood\", \"Exponential Likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in sample:\n",
    "    plt.plot(support, loglikelihood([x], model, pars), ls='--')\n",
    "\n",
    "support = np.linspace(0, 30, 300)[1:]\n",
    "plt.plot(support, loglikelihood(sample, model, pars)/len(sample), lw=5)\n",
    "label_plot(\"$\\lambda$\", \"Log Likelihood\", \"Exponential Log Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: what are these?\n",
    "\n",
    "- Histograms versus KDE (weighted sum of continuous distributions)? (bins versus bandwidth, edge effects)\n",
    "- Method of Moments: plug and play $E[\\bar x] = \\mu$? (nice for continuous, lower moments better)\n",
    "- likelihood function $10 e^{-\\lambda 10}$ of data point ? \n",
    "- likelihood function of a sample $10 e^{-\\lambda 10} \\times 5 e^{-\\lambda 5} \\times \\cdots$?\n",
    "- log-likelihood? (for numerical precision) \n",
    "- maximum likelihood estimation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "<br>\n",
    "# $$\\textbf{Beating the House}$$\n",
    "\n",
    "Blackjack can be legally beaten by keeping track of the probability of getting a high card (10,J,Q,K,A) compared to a low card (2,3,4,5,6). This is called $card$ $counting$.  In early 1979, four MIT students taught themselves card counting and along with a professional gambler and an investor who put up most of their capital (\\$5,000) went to Atlantic City for spring break.  They went again in December and then recruited a few more MIT students as \"students\" for a \"blackjack class\". The \"class\" continued to visit Atlantic City intermittently until May 1980 (when the students graduated), during which time they increased their capital four-fold.  \n",
    "\n",
    "<table align=\"center\">\n",
    "<tr><td>\n",
    "<table align=\"center\">\n",
    "<tr><td></td><td>House</td></tr>\n",
    "<tr><td>Game</td><td>Advantage</td></tr>\n",
    "<tr><td>Baccarat (no tie bets)</td><td>1.2%</td></tr>\n",
    "<tr><td>Craps (pass/come)</td><td>1.4%</td></tr>\n",
    "<tr><td>Blackjack (average player)</td><td>2.0%</td></tr>\n",
    "<tr><td>Video Poker (average player)</td><td>0.5-3.0%</td></tr>\n",
    "<tr><td>Roulette (double-zero)</td><td>5.3%</td></tr>\n",
    "<tr><td>Slots</td><td>5.0-10.0%</td></tr>\n",
    "<tr><td>Keno (average)</td><td>27.0%</td></tr>\n",
    "</table>\n",
    "</td><td>\n",
    "<table align=\"center\">\n",
    "<tr><td>\n",
    "<img src=\"stuff/vegas.png\" width=\"360px\" align=\"center\"> \n",
    "</tr></td>\n",
    "</table>\n",
    "</td></tr>\n",
    "</table>\n",
    "\n",
    "At about the same time, another guy, Bill Kaplan returned to Cambridge after successfully running a blackjack team in Las Vegas.  Kaplan earned his BA at Harvard in 1977 and was accepted into Harvard Business School but delayed admission while he ran the blackjack team. Kaplan ran his operation using funds he received upon graduation as Harvard's \"outstanding scholar-athlete\" and generated more than a 35 fold rate of return in less than nine months of play.  Kaplan continued to run his Las Vegas blackjack team as a sideline while attending Harvard Business School but by the time of his graduation the players were so \"burnt out\" the team disbanded. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# Central Limit Theorem (CLT)\n",
    "\n",
    "#### With _enough_ i.i.d. samples $X_i, i = 1, 2, \\cdots, n$  from _ANY_ distribution $p(\\mu, \\sigma^2), \\text{ i.e. } E[X_i] = \\mu, Var[X_i] = \\sigma^2 \\text{ for all } i,$ the sampling distribution of \n",
    "### $$\\begin{align}\n",
    "\\bar X &= \\frac{1}{n}\\sum_{i=1}^n X_i\\\\\n",
    "\\text{is $\\quad$}\\\\\n",
    "& \\bar X \\overset{\\tiny approx}{\\sim} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n",
    "&\\text{[if paramaterized by variance]}\\\\\n",
    "\\text{or $\\quad$}\\\\\n",
    "& \\bar X \\overset{\\tiny approx}{\\sim} N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right) &\\text{[if paramaterized by the standard deviation]}\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "# The amazing thing about the CLT is the _normality_ (not the variance result -- this is free for independent variables):\n",
    "\n",
    "\n",
    "\n",
    "## $$Var[\\bar X] = Var\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n Var\\left[X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "### (The formula governing decreasing variance for increasing $n$ is simply the result of the variance of an average)\n",
    "\n",
    "<br>\n",
    "## $$E[\\bar X] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n} \\sum_{i=1}^n E\\left[X_i\\right] = E\\left[X_i\\right] = \\mu$$\n",
    "### (The expected value of an average of i.i.d. random variables is the  [identical] expected values of those variables)\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "# Let's explore what \"_enough_ i.i.d. samples\" means..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"expon\"\n",
    "pars = {\"scale\":10}\n",
    "n = 30\n",
    "support = [0, 30]\n",
    "plot_distribution(model,support,pars,'k')\n",
    "plt.legend([model+\"\\n\"+str(pars)], loc='best')\n",
    "\n",
    "sample = generate_sample(n,model,pars)\n",
    "plot_sample(sample,support)\n",
    "plot_sample(sample,support, \"kde\", 1)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=30\n",
    "r=100\n",
    "plot_sample([np.mean(generate_sample(n, model,pars)) for j in range(r)], support)\n",
    "label_plot(\"\", \"\", \"Observed $\\\\bar X$'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "r=1000\n",
    "plot_sample([np.mean(generate_sample(n,model,pars)) for j in range(r)], support, \"kde\", .08)\n",
    "plot_distribution('norm', support,{\"loc\": 10, \"scale\": 10/np.sqrt(n)}, 'k')\n",
    "plt.legend([\"Observed Distribution of $\\\\bar X$'s\", \"Theoretical Distribution of $\\\\bar X$'s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So if $n$ is _large enough_ then $\\bar X \\overset{\\tiny approx}{\\sim} N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)$\n",
    "\n",
    "# and $$Pr\\left(\\mu - 1.96 \\frac{\\sigma}{\\sqrt{n}} < \\bar X < \\mu + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95$$\n",
    "\n",
    "# which we can rearrange to form the _pivot_ of this inequality \n",
    "# $$Pr\\left(\\bar X - 1.96 \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar X + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95$$\n",
    "\n",
    "### But this does not mean this is a probability about $\\mu$ -- it is still a probability about $\\bar X$; specifically, it is the probability that these bounds capture $\\mu$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Confidence Intervals useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=30\n",
    "sample = generate_sample(n,model,pars)\n",
    "plot_distribution('norm', support,{\"loc\": 10, \"scale\": 10/np.sqrt(n)},'k')\n",
    "plt.plot([np.mean(sample)-1.96*10/np.sqrt(n), np.mean(sample)+1.96*10/np.sqrt(n)], [.015]*2, 'r--')\n",
    "plt.legend([\"$\\\\bar X$'s are sampled from this distribution\", \"Pivot constructed from this distribution\"])\n",
    "plot_distribution('norm', support,{\"loc\": np.mean(sample), \"scale\": 10/np.sqrt(n)},'r')\n",
    "\n",
    "plt.plot([10]*2, [0, .2175], 'k')\n",
    "plt.plot([np.mean(sample)+1.96*10/np.sqrt(n)]*2, [0, .032], 'r')\n",
    "plt.plot([np.mean(sample)-1.96*10/np.sqrt(n)]*2, [0, .033], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=200\n",
    "n=300\n",
    "xbars = [np.mean(generate_sample(n,model,pars)) for r in range(r)]\n",
    "colrs = ['b' if abs(xbar - 10) < 1.96*10/np.sqrt(n) else 'r' for xbar in xbars]\n",
    "plt.vlines(range(r), xbars-1.96*10/np.sqrt(n), xbars+1.96*10/np.sqrt(n), colors=colrs)\n",
    "plt.hlines(10,0,r, colors='k')\n",
    "plt.ylim([5,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general form for a CLT based confidence interval is \n",
    "\n",
    "# $$Pr\\left(\\bar x - Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar x + Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right) = 1-\\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are Confidence Intervals useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# t-distribution\n",
    "\n",
    "### When $X_i, i = 1, 2, \\cdots, n$  are i.i.d. samples from a _NORMAL_ distribution $N(\\mu, \\sigma^2)$ \n",
    "# $$\\begin{align}\n",
    "\\frac{\\bar X - \\mu}{\\hat \\sigma/\\sqrt{n}} \\overset{\\tiny exactly}{\\sim} t_{n-1}\n",
    "\\end{align}$$\n",
    "\n",
    "# which we can use if we want to provide inerence on $\\mu$ but we don't know $\\sigma$\n",
    "\n",
    "### $n-1$ is a parameter of the t-distribution known as \"the degrees of freedom\"\n",
    "### Note that this is _only true_ when the samples are i.i.d. _NORMAL_ (and an approximation otherwise)\n",
    "### The t-distribution has heavier tails (is more spread out) than the normal distribution \n",
    "###  This is from the increased uncertainty of not knowing $\\sigma$ and estimating it with $\\hat \\sigma$ \n",
    "### This estimate $\\hat \\sigma$ is typically the sample standard deviation $S = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2}$\n",
    "### As the sample size n approaches 30 the t-distribution looks like the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,4])\n",
    "x = np.linspace(-5,5,500)\n",
    "\n",
    "plt.subplot(121)\n",
    "for df in range(1,30):\n",
    "    plt.plot(x,stats.norm.pdf(x,loc=0,scale=2.5/np.sqrt(df)),'-r',alpha=.25)\n",
    "plt.xlim([-5,5])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"CLT\")\n",
    "\n",
    "plt.subplot(122)\n",
    "for df in range(1,30):\n",
    "    plt.plot(x,stats.t.pdf(x,df=df),'-r',alpha=.25)\n",
    "plt.xlim([-5,5])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"t->N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general form for a t-distribution confidence interval is \n",
    "\n",
    "# $$Pr\\left(\\bar x - t_{n-1,\\alpha/2} \\frac{s}{\\sqrt{n}} < \\mu < \\bar x + t_{n-1,\\alpha/2} \\frac{s}{\\sqrt{n}}\\right) = 1-\\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Ostinsibly changing gears... \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"expon\"\n",
    "pars = {\"scale\":10}\n",
    "n = 100\n",
    "support = [0, 30]\n",
    "\n",
    "plot_distribution(model,support,pars,'k')\n",
    "plt.legend([model+\"\\n\"+str(pars)], loc='best')\n",
    "\n",
    "sample = generate_sample(n,model,pars)\n",
    "plot_sample(sample,support)\n",
    "plot_sample(sample,support, \"kde\", 1)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sample(sample,support)\n",
    "plot_sample(sample,support, \"kde\", 1)\n",
    "label_plot(\"Observed Value\", \"Density\", \"i.i.d. sample (n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrapped_sample = generate_sample(len(sample),population=sample)\n",
    "plot_sample(bootstrapped_sample, support, \"kde\", 1)\n",
    "plot_sample(bootstrapped_sample, support)\n",
    "label_plot(\"Observed Value\", \"Density\", \"Bootstrapped sample ($n_B$ = n = \" + str(n) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$: Original sample size\n",
    "# $n$: Also the size of a bootsrapped sample\n",
    "## _This approximates the \"sample size\" dynamics of the original sample_\n",
    "# $b$: Number of bootsrapped samples to take\n",
    "## _This improves estimation precision of the \"sample size $n$ averages\" distribution_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sample([np.mean(generate_sample(len(sample),population=sample)) for r in range(100)], support)\n",
    "label_plot(\"\", \"\", \"Observed $\\\\bar X$'s (from bootstrapped samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=10000\n",
    "sample = generate_sample(n,model,pars)\n",
    "bootstrapped_xbars = [np.mean(generate_sample(len(sample),population=sample)) for k in range(b)]\n",
    "plot_distribution('norm', support,{\"loc\": 10, \"scale\": 10/np.sqrt(n)},'k')\n",
    "plt.plot(list(np.percentile(bootstrapped_xbars, [2.5, 97.5])),[.04]*2,'r--')\n",
    "plt.legend([\"$\\\\bar X$'s are sampled from this distribution\", \"Pivot constructed from this distribution\"])\n",
    "plot_sample(bootstrapped_xbars, support, \"kde\", .175)\n",
    "plt.vlines(10,0,.4)\n",
    "plt.vlines(np.percentile(bootstrapped_xbars,2.5),0,.08,'r')\n",
    "plt.vlines(np.percentile(bootstrapped_xbars,97.5),0,.08,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bootstrapped sampling distribution will be centered on $\\bar x$ of the original sample\n",
    "### The variance of the bootstrapped sampling distribution will be about right!\n",
    "### So long as we use the same boostrap sample as as the original sample size\n",
    "### So we can use bootstrapped samples to create _bootstrapped confidence intervals_!\n",
    "# Yes, they are only as good as the samples they're based on \n",
    "# But this is just the same with the CLT approach: results are only ever as good as samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=200\n",
    "n=100\n",
    "b=1000\n",
    "\n",
    "for j in range(r):\n",
    "    sample = generate_sample(n,model,pars)\n",
    "    bootstrapped_xbars = [np.mean(generate_sample(len(sample),population=sample)) for k in range(b)]\n",
    "    tmp = np.percentile(bootstrapped_xbars, [2.5,97.5])\n",
    "    if all(tmp>10) or all(tmp<10):\n",
    "        colr='r'\n",
    "    else:\n",
    "        colr='b'\n",
    "    plt.vlines(j, *tmp, colors=colr)\n",
    "\n",
    "plt.hlines(10,0,r, colors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# WHY EVEN USE THE BOOTSTRAP CONFIDENCE INTERVAL?\n",
    "## Which type of confidence interval do you prefer?\n",
    "## Inference is only as good as the sample...\n",
    "## Samples should be unbiased population representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- What happens to $Var[\\bar X]$ as $n$ increases?\n",
    "- Under what conditions do you know something about the sampling distribution of $\\bar X$ and what do you know?\n",
    "- How could you demonstrate this?\n",
    "- Describe the construction process of a confidence interval to define what it is\n",
    "- How do you interpret a 95% confidence interval?\n",
    "- What is a t-distribution, what is it like, and why does it exist?\n",
    "- Compare the construction process of a _bootstrapped_ confidence interval to the original CLT-based confidencd interval in order to find the primariy differentiator between the two.\n",
    "- What is $n$ and what is $b$ with respect to a _bootstrapped_ confidence interval?\n",
    "- What is the $r$ in the above code?\n",
    "- Which type of confidence interval is better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# Review Questions\n",
    "\n",
    "- Compare and contrast KDE and Histograms\n",
    "- Compare and contrast Method of Moments and Maximum Likelihood Estimation\n",
    "- Explain the Central Limit Theorem\n",
    "- Explain Confidence Intervals\n",
    "- Explain how to use Bootstrapping to produce Confidence Intervals for $r$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slide_type": "markdown"
   },
   "source": [
    "# Previous Review Questions\n",
    "\n",
    "- Expected Value/Variance/Covariance/Correlation of a distribution and sample statistics analogues\n",
    "- Joint distribution, independence, conditional distributions, and the law of total probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize']=(5,3.4)\n",
    "\n",
    "games_played = np.arange(1,1000,10)\n",
    "fig = plt.figure(1)\n",
    "p = .506; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/ga\\\n",
    "mes_played)),label=\"Baccarat\")\n",
    "p = .507; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/ga\\\n",
    "mes_played)),label=\"Craps\")\n",
    "p = .51; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/gam\\\n",
    "es_played)),label=\"Blackjack\")\n",
    "p = .515; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/ga\\\n",
    "mes_played)),label=\"Video Poker\")\n",
    "p = .5265; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/g\\\n",
    "ames_played)),label=\"Roulette\")\n",
    "p = .55; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/gam\\\n",
    "es_played)),label=\"Slots\")\n",
    "p = .635; plt.plot(games_played, stats.norm.cdf(.5,p,np.sqrt(p * (1-p)/ga\\\n",
    "mes_played)),label=\"Keno\")\n",
    "plt.xlabel(\"Number of bets placed\")\n",
    "plt.ylabel(\"Proportion of gamblers with > $0\")\n",
    "plt.title(\"Successive $1 bets\")\n",
    "plt.legend(frameon = 1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
