{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## k-means, hierarchical, and more!\n",
    "\n",
    "#### Jack Bennetto\n",
    "#### May 26, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "#### Today's objectives:\n",
    "\n",
    "* Explain the difference between **supervised** and **unsupervised** learning\n",
    "* Implement a **k-means** algorithm for clustering\n",
    "* Discuss how **curse of dimensionality** affects clustering\n",
    "* Choose the best k using the **elbow method** or **silhouette scores**\n",
    "* Implement and interpret **hierarchical clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "#### Morning:\n",
    "\n",
    " * Supervised/unsupervised learning\n",
    " * Clustering\n",
    " * k-means algorithm\n",
    " * Curse of dimensionality\n",
    "\n",
    "#### Afternoon:\n",
    "\n",
    " * How to choose k\n",
    " * Hierarchical and other clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import scipy.stats as scs\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "\n",
    "So far we've mostly been doing supervise learning, when we try to predict some labels.\n",
    "\n",
    " * Linear & logistic regression with lasso or ridge regularization\n",
    " * Decision trees, bagging, random forest, boosting\t\n",
    " * SVM\n",
    " * kNN\n",
    "\n",
    "Label $==$ target $==$ endogenous variable $==$ dependent variable $==$ y\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "No labels. No target.\n",
    "\n",
    "Why use it?\n",
    "\n",
    " * EDA\n",
    " * Discovering latent variables\n",
    " * Feature engineering\n",
    " * Preprossessing\n",
    "\n",
    "The most common type of unsupervised learning is clustering, that we're talking about today.\n",
    "\n",
    "![](images/unsupervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Problem\n",
    "\n",
    "The goal of clustering is to divide the data into **distinct subgroups** such that observations within each group are similar.\n",
    "\n",
    "![](images/clusters.png)\n",
    "\n",
    "\n",
    "## Various Algorithms\n",
    "\n",
    "There are several approachs to clustering, each with variations.\n",
    "\n",
    "* k-means clustering\n",
    "* Hierarchical clustering\n",
    "* Density-based clustering (DBSCAN)\n",
    "* Distribution-based clustering\n",
    "* ...\n",
    "\n",
    "How do we measure how good the clustering is?\n",
    "\n",
    "## Within-Cluster Sum of Squares\n",
    "\n",
    "Measures the goodness of a clustering\n",
    "\n",
    "$$W(C) = \\sum_{k=1}^{K} \\frac{1}{K} \\sum_{C(i)=k}  \\sum_{C(j)=k} || x_i - x_j ||^2 $$\n",
    "\n",
    "where $K$ is the number of clusters, $C(i)$ is the cluster label of point $i$, and $x_i$ is the position of point $i$.\n",
    "\n",
    "Do you need to normalize/standardize the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means Algorithm\n",
    "\n",
    "The k-means algorithm involves repeatedly assigning points to clusters and then finding new clusters based on those points.\n",
    "\n",
    "* Choose a number of clusters k\n",
    "* Randomly assign each point to a cluster\n",
    "* Repeat:\n",
    "    * a\\. For each of k clusters, compute cluster *centroid* by taking\n",
    "mean vector of points in the cluster\n",
    "    * b\\. Assign each data point to cluster for which centroid is closest\n",
    "(Euclidean)\n",
    "\n",
    "...until clusters stop changing\n",
    "\n",
    "\n",
    "# k-means Algorithm\n",
    "\n",
    "![The k-means algorithm.](images/kmeans.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# k-means++\n",
    "\n",
    "Like many algoritms, it's easy to get caught in a local minimum.\n",
    "\n",
    "Consider the points below. How would you break them into two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "colors = 'k'\n",
    "#colors = ['r', 'b', 'r', 'b']\n",
    "ax.scatter([0, 0, 2, 2], [0, 1, 0, 1], s=200, c=colors)\n",
    "#ax.scatter([1, 1], [0, 1], marker='x', s=200, c=['r', 'b'])\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means finds a local minimum, and sometimes a bad one.\n",
    "\n",
    "How would we choose a better starting points?\n",
    "\n",
    "One alternative to this is to choose centroids based on random points, which will make it likely initial centroids will be farther apart.\n",
    "\n",
    "k-means++ is an extension of this idea, the same algorithm as k-means but with a different starting point.\n",
    "\n",
    " * Choose one point for first center.\n",
    " * Repeat:\n",
    "    \n",
    "    * Calculate distance from each point to the nearest center $d_i$\n",
    "    * Choose a point to be the next center, randomly, using a weighed probability $d_i^2$\n",
    "\n",
    " ... until k centers have been choosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "Random variation in extra dimensions can many hide significant differences between clusters.\n",
    "\n",
    "The more dimensions there are, the worse the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import function to plot histograms on top of each other cleanly\n",
    "from src.multihist import multihist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two clusters plotted in increasing numbers of dimensions. How much do they overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 500\n",
    "sep = 5.\n",
    "\n",
    "x = np.zeros((2, 2*count))\n",
    "x[0,:] = np.concatenate((scs.norm(-sep/2., 1).rvs(count), (scs.norm(sep/2., 1).rvs(count))))\n",
    "x[1, :] = scs.norm(0, 1).rvs(2*count)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[0, :count], x[1, :count], c='r', alpha=0.3)\n",
    "ax.scatter(x[0, count:], x[1, count:], c='b', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20, 30, 50, 100]:\n",
    "    x = np.zeros((dims, 2*count))\n",
    "    x[0,:] = np.concatenate((scs.norm(-sep/2., 1).rvs(count), (scs.norm(sep/2., 1).rvs(count))))\n",
    "    for d in range(1, dims):\n",
    "        x[d, :] = scs.norm(0, 1).rvs(2*count)\n",
    "\n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But...\n",
    "That assumes additional dimensions add nothing. What if each additional dimension adds an equal amount of signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20]:\n",
    "    \n",
    "    x = np.zeros((dims, 2*count))\n",
    "    for d in range(0, dims):\n",
    "        x[d,:] = np.concatenate((scs.norm(-sep/2., 1).rvs(count), (scs.norm(sep/2., 1).rvs(count))))\n",
    "\n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...except\n",
    "Assume that each new dimension has an exponentially decreasing amount of signal.\n",
    "\n",
    "Here additional dimension helps, but only up to a point. Later dimensions that are almost all noise make things worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20, 30, 50, 100]:\n",
    "    \n",
    "    x = np.zeros((dims, 2*count))\n",
    "    for d in range(0, dims):\n",
    "        x[d,:] = np.concatenate((scs.norm(-sep/2.*(.8**d), 1).rvs(count), (scs.norm(sep/2.*(.8**d), 1).rvs(count))))\n",
    "    \n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the correct number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = [(-2, -3, .75, 100),\n",
    "            (-5, -5, .75, 75),\n",
    "            (4, 3, 1.5, 200),\n",
    "            (1, -3, .5, 150),\n",
    "            (-1, 0.15, 0.75, 100)]\n",
    "k = len(clusters)\n",
    "n = sum([c[3] for c in clusters])\n",
    "\n",
    "x1 = np.array([])\n",
    "x2 = np.array([])\n",
    "\n",
    "for c in clusters:\n",
    "    x1 = np.concatenate([x1, (scs.norm(c[0], c[2]).rvs(c[3]))])\n",
    "    x2 = np.concatenate([x2, (scs.norm(c[1], c[2]).rvs(c[3]))])\n",
    "x = np.stack((x1, x2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[:,0], x[:,1], linewidths=0, color='k')\n",
    "ax.set_xlim(xmin=-9, xmax=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do you see?\n",
    "\n",
    "Let's try fitting them with k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KMeans(5)\n",
    "y = km.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[:,0], x[:,1], c=y, linewidths=0)\n",
    "ax.set_ylim(ymin=-9, ymax=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are basically the clusters we created the data from.\n",
    "\n",
    "# Choosing K\n",
    "\n",
    "Can we just use within-cluster sum of squares (WCSS) to choose k?\n",
    "\n",
    "\n",
    "More clusters $\\implies$ lower WCSS.\n",
    "\n",
    "Several measures for the \"best\" k - no easy answer\n",
    "\n",
    " * The Elbow Method\n",
    " * Silhouette Score\n",
    " * GAP Statistic\n",
    "\n",
    "First, let's cluster the data above with k-means with various values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxk = 13\n",
    "wcss = np.zeros(maxk)\n",
    "silhouette = np.zeros(maxk)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16,9))\n",
    "\n",
    "# flatten\n",
    "axes = [ax for axrow in axes for ax in axrow]\n",
    "\n",
    "for k, ax in zip(range(1,maxk), axes):\n",
    "    km = KMeans(k)\n",
    "    y = km.fit_predict(x)\n",
    "    ax.axis('off')\n",
    "    ax.scatter(x[:,0], x[:,1], c=y, linewidths=0, s=10)\n",
    "    ax.set_ylim(ymin=-9, ymax=8)\n",
    "    \n",
    "    \n",
    "    for c in xrange(0, k):\n",
    "        for i1, i2 in itertools.combinations([ i for i in xrange(len(y)) if y[i] == c ], 2):\n",
    "            wcss[k] += sum(x[i1] - x[i2])**2\n",
    "    wcss[k] /= 2\n",
    "    \n",
    "    if k > 1:\n",
    "        silhouette[k] = silhouette_score(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Method\n",
    "\n",
    "Let's start with the Elbow method. In this, we simply plot the within-cluster sum of squares and try to see what looks like an elbow.\n",
    "\n",
    "What looks best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(3,maxk), wcss[3:maxk], 'o-')\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"within-cluster sum of squares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is some metric we can maximize.\n",
    "\n",
    "What would that look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing K -- Silhouette Score\n",
    "\n",
    "For each point $x_i$:\n",
    "\n",
    " * $a(i)$ average dissimilarity of $x_i$ with points in the same cluster\n",
    " * $b(i)$ average dissimilarity of $x_i$ with points in the nearest cluster\n",
    "    * \"nearest\" means cluster with the smallest $b(i)$\n",
    "\n",
    "$$\\text{silhouette}(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))} $$\n",
    "\n",
    "What's the range of silhouette scores?\n",
    "\n",
    "The silhouette score of a clustering is the average of silhouette score of all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,maxk), silhouette[2:maxk], 'o-')\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"silhouette score\")\n",
    "#ax.set_ylim(ymin=0.0, ymax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible sillhoiette scores:\n",
    " * near 1: very small tight cluster.\n",
    " * 0: at the edge of two clusters; could be in either.\n",
    " * < 0: oops.\n",
    "\n",
    "The higher the the average silhouette score, the tigher and more separated the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Graph\n",
    "\n",
    "A silhouette graph is a representation of the silhouette score of ever data point, grouped first by cluster and then in decresing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = x\n",
    "range_n_clusters = xrange(2,10)\n",
    "\n",
    "# taken from sklearn\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing K -- GAP Statistic\n",
    "\n",
    "For each $K$, compare $W_K$ (within-cluster sum of squares) with that of randomly generated \"reference distributions\"\n",
    "\n",
    "\n",
    "Generate B distributions\n",
    "\n",
    "$$Gap(K) = \\frac{1}{B} \\sum_{b=1}^B \\log{W_{Kb}} - \\log{W_K}$$\n",
    "\n",
    "Choose smallest K such that $Gap(K) \\ge Gap(K+1) - s_{N+1}$\n",
    "\n",
    "where $s_{K}$ is the standard error of $Gap(K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "With many clustering methods the goal is to divide the data into a specific number of clusters. With hierarchical clustering we group data points as leaves on a tree, clustering them into larger and larger groups.\n",
    "\n",
    "How would we group these?\n",
    "\n",
    "![](images/letters-ungrouped.png)\n",
    "\n",
    "\n",
    "<table><tr><td><img src='images/letters-grouped.png'></td><td><img src='images/letters-dendrogram.png'></td></tr></table>\n",
    "\n",
    "\n",
    "# Hierarchical Clustering\n",
    "\n",
    "The basic algoritm of hierarchical clustering is\n",
    "\n",
    " * Assign each point to its own cluster\n",
    " * Repeat:\n",
    "\n",
    "   * Compute distances between clusters\n",
    "   * Merge closest clusters\n",
    "\n",
    " ...until all are merged\n",
    "\n",
    "How do we define dissimilarity between clusters?\n",
    "\n",
    "## Linkage\n",
    "\n",
    "It's easy to talk about the distance (or dissimalarity) between two points, but between clusters it's less clear. There are a few different measures used.\n",
    "\n",
    "* **Complete:** Maximum pairwise dissimilarity between points in clusters -- good\n",
    "* **Average:** Average of pairwise dissimilarity between points in clusters -- also good\n",
    "* **Single:** Minimum pairwise dissimilarity between points in clusters -- not as good; can lead to long narrow clusters\n",
    "* **Centroid:** Dissimilarity between centroids -- used in genomics; risk of inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with k-means and other methods\n",
    "\n",
    "k-means has limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "r = np.concatenate([scs.norm(7,1).rvs(250),\n",
    "                    scs.norm(2,1).rvs(50)])\n",
    "a = scs.uniform(0, 6.28).rvs(300)\n",
    "x = r * np.cos(a)\n",
    "y = r * np.sin(a)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "ax.scatter(x, y, color=\"r\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will k-means do here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "With DBSCAN (Density-Based Spacial Clustering of Applications with Noise) we don't specify the number of clusters. Instead we specify:\n",
    "\n",
    " * $\\epsilon$: distance between points for them to be connected\n",
    " * minPts: number of connected points for a point to be a \"core\" point\n",
    "\n",
    "A cluster is all connected core points, plus others within $\\epsilon$ of one of those. Other points are noise.\n",
    "\n",
    "Let's tackle the above problem with DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 1.1\n",
    "minpts = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "distances = squareform(pdist(np.stack([x,y], axis=1)))\n",
    "connected = distances < epsilon\n",
    "ax.scatter(x, y, color=\"k\", alpha=0.3)\n",
    "for i in xrange(len(x)):\n",
    "    for j in xrange(i):\n",
    "        if connected[i,j]:\n",
    "            ax.plot([x[i], x[j]], [y[i], y[j]], 'k', lw=0.5)\n",
    "\n",
    "coreindices = np.where(connected.sum(axis=0) > (minpts))[0]\n",
    "ax.scatter(x[coreindices], y[coreindices], s=150, c='r')\n",
    "\n",
    "for i in coreindices:\n",
    "    for j in coreindices:\n",
    "        if i > j and connected[i,j]:\n",
    "            ax.plot([x[i], x[j]], [y[i], y[j]], 'k', lw=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution-based clustering\n",
    "\n",
    "With distribution-based clustering we assume some fixed number of clusters, and assume they follow some (often normal) distribution. We then try to find the parameters that have the **maximum likelihood** of producing these data.\n",
    "\n",
    "This is more difficult then other problems we've seen because we don't know which point came from which distribution. We need to add some hidden variables to the problem: the probability each point came from each distribution. We can solve this by an **expectation-maximization** (EM) algorithm in which we alternate between expectation steps (where we calculate the hidden variables) and maximization steps (in which we calculate the maximum-likelihood parameters assuming the hidden variables are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npts = 100\n",
    "actual_mus = 1, 5\n",
    "actual_sds = 1.5, 0.5\n",
    "x = np.concatenate([scs.norm(actual_mus[0], actual_sds[0]).rvs(npts/2),\n",
    "                    scs.norm(actual_mus[1] ,actual_sds[1]).rvs(npts/2)])\n",
    "y = np.ones(npts)/2.\n",
    "fig, ax = plt.subplots(figsize=(10,1))\n",
    "ax.scatter(x[:npts/2], y[:npts/2], c='b', alpha=0.1, s=100)\n",
    "ax.scatter(x[npts/2:], y[npts/2:], c='r', alpha=0.1, s=100)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first step is to make guesses for the two starting distrubitions. Let's go with N(0,1) and N(2,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mus = [-1, 0]\n",
    "sds = [1, 1]\n",
    "dists = [scs.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "actual_dists = [scs.norm(mu, sd) for mu, sd in zip(actual_mus, actual_sds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_em(x, y, dists, actual_dists):\n",
    "    xpts = np.linspace(-5,10,100)\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    npts = len(x)\n",
    "    ax.scatter(x[:npts/2], y[:npts/2], c='b', alpha=0.1, s=100, label=\"sample from dist 0\")\n",
    "    ax.scatter(x[npts/2:], y[npts/2:], c='r', alpha=0.1, s=100, label=\"sample from dist 1\")\n",
    "    ax.plot(xpts, dists[0].pdf(xpts), 'b', label=\"estimated dist 0\")\n",
    "    ax.plot(xpts, 1-dists[1].pdf(xpts), 'r',  label=\"estimated dist 1\")\n",
    "    ax.plot(xpts, actual_dists[0].pdf(xpts), 'b:',  label=\"actual dist 0\")\n",
    "    ax.plot(xpts, 1-actual_dists[1].pdf(xpts), 'r:',  label=\"actual dist 1\")\n",
    "    ax.set_ylabel(\"Probability data from dist 1\")\n",
    "    ax.legend()\n",
    "\n",
    "def print_stats(actual_mus, actual_sds, mus, sds):\n",
    "    #for i in (0, 1):\n",
    "    print(\"      actual  estimate\")\n",
    "    print(\"mu_0  {0:.2f}    {1:.3f}\".format(actual_mus[0], mus[0]))\n",
    "    print(\"mu_1  {0:.2f}    {1:.3f}\".format(actual_mus[1], mus[1]))\n",
    "    print(\"sd_0  {0:.2f}    {1:.3f}\".format(actual_sds[0], sds[0]))\n",
    "    print(\"sd_1  {0:.2f}    {1:.3f}\".format(actual_sds[1], sds[1]))\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we're going to calculate the relative likelihood that each point will be in `dist[1]` (the upper distribution, as opposed to `dist[0]`, the lower one).\n",
    "\n",
    "We'll plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we update each distribution to be (normal) distribution that has the maximum likelihood of generating the data. We'll weight each point by the probability (calculated above) of being in that distribution.\n",
    "\n",
    "We can just use the (weighted) means and standard deviations for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_stats(x, weights):\n",
    "    wtotal = weights.sum()\n",
    "    wmean = (x * weights).sum() / wtotal\n",
    "    wsd = (((weights*(x - wmean))**2).sum() / wtotal)**0.5\n",
    "    return wmean, wsd\n",
    "\n",
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [scs.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [scs.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [scs.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "    mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "    mus[1], sds[1] = weighted_stats(x, y)\n",
    "    dists = [scs.norm(mu, sd) for mu, sd in zip(mus, sds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_em(x, y, dists, actual_dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
