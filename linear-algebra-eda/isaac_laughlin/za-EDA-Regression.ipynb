{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars_multivariate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a very high level overview\n",
    "- What do the first few rows look like?\n",
    "- What are the different columns?\n",
    "- Different data types\n",
    "- Data summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cars.head()\n",
    "# cars.info()\n",
    "# cars.shape\n",
    "# cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the variable types make sense? Notice that HP has variable type 'object' - this doesn't seem right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars['horsepower'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the values that we can see look like numbers. If they were all numbers, Pandas should have figured out the correct data type---there must be a weird value in there somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Force convert horsepower to numeric\n",
    "conv = cars[['horsepower']].convert_objects(convert_numeric=True)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out the bad values\n",
    "cars.ix[conv['horsepower'].isnull(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload the data, but tell pandas to treat '?' as missing\n",
    "cars = pd.read_csv('data/cars_multivariate.csv', na_values=['?'])\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's just drop those rows\n",
    "cars = cars.ix[cars.horsepower.notnull(), :]\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What about origin? model?\n",
    "cars.origin.value_counts()\n",
    "# cars['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars['origin'] = cars['origin'].astype(str)\n",
    "cars['model'] = cars['model'].astype(str)\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set has 5-6 numeric variables and 3-4 categorical variables (cylinders is kind of a wild card). Sometimes it is good to keep track of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['mpg','cylinders','displacement','horsepower','weight','acceleration']\n",
    "cat_vars = ['model', 'origin', 'car_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate-Numeric Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "ax = cars[num_vars].hist(bins=10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "fig, axes = plt.subplots(3,2)\n",
    "for ax, var in zip(axes.ravel(), num_vars):\n",
    "    ax.boxplot(cars[var])\n",
    "    ax.set_title(var)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical\n",
    "agg = cars.groupby('origin').apply(len)\n",
    "agg.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical vs Numeric\n",
    "agg = cars.groupby('origin')['mpg'].mean()\n",
    "agg.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical vs categorical vs numeric\n",
    "agg = cars.groupby(['origin','cylinders'])['mpg'].mean()\n",
    "print agg\n",
    "agg = agg.unstack(level='cylinders')\n",
    "print agg\n",
    "agg.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric vs Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatterplot matrix\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "ax = scatter_matrix(cars[num_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use binning to see relationships more clearly\n",
    "cars['binned_acceleration'] = pd.cut(cars.acceleration, bins=7)\n",
    "agg = cars.groupby('binned_acceleration')['mpg'].mean()\n",
    "agg.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot by category\n",
    "origins = cars['origin'].unique()\n",
    "for origin in origins:\n",
    "    plt.plot(cars.loc[cars.origin==origin,'acceleration'], \n",
    "             cars.loc[cars.origin==origin,'mpg'], \n",
    "             linestyle='',\n",
    "             marker='o',\n",
    "             alpha=.7,\n",
    "             label=origin)\n",
    "plt.xlabel('acceleration')\n",
    "plt.ylabel('mpg')\n",
    "plt.legend(numpoints=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical vs Categorical\n",
    "pd.crosstab(cars['origin'], cars['model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Model means model year --- we probably should have left this as numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Heat-map\n",
    "import seaborn as sns\n",
    "\n",
    "agg = cars.groupby(['origin','model'])['mpg'].mean()\n",
    "ax = sns.heatmap(agg.unstack(level='model'), annot=True)\n",
    "ax.set_title('MPG by origin and model year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single variable regression\n",
    "cars.plot('weight','mpg',kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is just the fancy term for finding the line of best fit. If I was going to eyeball it from this data, I would draw the line from (1000,40) through (5500,5).\n",
    "\n",
    "In other words, we are looking for the slope and intercept that defines a line that fits the data as well as possible.\n",
    "\n",
    "'As well as possible' means that we are trying to minimize the mean-squared-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a guess at the line of best fit\n",
    "first_point = [1000,45]\n",
    "second_point = [5500, 0]\n",
    "\n",
    "# Solve \n",
    "def get_line_equation(p1, p2):\n",
    "    \"\"\"\n",
    "    Solve the system of equations:\n",
    "    y1 = m*x1 + b\n",
    "    y2 = m*x2 + b\n",
    "    \n",
    "    Input:\n",
    "    p1: first point [x1, y1]\n",
    "    p2: second point [x2, y2]\n",
    "    \n",
    "    returns: slope, intercept\n",
    "    \"\"\"\n",
    "    X = [[p1[0], 1], [p2[0], 1]]\n",
    "    y = [[p1[1]], [p2[1]]]\n",
    "    soln = np.linalg.solve(X,y)\n",
    "    return  soln[0][0], soln[1][0]\n",
    "\n",
    "slope, intercept = get_line_equation(first_point, second_point)\n",
    "\n",
    "\n",
    "# Plot the line along with the data\n",
    "ax = cars.plot('weight','mpg',kind='scatter')\n",
    "xx = np.linspace(1000, 5500, 100)\n",
    "ax.plot(xx, xx*slope + intercept, color='red', lw=3)\n",
    "ax.set_xlim([1000,5500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we measure the error? The typical choice is to use mean squared error. The error for a given data point is the difference between the observed value and the predicted value\n",
    "$$\n",
    "MSE := \\frac{1}{n} \\sum_{i=1}^n (y_i - (mx_i + b))^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "\n",
    "def mean_squared_error(X, y, m, b):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error, on the data (X,y), \n",
    "    of the model defined by slope m, and intercept b.\n",
    "    \"\"\"\n",
    "    pred = X*m + b\n",
    "    error = y - pred\n",
    "    mse = np.mean(error**2)\n",
    "    return mse\n",
    "\n",
    "mean_squared_error(cars['weight'], cars['mpg'], slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##R-Squared\n",
    "\n",
    "Mean squared error is a good error metric, but it is not comparable across different data sets. For this we use a scaled version called $R^2$. \n",
    "\\begin{align}\n",
    "    R^2 &:= 1 - \\frac{SS_{res}}{SS_{tot}} \\\\\n",
    "    &= 1 - \\frac{\\sum_{i=1}^n (y_i - (mx_i + b))^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "\\end{align}    \n",
    "\n",
    "Where $SS_{res}$ is the sum of the squared residuals and $SS_{tot}$ is the total sum of squares. $R^2$ can be interpreted as the fraction of the variance in the data that is explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate r-squared\n",
    "\n",
    "def r_squared(X, y, m, b):\n",
    "    \"\"\"\n",
    "    Compute the r-squared, on the data (X,y), \n",
    "    of the model defined by slope m, and intercept b.\n",
    "    \"\"\"\n",
    "    pred = X*m + b\n",
    "    resid = y - pred\n",
    "    rsquared = 1 - np.sum(resid**2)/np.sum((y-y.mean())**2)\n",
    "    return rsquared\n",
    "\n",
    "r_squared(cars['weight'], cars['mpg'], slope, intercept)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares\n",
    "It turns out that we can find the slope and intercept which *minimize* the mean squared error, using a procedure called ordinary least squares\n",
    "\n",
    "Ordinary least squares is implemented in the statsmodels package. The advantage of this package is that we also have access to a number of *regression diagnostics.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Choose the predictor and add a constant term\n",
    "# (allow for an intercept)\n",
    "X = pd.DataFrame({'weight' : cars['weight']})\n",
    "X = sm.add_constant(X)\n",
    "y = cars['mpg']\n",
    "\n",
    "# Create a linear regression object\n",
    "regressor = sm.OLS(y,X)\n",
    "regressor = regressor.fit()\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "**coef** - The values of the coefficients in the model\n",
    "\n",
    "**$P>|t|$** - The p-value of the null hypothesis that a specific parameter is zero.\n",
    "\n",
    "**R-Squared** - Proportion of variance explained by the model. Measured on a scale from 0 (bad) to 1 (good)\n",
    "\n",
    "**Prob (F-statistic)** - p-value of the F-statistic. This is the probability of the null hypothesis that *all parameters in the model are zero*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the line along with the data\n",
    "slope = -.0076\n",
    "intercept = 46.2165\n",
    "ax = cars.plot('weight','mpg',kind='scatter')\n",
    "xx = np.linspace(1000, 5500, 100)\n",
    "ax.plot(xx, xx*slope + intercept, color='red', lw=3)\n",
    "ax.set_xlim([1000,5500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
