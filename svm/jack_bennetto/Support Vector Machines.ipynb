{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "### Jack Bennetto\n",
    "#### February 2, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    " * Explain margins, support vectors, and hyperplanes\n",
    " * Compute a linear SVC model, tuning C\n",
    " * Compute a non-linear SVM, tuning $\\gamma$ or degree\n",
    " * Explain the relationship of all the hyperparameters to bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Morning agenda\n",
    "\n",
    " * SVM as a classifier\n",
    " * Decision boundaries\n",
    " * Maximum-margin classification\n",
    " \n",
    "Afternoon agenda\n",
    " * Soft boundaries and scalar-vector classification\n",
    " * Additional features for non-linear boundaries\n",
    " * Kernels and SVMs\n",
    " * Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morning Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is SVM?\n",
    "\n",
    "The SVM was first developed in the '60s by Vladimir Vapnik, it wasn't well known until the '90s.\n",
    "\n",
    "Advantages\n",
    " * Best model for well-separated data \n",
    " * Works well for for high-dimensional data\n",
    " * Non-linear kernels can match non-linear boundaries\n",
    " \n",
    "Disadvantages\n",
    " * Doesn't do well at providing probability\n",
    " * Not very interpretable, or visualizable for high-dimensional data\n",
    " * Slow to train\n",
    " * Need to standardize/normalize features\n",
    "\n",
    "\n",
    "\n",
    "SVM can also be used for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_boundary(ax, b0, b1, b2, margin=0, ls='-', color='k', alpha=1.0):\n",
    "    '''\n",
    "    Plot a decision boundary on an existing axis\n",
    "    '''\n",
    "        \n",
    "    # save the limits, so the decision boundary doesn't expand them\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # boundary_x are the x axis values for the boundary, from X[0]\n",
    "    # boundary_y are the y axis values for the boundary, from X[1]\n",
    "    boundary_x = np.zeros(4)\n",
    "    boundary_y = np.zeros(4)\n",
    "    boundary_x[:2] = xlim[0], xlim[1]\n",
    "    boundary_y[:2] = (margin - b0 - b1 * boundary_x[:2]) / b2\n",
    "    boundary_y[2:] = ylim[0], ylim[1]\n",
    "    boundary_x[2:] = (margin - b0 - b2 * boundary_y[2:]) / b1\n",
    "    \n",
    "    # we could just plot all the points,\n",
    "    # but if we do that with a dotted line it plots over itself\n",
    "    irng = [boundary_x.argmin(), boundary_x.argmax()]\n",
    "    ax.plot(boundary_x[irng], boundary_y[irng], color=color, ls=ls, alpha=alpha)\n",
    "    # restore the saved limits\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundaries\n",
    "\n",
    "SVM is focused on situations \n",
    "\n",
    "Let's choose some points, plot them and consider a couple possible boundaries.\n",
    "\n",
    "For the rest of the morning, we'll only consider cases where we have two classes that can be completely separated with a single line. In the afternoon we'll extend that so include soft margins and non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "npts = 100\n",
    "X_a = np.zeros((npts, 2))\n",
    "X_a[npts/2:, 0] = scs.norm(0,1).rvs(npts/2)\n",
    "X_a[npts/2:, 1] = scs.norm(0,5).rvs(npts/2)\n",
    "X_a[:npts/2, 0] = scs.norm(24,10).rvs(npts/2)\n",
    "X_a[:npts/2, 1] = scs.norm(0,1).rvs(npts/2)\n",
    "# rotate a bit\n",
    "X_a = X_a.dot([[1,-.2],[.2,1]])\n",
    "y_a = np.zeros(npts, dtype='int32')\n",
    "y_a[npts/2:] = 1\n",
    "\n",
    "model_lr_a = LogisticRegression(intercept_scaling=100)\n",
    "model_lr_a.fit(X_a, y_a)\n",
    "model_sv_a = SVC(kernel='linear')\n",
    "model_sv_a.fit(X_a, y_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.scatter(X_a[:,0], X_a[:,1], color=np.array(['r', 'b'])[y_a])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plot_boundary(ax, model_lr_a.intercept_[0], model_lr_a.coef_[0][0], model_lr_a.coef_[0][1], color='k')\n",
    "plot_boundary(ax, model_sv_a.intercept_[0], model_sv_a.coef_[0][0], model_sv_a.coef_[0][1], color='k')\n",
    "ax.plot([3.41,3.41],[-15,10], color='k')\n",
    "#plot_boundary(ax, 4.17468436879, -1.00207931899, 0.200425620857, color='g')\n",
    "\n",
    "ax.text(2.4,8.8, 'a', fontsize=16)\n",
    "ax.text(4.6,8.8, 'b', fontsize=16)\n",
    "ax.text(7.2,8.8, 'c', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which line do you like best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three lines represent three different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-margin classification\n",
    "\n",
    "One approach to choosing the best decision boundary is find the one that's the farthest from any of the points. The distance from the nearest points is called the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.scatter(X_a[:,0], X_a[:,1], color=np.array(['r', 'b'])[y_a])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "b0 = model_sv_a.intercept_[0]\n",
    "b1 = model_sv_a.coef_[0][0]\n",
    "b2 = model_sv_a.coef_[0][1]\n",
    "plot_boundary(ax, b0, b1, b2, color='k')\n",
    "plot_boundary(ax, b0, b1, b2, margin=-1, ls=':', color='k')\n",
    "plot_boundary(ax, b0, b1, b2, margin=1,  ls=':', color='k')\n",
    "ax.scatter(model_sv_a.support_vectors_[:,0], model_sv_a.support_vectors_[:,1], s=200, edgecolors='black', facecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points at the margins (circled) are called *support vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperplanes\n",
    "\n",
    "A line is great when we have two features, but what if we have more?\n",
    "\n",
    "A **hyperplane** is a affine subspace with a dimension of one less than the ambient space. That's what we need for a decision boundary.\n",
    "\n",
    "If we just have one feature, that's a point. If we have two features (as above) the hyperplane is a line. In three dimensions it's a plane, and in four dimensions...\n",
    "\n",
    "Mathematically, a hyperplane in $p$-dimensional space can be described by\n",
    "\n",
    "$$\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...  + \\beta_px_p = 0$$\n",
    "\n",
    "**Warning: For most classification problems we use $y_i \\in \\{0, 1\\}$. For SVC we use $y_i \\in \\{-1, 1\\}$.**\n",
    "\n",
    "So\n",
    "\n",
    "$\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip} > 0$ for $y_i = +1$\n",
    "\n",
    "$\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip} < 0$ for $y_i = -1$\n",
    "\n",
    "or\n",
    "\n",
    "$y_i(\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip}) > 0$\n",
    "\n",
    "We can describe the MMC as finding the maximum value of $M$ for which\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip}) \\ge M \\;\\forall\\; i$$\n",
    "\n",
    "with the constraint \n",
    "\n",
    "$$\\sum_{j=1}^p \\beta_j= 1$$\n",
    "\n",
    "\n",
    "(To see that, recall that in general $\\frac{\\bar x_i \\cdot \\bar v}{||\\bar v||}$ is length of the projection of $\\bar x_i$ along the vector $\\bar v$, or the distance from the hyperplane $\\bar v \\cdot \\bar x = 0$.)\n",
    "\n",
    "Note that for the support vectors on the margin, we have\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip}) - M = 0$$\n",
    "\n",
    "That's a constrained-minimization problem, so we use Lagrange multipliers. In the end, the coefficients will be linear combinations of the $x_i$ using the multipliers $\\alpha_i$. Most importantly, the lagrangian is dependent on the dot product of the $x_i$'s. While there isn't a closed-form, it is convex so solving numerically won't get stuck in a local maximum.\n",
    "\n",
    "\n",
    "One helpful video on the math (that uses a slightly different formalism): https://www.youtube.com/watch?v=_PwhiWxHK8o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for MMC pre-processing will be necessary to normalize or standardize the features.\n",
    "\n",
    "This afternoon we'll discuss a few generalizations to MMC for situations when the classes aren't separable by a hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margins\n",
    "\n",
    "So far, we've only considered the case in which there exists a hyperplane that can divide the two classes. In the real world, even with well-separated classes, we'll always end up with points on the wrong side. To deal with that we'll add a cost to points that violate the margin (or, equivalently, allow a certain 'budget' for points to go past the margin).\n",
    "\n",
    "First, a function to generate a bunch of points from distributions I choose that might overlap a bit, depending on the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_points(seed):\n",
    "    '''Generate 100 bunch of points based on a seed in 2 different classes/distributions,\n",
    "    choosen so they might overlap slightly.'''\n",
    "    np.random.seed(seed)\n",
    "    npts = 100\n",
    "    X = np.zeros((npts, 2))\n",
    "    X[npts/2:, 0] = scs.norm(0,1.5).rvs(npts/2)\n",
    "    X[npts/2:, 1] = scs.norm(0,1.5).rvs(npts/2)\n",
    "    X[:npts/2, 0] = scs.norm(4,1.5).rvs(npts/2)\n",
    "    X[:npts/2, 1] = scs.norm(6,1.5).rvs(npts/2)\n",
    "    y = np.zeros(npts, dtype='int32')\n",
    "    y[npts/2:] = 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def projection(b0, b1, b2, x1, x2):\n",
    "    '''Find projection of a point to a line'''\n",
    "    x1p = (b2*( b2*x1 - b1*x2) - b1*b0) / (b1**2 + b2**2)\n",
    "    x2p = (b1*(-b2*x1 + b1*x2) - b2*b0) / (b1**2 + b2**2)\n",
    "    m = np.sqrt(b1**-2 + b2**-2)/2\n",
    "    dist = np.sqrt((x1-x1p)**2 + (x2-x2p)**2) / m\n",
    "    return x1p, x2p, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "X, y = generate_points(8)\n",
    "model_sv = SVC(kernel='linear', C=C)\n",
    "model_sv.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(X[:,0], X[:,1], color=np.array(['r', 'b'])[y], s=8)\n",
    "ax.set_aspect('equal')\n",
    "b0 = model_sv.intercept_[0]\n",
    "b1 = model_sv.coef_[0][0]\n",
    "b2 = model_sv.coef_[0][1]\n",
    "plot_boundary(ax, b0, b1, b2, color='k')\n",
    "plot_boundary(ax, b0, b1, b2, margin=-1, ls=':', color='k')\n",
    "plot_boundary(ax, b0, b1, b2, margin=1,  ls=':', color='k')\n",
    "ax.scatter(model_sv.support_vectors_[:,0], model_sv.support_vectors_[:,1], s=200, edgecolors='black', facecolors='none')\n",
    "total_slack=0.\n",
    "for i, support in enumerate(model_sv.support_vectors_):\n",
    "    margin = np.sign(model_sv.dual_coef_[0][i])\n",
    "    x1p, x2p, dist = projection(b0-margin, b1, b2, support[0], support[1]) \n",
    "    ax.plot([support[0], x1p], [support[1], x2p], 'g')\n",
    "    total_slack += dist\n",
    "ax.set_title('Soft margins with SVC, C={0}, total slack={1:.2f}'.format(C, total_slack))\n",
    "ax.set_xlim((-2,5))\n",
    "ax.set_ylim((0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *slack* associated with $x_i$ is $\\epsilon_i$ is distance that any given point is past the margin, divided by the width of the margin M. So $\\epsilon_i$ is\n",
    "\n",
    " * 0 if $x_i$ is correctly classified, either right on the margin or outside it,\n",
    " * 1 if $x_i$ is at the decision boundary,\n",
    " * 2 if $x_i$ is at the opposite margin, etc.\n",
    "\n",
    "We can allow a certain amount of slack C:\n",
    "\n",
    "$$\\sum_{i=1}^n \\epsilon_i = budget$$\n",
    "\n",
    "while requiring\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1x_{i1} + ...  + \\beta_px_{ip}) \\ge M (1 - \\epsilon_i)\\;\\forall\\; i$$\n",
    "\n",
    "**Warning: C is used inconsitently in sklearn**\n",
    "\n",
    "To solve this again we follow Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Penalty term\n",
    "\n",
    "Let's investigate what happens when try different values for C for the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def svc_plots(X, y):\n",
    "    '''Fit an SVC and plot a graph for various of different values of the penalty'''\n",
    "    fig, axes = plt.subplots(1,5,figsize=(20,8))\n",
    "    for ax, C in zip(axes, [100, 10, 1, .1, .01]):\n",
    "        model_sv = SVC(kernel='linear', C=C)\n",
    "        model_sv.fit(X, y)\n",
    "        ax.scatter(X[:,0], X[:,1], color=np.array(['r', 'b'])[y], s=5)\n",
    "        ax.set_aspect('equal')\n",
    "        b0 = model_sv.intercept_[0]\n",
    "        b1 = model_sv.coef_[0][0]\n",
    "        b2 = model_sv.coef_[0][1]\n",
    "        plot_boundary(ax, b0, b1, b2, color='k')\n",
    "        plot_boundary(ax, b0, b1, b2, margin=-1, ls=':', color='k')\n",
    "        plot_boundary(ax, b0, b1, b2, margin=1,  ls=':', color='k')\n",
    "        ax.scatter(model_sv.support_vectors_[:,0], model_sv.support_vectors_[:,1], s=200, edgecolors='black', facecolors='none')\n",
    "        ax.set_title('C = {}'.format(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y = generate_points(101)\n",
    "svc_plots(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y = generate_points(99)\n",
    "svc_plots(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second row we're taking a different sample from the same distribution and finding the decision boundary. The decision boundaries on the right-hard (C=0.01) graphs are pretty similar for the two different samples, while for the left-hand graphs (C=100) they are pretty different. One the the other hand, the right-hand graphs are pretty far for the \"true\" boundary. This suggests a bias-variance tradeoff.\n",
    "\n",
    "Let's get a lot more random samples and plot the decision boundaries to see the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5,figsize=(20,8))\n",
    "for ax, C in zip(axes, [100, 10, 1, .1, .01]):\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('C = {}'.format(C))\n",
    "    ax.set_xlim((-4,10))\n",
    "    ax.set_ylim((-4,12))\n",
    "\n",
    "for seed in xrange(100):\n",
    "    X,y = generate_points(seed)\n",
    "    for ax, C in zip(axes, [100, 10, 1, .1, .01]):\n",
    "        model = SVC(kernel='linear', C=C)\n",
    "        model.fit(X, y)\n",
    "        plot_boundary(ax, model.intercept_[0], model.coef_[0][0], model.coef_[0][1], color='k', alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple 1-dimensional classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = scs.uniform(-10,20).rvs(50)\n",
    "y = ((x > 4) | (x < -4)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = scs.uniform(-0.03, .06).rvs(50)\n",
    "fig, ax = plt.subplots(figsize=(8,.5))\n",
    "ax.scatter(x, jitter, alpha=0.5, color=np.array(['r', 'b'])[y])\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x2 = np.power(x, 2)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, x2, alpha=0.5, color=np.array(['r', 'b'])[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit an SVC model and get the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(np.stack([x, x2]).T, y)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, x2, alpha=0.5, color=np.array(['r', 'b'])[y] )\n",
    "plot_boundary(ax, model.intercept_[0], model.coef_[0][0], model.coef_[0][1], color='k')\n",
    "plot_boundary(ax, model.intercept_[0], model.coef_[0][0], model.coef_[0][1], margin=-1, ls=':', color='k')\n",
    "plot_boundary(ax, model.intercept_[0], model.coef_[0][0], model.coef_[0][1], margin=1,  ls=':', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npts = 1000\n",
    "x1 = scs.uniform(-10, 20).rvs(npts)\n",
    "x2 = scs.uniform(-10, 20).rvs(npts)\n",
    "#y = ((x1**2 + x2**2) > 50).astype(int)\n",
    "y = ((x1**2 - x2**2) > 10).astype(int)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "for ax, y in zip(axes, [(x1**2 + x2**2) > 40,\n",
    "                        (x1**2 - x2**2) > 10,\n",
    "                        (x1*x2*x2 - x1**3) < 50\n",
    "                       ]):\n",
    "    y = y.astype(int)\n",
    "    ax.set_xlim(-10,10)\n",
    "    ax.set_ylim(-10,10)\n",
    "\n",
    "    ax.scatter(x1, x2, color=np.array(['r','b'])[y], s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can handle all of these sorts of boundaries by adding lots of extra dimensions. That's great, except we really need to add many, many dimensions.\n",
    "\n",
    "Suppose we have 3 features and want to consider all 2nd-order polynomials. How many more features will we need to add?\n",
    "\n",
    "Here's a video showing this in 3-d space: https://www.youtube.com/watch?v=3liCbRZPrZA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "It turns out that the math above only depends on the dot products of the vectors with each other. We could replace that with a different kernel and get different results.\n",
    "\n",
    "$$\\begin{align}\n",
    "K(x_i, x_{i'}) & = \\sum_{j=1}^p x_{ij} x_{i'j} &\\text{Linear Kernel}\\\\\n",
    "K(x_i, x_{i'}) & = (1 + \\sum_{j=1}^p x_{ij} x_{i'j})^d & \\text{Polynomial Kernal}\\\\\n",
    "K(x_i, x_{i'}) & = \\exp(-\\gamma(\\sum_{j=1}^p (x_{ij} x_{i'j})^2) & \\text{Radial Basis Function Kernel} \n",
    "\\end{align}$$\n",
    "\n",
    "Polynomial kernels require the degree for a hyperparameter.\n",
    "\n",
    "How will this affect bias and variance?\n",
    "\n",
    "RBF kernels require a hyperparameter $\\gamma$. Larger $\\gamma$ will tighten the boundaries, so...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "Up until now, we've been focusing on binary classification.\n",
    "\n",
    "SVM (like Logistic Regression) doesn't naturally fit to a multiple target classes. There are a couple approaches to expanding a binary classifier to a multi-class problem.\n",
    "\n",
    "#### One-vs-rest (OvR)\n",
    "\n",
    "The fastest approach is to build one model for each class, comparing each class to everything else. For any point we choose the class that's the most favored by the corresponding model. This is used in sklearn's LogisticRegression as well as LinearSVC.\n",
    "\n",
    "#### One-vs-one (OvO)\n",
    "\n",
    "The other approach is to build a model for each of the $k \\choose 2$ pairs of classes, training each on just those samples that are in one class or the other. In this approach the overall model predicts the class that is choosen by the most models. This is used by sklearn's SVC and NuSVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities\n",
    "\n",
    "Sklearn's SVC doesn't generate probabilities by default; to do so you need to create the model with `probability=True`. In this case it uses cross-validation to fit a logistic function, which slows down the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced classes\n",
    "\n",
    "If you have seriously unbalanced classes and a small C (i.e., soft margin) the smaller class might get overwelmed. One way to deal with this is with the `class_weight` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other loss functions\n",
    "\n",
    "SVM uses a \"hinge loss\" function, but others are possible with the LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpts = np.linspace(-3,3,100)\n",
    "yhinge = 1 - xpts\n",
    "yhinge[yhinge < 0] = 0\n",
    "ylogit = 1/(1+np.exp(xpts))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xpts, yhinge)\n",
    "ax.plot(xpts, ylogit)\n",
    "\n",
    "ax.set_ylim((-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
