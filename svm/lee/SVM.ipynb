{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM (support vector machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "from svm_ import find_support,plot_svm,plot_boundary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('svm_data.pkl','r') as f:\n",
    "    x,y = pickle.load(f)\n",
    "with open('svm_labels.pkl','r') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "c1 = '#FF6DB6'\n",
    "c2 = '#006DDB'\n",
    "students = [\n",
    "    'Anna Wolak',\n",
    "    'Anna Novakovska',\n",
    "    'Bing Liang',\n",
    "    'Carlos Gallardo',\n",
    "    'Erin Wolpert',\n",
    "    'Evan Li',\n",
    "    'Jane Huston',\n",
    "    'Jerry Tsai',\n",
    "    'Keita Broadwater',\n",
    "    'Kerry Levenberg',\n",
    "    'Matthew Bohan',\n",
    "    'Miles Monaghan',\n",
    "    'Minoo Beyzavi',\n",
    "    'Moses Marsh',\n",
    "    'Neal Riordan',\n",
    "    'Omid Saremi',\n",
    "    'Robert Senseman',\n",
    "    'Sami Rubenfeld',\n",
    "    'Simon Petre',\n",
    "    'Somaye Yari',\n",
    "    'Srinivas Moparthy',\n",
    "    'Sunakshi Bhatia',\n",
    "    'Thomas Dobbs',\n",
    "]\n",
    "def random_student():\n",
    "    return np.random.choice(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x[labels==1],y[labels==1],color=c1,edgecolor=c1)\n",
    "plt.scatter(x[labels==-1],y[labels==-1],color=c2,edgecolor=c2)\n",
    "plt.xlabel('feature1')\n",
    "plt.ylabel('feature2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how can we classify this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = np.linspace(-5,5)\n",
    "plt.scatter(x[labels==1],y[labels==1],color=c1,edgecolor=c1)\n",
    "plt.scatter(x[labels==-1],y[labels==-1],color=c2,edgecolor=c2)\n",
    "plt.xlabel('feature1')\n",
    "plt.ylabel('feature2')\n",
    "\n",
    "plt.plot(v,-3*v,'k--')\n",
    "plt.xlim(-6,6)\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## we want the line that divides negative from positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### could get that line from logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ${{1}\\over{1+e^{-(\\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2)}}} = t$  \n",
    "# $ e^{-(\\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2)} = 1/t - 1 $  \n",
    "# $ \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2 = ln(1/t-1) $  \n",
    "# $ x_2 => y, x_1 => x$  \n",
    "# $ y = {ln(1/t-1)\\over{\\beta_2}}-({{\\beta_1}\\over{\\beta_2}}x + {{\\beta_0}\\over{\\beta_2}})$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision(X,y,threshold=0.5,line_color='k'):\n",
    "    colors = [c1 if i == 1 else c2 for i in y]\n",
    "    m = LogisticRegression().fit(X,y)\n",
    "    mod = np.log(1./threshold - 1)/m.coef_[0][1]\n",
    "    slope = -1.*m.coef_[0][0]/m.coef_[0][1]\n",
    "    intercept = -1.*m.intercept_[0]/m.coef_[0][1] + mod\n",
    "    v = np.linspace(-5,5,100)\n",
    "    plt.scatter(X[:,0],X[:,1],color=colors,edgecolor=colors)\n",
    "    plt.plot(v,slope*v+intercept,color=line_color)\n",
    "    plt.xlabel('feature1')\n",
    "    plt.ylabel('feature2')\n",
    "\n",
    "    plt.ylim(-6,6)\n",
    "    plt.xlim(-6,6)\n",
    "    \n",
    "    return slope,intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([x,y]).T\n",
    "plot_decision(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision(X,labels)\n",
    "plot_decision(X,labels,0.8,'m')\n",
    "plot_decision(X,labels,0.2,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = [c1 if i == 1 else c2 for i in labels]\n",
    "plt.scatter(X[:,0],X[:,1],color=colors,edgecolor=colors)\n",
    "plt.plot(v,-3.*v,'k')\n",
    "plt.plot(v,-18*v-1,'g')\n",
    "plt.plot(v,-1.6*v + 0.3,'m')\n",
    "plt.xlabel('feature1')\n",
    "plt.ylabel('feature2')\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In fact there are an infinite number of lines that we could use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what conditions might we want to impose on the line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_plot(X,colors):\n",
    "    plt.scatter(X[:,0],X[:,1],color=colors)\n",
    "\n",
    "    plt.plot(v,-3*v-3.08,'k')\n",
    "    plt.plot(v,-3*v + 3.08,'k')\n",
    "    plt.plot(v,-3*v,'k--')\n",
    "\n",
    "    plt.xlabel('feature1')\n",
    "    plt.ylabel('feature2')\n",
    "\n",
    "    plt.ylim(-6,6)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_plot(X,colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how can we turn this idea into math?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a plane through the origin, with normal vector $\\vec{w}$, and an unknown point $\\vec{u}$:  \n",
    "if $\\vec{w} \\cdot \\vec{u} > 0$ then point is on right side of plane  \n",
    "if the plane is not through the origin, then there exists a b:  \n",
    "if $\\vec{w} \\cdot \\vec{u} + b> 0$ then the point is on the right side of plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduce notation for positive and negative:  \n",
    "$y_i = 1 \\iff y_i \\text{ is a positive example}$  \n",
    "$y_i = -1 \\iff y_i \\text{ is a negative example}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then:  \n",
    "$y_i * (\\vec{w} \\cdot \\vec{u} + b) \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but what about the margin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "support,s_labels = find_support(X,-3,0)\n",
    "make_plot(X,colors)\n",
    "plt.scatter(support[:,0],support[:,1],color='none',s=150,edgecolor='k')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can assert we want:   \n",
    "$y_i * (\\vec{w} \\cdot \\vec{x} + b) \\ge 1$  \n",
    "where $\\vec{x}$ is the set of points in our data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:  \n",
    "$y_i * (\\vec{w} \\cdot \\vec{x}_i + b) = 1 \\text{ for } \\vec{x}_i \\text{ on the margin}$  \n",
    "on the margin is the point, or points, closest to the separating hyperplane  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what does this mean for finding w (and b)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how wide is the margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$width = (\\vec{x}_+ - \\vec{x}_-)$, where $\\vec{x}_+,\\vec{x}_-$ are support vectors for positive or negative examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but, this width might be angled across the street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "support,s_labels = find_support(X,-3,0)\n",
    "make_plot(X,colors)\n",
    "plt.scatter(support[:,0],support[:,1],color='none',s=150,edgecolor='k')\n",
    "plt.plot([support[0,0],support[1,0]],[support[0,1],support[1,1]])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, lets define:  \n",
    "$width_\\bot = (\\vec{x}_+ - \\vec{x}_-) \\cdot {\\vec{w}\\over{||w||}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try and be a little tricky:  \n",
    "$y_+ * (\\vec{x}_+ \\cdot \\vec{w} + b) = 1$  \n",
    "$y_- * (\\vec{x}_- \\cdot \\vec{w} + b) = 1$  \n",
    "so:  \n",
    "$\\vec{x}_+ \\cdot \\vec{w} = 1 - b$  \n",
    "$\\vec{x}_- \\cdot \\vec{w} = -1 - b$  \n",
    "so:  \n",
    "$width_\\bot = {(1 - b + 1 + b)\\over{||w||}}$  \n",
    "$width_\\bot = {2\\over{||w||}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is really nice, now to make the biggest street possible we just need to maximize $2/||w||$  \n",
    "or equivalently:  \n",
    "minimize $\\frac{1}{2}||w||^2$  \n",
    "subject to $y_i(\\vec{w} \\cdot \\vec{x} + b) \\ge 1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##how do we maximize/minimize functions with constraints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathscr{L} = \\left(\\frac{1}{2}\\right) ||w||^2 - \\sum_{i} \\left[\\alpha_i (y_i (\\vec{w} \\cdot \\vec{x}_i + b)-1)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after some math..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\max\\limits_{\\alpha \\ge 0, \\sum\\limits_i[\\alpha_i y_i] = 0} \\left (\\sum_{i} \\alpha_i - \\left(\\frac{1}{2}\\right) \\sum_{i,j} \\left[\\alpha_i \\alpha_j  y_i  y_j  (\\vec{x}_i \\cdot \\vec{x}_j) \\right]\\right )$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, for an unknown point $\\vec{u}$ we would check:  \n",
    "$y_u = sign(\\vec{w} \\cdot \\vec{u} + b)$  \n",
    "  \n",
    "we can update our prediction function to:  \n",
    "$y_u = sign(\\sum\\limits_i [\\alpha_i y_i (\\vec{x}_i \\cdot \\vec{u})] + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to calculate $\\vec{w}$ and $b$:  \n",
    "$ \\vec{w} = \\sum_\\limits{i} [\\alpha_i y_i \\vec{x}_i] $  \n",
    "$ b = y_k - \\vec{w} \\cdot \\vec{x}_k \\text{ for any } \\vec{x}_k \\text{ with } \\alpha_k \\neq 0 $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization is now a quadratic optimization in $\\alpha$  \n",
    "importantly this, and the decision function, only depend on the dot products of our input data, we will talk more about what that means this afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision(svc, show=True):\n",
    "    # get the separating hyperplane\n",
    "    w = svc.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (svc.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors\n",
    "    margin = 1. / np.sqrt(np.sum(svc.coef_ ** 2))\n",
    "    yy_down = yy + a * margin\n",
    "    yy_up = yy - a * margin\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.plot(xx, yy, 'k--')\n",
    "    plt.plot(xx, yy_down, 'k')\n",
    "    plt.plot(xx, yy_up, 'k')\n",
    "    \n",
    "    if show:\n",
    "        print 'slope of decision boundary: {}'.format(a)\n",
    "        print 'margin width: {}'.format(2*margin)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X,labels)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],color=colors)\n",
    "plot_svc_decision(svc,True)\n",
    "svc_sp = svc.support_vectors_\n",
    "\n",
    "plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print svc.support_vectors_\n",
    "print '-'*28\n",
    "print X[svc.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print svc.coef_[0]\n",
    "for ind in svc.support_:\n",
    "    b = labels[ind]-svc.coef_[0].dot(X[ind])\n",
    "    print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:,1] = X[:,1]*1500\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X,labels)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],color=colors)\n",
    "plot_svc_decision(svc,True)\n",
    "svc_sp = svc.support_vectors_\n",
    "\n",
    "plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "\n",
    "plt.ylim(-6*1500,6*1500)\n",
    "X[:,1] = X[:,1]/1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print svc.support_vectors_\n",
    "print svc.coef_[0]\n",
    "print svc.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dimensions\n",
    "### this will come up again later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dimensions](ryan_dim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# afternoon\n",
    "## soft margins and kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('svm_data_insep.pkl','r') as f:\n",
    "    xi,yi = pickle.load(f)\n",
    "with open('svm_labels_insep.pkl','r') as f:\n",
    "    labelsi = pickle.load(f)\n",
    "colorsi = [c1 if i == 1 else c2 for i in labelsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xi = np.array([xi,yi]).T\n",
    "\n",
    "plt.scatter(Xi[:,0],Xi[:,1],color=colorsi)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what now?\n",
    "### can we use maximum margin for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of looking at $y_i * (\\vec{w} \\cdot \\vec{u} + b) \\ge 0$  \n",
    "we use $y_i * (\\vec{w} \\cdot \\vec{u} + b) \\ge 1 - \\xi_i$  \n",
    "$\\xi_i$ are called slack variables  \n",
    "  \n",
    "they penalize samples that are either within the margin, or even on the wrong side of the boundary  \n",
    "  \n",
    "the resulting classifier, a soft-margin svm, now makes trade off between maximizing correct classification and making the widest possible margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc2 = SVC(C=1,kernel='linear')\n",
    "svc2.fit(Xi,labelsi)\n",
    "\n",
    "plt.scatter(Xi[:,0],Xi[:,1],color=colors)\n",
    "plot_svc_decision(svc2,True)\n",
    "# svc_sp = svc2.support_vectors_\n",
    "\n",
    "# plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "print 'accuracy: {}'.format(svc2.score(Xi,labelsi))\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc2 = SVC(C=0.001,kernel='linear')\n",
    "svc2.fit(Xi,labelsi)\n",
    "\n",
    "plt.scatter(Xi[:,0],Xi[:,1],color=colors)\n",
    "plot_svc_decision(svc2,True)\n",
    "svc_sp = svc2.support_vectors_\n",
    "\n",
    "plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "print 'accuracy: {}'.format(svc2.score(Xi,labelsi))\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now, support vectors are all points within the margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimization problem changes to:  \n",
    "minimize: $\\frac{1}{2}||w||^2 + C\\sum\\limits_i\\xi_i$  \n",
    "constraint: $y_i * (\\vec{w} \\cdot \\vec{u} + b) \\ge 1 - \\xi_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dual form looks (almost) the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\max\\limits_{0 \\le \\alpha \\le C, \\sum\\limits_i[\\alpha_i y_i] = 0} \\left (\\sum_{i} \\alpha_i - \\left(\\frac{1}{2}\\right) \\sum_{i,j} \\left[\\alpha_i \\alpha_j  y_i  y_j  (\\vec{x}_i \\cdot \\vec{x}_j) \\right]\\right )$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have introduced a maximum value on the alphas\n",
    "this allows for misclassifications, and for wider margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svct = SVC(C=0.01,kernel='linear')\n",
    "svct.fit(X,labels)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],color=colors)\n",
    "plot_svc_decision(svct,True)\n",
    "svc_sp = svct.support_vectors_\n",
    "\n",
    "plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svct = SVC(C=0.01,kernel='linear')\n",
    "svct.fit(Xi,labelsi)\n",
    "\n",
    "plt.scatter(Xi[:,0],Xi[:,1],color=colors)\n",
    "plot_svc_decision(svct,True)\n",
    "svc_sp = svct.support_vectors_\n",
    "\n",
    "plt.scatter(svc_sp[:,0],svc_sp[:,1],color='none',s=150,edgecolor='k')\n",
    "plt.ylim(-6,6)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember our optimization and decision rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\max\\limits_{0 \\le \\alpha \\le C, \\sum\\limits_i[\\alpha_i y_i] = 0} \\left (\\sum_{i} \\alpha_i - \\left(\\frac{1}{2}\\right) \\sum_{i,j} \\left[\\alpha_i \\alpha_j  y_i  y_j  (\\vec{x}_i \\cdot \\vec{x}_j) \\right]\\right )$  \n",
    "$y_u = sign(\\sum\\limits_i [\\alpha_i y_i (\\vec{x}_i \\cdot \\vec{u})] + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel functions allow us to get the result of the dot product of vectors after being transformed to a higher dimensional space, without actually needing to know what that transformation is  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Phi(\\vec{x})$ is some transformation from space of dimension d to space of dimenstion d+n  \n",
    "then $K(\\vec{x}_1,\\vec{x}_2) = \\langle\\Phi(\\vec{x}_1),\\Phi(\\vec{x}_2)\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since our decision rule and optimization only depend on dot products between our input data we can use kernels to transform our data to higher dimensions, without having to explicitly compute the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many kernels to potentially use, but some of the most common\n",
    "are:  \n",
    "  \n",
    "linear:  \n",
    "$K(\\vec{x}_i,\\vec{x}_j) = \\vec{x}_i \\cdot \\vec{x}_j$  \n",
    "  \n",
    "polynomial (of degree d):  \n",
    "$K(\\vec{x}_i,\\vec{x}_j) = (1 + \\vec{x}_i \\cdot \\vec{x}_j)^d$  \n",
    "  \n",
    "radial basis function (gaussian):  \n",
    "$K(\\vec{x}_i,\\vec{x}_j) = \\exp(\\gamma * ||\\vec{x}_i - \\vec{x}_j||^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://rvlasveld.github.io/images/oc-svm/visualization.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('rbf_data3.pkl','r') as f:\n",
    "    x_rbf,y_rbf = pickle.load(f)\n",
    "with open('rbf_labels3.pkl','r') as f:\n",
    "     labels_rbf = pickle.load(f)\n",
    "        \n",
    "colors_rbf = [c1 if i == 1 else c2 for i in labels_rbf]\n",
    "X_rbf = np.array([x_rbf,y_rbf]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=1,gamma=0.1).fit(X_rbf,labels_rbf)\n",
    "plot_boundary(svm,X_rbf)\n",
    "plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=1,gamma=1).fit(X_rbf,labels_rbf)\n",
    "plot_boundary(svm,X_rbf)\n",
    "plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for g in [0.1,0.3,1,3,10]:\n",
    "    plt.figure()\n",
    "    svm = SVC(C=1,gamma=g).fit(X_rbf,labels_rbf)\n",
    "    plot_boundary(svm,X_rbf)\n",
    "    plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=1,gamma=250).fit(X_rbf,labels_rbf)\n",
    "plot_boundary(svm,X_rbf)\n",
    "plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [1E-1,1,10,100]:\n",
    "    plt.figure()\n",
    "    svm = SVC(C=c,gamma=3).fit(X_rbf,labels_rbf)\n",
    "    plot_boundary(svm,X_rbf)\n",
    "    plt.scatter(X_rbf[:,0],X_rbf[:,1],color=colors_rbf)\n",
    "\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid search](ryan_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two main methods are used for multiclass svm:  \n",
    "**one vs all:** train N models, assign points based on which model has largest value for $\\vec{w} \\cdot \\vec{x} + b$    \n",
    "**one vs one:** train $N * (N-1)\\over{2}$ models, assign points based on majority vote of classes it has been assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final notes/thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svm algorithm is ~$O(\\text{rows}^2 * \\text{cols})$ [i couldn't find clear numbers on the order, but regardless of specifics it is rather slow, and slower for large n]  \n",
    "- however, solution space can end up being very sparse, which is useful for situations where p >> n  \n",
    "- be careful using svm on very large data sets  \n",
    "- the rbf kernel is very powerful, but also prone to overfitting, use with caution  \n",
    "- in researching this lecture i discovered that people do all kinds of interesting/crazy things with svm  \n",
    "    - single class svms can be used for anonomly/outlier detection (is implemented in sklearn)  \n",
    "    - svm clustering is something that exists, but that i still don't really have a good handle on  \n",
    "    - svm regression is done, also not entirely sure how that works  \n",
    "    - you can include multiclass and cross validation scores in the dual space optimization algorithm, although i am not suggesting anyone actually try this  \n",
    "    - i found some pretty extensive discussion of svm's used with genomic data, not sure if this is actually wide spread or there is just a vocal subset of users  \n",
    "- it is not really possible to get probabilites out of svm models, which is annoying as people often want these  \n",
    "- svm's have a natural fit with problems where there is a natural sense of the margin\n",
    "    - for example, while i have not seen this done, it seems like svm's could work well for models where there are two clear classes, and then a zone of results that would like a human to check (for example fraud detection) could then tune your parameters to get a margin that matches what types of samples you would want human interaction with\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
