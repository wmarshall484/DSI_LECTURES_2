{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's an SVM?\n",
    "\n",
    "A classification algorithm.\n",
    "\n",
    "Remember LogisticRegression?\n",
    " * Linear model whose parameters derived from maximizing log-likelihood of odds ratios.\n",
    " * Outputswere probability estimates.\n",
    " \n",
    "Why do we need SVM?\n",
    " * Imagine a trivial case where we have two categories of points that are completely\n",
    "separable.\n",
    " * Why should we choose the logistic regression line out of all the possible options? Maybe\n",
    "some other line would be best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Let's let L be an _affine set_, a hyperplane that doesn't pass through the origin.\n",
    "\n",
    "$$ L = \\{x:f(x) = \\beta_0 + \\beta^Tx=0\\} $$\n",
    "\n",
    "$ f(x) $ is proprtional to the signed distance from $ x $ to the hyperplane.\n",
    "\n",
    "Remembering that \"the projection of $ x $ onto $ \\beta $\" is:\n",
    "\n",
    "$$ proj_\\beta(x) = x \\cdot \\frac{\\beta}{|\\beta|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Separating Hyperplanes\n",
    "\n",
    "If our data is \"perfectly separable\"\n",
    "\n",
    "$$ max_{\\beta, \\beta_0} M $$\n",
    "subject to:\n",
    "$$ y_i(x_i^T\\beta + \\beta_0) \\ge M, \\forall i = 1,...,N $$\n",
    "$$ ||\\beta|| = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximizing the Margin\n",
    "\n",
    "The optimal separating hyperplane gives us an alternative separating line to the logistic regression line.\n",
    "\n",
    "Why would we want an alternative?\n",
    "\n",
    "* Boundary points (support) are most important\n",
    "* Consequences for prediction\n",
    "* Convergence problems for Logistic Regression\n",
    "* Cares about \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non Separable Case\n",
    "\n",
    "What if our data is not separable? This means we can't satisfy these constraints for all $ i $.\n",
    "\n",
    "$$ y_i(x_i^T\\beta + \\beta_0) \\ge M, \\forall i = 1,...,N $$\n",
    "\n",
    "Which we'll discuss this afternoon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Morning Sprint Notes:\n",
    "\n",
    "1. You will compare Logistic Regression with Maximal Margin Classifier.\n",
    "1. You'll be plotting boundaries, remember `matplotlib` function signature.\n",
    "1. Keep in mind separable data is a _very special case_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Objectives:\n",
    "\n",
    "* Differentiate SVM from Maximal Margin Classfier\n",
    "* Derive in completely separating case.\n",
    "* Write full specification and understand the components of that specification\n",
    "* Extend understanding to include the \"kernel trick\"\n",
    "* Learn what's necessary to use SVMs in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imperfectly Separable\n",
    "\n",
    "$$ max_{\\beta, \\beta_0}M $$\n",
    "subject to:\n",
    "$$ ||\\beta|| = 1 $$\n",
    "$$ y_i(x_i^T\\beta + \\beta_0) \\ge M(1-\\xi_i)\\forall i$$\n",
    "$$ \\xi_i \\ge 0, \\sum_i \\xi_i <\\ constant $$\n",
    "\n",
    "We can transform that into a minimization problem that looks like:\n",
    "\n",
    "$$ min_{\\beta, \\beta_0} \\frac{1}{2}||\\beta||^2 + C\\sum_{i=1}^N \\xi_i $$\n",
    "subject to:\n",
    "$$ \\xi \\ge 0 $$\n",
    "and\n",
    "$$ y_i(x_i^T\\beta+\\beta_0) \\ge 1 - \\xi_i \\forall i $$\n",
    "\n",
    "Which of the terms above is a cost function and which is the margin?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hinge Loss\n",
    "\n",
    "$$ y_i(x_i^T\\beta+\\beta_0) \\ge 1 - \\xi_i $$\n",
    "$$ 1 - y_i(x_i^T\\beta+\\beta_0) \\le \\xi_i $$\n",
    "\n",
    "So what does this function look like if things are classified correctly (incorrectly)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical Concerns\n",
    "\n",
    "* Class imbalance ( `class_weight` argument in sklearn)\n",
    "* Scaling (as usual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Woop dee doo\n",
    "## Another linear classfier\n",
    "\n",
    "What if our dat is non-linear? Of course we could use basis transformations like those we used with logistic regression for example. Something like:\n",
    "    \n",
    "$$ \\hat{f}(x) = h(x)^T\\hat{\\beta} + \\hat{\\beta_0} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ h: N \\rightarrow V $$\n",
    "\n",
    "But what transformations? How many? What if I want to use a lot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![kerneltrick](https://camo.githubusercontent.com/cc731fecfbaf6cfb597bbf308eeac964ca9daffc/687474703a2f2f72766c617376656c642e6769746875622e696f2f696d616765732f6f632d73766d2f76697375616c697a6174696f6e2e676966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The \"trick\"\n",
    "\n",
    "We can use the \"kernel trick\" to avoid explicitly evaluating $h$ for some cases.\n",
    "\n",
    "There are certain kernel functions $k: NxN \\rightarrow \\mathbb{R} $, which can be expressed as an inner product in some other (much) higher-dimensional space.\n",
    "\n",
    "$$ k(x,x') = \\langle h(x), h(x') \\rangle v $$\n",
    "\n",
    "Since the inner product gives us a notion of similarity, which is related to the inverse of distance, we will have a solution for:\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\sum_{i=1}^N \\alpha_ik(x, x_i) $$\n",
    "\n",
    "Computing $k(.,.) $ can often be cheaper than directly computing the basis transforms $ h(.) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can I use my favorite basis transforms?\n",
    "\n",
    "Popular kernels include:\n",
    "    \n",
    "Radial Basis Function (`rbf`) or Gaussian\n",
    "$$ K(x,y) = exp(\\gamma||x-y||^2) $$\n",
    "\n",
    "Polynomial\n",
    "$$ K(x,y) = (a + x^Ty)^d $$\n",
    "\n",
    "and Sigmoid (Hyperbolic Tangent)\n",
    "\n",
    "$$ tanh(\\gamma\\langle x, y\\rangle + r) $$\n",
    "\n",
    "Other kernels are possible, but in practice RBF is commonly used on a wide range, and found to perform well.\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why use SVM or kernels?\n",
    "\n",
    "1. Sparsity of solutions achievable via l1 penalty.\n",
    "1. Rows and columns: lots of columns may allow linearity, few rows may require it.\n",
    "1. Interpretability of coefficients in linear case.\n",
    "1. Use of kernels may be effective for prediction.\n",
    "1. When margin might be the right idea?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-class classification\n",
    "\n",
    "So far, we've talked about how to classify two-class data, but what if there are more than two classes?\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "* One vs. Rest\n",
    "* One vs. One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One vs. Rest\n",
    "\n",
    "If we have $ K $ classes, train $ K $ models. Let $f_k$ be the kth model. To choose a class for our problem we simply:\n",
    "\n",
    "$$ f(x) = argmax_kf_k(x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One vs. One\n",
    "\n",
    "If we have $ K $ classes, train $ K(K-1)/2 $ models.\n",
    "\n",
    "Then let\n",
    "\n",
    "$$ f(x) = argmax_k(\\sum_j f_{kj}(x)) $$\n",
    "\n",
    "for example, choose the case with the maximum number of votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OvR vs. OvO\n",
    "\n",
    "## OvR\n",
    "\n",
    "* Requires probabilities\n",
    "* Trains K models, with N rows each.\n",
    "\n",
    "## OvO\n",
    "* Votes may have ties\n",
    "* Trains $ K(K-1)/2 $ models, but models have only $ 2 \\frac{N}{K} $ rows in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real examples\n",
    "\n",
    "Let's see a real example of logistic regression vs. SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
