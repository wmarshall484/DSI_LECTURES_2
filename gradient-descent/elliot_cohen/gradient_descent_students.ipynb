{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cbook\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LightSource\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy can be used to check your calculus\n",
    "\n",
    "A simple derivative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3*x**2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate symbol first\n",
    "x = sympy.symbols('x')\n",
    "# take derivative\n",
    "sympy.diff(x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x**3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indefinite integral\n",
    "sympy.integrate(3*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definite integral\n",
    "sympy.integrate(3*x**2, (x, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sympy.Matrix(sympy.symbols('a b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [a],\n",
       " [b]]), Matrix([[a, b]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2*a + 2*b, 2*a + 2*b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sympy.diff(sum(m*m.T), i) for i in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[a**2,  a*b],\n",
       "[ a*b, b**2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for reference...this is what this looks like...\n",
    "m*m.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in one dimension\n",
    "\n",
    "Let's start with an overly simplified example, a cost function of $ y = 3x^2 $\n",
    "\n",
    "In this case, 'x' is our parameter that we are trying to minimize. \n",
    "\n",
    "First, you'll need to write a function that computes the gradient of the cost function at a given x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(x):\n",
    "    return 3*x**2\n",
    "\n",
    "def grad_cost_func(x):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from calculus that we can find the minimum of this function at x = 0. \n",
    "\n",
    "For demonstation purposes, we pretend that we don't know this, and we will start with a guess of x = 5 for the minimum. :)\n",
    "\n",
    "Let's plot this, for a visual..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# keeping our guesses in a list for reasons that will become clear later....\n",
    "guesses = [5]\n",
    "\n",
    "def plot_cost(x_guess):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111) \n",
    "    x = np.linspace(-6, 6)\n",
    "    y = cost_func(x)\n",
    "    ax.plot(x, y)\n",
    "    y_guess = [cost_func(xg) for xg in x_guess]\n",
    "    ax.plot(x_guess, y_guess, 'ro')\n",
    "    \n",
    "    labels = ['step {}'.format(i) for i in range(len(x_guess))]\n",
    "    for label, x, y in zip(labels, x_guess, y_guess):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-20, 20),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "   \n",
    "plot_cost(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this one-dimensional case, we can visually inspect and see that we are pretty far from the minimum! \n",
    "\n",
    "Let's compute the gradient at this point to update our \"guess\" for x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "grad = grad_cost_func(guesses[-1])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is huge at our guess! If we simply adjusted by that amount, we would overshoot the minimum by far! This is why we use the learning rate to adjust our guess by small steps. Let's try a learning rate of .05, so we don't go too far. Since we are doing gradient DESCENT, we should subtract the gradient from our guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-83c4f2da4e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mguess_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# let's keep our guesses in a list...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mguesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "guess_update = guesses[-1] - learning_rate*grad\n",
    "guesses.append(guess_update)  # let's keep our guesses in a list...\n",
    "\n",
    "print(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot including our new guess..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot closer. Let's do this three more times and see where we land..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    grad = grad_cost_func(guesses[-1])\n",
    "    guess_update = guesses[-1] - learning_rate*grad\n",
    "    guesses.append(guess_update)\n",
    "\n",
    "plot_cost(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as we get closer to the minimum, the gradient gets smaller and so the guesses step more slowly. This is a good thing - a natural property that makes it harder for us to overshoot!\n",
    "\n",
    "Let's see if 5 more times does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    grad = grad_cost_func(guesses[-1])\n",
    "    guess_update = guesses[-1] - learning_rate*grad\n",
    "    guesses.append(guess_update)\n",
    "\n",
    "plot_cost(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks pretty close, let's inspect the list to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't quite get to the minimum. It is left as an exercise to the reader to determine how many more steps it will take to get to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cost functions\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^N (y_i - h_\\theta(x_i))^2 $$\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "$$ \\ln p(\\vec{y}|X;\\theta) = \\sum_{i=1}^N (y_i \\ln h_\\theta (x_i) + (1- y_i)\\ln(1- (h_\\theta (x_i))) $$\n",
    "\n",
    "\n",
    "In your assignment, you will be working through the gradient descent algorithm for logistic regression. Let's work through an example using a linear regression. \n",
    "\n",
    "First, we'll instantiate a small \"dataset\" X, with 2 features and 10 rows, and some beta coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((10, 2))\n",
    "true_betas = np.array([3, 4]).reshape(-1, 1) # go ahead and turn this into a column vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when doing gradient descent, we know our Xs and ys, but are actually trying to solve for the best values of beta. So we need to calculate the y that we will use  \n",
    "\n",
    "Note - we already know the correct answer for our coefficients, which is a good way to practice our algorithm - we already know the correct answer! Note that since there does exist a set of betas that X dot beta is exactly y, we expect this to converge absolutely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.dot(true_betas).reshape(-1, 1) # keep this as a column vector so linear algebra goes smoothly\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reminder: Gradient Formula\n",
    "* The gradient is the multivariate analogue of the derivative.\n",
    "    * More precisely, it's the length $n$ vector of partial derivatives where $ f: \\mathbb{R}^n \\rightarrow \\mathbb{R} $.\n",
    "* Geometrically the gradient is the direction of steepest descent.\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^P\\frac{\\partial f}{\\partial x_i} e_i$$\n",
    "\n",
    "How can we use the direction of steepest descent to find the minimum of our function?\n",
    "\n",
    "How about the maximum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Gradient Descent\n",
    "\n",
    "**1. Compute objective function.** We've done this. We are doing a linear regression, so we will use the sum of squared errors as our cost function. \n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^N (y_i - h_\\theta(x_i))^2 $$\n",
    "\n",
    "**2. Compute gradient of objective function.** Compute the gradient of this below. You may find it helpful to break this into components, i.e.\n",
    "$$ J(\\theta) = (y_1 - x_1\\beta_1- x_2\\beta_2)^2 $$\n",
    "but you should return the matrix formulation. \n",
    "\n",
    "Put the gradient in a function that will return the gradient vector for given X, betas, and y. This vector should have the same shape as your coefficient array!\n",
    "\n",
    "Remember the product and chain rules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse_grad_func(X, beta_guess, y):\n",
    "    # your code here\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in practice, we won't know our coefficients, so we'll need to start with a \"random guess\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_guess = np.ones((2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the cost function. \n",
    "\n",
    "Note that this cost function is very smooth and does have an absolute minimum, because of the way we set the problem up. \"Real\" cost functions will lack this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_surface(betas):\n",
    "    return 0.5*((y - X.dot(betas))**2).sum()\n",
    "\n",
    "def plot_cost_surface():\n",
    "    beta1_space = np.linspace(-10, 10).reshape(-1, 1)\n",
    "    beta2_space = np.linspace(-10, 10).reshape(-1, 1)\n",
    "    \n",
    "    cost_surface_z = [cost_func_surface(np.array([beta1, beta2]).reshape(-1, 1)) for beta1 in beta1_space for beta2 in beta2_space] \n",
    "\n",
    "    beta1_space, beta2_space = np.meshgrid(beta1_space, beta2_space)\n",
    "\n",
    "    cost_surface_z = np.array(cost_surface_z).reshape(50,50)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='3d'))\n",
    "\n",
    "    ls = LightSource(270, 45)\n",
    "    # To use a custom hillshading mode, override the built-in shading and pass\n",
    "    # in the rgb colors of the shaded surface calculated from \"shade\".\n",
    "    rgb = ls.shade(cost_surface_z, cmap=cm.rainbow, vert_exag=0.1, blend_mode='soft')\n",
    "\n",
    "    surf = ax.plot_surface(beta1_space, beta2_space, cost_surface_z, facecolors=rgb)\n",
    "    ax.set_xlabel('beta 1')\n",
    "    ax.set_ylabel('beta 2')\n",
    "    ax.set_zlabel('cost')\n",
    "\n",
    "    \n",
    "plot_cost_surface()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Gradient Descent\n",
    "\n",
    "**3. Adjust weights/parameters by the gradient of the cost function scaled by the learning rate** We should update our betas given the gradient of the cost function that we just computed, scaled by our learning rate. This is gradient DESCENT, so we should subtract our update. \n",
    "\n",
    "Write a function that performs a parameter update and returns updated parameters (This is very simple - should be a one liner, use numpy. DO NOT OVERTHINK THIS!). Since our problem is simple, the default learning rate is (relatively) high at 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paramater_update(betas_old, grad, lr=0.02):\n",
    "    # your code here\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one update step on our parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = sse_grad_func(X, beta_guess, y)\n",
    "\n",
    "beta_1_iter = paramater_update(beta_guess, grad)\n",
    "\n",
    "print('New values of beta after 1 update: {}'.format(beta_1_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together! \n",
    "\n",
    "If you have done your math right, this should be a step in the right direction. But it takes some iterating to get to the correct answer. \n",
    "\n",
    "Write a function with a for loop below to perform the full gradient descent. Experiment with values of max_iter to see how long it takes to converge to the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gradient_descent(X, y, beta_guess = np.ones((2,1)), lr = .02, max_iter = 100): \n",
    "    # your code here\n",
    "    # return params\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Gradient Descent Convergence Criterion\n",
    "We did the simplest case of convergence criteria, a set number of iterations. In practice, you would want to use a more sophisticated convergence criterion - i.e. stopping iterations when your result stops significantly improving.\n",
    "\n",
    "Copy and paste your function from into two blocks below, and improve by adding the following stopping criteria in addition to maximum iterations. Feel free to adjust the default paramaters if you don't like the results!\n",
    "\n",
    "* Change in cost function $ (cost_{old} - cost_{new}) / cost_{old} < \\epsilon $\n",
    "* Magnitude of gradient < $ \\epsilon $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this will likely require a helper function to compute the cost!\n",
    "\n",
    "def gradient_descent_cost_fcn(X, y, beta_guess = np.ones((2,1)), lr = .02, max_iter = 1000, epsilon = .00001):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_grad_mag(X, y, beta_guess = np.ones((2,1)), lr = .02, max_iter = 1000, epsilon = .01):\n",
    "    # your code here\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus nice-to-know...Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(f_prime, f_double_prime, initial_guess,\n",
    "                   threshold, max_iter):\n",
    "    x = float(initial_guess)\n",
    "    iterations = 0\n",
    "    x_history = [x]\n",
    "    while f_prime(x) > threshold and iterations < max_iter:\n",
    "        x = x - f_prime(x)/f_double_prime(x)\n",
    "        x_history.append(x)\n",
    "        iterations += 1\n",
    "    print('newtons method took %s iterations'%iterations)\n",
    "    return x_history\n",
    "\n",
    "newton_x_history = newtons_method(f_prime, \n",
    "                                  f_double_prime,\n",
    "                                  initial_guess, \n",
    "                                  threshold,\n",
    "                                  max_iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
