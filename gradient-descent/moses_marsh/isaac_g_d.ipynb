{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization Methods\n",
    "## Gradient Descent\n",
    "### Objectives\n",
    "\n",
    "At the end of the day you should be able to:\n",
    "\n",
    "1. Know the purpose of gradient descent, and name some specific applications we already know.\n",
    "1. Write pseudocode of the gradient descent and stochastic gradient descent algorithms.\n",
    "1. Compare and contrast batch and stochastic gradient descent - the algorithms, costs, and benefits.\n",
    "1. Draw Gradient Descent cartoons!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "Generally: find the minimum or maximum of a function.\n",
    "\n",
    "## Example 1:\n",
    "\n",
    "Find the parameters of a model which maximize the likelihood of data.\n",
    "\n",
    "## Example 2:\n",
    "\n",
    "Find the parameters of a model which minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What can we optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "x = np.arange(-5,5, .001)\n",
    "funcs = [-(x**2), x**3, exp(x)]\n",
    "fig, subps = plt.subplots(1,len(funcs))\n",
    "fig.set_size_inches(12,6)\n",
    "for ax, func in zip(subps, funcs):\n",
    "    ax.plot(x, func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost functions\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N (y_i - h_\\theta(x_i))^2 $$\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "$$ \\ln p(\\vec{y}|X;\\theta) = \\sum_{i=1}^N (y_i \\ln h_\\theta (x_i) + (1- y_i)\\ln(1- (h_\\theta (x_i))) $$\n",
    "Which comes from the binomial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding an optimium\n",
    "\n",
    "## Calculus\n",
    "\n",
    "?\n",
    "\n",
    "## Find where the derivative is 0, numerically rather than analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Gradient\n",
    "* The gradient is the multivariate analogue of the derivative.\n",
    "    * More precisely, it's the length $n$ vector of partial derivatives where $ f: \\mathbb{R}^n \\rightarrow \\mathbb{R} $.\n",
    "* Geometrically the gradient is the direction of steepest descent.\n",
    "\n",
    "$$ \\nabla f = \\sum_{i=1}^P\\frac{\\partial f}{\\partial x_i} e_i$$\n",
    "\n",
    "How can we use the direction of steepest descent to find the minimum of our function?\n",
    "\n",
    "How about the maximum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "Follow the direction of steepest descent until convergence.\n",
    "\n",
    "But the gradient gives us the _direction_ of steepest descent, is this sufficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent Algorithm\n",
    "\n",
    "$$ \\theta_{i+1, j} = \\theta_{i, j} + \\alpha \\frac{\\partial J((\\vec\\theta_i))}{\\partial \\theta_{i, j}} $$\n",
    "\n",
    "* $ \\alpha $: Learning rate\n",
    "* $ i $: Iteration\n",
    "* $ j $: Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Pseudo code, raises errors.\n",
    "new_params = dict((i,0) for i in range(k)) #Initialize parameters\n",
    "while not has_converged:\n",
    "    params = copy(new_params)\n",
    "    for theta in params:\n",
    "        new_params[theta] -= learning_rate * gradient(theta, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent Convergence Criterion\n",
    "* Max number of iterations\n",
    "* Change in cost function $ (cost_{old} - cost_{new})/cost_{old} < \\epsilon $\n",
    "* Magnitude of gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demonstration: see live notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Caveats\n",
    "\n",
    "* Requires differentiable, convex function\n",
    "* Only finds global optimum on globally convex function\n",
    "* Converges linearly for strongly convex functions.\n",
    "* May not converge for weakly convex functions.\n",
    "* Requires feature scaling\n",
    "* Learning rate must be chosen (well)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Motivation\n",
    "Problems with Gradient Descent\n",
    "\n",
    "* Memory (data needs to fit)\n",
    "* Processor (cost function over all rows is expensive)\n",
    "* Online (making updates requires complete restart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition\n",
    "\n",
    "Remember the linear regression cost function:\n",
    "    \n",
    "?\n",
    "\n",
    "The _expected cost_ of a single randomly chosen observation is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SGD\n",
    "\n",
    "Use a cost function based on a single randomly chosen observation:\n",
    "\n",
    "$$ J(\\theta) = (h_\\theta(x_i) - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why use SGD?\n",
    "\n",
    "* Big data\n",
    "* Converges faster on average the GD\n",
    "* Can work online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other flavors\n",
    "\n",
    "* \"batch\" is sometimes used to refer to regular GD.\n",
    "* guess what minibatch is?\n",
    "* Online SGD: take a step every time you get a new observation\n",
    " * Optionally sunset old observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Newton-Raphson Method\n",
    "\n",
    "A fancy way of choosing our learning rate $\\alpha$ in GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\theta_{i+1} = \\theta_i - \\frac{1}{J''(\\theta)}J'(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Newton-Raphson\n",
    "\n",
    "$$ \\theta_{i+1} = \\theta_{i} - [H(f(x)]^{-1}\\nabla f(x) $$\n",
    "\n",
    "What might prevent us from using this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hessian has to be computed, and inverted.\n",
    "* Can diverge with a bad starting position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The universally best optimization method to always use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('the_universally_best_optimization_method_to_always_use.secret') as f:\n",
    "    use(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why should I care?\n",
    "\n",
    "Q: Will you ever invent a new cost function that you need to optimize directly?\n",
    "\n",
    "A: Not too often\n",
    "\n",
    "But, optimization underlies everything we do as data scientists, so understanding these concerns will help you use high-level tools. Knowing what optimization approach is used for different methods is a good signal of competence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "help(lm.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
