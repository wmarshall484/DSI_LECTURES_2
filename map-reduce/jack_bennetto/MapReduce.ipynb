{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop and MapReduce\n",
    "### Jack Bennetto\n",
    "#### April 10, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this class, we will be able to:\n",
    "\n",
    "- Explain how HDFS stores large files on a cluster.\n",
    "- Describe MapReduce, and how it relates to Hadoop.\n",
    "- Explain types of problems which benefit from MapReduce.\n",
    "- Write MapReduce (multi-step) jobs in python using MRJob.\n",
    "- Speed up MapReduce using combiners, map-only jobs, and counters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Morning\n",
    " * Big data\n",
    " * HDFS\n",
    " * MapReduce\n",
    " * Examples with MRJob\n",
    "\n",
    "Afternoon\n",
    " * Hive & Pig\n",
    " * Combiners\n",
    " * Map-only jobs and counters\n",
    " * Multi-step jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "(and caveats)\n",
    "\n",
    "## Big Data\n",
    "\n",
    "\"Big data is *high volume*, *high velocity* and/or *high variety* information assets that require new forms of processing to enable enhanced decision making, in insight discovery and process optimization.\" – Gartner, Inc.\n",
    "\n",
    "In particular, we often talk about data too large to fit in the memory of a single computer, and that we need to process it faster than we can with a single machine. In this case we need a distributed (as opposed to local) architecture that will spread the data across the disks, memory, and processors of multiple machines.\n",
    "\n",
    "Distributed systems are great, but have some challenges.\n",
    "\n",
    "Local                     | Distributed\n",
    "--------------------------|-------------\n",
    "Simple to understand and program     | Hard to understand and program; not always appropriate\n",
    "No communications overhead | Overhead passing data between computers\n",
    "Limited memory, cpu, disk  | Lots of memory, cpu, disks\n",
    "Single point of failure    | May be fault tolerant\n",
    "\n",
    "\n",
    "### Big-data problem\n",
    "\n",
    "We have a 100 TB of sales data that looks like this:\n",
    "\n",
    "ID    |Date          |Store  |State |Product   |Amount\n",
    "--    |----          |-----  |----- |-------   |------\n",
    "101   |11/13/2014    |100    |WA    |331       |300.00\n",
    "104   |11/18/2014    |700    |OR    |329       |450.00\n",
    "\n",
    "What are some of the questions we could answer if we could process this huge data set?\n",
    "\n",
    "- How many transactions were there by store, by state?\n",
    "- How many transactions were there by product?\n",
    "- How many transactions were there by week, month, year?\n",
    "- How many transactions were there by store, state, product, month?\n",
    "- How much revenue did we make by store, state?\n",
    "- How much revenue did we make by product?\n",
    "- How much revenue did we make by week, month, year?\n",
    "- How much revenue did we make by store, state, product, month?\n",
    "\n",
    "### Statistical Uses\n",
    "\n",
    "Why are these interesting?\n",
    "\n",
    "- These questions can help us figure out which products are selling\n",
    "  in which markets, at what time of the year.\n",
    "- Using statistical algorithms such as regression or random forests we\n",
    "  can predict sales.\n",
    "\n",
    "What kinds of sales can we predict?\n",
    "  \n",
    "- How much of each product will sell in each store next week.\n",
    "- How much of each product to stock in inventory.\n",
    "- If there are any large-scale trends.\n",
    "- If there are any blips in the data.\n",
    "\n",
    "### Engineering Problem\n",
    "\n",
    "To answer these questions we have to solve two problems:\n",
    "\n",
    "- Store 100 TB of data\n",
    "- Process 100 TB of data\n",
    "\n",
    "Here is our starting point:\n",
    "\n",
    "- To solve this problem we have been provided with 1000 commodity Linux servers.\n",
    "- How can we organize these machines to store and process this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Intro\n",
    "\n",
    "Hadoop is a cluster operating system. It is made up of:\n",
    "\n",
    "- HDFS, which coordinates storing large amounts of data on a\n",
    "  cluster.\n",
    "\n",
    "- MapReduce which coordinates processing data across a cluster of\n",
    "  machines.\n",
    "\n",
    "### Google Papers\n",
    "\n",
    "Hadoop, HDFS, and MapReduce are open source implementations of the\n",
    "ideas in these papers from Google and Stanford.\n",
    "\n",
    "- Paper #1: [2003] The Google File System     \n",
    "    <http://research.google.com/archive/gfs-sosp2003.pdf>\n",
    "\n",
    "- Paper #2: [2004] MapReduce: Simplified Data Processing on Large Clusters    \n",
    "    <http://research.google.com/archive/mapreduce-osdi04.pdf>\n",
    "\n",
    "- Paper #3: [2006] Bigtable: A Distributed Storage System for Structured Data\n",
    "    <http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf>\n",
    "\n",
    "\n",
    "### Hadoop Analogy\n",
    "\n",
    "System     |Analogy\n",
    "------     |-------\n",
    "Hadoop     |Cluster Operating System\n",
    "HDFS       |Cluster Disk Drive\n",
    "MapReduce  |Cluster CPU\n",
    "\n",
    "- Hadoop clusters are made up of commodity Linux machines.\n",
    "- Each machine is weak, limited, and may fail.\n",
    "- Hadoop combines these machines into something more powerful than any part.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS\n",
    "\n",
    "<img src=\"images/hdfs-data-distribution.png\">\n",
    "\n",
    "### HDFS Notes\n",
    "\n",
    "- HDFS breaks up large files into 128 MB blocks.\n",
    "\n",
    "- The system stores 3 replicas of each block.\n",
    "\n",
    "- When a machine goes down the NameNode daemon makes the DataNode\n",
    "  daemons rereplicate the lost blocks.\n",
    "  \n",
    "Questions:\n",
    "\n",
    "* What are the implications of a block sizes that large?\n",
    "\n",
    "* In this picture how many machines can crash before we lose data?\n",
    "\n",
    "* If a machines crashes, the system rereplicates the lost blocks, and\n",
    "then the machine rejoins the cluster. What happens to the block replication\n",
    "count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "Question:\n",
    "\n",
    "Suppose you want to count how many of each word, totaled across a large number of large text files. How would you spread the work across multiple machines?\n",
    "\n",
    "<img src=\"images/map-reduce-key-partition.png\">\n",
    "\n",
    "MapReduce Notes\n",
    "---------------\n",
    "\n",
    "How does MapReduce work?\n",
    "\n",
    "- The developer provides mapper and reducer code.\n",
    "- The mapper function transforms individual records and attaches a key to each record.\n",
    "- All the records with the same key end up on the same reducer.\n",
    "- For each key the reduce function combines the records with that key.\n",
    "\n",
    "Which machines run mappers and which run reducers?\n",
    "\n",
    "- The JobTracker tries to run the mappers on the machines where the\n",
    "  blocks of input data are located.\n",
    "- This is called data locality – ideally, the mapper does not need to\n",
    "  pull data across the network.\n",
    "- The reducers are assigned randomly to machines which have memory and\n",
    "  CPUs currently available.\n",
    "\n",
    "Questions:\n",
    "\n",
    " * How many mappers does each job get?\n",
    " * How many reducers does each job get?\n",
    " * Suppose I want to find out how many sales transactions are in a data set for each state. What key should the mapper output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Using MRJob\n",
    "\n",
    "\n",
    "Hadoop is technically a Java library from Apache. You probably don't want to use it in Java, because it's not easy.\n",
    "\n",
    "The following is a \"simple\" word count on a text file.\n",
    "\n",
    "\n",
    "```java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "    public static class TokenizerMapper\n",
    "           extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "\n",
    "        public void map(Object key, Text value, Context context\n",
    "                        ) throws IOException, InterruptedException {\n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            while (itr.hasMoreTokens()) {\n",
    "                word.set(itr.nextToken());\n",
    "                context.write(word, one);\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "\n",
    "    public static class IntSumReducer\n",
    "           extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "        private IntWritable result = new IntWritable();\n",
    "\n",
    "        public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                           Context context\n",
    "                           ) throws IOException, InterruptedException {\n",
    "            int sum = 0;\n",
    "            for (IntWritable val : values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            result.set(sum);\n",
    "            context.write(key, result);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"word count\");\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        job.setCombinerClass(IntSumReducer.class);\n",
    "        job.setReducerClass(IntSumReducer.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## MRJob\n",
    "\n",
    "MRJob is a Python wrapper around Hadoop created by Yelp (luckily for you). It makes your life *a lot* easier.\n",
    "\n",
    "Here's the same program in python.\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield (word, 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()\n",
    "```\n",
    "\n",
    "## Sales Data\n",
    "\n",
    "Here is the sales data we are going to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions By State\n",
    "\n",
    "Q: How many transactions were there for each state?\n",
    "\n",
    "We can't run MapReduce from jupyter, so we'll write `.py` file and run it.\n",
    "\n",
    "This includes a class with two functions, a mapper and a reducer. The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SaleCount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        store = fields[2]\n",
    "        yield (state, 1)\n",
    "        \n",
    "    def reducer(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    SaleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/step-0-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/step-0-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/step-0-reducer_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180403.235707.063161\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t3\r\n",
      "\"OR\"\t1\r\n",
      "\"WA\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* Suppose instead of counting transactions by state we want to count\n",
    "transactions by store. What should we change in the code above?\n",
    "* Suppose instead of counting transactions we want to find total\n",
    "revenue by state. What should we change in the code above?\n",
    "\n",
    "\n",
    "## Using MapReduce For Statistics\n",
    "\n",
    "- Using MapReduce we can calculate statistics for any factors.\n",
    "- Our factor or condition becomes the key.\n",
    "- The parameter that we want to calculate the statistic on becomes\n",
    "  the value.\n",
    "- The reducer contains the logic to apply the statistic.\n",
    "- The statistic can be sum, count, average, stdev, etc.\n",
    "\n",
    "## Using MRJob for Word Count\n",
    "\n",
    "First, we create an input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "this is the second line\n",
    "this is the third line\n",
    "hello again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the `WordCount.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class WordCount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "            \n",
    "    def reducer(self, word, counts): \n",
    "        yield word, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    WordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/step-0-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/step-0-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/step-0-reducer_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/WordCount.jackbennetto.20180404.000115.516612\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py input.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"again\"\t1\r\n",
      "\"hello\"\t2\r\n",
      "\"is\"\t2\r\n",
      "\"line\"\t2\r\n",
      "\"second\"\t1\r\n",
      "\"the\"\t2\r\n",
      "\"third\"\t1\r\n",
      "\"this\"\t2\r\n",
      "\"world\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Notes\n",
    "\n",
    "- WordCount is used as a standard distributed application\n",
    "\n",
    "- A large corpus can require more storage than the disk on a single\n",
    "  machine\n",
    "\n",
    "- WordCount generalizes to other counting applications such as\n",
    "  counting clicks by category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Lecture\n",
    "\n",
    "## MapReduce Abstractions\n",
    "\n",
    "### Why Hive and Pig\n",
    "\n",
    "- Instead of writing MapReduce programs what if we could write SQL.\n",
    "- Hive and Pig let you write MapReduce programs in SQL-like languages.\n",
    "- These are then converted to MapReduce on the fly.\n",
    "- We will look at Spark SQL tomorrow, which fills the same niche.\n",
    "\n",
    "### Hive Example\n",
    "\n",
    "```sql\n",
    "SELECT user.*\n",
    "FROM user\n",
    "WHERE user.active = 1;\n",
    "```\n",
    "\n",
    "### Hive\n",
    "\n",
    "- Hive was developed at Facebook.\n",
    "- It translates SQL to generate MapReduce code.\n",
    "- Its dialect of SQL is called HiveQL.\n",
    "- Data scientists can use SQL instead of MapReduce to process data.\n",
    "\n",
    "### Pig Example\n",
    "\n",
    "```pig\n",
    "user = LOAD 'user';\n",
    "active_user = FILTER user BY active == 1;\n",
    "dump active_user;\n",
    "```\n",
    "\n",
    "### Pig\n",
    "\n",
    "- Pig was developed at Yahoo.\n",
    "- It solves the same problem as Hive.\n",
    "- Pig uses a custom scripting language called PigLatin instead of SQL.\n",
    "- PigLatin resembles scripting languages like Python and Perl.\n",
    "- Pig is frequently used for processing unstructured or badly formed\n",
    "  data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced MapReduce Applications\n",
    "\n",
    "### Combiner\n",
    "\n",
    "- Communication between nodes is can be a major bottlenex in MapReduce.\n",
    "- Reduce the records before shuffling them saves bandwidth/disk usage.\n",
    "- The *combiner* shrinks the numer of records locally before shuffling.\n",
    "- A reducer can only be used as a combiner if it is commutative and associative.\n",
    "\n",
    "### Transactions By State Using Combiner\n",
    "\n",
    "Q: How many transactions were there for each state?\n",
    "\n",
    "- Create the `SaleCountFast.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SaleCountFast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCountFast.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SaleCountFast(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        yield (state, 1)\n",
    "        \n",
    "    def combiner(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "        \n",
    "    #def reducer(self, state, counts): \n",
    "        #yield state, sum(counts)\n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    SaleCountFast.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372/step-0-mapper_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountFast.jackbennetto.20180404.000451.662372\n"
     ]
    }
   ],
   "source": [
    "!python SaleCountFast.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t3\r\n",
      "\"OR\"\t1\r\n",
      "\"WA\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    " * Can we use the reduce function as a combiner if we are calculating\n",
    "the total number of sales transactions per state?\n",
    " * Can we use the reduce function as a combiner if we are calculating\n",
    "the average transaction revenue per state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Map-Only Job To Clean Data\n",
    "\n",
    "Q: Write an application that extracts all the `CA` sales records.\n",
    "\n",
    "- This only requires transforming records, without consolidating them.\n",
    "\n",
    "- Any time we don't have to consolidate records we can use a *Map\n",
    "  Only* job.\n",
    "\n",
    "- Create the `SaleExtract.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SaleExtract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleExtract.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class SaleExtract(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'): \n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        if state != 'CA': \n",
    "            return\n",
    "        yield (state, line)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleExtract.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834\r\n",
      "\r\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\r\n",
      "\r\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834/step-0-mapper_part-00000\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834/step-0-mapper_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834/output/part-00000\r\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834/output\r\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleExtract.jackbennetto.20180404.000502.348834\r\n"
     ]
    }
   ],
   "source": [
    "!python SaleExtract.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t\"102    11/15/2014     203     CA     321        200.00\"\r\n",
      "\"CA\"\t\"106    11/19/2014     202     CA     331        330.00\"\r\n",
      "\"CA\"\t\"105    11/19/2014     202     CA     321        200.00\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Only Applications\n",
    "\n",
    "Here are some other applications of map-only jobs.\n",
    "\n",
    "- Web-crawler that finds out how many jobs are on Craigslist for a\n",
    "  particular keyword.\n",
    "- Application that maps property addresses to property back-taxes by\n",
    "  scraping county databases.\n",
    "\n",
    "Question:\n",
    "\n",
    " * Do map-only applications shuffle and sort the data?\n",
    "\n",
    "\n",
    "### Counters\n",
    "\n",
    "Q: Count how many transactions there were in California and Washington.\n",
    "\n",
    "- One way to solve this problem is to use a MapReduce application we did before.\n",
    "- However, if we have a fixed number of categories we want to count we can use counters.\n",
    "- If we use counters we no longer need a reduce phase, and can use a map-only job.\n",
    "- MapReduce has a limit of 120 counters so this cannot be used to count an unknown number of categories.\n",
    "\n",
    "Create the `SaleCount1.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SaleCount1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount1.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class SaleCount1(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'): \n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        if state == 'CA':\n",
    "            self.increment_counter('State', 'CA', 1)\n",
    "        if state == 'WA':\n",
    "            self.increment_counter('State', 'WA', 1)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleCount1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  State:\n",
      "    CA: 3\n",
      "    WA: 2\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566/step-0-mapper_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount1.jackbennetto.20180404.000506.966566\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount1.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There should not be any output. The counter values were printed when\n",
    "  the job was executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Notes\n",
    "\n",
    "- Counters can be incremented in both the map and the reduce phase.\n",
    "- Counter values from all the machines participating in a MapReduce\n",
    "  job are aggregated to compute job-wide value.\n",
    "- Counter values are printed out when the job completes and are also\n",
    "  accessible on the Hadoop Web UI that stores job history.\n",
    "- Counters are have a group name and a counter name.\n",
    "- Group names help organize counters.\n",
    "- Here is how we increment a counter:\n",
    "  `self.increment_counter(group_name, counter_name, 1)`\n",
    "\n",
    "Questions:\n",
    "\n",
    "SalesStrategy Inc employs 100,000 part-time sales partners to sell\n",
    "their products. The salespeople get monthly bonuses based on the\n",
    "number of transactions they ring up. Should SalesStrategy use counters\n",
    "to calculate these bonuses? Why or why not?\n",
    "\n",
    "### Map-Only Job Observations\n",
    "\n",
    "- Map-only jobs are the multi-machine equivalent of the\n",
    "  multi-threading and multi-processing exercises we did earlier.\n",
    "- Like our multi-threading and multi-processing applications, map-only\n",
    "  jobs break up a larger problem into smaller chunks and then work on\n",
    "  a particular chunk.\n",
    "- Any time we have a problem where we don't need to reconcile or\n",
    "  consolidate records we should use map-only jobs.\n",
    "- Map-only jobs are much faster than regular MapReduce jobs.\n",
    "\n",
    "Questions:\n",
    "\n",
    "\n",
    "Q: Why are map-only jobs faster than regular MapReduce jobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Jobs Together\n",
    "\n",
    "It's possible to chain multiple MapReduce steps together. Each mapper take key-value pairs from the function before it, and each reducer takes the prior pairs aggregated by keys.\n",
    "\n",
    "Rather than override the `mapper` and `reducer` methods, we need to override the `steps` method to returns a list of `MRStep`s, each with (optionally) a mapper, combiner, and reducer. First, a simple example that only returns the results of the sales data for states with amounts greater than 500.00 (similar to a HAVING command in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SaleCountTwoStep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCountTwoStep.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class SaleCount(MRJob):\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        amount = float(fields[5])\n",
    "        yield (state, amount)\n",
    "        \n",
    "    def reducer1(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "    \n",
    "    def having(self, state, amount):\n",
    "        if amount > 500:\n",
    "            yield state, amount\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.having)\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    SaleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run locally and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-0-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-0-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/step-1-mapper_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCountTwoStep.jackbennetto.20180404.000516.647728\n",
      "\"CA\"\t730.0\n",
      "\"WA\"\t1050.0\n"
     ]
    }
   ],
   "source": [
    "!python SaleCountTwoStep.py sales.txt > output.txt\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tricker example that takes advantage of the sorting in MapReduce.\n",
    "\n",
    "Q: Find word frequencies and sort the result by frequency. \n",
    "\n",
    "This requires running two MapReduce jobs.\n",
    " - The first will calculate word frequencies.\n",
    " - The second will sort them.\n",
    "\n",
    "\n",
    "First, Create `MostUsedWords.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MostUsedWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostUsedWords.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MostUsedWords(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        count_sum = '%03d'%sum(counts) \n",
    "        yield (count_sum, word)\n",
    "\n",
    "    def reducer_sort(self, count, words):\n",
    "        for word in words:\n",
    "            yield (word, count)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_sort)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostUsedWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-0-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-0-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-1-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-1-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/step-1-reducer_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/output/part-00000\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/MostUsedWords.jackbennetto.20180404.000535.593879\n"
     ]
    }
   ],
   "source": [
    "!python MostUsedWords.py input.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"again\"\t\"001\"\r\n",
      "\"second\"\t\"001\"\r\n",
      "\"third\"\t\"001\"\r\n",
      "\"world\"\t\"001\"\r\n",
      "\"hello\"\t\"002\"\r\n",
      "\"is\"\t\"002\"\r\n",
      "\"line\"\t\"002\"\r\n",
      "\"the\"\t\"002\"\r\n",
      "\"this\"\t\"002\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop Streaming API\n",
    "-----------------------\n",
    "\n",
    "- Why are we left-padding the amount with zeros? \n",
    "\n",
    "- MRJob is a wrapper around the Hadoop Streaming API.\n",
    "\n",
    "- The Hadoop Streaming API converts all intermediate types to strings for comparison.\n",
    "\n",
    "- So `123` will be smaller than `59` because it starts with `1` which\n",
    "  is less than `5`.\n",
    "  \n",
    "- To get around this in MRJob if we want our data to sort numerically\n",
    "  we have to left-pad the numbers with zeros.\n",
    "\n",
    "\n",
    "### Sorting Sales Data\n",
    "\n",
    "Q: Find the total sales per state and then sort by sales to find the\n",
    "state with the highest sales total.\n",
    "\n",
    "- We can use a multi-step MRJob to do this.\n",
    "\n",
    "- Sort sales data using two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "class SaleCount(MRJob):\n",
    "   \n",
    "    def mapper1(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        amount = float(fields[5])\n",
    "        state = fields[3]\n",
    "        yield (state, amount)\n",
    "\n",
    "    def reducer1(self, state, amounts):\n",
    "        amount = '%07.2f'%sum(amounts) \n",
    "        yield (state, amount)\n",
    "    \n",
    "    def mapper2(self, state, amount):\n",
    "        yield (amount, state)\n",
    "\n",
    "    def reducer2(self, amount, states):\n",
    "        for state in states: \n",
    "            yield (state, amount)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2)\n",
    "        ]\n",
    "if __name__ == '__main__': \n",
    "    SaleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-0-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-0-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-0-reducer_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-0-reducer_part-00001\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-mapper_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-mapper_part-00001\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-mapper-sorted\n",
      "> sort /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-mapper_part-00000 /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-mapper_part-00001\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-reducer_part-00000\n",
      "writing to /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-reducer_part-00001\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-reducer_part-00000 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/output/part-00000\n",
      "Moving /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/step-1-reducer_part-00001 -> /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/output/part-00001\n",
      "Streaming final output from /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820/output\n",
      "removing tmp directory /var/folders/m9/8htjs0dj34d0qgtnbq5w_2fm0000gn/T/SaleCount.jackbennetto.20180404.000544.608820\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount.py sales.txt --jobconf mapred.reduce.tasks=2 > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"OR\"\t\"0450.00\"\r\n",
      "\"CA\"\t\"0730.00\"\r\n",
      "\"WA\"\t\"1050.00\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: an Aside on Generators\n",
    "\n",
    "Most python functions run and return a value. That value might be a single number or string or it might be a list or dictionary. It might be a tuple, effectively returning multiple things. Or it might return `None`, effectively nothing at all (this is implicit if there isn't a return statement. But in all these cases, the return value is one object returned all at once.\n",
    "\n",
    "The `yield` statement allows a function to return many object as they are available, returning a bit at a time. Any function with a `yield` returns a **generator**. Let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_arguments_as_generator(a, b, c):\n",
    "    yield a\n",
    "    yield b\n",
    "    yield c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object return_arguments_as_generator at 0x1153baa98>\n"
     ]
    }
   ],
   "source": [
    "g = return_arguments_as_generator(3, 4, 5)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over a generator with a for loop, or convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for x in g:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(return_arguments_as_generator(10, 20, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's happening here? When we call the function it doesn't actually runAt this point, the function hasn't actually run yet. Calling the `next` function one the generator advances the function until it hits a `yield` statement and returns the `yield`ed value.\n",
    "\n",
    "Note we have to recreate the generator; the old one is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = return_arguments_as_generator(7, 8, 9)\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function reaches a `return` statement (either explicitly, or implicitly by reaching the end of the function) it raises a `StopIteration` exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5f315c5de15b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often, generators are used to return values from a loop. We could write a version of the `range` function like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_range(start, end, step=1):\n",
    "    '''Similar to buildin range function,\n",
    "    but both start and end are required,\n",
    "    and step must be positive'''\n",
    "    i = start\n",
    "    while i < end:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_range(3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
