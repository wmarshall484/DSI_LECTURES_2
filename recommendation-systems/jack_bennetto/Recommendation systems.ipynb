{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommandation Systems\n",
    "#### Jack Bennetto\n",
    "\n",
    "(partially converted from slides - do not use as is)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Today's objectives:\n",
    "\n",
    "*   Describe primary approaches to recommender systems\n",
    "*   Build a recommender using collaborative filtering and similarity\n",
    "*   Build a recommender using collaborative filtering and matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Today's plan:\n",
    "\n",
    "1.  Overview of types of recommender systems\n",
    "2.  Content-based recommender systems\n",
    "3.  Collaborative filtering with similarity\n",
    "4.  Collaborative filtering with matrix factorization\n",
    "5.  Best practices\n",
    "6.  Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "A couple references, from the machine learning perspective:\n",
    "\n",
    "*   [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n",
    "*   [Recommender Systems: An Introduction](http://www.amazon.com/Recommender-Systems-Introduction-Dietmar-Jannach/dp/0521493366/ref=sr_1_1?ie=UTF8&qid=1448251793&sr=8-1&keywords=recommender+systems+an+introduction)\n",
    "*   [Music Recommendation and Discovery: The Long Tail, Long Fail, and Long Play in the Digital Music Space](http://www.amazon.com/Music-Recommendation-Discovery-Digital-Space/dp/3642132863/ref=sr_1_1?ie=UTF8&qid=1448251867&sr=8-1&keywords=recommender+systems+music)\n",
    "*   [Matrix Factorization Techniques for Recommender Systems](http://www.computer.org/csdl/mags/co/2009/08/mco2009080030.html)\n",
    "*   [Amazon.com recommendations: Item-to-Item Collaborative Filtering](http://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf)\n",
    "*   Dato/GraphLab documentation & blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Introduction\n",
    "\n",
    "\n",
    "\n",
    "##  Recommendation business problem\n",
    "\n",
    "Recommendation problem takes several forms:\n",
    "\n",
    "*   Goal of recommender:\n",
    "    -   predict missing ratings\n",
    "    -   May be sufficient to just predict a subset of items with high expected rankings\n",
    "    *   May be sufficient to just predict general trends, such as *trending* news\n",
    "*   Long-tail:\n",
    "    -   Scarcity $\\Rightarrow$ brick & mortar stocks items based on average user\n",
    "    -   Online $\\Rightarrow$ cater to individual, not average user $\\Rightarrow$ stock everything, both popular and long tail\n",
    "*   Often described as *personalization*\n",
    "*   Examples: Movies (Netflix), Products (Amazon), Music (Pandora), and News articles (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Approaches to recommender systems\n",
    "\n",
    "There are several approaches to building a recommender.\n",
    "\n",
    "There are several approaches to building a recommender.\n",
    "\n",
    "A **content-based recommender** looks at the characteristics of the users and items to determine what to recommend. For instance, suppose your open a new MovieBinge account and it asks you for your favorite genres. If you say you like Science Fiction and Comedy, it might recommend *Galaxy Quest*.\n",
    "\n",
    "**Collaborative filtering** uses the \"ratings\" of you and other users to determine what movies to watch. For example, if you say you like *Mars Attacks!*, and many people who like *Mars Attacks!* also like *Galaxy Quest*, the recommender might suggest you watch *Galaxy Quest*.\n",
    "\n",
    "A **Hybrid** recommender uses \n",
    "\n",
    "*   Hybrid: Content-based + Collaborative filtering\n",
    "*   Applications:\n",
    "    -   Product recommendations\n",
    "    -   Movie recommendations\n",
    "    -   News articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data\n",
    "\n",
    "Typically, data is a *utility* (*rating*) matrix, which captures user preferences/well-being:\n",
    "\n",
    "*   User rating of items\n",
    "*   User purchase decisions for items\n",
    "*   Most items are unrated $\\Rightarrow$ matrix is sparse\n",
    "*   Unrated are coded as 0 or missing\n",
    "*   Use recommender:\n",
    "    -   Determine which attributes users think are important\n",
    "    -   Predict ratings for unrated items\n",
    "    -   Better than trusting 'expert' opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Types of data\n",
    "\n",
    "Data can be:\n",
    "\n",
    "*   *Explicit*:\n",
    "    -   User provided ratings (1 to 5 stars)\n",
    "    -   User like/non-like\n",
    "*   *Implicit*:\n",
    "    -   Infer user-item relationships from behavior\n",
    "    -   More common\n",
    "    -   Example: buy/not-buy; view/not-view\n",
    "*   To convert implicit to explicit, create a matrix of 1s (yes) and 0s (no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example:    explicit utility matrix\n",
    "\n",
    "Example 9.1 in [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf):\n",
    "\n",
    "|       | HP1   | HP2   | HP3   | TW    | SW1   | SW2   | SW3   |\n",
    "| :---- | :---- | :--   | :--   | :--   | :--   | :--   | :--   |\n",
    "| A     |  4    |       |       | 5     | 1     |       |       |\n",
    "| B     |  5    | 5     | 4     |       |       |       |       |\n",
    "| C     |       |       |       | 2     | 4     | 5     |       |\n",
    "| D     |       | 3     |       | 5     | 1     |       | 3     |\n",
    "\n",
    "---\n",
    "\n",
    "##  Example:    implicit utility matrix\n",
    "\n",
    "Based on example 9.1 in [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf):\n",
    "\n",
    "|       | HP1   | HP2   | HP3   | TW    | SW1   | SW2   | SW3   |\n",
    "| :---- | :---- | :--   | :--   | :--   | :--   | :--   | :--   |\n",
    "| A     |  1    |       |       | 1     | 1     |       |       |\n",
    "| B     |  1    | 1     | 1     |       |       |       |       |\n",
    "| C     |       |       |       | 1     | 1     | 1     |       |\n",
    "| D     |       | 1     |       | 1     | 1     |       | 1     |\n",
    "\n",
    "---\n",
    "\n",
    "#  Content-Based Recommenders\n",
    "\n",
    "---\n",
    "\n",
    "##  Overview of content-based recommenders\n",
    "\n",
    "Use features to determine similarity:\n",
    "\n",
    "*   Recommend based on item properties/characteristics\n",
    "1.  Construct item *profile* of characteristics\n",
    "2.  Construct item features:\n",
    "    -   Text: use TF-IDF and use top $N$ features or features over a cutoff\n",
    "    -   Images: use tags -- only works if tags are frequent & accurate\n",
    "3.  Compute document similarity: Jaccard, Cosine\n",
    "4.  Construct user profile\n",
    "\n",
    "---\n",
    "\n",
    "##  Item profile\n",
    "\n",
    "*   Consists of (feature, value) pairs\n",
    "*   Consider setting feature to 0 or 1\n",
    "*   Consider how to scale non-Boolean features\n",
    "\n",
    "---\n",
    "\n",
    "##  User profile\n",
    "\n",
    "*   Describes user preferences (utility matrix)\n",
    "*   Consider how to aggregate item features per user:\n",
    "    -   Compute \"weight\" a user puts on each feature\n",
    "    -   E.g., \"Julia Roberts\" feature = average rating for films with \"Julia Roberts\"\n",
    "*   Normalize: subtract average utility per user\n",
    "    -   E.g., \"Julia Roberts\" feature = average rating for films with \"Julia Roberts\" - average rating\n",
    "\n",
    "---\n",
    "\n",
    "##  Content-based recommendations\n",
    "\n",
    "*   Compute (cosine) distance between user profile and item profiles\n",
    "*   May want to bucket items first using random-hyperplane and locality-sensitivity-hashing (LSH)\n",
    "*   ML approach:\n",
    "    -   Use random forest or equivalent to predict on a per-user basis\n",
    "    -   Computationally intensive -- usually only feasible for small problems\n",
    "\n",
    "---\n",
    "\n",
    "#   Collaborative filtering using similarity\n",
    "\n",
    "---\n",
    "\n",
    "##  Overview of CF using similarity\n",
    "\n",
    "Use similarity to recommend items:\n",
    "\n",
    "*   Make recommendations based on similarity:\n",
    "    -   Between users\n",
    "    -   Between items\n",
    "*   Similarity measures:\n",
    "    -   Pearson\n",
    "    -   Cosine\n",
    "    -   Jaccard\n",
    "\n",
    "---\n",
    "\n",
    "##  Types of collaborative filtering\n",
    "\n",
    "Two types of similarity-based CF:\n",
    "\n",
    "*   *User-based*: predict based on similarities between users\n",
    "    -   Performs well, but slow if many users\n",
    "    -   Use item-based CF if $|Users| \\gg |Items|$\n",
    "*   *Item-based*: predict based on similarities between items\n",
    "    -   Faster if you precompute item-item similarity\n",
    "    -   Usually $|Users| \\gg |Items| \\, \\Rightarrow$  item-based CF is most popular\n",
    "    -   Items tend to be more stable:\n",
    "        -   Items often only in one category (e.g., action films)\n",
    "        -   Stable over time\n",
    "        -   Users may like variety or change preferences over time\n",
    "        -   Items usually have more ratings than users $\\Rightarrow$ items have more stable average ratings than users\n",
    "\n",
    "---\n",
    "\n",
    "##  Collaborative filtering recipe\n",
    "\n",
    "Compute predictions by similarity:\n",
    "\n",
    "1.  Normalize (demean) utility matrix\n",
    "1.  Reduce dimensionality: SVD, NMF, or UV (optional)\n",
    "1.  Compute similarity of users or items\n",
    "1.  Predict ratings for unrated items\n",
    "1.  Add prediction to average rating of user/item\n",
    "\n",
    "Note:\n",
    "\n",
    "*   Precompute utility matrix for each user -- it is relatively stable\n",
    "*   Only compute predictions at runtime\n",
    "\n",
    "---\n",
    "\n",
    "##  Review: measuring similarity\n",
    "\n",
    "Example 9.1 in [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf):\n",
    "\n",
    "|       | HP1   | HP2   | HP3   | TW    | SW1   | SW2   | SW3   |\n",
    "| :---- | :---- | :--   | :--   | :--   | :--   | :--   | :--   |\n",
    "| A     |  4    |       |       | 5     | 1     |       |       |\n",
    "| B     |  5    | 5     | 4     |       |       |       |       |\n",
    "| C     |       |       |       | 2     | 4     | 5     |       |\n",
    "| D     |       | 3     |       | 5     | 1     |       | 3     |\n",
    "\n",
    "*   What is the Jaccard distance between A & B? A & C?\n",
    "*   What is the Cosine distance between A & B? A & C?\n",
    "*   See text for examples with normalization and rounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Choosing a similarity measure\n",
    "\n",
    "Chose the appropriate similarity measure for your data:\n",
    "\n",
    "*   Cosine:\n",
    "    -   Use for ratings (non-Boolean) data\n",
    "    -   Treat missing ratings as $0$\n",
    "    -   Cosine + de-meaned data is the same as Pearson\n",
    "*   Jaccard:\n",
    "    -   Use only Boolean (e.g., buy/not buy) data\n",
    "    -   Loses information with ratings data\n",
    "\n",
    "Then compute *similarity matrix* of pair-wise similarities between items (users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predict ratings from similarity\n",
    "\n",
    "Predict using a similarity-weighted average of ratings:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\dfrac{\\underset{j\\in I_u}{\\sum} similarity(i,j) \\cdot R_{uj} }{\\underset{j \\in I_u}{\\sum } similarity(i,j) }$$\n",
    "\n",
    "where\n",
    "\n",
    "*   $\\hat{r}_{ui}$ is user $u$'s predicted rating for item $i$\n",
    "*   $I_u \\equiv$  set of items rated by $u$\n",
    "*   $R_{uj}$ is utility matrix, i.e., $R_{uj} \\equiv$ user $u$'s rating of item $j$\n",
    "\n",
    "$\\Rightarrow$ Compute similarity between items!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check for mastery\n",
    "\n",
    "How would you modify the prediction formula below for a user-based recommender?\n",
    "\n",
    "$$\\hat{r}_{ui} = \\dfrac{\\underset{j\\in I_u}{\\sum} similarity(i,j) \\cdot R_{uj} }{\\underset{j \\in I_u}{\\sum } similarity(i,j) }$$\n",
    "\n",
    "Hint: should you compute similarity between users or items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recommend best items\n",
    "\n",
    "Recommend items with highest predicted rating:\n",
    "\n",
    "*   Sort predicted ratings $\\hat{r}_{ui}$\n",
    "*   Optimize by only searching a neighborhood which contains the $n$ items most similar to $i$\n",
    "*   Beware:\n",
    "    -   Consumers like variety\n",
    "    -   Don't recommend every Star Trek film to someone who liked first film\n",
    "    -   Best to offer several different types of item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dimensionality reduction (optional)\n",
    "\n",
    "May use SVD or similar method to reduce dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aff419407838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#  Set n_top_eig to capture most of the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_top_eig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_top_eig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm_new_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_top_eig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm_sigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm_ratings' is not defined"
     ]
    }
   ],
   "source": [
    "U, Sigma, VT = np.linalg.svd(m_ratings)\n",
    "#  Set n_top_eig to capture most of the variance\n",
    "m_sigma = np.mat(np.eye(n_top_eig) * Sigma[:n_top_eig])\n",
    "m_new_ratings = m_ratings.T * U[:, :n_top_eig] * m_sigma.I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Application of Dimensionality Reduction in Recommender System -- A Case Study](http://files.grouplens.org/papers/webKDD00.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Collaborative filtering using matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Collaborative filtering using matrix factorization\n",
    "\n",
    "Predict ratings from *latent factors*:\n",
    "\n",
    "*   Compute latent factors $q_i$ and $p_u$ via matrix factorization\n",
    "*   *Latent factors* are unobserved user or item attributes:\n",
    "    -   Describe some user or item concept\n",
    "    -   Affect behavior\n",
    "    -   Example: escapist vs. serious, male vs. female films\n",
    "*   Predict rating: `$\\hat{r}_{ui} = q_i^T p_u$`\n",
    "*   Assumes:\n",
    "    -   Utility matrix is product of two simpler matrices (long, thin):\n",
    "    -   $\\exists$ small set of users & items which characterize behavior\n",
    "    -   Small set of features determines behavior of most users\n",
    "*   Can use NMF, $U V$, or SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Review: SVD\n",
    "\n",
    "Q:  What is SVD?\n",
    "\n",
    "Q:  How do you compute it? (optional)\n",
    "\n",
    "Q:  How do you compute the variance in the data that a factor explains?\n",
    "\n",
    "Q:  What do the different matrices in decomposition represent?\n",
    "\n",
    "Q:  How can you use it to reduce dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Review: NMF\n",
    "\n",
    "Q:  What is NMF?\n",
    "\n",
    "Q:  How do you compute it?\n",
    "\n",
    "Q:  What do the different matrices in decomposition represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SVD vs. NMF\n",
    "\n",
    "SVD:\n",
    "\n",
    "*   Must know all ratings -- i.e., no unrated items\n",
    "*   Assumes can minimize squared Frobenius norm\n",
    "*   Very slow if matrix is large & dense\n",
    "\n",
    "NMF:\n",
    "\n",
    "*   Can estimate via alternating least squares (ALS) or stochastic gradient descent (SGD)\n",
    "*   Must regularize\n",
    "*   Can handle big data, biases, interactions, and time dynamics\n",
    "\n",
    "\n",
    "##  Using NMF in recommendation systems\n",
    "\n",
    "NMF is a 'best in class' option for many recommendation problems:\n",
    "\n",
    "*   Includes overall, user, & item bias as well as latent factor interactions\n",
    "*   Can fit via SGD or ALS\n",
    "*   No need to impute missing ratings\n",
    "*   Use regularization to avoid over-fitting\n",
    "*   Can handle time dynamics, e.g., changes in user preferences\n",
    "*   Used by winning entry in Netflix challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NMF problem formulation\n",
    "\n",
    "To factor the utility matrix:\n",
    "\n",
    "$$\\underset{\\{q_i, p_u\\}}{\\mathtt{argmin}} \\underset{(u,i)\\in \\mathcal{K}}{\\sum}(r_{ui} - q_i^T p_u)^2 + \\lambda (\\left\\Vert q_i \\right\\Vert{}^2 + \\left\\Vert p_u \\right\\Vert{}^2)$$\n",
    "\n",
    "where\n",
    "\n",
    "*   $\\mathcal{K} \\equiv$ all $(u,i)$ in the training set with known ratings\n",
    "*   $\\lambda$ is amount of regularization\n",
    "*   $r_{ui}$ is user $u$'s rating of item $i$\n",
    "*   $p_u$ is latent factor for user $u$\n",
    "*   $q_i$ is latent factor for item $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NMF problem formulation with bias\n",
    "\n",
    "Should account for bias:\n",
    "\n",
    "$$\\underset{\\{q_i, p_u, \\mu, b_u, b_i\\}}{\\mathtt{argmin}} \\underset{(u,i)\\in \\mathcal{K}}{\\sum}(r_{ui} - \\mu - b_u - b_i - q_i^T p_u)^2 + \\lambda (\\left\\Vert q_i \\right\\Vert{}^2 + \\left\\Vert p_u \\right\\Vert{}^2 + b_u^2 + b_i^2)$$\n",
    "\n",
    "where\n",
    "\n",
    "*   $\\mu$:  overall bias (average rating)\n",
    "*   $b_u$:  user bias\n",
    "*   $b_i$:  item bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimating NMF\n",
    "\n",
    "Two methods to estimate NMF factors:\n",
    "\n",
    "*   Stochastic gradient descent (SGD):\n",
    "    -   Easier and faster than ALS\n",
    "    -   Must tune learning rate\n",
    "    -   Sometimes called 'Funk SGD' after originator\n",
    "*   Alternating least squares (ALS):\n",
    "    -   Use least squares, alternate between fixing $q_i$ and $p_u$\n",
    "    -   Available in Spark/MLib\n",
    "    -   Fast if you can parallelize\n",
    "    -   Better for implicit (non-sparse) data\n",
    "*   Beware of local optima!\n",
    "\n",
    "---\n",
    "\n",
    "##  NMF ProTips\n",
    "\n",
    "To get best performance with NMF:\n",
    "\n",
    "*   Model bias (overall, user, and item)\n",
    "*   Model time dynamics, such as changes in user preferences\n",
    "*   Add side or implicit information to handle cold-start\n",
    "*   See [Matrix Factorization Techniques for Recommender Systems](http://www.computer.org/csdl/mags/co/2009/08/mco2009080030.html)\n",
    "\n",
    "---\n",
    "\n",
    "##  Building a recommender with NMF\n",
    "\n",
    "*   Supports many types of recommenders\n",
    "*   Provides (near) best in class performance\n",
    "*   Reasonable licensing terms\n",
    "*   To improve performance, focus on:\n",
    "    -   Data collection and quality\n",
    "    -   Cold-start problem\n",
    "    -   Feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "#   Best practices\n",
    "\n",
    "---\n",
    "\n",
    "##  Overview:\n",
    "\n",
    "Will discuss:\n",
    "\n",
    "*   Cold-start problem\n",
    "*   Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The cold-start problem\n",
    "\n",
    "Difficult to build a recommender without ratings:\n",
    "\n",
    "*   *Cold-start* problem:\n",
    "    -   Need utility matrix to recommend\n",
    "    -   Can ask users to rate items\n",
    "    -   Infer ratings from behavior, e.g., viewing an item\n",
    "*   Must also handle new users and new items\n",
    "*   Approaches:\n",
    "    -   Use ensemble of (bad) recommenders until you have enough ratings\n",
    "    -   Use content-based recommender\n",
    "    -   Exploit implicit, tag, and other side data\n",
    "    -   Use `ItemSimilarityModel` until you have enough rating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation issues\n",
    "\n",
    "Choose right evaluation criteria:\n",
    "\n",
    "-   Historically, used RMSE or MAE\n",
    "-   But, only care about predicting top $n$ items\n",
    "    -   Should you compute metric over all missing ratings in test set?\n",
    "    -   No need to predict items undesirable items well\n",
    "-   *Precision at n*: percentage of top $n$ predicted ratings that are 'relevant'\n",
    "-   *Recall at n*:    percentage of relevant items in top $n$ predictions\n",
    "-   Lift or hit rate are more relevant to business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation issues\n",
    "\n",
    "Evaluation is difficult:\n",
    "\n",
    "*   Performance of recommender should be viewed in context of *user experience* (UX)\n",
    "*   $\\Rightarrow$ run A/B test on entire system\n",
    "*   Cross validation is hard:\n",
    "    -   What do you use for labels because of missing data?\n",
    "    -   Users choose to rate only some items $\\Rightarrow$ selection bias\n",
    "    -   Not clear how to fix this bias, which is always present\n",
    "*   Beware of local optima $\\Rightarrow$ use multiple starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross-validation\n",
    "\n",
    "Cross-validation (for item-based recommender):\n",
    "\n",
    "*   Randomly sample ratings to use in training set\n",
    "*   Split on users\n",
    "*   Be careful if you split temporally\n",
    "*   Do not split on items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recommender issues\n",
    "\n",
    "Building a production recommender is also challenging:\n",
    "\n",
    "*   Part of entire UX\n",
    "*   Should consider:\n",
    "    -   Diversity of recommendations\n",
    "    -   Privacy of personal information\n",
    "    -   Security against attacks on recommender\n",
    "    -   Social effects\n",
    "    -   Provide explanations\n",
    "*   See [Recommender systems: from algorithms to user experience](http://files.grouplens.org/papers/algorithmstouserexperience.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computational\n",
    "\n",
    "Computation tips:\n",
    "\n",
    "*   Compute offline:\n",
    "    -   Matrix factorization\n",
    "    -   Similarity matrix\n",
    "    -   User/item neighborhoods (via clustering)\n",
    "*   Compute predicted ratings/rankings live\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "\n",
    "You should now be able to explain:\n",
    "\n",
    "*   Content-based vs. collaborative filtering recommenders?\n",
    "*   Item-based vs. user-based CF?\n",
    "*   Compute measures of similarity (Jaccard, Pearson, cosine)?\n",
    "*   State which GraphLab recommender model is right for which problem?\n",
    "*   Describe how to tune and evaluate a recommender?\n",
    "*   Explain how to overcome the cold-start problem\n",
    "\n",
    "---\n",
    "\n",
    "#   Appendix:   similarity measures\n",
    "\n",
    "---\n",
    "\n",
    "##  Similarity measures\n",
    "\n",
    "Recommenders use distance to quantify similarity:\n",
    "\n",
    "*   Cosine similarity:\n",
    "    -   $cosine(x,y) = \\cos \\theta = \\dfrac{\\mathbf{x} \\cdot \\mathbf{y}}{\\lVert \\mathbf{x} \\rVert \\cdot \\lVert \\mathbf{y} \\rVert}$\n",
    "    -   $similarity(x,y) = \\frac{1}{2} + \\frac{1}{2} \\cdot cosine(x,y)$\n",
    "    -   Same as Pearson if you de-mean data\n",
    "    -   Treat blanks as $0$\n",
    "*   Jaccard distance:  \n",
    "    -   Jaccard index: $J(A,B) = \\dfrac{\\left| A \\cap B \\right|}{ \\left| A \\cup B \\right| } = \\dfrac{\\left| A \\cap B \\right|}{ \\left|A \\right| + \\left| B \\right| - \\left| A \\cap B \\right| }$\n",
    "    -   Jaccard distance: $d_J(A,B) = 1 - J(A,B)$\n",
    "    -   Use for binary data\n",
    "    -   Loses information with non-Boolean data\n",
    "    -   Example:\n",
    "        -   Let `$U_k \\equiv \\left\\{ i \\in \\text{ Users } | R_{ik} \\neq 0 \\right\\}$`, i.e. user $i$ rated item $k$\n",
    "        -   `$similarity(a,b) = J(U_a, U_b) = \\dfrac{\\left| U_a \\cap U_b \\right|}{ \\left| U_a \\cup U_b \\right| }$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Other distance measures\n",
    "\n",
    "Two other measures of similarity:\n",
    "\n",
    "*   Similarity:\n",
    "    -   Constructed from Euclidean distance so $similarity(x,y) \\in (0,1)$\n",
    "    -   $similarity(x,y) = \\dfrac{1}{1 + \\lVert \\mathbf{x-y} \\rVert}$\n",
    "*   Pearson correlation: $pearson(x,y) = \\dfrac{\\mathtt{cov}(x,y)}{\\sigma(x) \\cdot \\sigma(y)}$\n",
    "    -   Renormalize to be in (0,1): $similarity(x,y) = \\frac{1}{2} + \\frac{1}{2} \\cdot pearson(x,y)$\n",
    "    -   Use Numpy `corrcoef()`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Appendix: matrix factorization\n",
    "\n",
    "---\n",
    "\n",
    "##  Review: matrix factorization (1/4)\n",
    "\n",
    "Use matrix factorization to predict ratings:\n",
    "\n",
    "*   Discover *latent factors*, unobserved characteristics which determine behavior\n",
    "*   Reduce dimension\n",
    "*   Consider: SVD, UV, or NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Review: SVD (2/4)\n",
    "\n",
    "Decompose rating matrix, $M$, into $U \\cdot \\Sigma \\cdot V^T$\n",
    "\n",
    "*   $U$:    $m \\times d$ unitary matrix, represents user latent factors\n",
    "*   $\\Sigma$:\n",
    "    -   $d \\times d$ diagonal matrix of singular values\n",
    "    -   $\\Sigma^2$ is the variance of each factors\n",
    "*   $V^T$:  \n",
    "    -   $d \\times n$ matrix\n",
    "    -   Transpose of item latent factors\n",
    "*   Keep only factors which explain the top ~90% of variance\n",
    "*   Caveat: doesn't work with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Review: UV (3/4)\n",
    "\n",
    "Decompose rating matrix, $M$, into $U \\cdot V$\n",
    "\n",
    "*   $U$:    $m \\times d$ unitary matrix, represents user latent factors\n",
    "*   $V$:    $d \\times n$ matrix\n",
    "*   Can fit via stochastic gradient descent or alternating least squares (ALS)\n",
    "*   Use regularization to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $U \\cdot V$ decomposition\n",
    "\n",
    "$M$ is an $m$ by $n$ matrix\n",
    "\n",
    "*   $M \\approx U \\cdot V$, $U$ is $m$ by $d$ and $V$ is $d$ by $n$\n",
    "*   Use entries from $U \\cdot V$ to predict missing ratings\n",
    "*   Fit by minimizing RMSE of $M - U \\cdot V$:\n",
    "    -   Has multiple local optima\n",
    "    -   Use multiple starts & algorithms\n",
    "        -   Start from `$\\sqrt{\\dfrac{ \\mathtt{ave} \\left( \\left\\{ m_{ij} \\in M | m_{ij} \\neq 0 \\right\\} \\right) }{d}}$`\n",
    "        -   Perturb for other starts\n",
    "        -   Vary path for visiting elements during optimization\n",
    "    -   Compute via ALS or update rule\n",
    "        -   Minimize RMSE of `$\\sum(m_{ij} - (U \\cdot V)_{ij})^2$`\n",
    "        -   Overfitting\n",
    "        -   Use (stochastic) gradient descent to optimize\n",
    "\n",
    "---\n",
    "\n",
    "##  Review: NMF (4/4)\n",
    "\n",
    "Non-negative matrix factorization :\n",
    "\n",
    "*   Includes overall, user & item bias as well as latent factor interactions\n",
    "*   Can fit via stochastic gradient descent or alternating least squares (ALS)\n",
    "*   Use regularization to avoid overfitting\n",
    "*   Used by winning entry in Netflix challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_prime(n):\n",
    "    \"\"\"Checks if a number is prime.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: an Int\n",
    "    Returns\n",
    "    -------\n",
    "    Bool\n",
    "    \"\"\"\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for i in xrange(3, int(math.sqrt(n)) + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import multiprocessing\n",
    "import itertools\n",
    "from timeit import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primes_parallel(number_range):\n",
    "    pool = multiprocessing.Pool()\n",
    "    output = pool.map(check_prime, number_range)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes_parallel(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, figsize=(6,6))\n",
    "\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        ax.plot(scs.norm(0,1).rvs(100), scs.norm(0,1).rvs(100), '.')\n",
    "        pass\n",
    "        ax.margins(0.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(bottom=0.1, top=0.3, wspace=0.7, hspace=0.7)\n",
    "\n",
    "fig.suptitle(\"Foo\")\n",
    "fig.subtitle(\"Foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
