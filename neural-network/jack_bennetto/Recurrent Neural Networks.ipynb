{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "### and Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks we've talked about so far operate all at once. You have an input data point (or points) and the network, and the values propagate over the connections \n",
    "\n",
    "In this lesson we'll discuss **recurrent neural networks** (RNNs) which use time-series data as an input. The state of the network at a given point in time is based not just on the input layer at that time, but the state of the network from the previous point in time. So \n",
    "\n",
    "If a simple neural network looks like\n",
    "\n",
    "<img src=\"img/simple-mlp.png\" width=300>\n",
    "\n",
    "A recurrent network looks like\n",
    "\n",
    "\n",
    "<img src=\"img/rnn.png\" width=300>\n",
    "\n",
    "Where there's a connection from the hidden layer back to itself.\n",
    "\n",
    "We can unroll this in time, showing each row as a separate time step.\n",
    "\n",
    "<img src=\"img/rnn-unrolled-labeled.png\" width=350>\n",
    "\n",
    "Note that there are only three sets of weights: the vertical arrows, the horizontal arrows on the left, and the horizontal arrows on the right. To fit this model you fit those three sets of weights to the inputs $x_0$, $x_1$, $x_2$, and $x_3$ and minimize the loss between the output $\\hat{x_4}$ and the label $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' try this in `keras`!\n",
    "\n",
    "Suppose we have some time-series data and we want to build a model so, given a bunch of points, we can predict the next point. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pts = 500\n",
    "t = np.linspace(0, 15 * 6, n_pts)\n",
    "sin_t = np.sin(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "ax.plot(t, sin_t, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except that's way to easy. Let's add some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sin_t_noisy = np.sin(t) + stats.norm(0, 0.5).rvs(n_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "ax.plot(t, sin_t_noisy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the data into a format we can put into an RNN. Each training data point should consist of a sequence of consecutive values for our data (for input) and the next value of our data (for output).\n",
    "\n",
    "First we'll write a function to consider every possible group of 50 values followed by one value (for the output) along our time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windowize_data(data, n_prev):\n",
    "    n_predictions = len(data) - n_prev\n",
    "    y = data[n_prev:]\n",
    "    # this might be too clever\n",
    "    indices = np.arange(n_prev) + np.arange(n_predictions)[:, None]\n",
    "    x = data[indices, None]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll write a function split the data into training and testing sets. Because it's time-series data we have to do that sequentially rather than shuffling it. They should be completely separate and not overlap, so the the training data isn't used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_and_windowize(data, n_prev, fraction_test=0.3):\n",
    "    n_predictions = len(data) - 2*n_prev\n",
    "    \n",
    "    n_test  = int(fraction_test * n_predictions)\n",
    "    n_train = n_predictions - n_test   \n",
    "    \n",
    "    x_train, y_train = windowize_data(data[:n_train], n_prev)\n",
    "    x_test, y_test = windowize_data(data[n_train:], n_prev)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prev = 50\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_and_windowize(sin_t_noisy, n_prev)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our model. We'll put two RNN layers.\n",
    "\n",
    "<img src=\"img/rnn-2-layer-unrolled.png\" width=450>\n",
    "\n",
    "Note that for the last layer we aren't going to fit all the outputs, but just the last one, so we set `return_sequences=False`. For the previous layer that feeds into that we need the output of each step, so `return_sequences=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(32, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(keras.layers.SimpleRNN(32, return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape is `(n_prev, 1)` because we're training with `n_prev` prior time points and only have a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what predictions we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(t[-len(y_test):], y_pred, 'b.-', label='predictions', lw=0.5)\n",
    "ax.plot(t[-len(y_test):], y_test, 'r.', label='actual')\n",
    "ax.plot(t[-len(y_test):], np.sin(t[-len(y_test):]), 'g-', label='ideal')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so great. Note, however, that the blue predictions do vary less than the red data points, so at least it averaged out some of the noise. \n",
    "\n",
    "One of the difficulties with traditional RNNs is what's called the \"vanishing gradients problem.\" For neural networks (this is 50 levels deep!) the effect of the input at the beginning exponentially shrinks with the depth of the network. This makes it very hard to remember details from the disThe the signal from each successively earlier point is typically smaller (or maybe larger) than the previous is that while they can \"remember\" what happened recently, \n",
    "\n",
    "There are other architectures of RNNs that will do a better job. One is a Long Short Term Memory (LSTM) network; a good post detailing them is at [http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(32, return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(t[-len(y_test):], y_pred, 'b.-', label='predictions', lw=0.5)\n",
    "ax.plot(t[-len(y_test):], y_test, 'r.', label='actual')\n",
    "ax.plot(t[-len(y_test):], np.sin(t[-len(y_test):]), 'g-', label='ideal')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "RNNs can also be used for classification. Rather than predicting the next step after a sequence as the output, we predict a class (or rather, a probability). Let's try two sequences, sine waves of slightly difference frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pts = 500\n",
    "t = np.linspace(0, 15 * 6, n_pts)\n",
    "sin_11t_noisy = np.sin(1.1*t) + stats.norm(0, 0.5).rvs(n_pts)\n",
    "sin_t_noisy = np.sin(t) + stats.norm(0, 0.5).rvs(n_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't care about the next value any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train1, x_test1, _, _ = split_and_windowize(sin_t_noisy, n_prev)\n",
    "x_train2, x_test2, _, _ = split_and_windowize(sin_11t_noisy, n_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, the `y`s are the labels of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate([x_train1, x_train2])\n",
    "x_test = np.concatenate([x_test1, x_test2])\n",
    "y_train = np.concatenate([np.zeros(x_train1.shape[0]), np.ones(x_train2.shape[0])])\n",
    "y_test = np.concatenate([np.zeros(x_test1.shape[0]), np.ones(x_test2.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a sigmoid activation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(32, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(32, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(32, return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "ax.hist(y_pred[y_test == 0], alpha=0.3, bins=20, label=\"0\")\n",
    "ax.hist(y_pred[y_test == 1], alpha=0.3, bins=20, label=\"1\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we could use an RNN to figure out the frequency of a signal. Here we'll just create a lot of sequences (with noise), each with a different frequency and starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts = 50\n",
    "n_sequences = 1000\n",
    "length = 10  # length of each sequence\n",
    "xpts = np.linspace(0, length, n_pts)\n",
    "offsets = stats.uniform(0, 2*np.pi).rvs(n_sequences)[:, None]\n",
    "freqs = stats.uniform(1,4).rvs(n_sequences)[:, None]\n",
    "signals = np.sin(xpts*freqs + offsets) + stats.norm(0, 0.3).rvs((n_sequences, n_pts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(7, 1, figsize=(12,10))\n",
    "for i, ax in zip(range(7), axs):\n",
    "    ax.plot(xpts, signals[i], '.-', lw=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "#model.add(keras.layers.LSTM(32, input_shape=(n_pts, 1), return_sequences=True))\n",
    "#model.add(keras.layers.LSTM(32, return_sequences=False))\n",
    "model.add(keras.layers.LSTM(32, input_shape=(n_pts, 1), return_sequences=False))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(signals[:, :, None], freqs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,  batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, predict_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
