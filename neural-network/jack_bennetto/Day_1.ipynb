{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks, Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Morning Objectives\n",
    "* Know the advantages and disadvantages of neural networks.\n",
    "* Build a simple predictive neural network in keras.\n",
    "* Know ways to regularize a neural network.\n",
    "\n",
    "\n",
    "Afternoon\n",
    "* Understand the purpose of backpropagation.\n",
    "* Build an autoencoder.\n",
    "\n",
    "## Agenda\n",
    "\n",
    "Morning\n",
    "\n",
    " * Overview\n",
    " * History\n",
    " * Introduction\n",
    " * Fully connected networks\n",
    " * Example in keras\n",
    " * Activation functions\n",
    " * Regularization\n",
    " \n",
    "Afternoon\n",
    " * Backpropagation\n",
    " * Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to install keras today. Doing\n",
    "\n",
    "```conda install keras```\n",
    "\n",
    "should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "import sklearn\n",
    "from sklearn import preprocessing, model_selection\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Neural Networks?\n",
    "\n",
    "Neural Networks were originally developed in the 1950's using the neurons in the brain as inspiration.\n",
    "\n",
    "In the brain, we have neurons connected together by dendrons of different strengths, and these strength change as we learn. The connections are non-linear; a neural doesn't activate at all until it has sufficient input.\n",
    "\n",
    "But don't get too caught up in the analogy. Brains are an inspiration, not a model, and trying to fit to closely to them hasn't always helped researchers.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "I'm using the term *neural network* throughout the lecture. The term *deep learning* has become popular as well in recent years, in part to move away from the idea that these are based on the brain, and in part to emphasize the depth that's become possible. Sometimes people talk about *artificial neural networks* (ANN) to distinguish them from the biological ones. And a couple decades ago the terms *connectionism* and *parallel distributed processing* (http://cognet.mit.edu/book/parallel-distributed-processing) were popular.\n",
    "\n",
    "I'll use the *neuron* or *node* to describe an individual value. *Perceptron* is another common term for the same concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Neural Networks?\n",
    "\n",
    "Neural networks are a class of models that can be used for a variety of purposes. Mostly they are supervised-learning models used for either classification or regression, but they can be designed for other applications as well such as unsupervised learning or reinforcement learning. They perform well with high-dimensional data such as images, audio, and text.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    " * Hard to design and tune\n",
    " * Seriously, really hard to design and tune\n",
    " * Slow to train\n",
    " * Uninterpretable\n",
    " * Easy to overfit (need a lot of data)\n",
    " \n",
    "Advantages\n",
    "\n",
    " * Works well with high-dimensional data\n",
    " * Can find almost anything, when designed and trained correctly\n",
    " * Online training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Stage 1: ('50s-60s) Early understanding of the brain and development of computers ('40s-60s)\n",
    "\n",
    "Stage 2: ('80s-'90s) Understanding of backpropagation, recognition that neural networks could be used for associative memory, first recurrent neural networks\n",
    "\n",
    "Stage 3: (2005-) Growth of GPUs, better algorithms for training deep networks, more designs, more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "A neural network can be expressed as a directed graph. The nodes of the graph are neurons. Neurons can either be **input**, **hidden**, or **output** neurons. The input nodes are set to the feature, and (once all the calculations are done) the value of the output nodes will be the predicted labels.\n",
    "\n",
    "The connections between the neurons are either **weights** or **activation functions**. If the connections into a neuron are weights, then the value for that neuron is the sum over all incoming connects of the weight times the value of the previous neuron.\n",
    "\n",
    "<img src=\"img/2-1.png\" alt=\"1-layer network\" style=\"height: 180px;\"/>\n",
    "\n",
    "Here we have two input nodes ($x_1$ and $x_2$), a single output node ($y$), and weights between them ($W_1$ and $W_2$). So here\n",
    "\n",
    "$$y = W_1 x_1 + W_2 x_2$$\n",
    "\n",
    "Question: does that look familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best values for the weights, we need to train the model. This is done with some sort of gradient descent to minimize the error in predicting the training data. In this case we might define the error (the *loss function*) as the sum of squares of the difference of actual and predicted values of y.\n",
    "\n",
    "The other option for a connection is an activation function. Here the value of a node is simply some fixed function of the of the previous node. There are a number of common ones; one of these is a the logistic function. Sometimes the term sigmoid function is used instead, though some (i.e., myself) use \"sigmoid\" to mean any function that sort of looks like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "xpts = np.linspace(-5, 5, 100)\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(xpts, logistic(xpts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add that in, along with a constant intercept term.\n",
    "\n",
    "<img src=\"img/2-1-logit.png\" alt=\"1-layer network\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write that as\n",
    "\n",
    "$$y = \\frac{1}{1 + e^{-(W_1 x_1 + W_2 x_2 + b)}}$$\n",
    "\n",
    "Question: does that look familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to find the best weights we use something like gradient descent, though if we want to use it to predict something is in one class or another, we'd use log loss (a.k.a. cross entropy) as our loss function.\n",
    "\n",
    "Question: could we use accuracy as a loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "\n",
    "So far we haven't done anything new. Neural networks are more powerful if we introduce additional layers.\n",
    "\n",
    "<img src=\"img/2-2-1-logit.png\" alt=\"2-layer network\" style=\"height: 400px;\"/>\n",
    "\n",
    "The nodes $h_1$ and $h_2$ are called hidden nodes.\n",
    "Now we have a bunch of terms to fit, weights for each connection plus intercept terms.\n",
    "\n",
    "Discussion: Why did we add those logistic functions in the middle of the network? What if we hadn't included them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simulation\n",
    "\n",
    "Let's explore a simple neural network being used for classification at https://playground.tensorflow.org\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Networks\n",
    "\n",
    "Much of the challenge of creating neural networks is deciding how to organize the nodes. The simplest type is called a **fully connected network**, in which nodes are organized into layers. Each layer (after the input) is a fully connected layer, with each of its nodes connected to the each node in the previous. Each layer may have an associated activation function, which is used for each node in the layer.\n",
    "\n",
    "The more hidden layers a network has, the more powerful it is and the more it can predict. But it will also be slower to train and easier to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple  neural network in keras on some data we can understand. Usually neural networks are used for fancier stuff, but this is just a simple 1-feature regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 500\n",
    "x_train = stats.uniform(-5, 10).rvs(npts)\n",
    "y_train = np.sin(x_train) + stats.norm(0, 1.0).rvs(npts)\n",
    "x_test = stats.uniform(-5, 10).rvs(npts)\n",
    "y_test = np.sin(x_test) + stats.norm(0, 1.0).rvs(npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x_train, y_train, s=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic object in keras is called a `model`; you create a model first and add layers onto it. The simplest one to use is a `Sequential` model, which is just a bunch of layers. Let's make one of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off we'll make the simplest network we can, without any hidden layers, just a single output layer. We need to specify the number of `units` in that layer (1) as well as the number of input units with `input_dim` (also 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1, input_dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - we built a neural network! Now we compile it (specifying `loss` function).\n",
    "\n",
    "**Question:** What do you think `optimizer='sgd'` means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the model. We train the data in batches of multiple points at once, running through the entire dataset (epochs) several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a picture!\n",
    "\n",
    "Our data only range from -5 to 5, but we'll plot a bit beyond that. We'll make a function since we'll do this a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, ax=None, function=np.sin, xlim=(-7, 7),  data_lim=(-5, 5)):\n",
    "    x_actual = np.linspace(*xlim, 500)\n",
    "    y_actual = function(x_actual)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    y_pred = model.predict(x_actual)\n",
    "    ax.plot(x_actual,y_pred, 'r-', label='model predictions')\n",
    "    ax.plot(x_actual,y_actual, 'b-', lw=0.3, label='actual expected values')\n",
    "\n",
    "    ax.axvline(data_lim[0], color='k', label='limits of data')\n",
    "    ax.axvline(data_lim[1], color='k')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.legend()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "plot_results(model, ax=ax)\n",
    "#ylim = ax.get_ylim()\n",
    "#ax.scatter(x_train, y_train, s=3, label='data')\n",
    "#ax.set_ylim(ylim)\n",
    "#ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically just did linear regression.\n",
    "\n",
    "Let's make a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hidden_layer = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add a layer to it, taking us from a single input unit to 5 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 5\n",
    "model_hidden_layer.add(Dense(units=n_hidden, input_dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second layer will connect use to the output with a single output value. Note we do not need to specify the `input_dim` since it's inferred from the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hidden_layer.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the model and fit it, as before, and plot the results. We're going to specify additional `metrics` to display when compiling, and also a test set to validate the result (again, just for display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hidden_layer.compile(loss='mean_squared_error',\n",
    "             optimizer='sgd',\n",
    "             metrics=['mean_absolute_error'])\n",
    "model_hidden_layer.fit(x_train, y_train,\n",
    "                       epochs=5,\n",
    "                       batch_size=32,\n",
    "                       validation_data=(x_test, y_test))\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_results(model_hidden_layer, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was disappointing.\n",
    "\n",
    "What went wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing non-linearity\n",
    "\n",
    "The problem is that our network is completely linear, so no matter what the weights the output is a linear function of the input. In order to fix this, we need to provide some sort of non-linear function at each of the layers.\n",
    "\n",
    "Above we used a logistic function for our activation function. In the past that was the most popular activation function, for a couple reasons. First, it's differentiable everywhere. Second, the finite range was seen as a model for the activation of neurons in the brain. Both of these were seen as Good Things.\n",
    "\n",
    "It turns out neither was that important, and today the ReLU function is generally more popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts = np.linspace(-5, 10, 500)\n",
    "sess = tf.InteractiveSession()\n",
    "functions = [keras.backend.relu,\n",
    "             keras.backend.elu,\n",
    "             keras.backend.softplus,\n",
    "             keras.backend.softsign,\n",
    "             keras.backend.tanh,\n",
    "             keras.backend.sigmoid            ]\n",
    "\n",
    "fig,axes = plt.subplots(len(functions), figsize=(12,12))\n",
    "\n",
    "for ax, func in zip(axes, functions):\n",
    "    ax.plot(xpts, func(xpts).eval(), label=func.__name__)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xlim(xpts.min(), xpts.max())\n",
    "\n",
    "fig.suptitle('Various Activation Functions', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation units\n",
    "\n",
    "To add an activation function in keras, we just add it as an additional layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activation = Sequential()\n",
    "n_hidden = 5\n",
    "model_activation.add(Dense(units=n_hidden, input_dim=1))\n",
    "model_activation.add(Activation('relu'))\n",
    "model_activation.add(Dense(units=1))\n",
    "model_activation.compile(loss='mean_squared_error',\n",
    "             optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activation.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "plot_results(model_activation, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Since neural networks (particularly fully connected networks) have a large number of parameters, they are fairly easy to overfit. This is most serious in situations in which we have more features than data.\n",
    "\n",
    "Question: is that the case here?\n",
    "\n",
    "Question: what's an example of such a problem?\n",
    "\n",
    "To avoid this most neural networks include some sort of regularization. One approach is the same sort of L1 or L2 regularization used in linear and logistic regression. Another approach is dropout.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "The most popular approach is adding dropout layers. A dropout layer has no effect during prediction, acting like an identity matrix, but during training causes nodes to fail randomly. This forces the network to include redundancy.\n",
    "\n",
    "It won't make much of a different here, but this is what it would look like in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 20\n",
    "x_train = np.linspace(-5, 5, npts)\n",
    "y_train = np.sin(x_train) + stats.norm(0, 1.0).rvs(npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x_train, y_train, s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activation = Sequential()\n",
    "n_hidden = 50\n",
    "model_activation.add(Dense(units=n_hidden, input_dim=1, activation='relu'))\n",
    "model_activation.add(Dense(units=1))\n",
    "model_activation.add(Dropout(rate=0.5))\n",
    "model_activation.compile(loss='mean_squared_error',\n",
    "             optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activation.fit(x_train, y_train, epochs=5000, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "plot_results(model_activation, ax=ax)\n",
    "ax.scatter(x_train, y_train, s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "We've glossed over how you actually train the network\n",
    "Calculating the output based on the inputs is sometimes called **forward propagation**, in which the signal is propagated from one layer to the next. To update the weights in the model we use an algorithm called **backpropagation**. In this, we compare the predicted output with the expected value for a set of inputs to find the output error, and propagate the error backwards, from one layer to the previous one, based on the gradient of the intervening functions and weights. Once we have the error at each node we can use gradient descent or some related algorithm to adjust the weights at that node.\n",
    "\n",
    "We aren't going to do that manually, but if you're interested Jeff's lecture at https://github.com/zipfian/DSI_Lectures/blob/master/neural-network/jfomhover/Perceptron%20-%20Notes%20-%20v3.ipynb does a great job exploring that in more detail. TensorFlow (or any other NN framework) all take care of that automatically.\n",
    "\n",
    "That said, it's an important algorithm that was critical to the development of NN and at some point it's worth studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "An autoencoder a unsupervised learning model to create a low-dimensional representation of data. We'll do this by creating a multi-layer network, with one of the hidden layers having a small number of nodes.\n",
    "\n",
    "Think of this like PCA, but better because it learns the overall structure of the data rather than just looking in the directions in which things vary the most.\n",
    "\n",
    "There are a couple reasons we might want to create a low-dimensional representation. One, we might want to visualize data. It's hard to visualize high-dimensional data; reducing it to two important dimensions helps.\n",
    "\n",
    "Second, we might want to remove noise, focusing on the features in which the data varies. There are reasons to be skeptical of this, as discussed in the PCA lectures, but autoencoders are generally better than PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we'll use some data from two thousand recipes of cookies, pastries, and pizzas. The features are ingredients: 1 if the ingredient is present in the recipe, 0 if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = pd.read_excel('data/cookieclassifier_data_matrix.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cookies.values\n",
    "y = cookies.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "y_numeric = le.transform(y)\n",
    "y_labels = le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                    y_numeric,\n",
    "                                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60,  activation='relu', input_shape=(133,)))\n",
    "model.add(Dense(40,  activation='relu'))\n",
    "model.add(Dense(20,  activation='relu'))\n",
    "model.add(Dense(2,    activation='linear', name=\"bottleneck\"))\n",
    "model.add(Dense(20,  activation='relu'))\n",
    "model.add(Dense(40,  activation='relu'))\n",
    "model.add(Dense(60,  activation='relu'))\n",
    "model.add(Dense(133,  activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=0, \n",
    "                    validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(model.input, model.get_layer('bottleneck').output)\n",
    "z_enc = encoder.predict(X_train)  # bottleneck representation\n",
    "r_enc = model.predict(X_train)        # reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for label, class_name in zip(y_labels, le.classes_):\n",
    "    z0 = z_enc[:, 0][y_train == label]\n",
    "    z1 = z_enc[:, 1][y_train == label]\n",
    "    ax.scatter(z0, z1, label=class_name)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you get crashes here, `conda install nomkl` helped me)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also build a decoder to reconstruct the data from a point on the grid, finding the corresponding \"recipe\". It isn't that useful here, but I'm providing it as an example. Note that it uses keras's functional interface, which is very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(2,))\n",
    "\n",
    "decoding_layers = model.layers[4](encoded_input)\n",
    "decoding_layers = model.layers[5](decoding_layers)\n",
    "decoding_layers = model.layers[6](decoding_layers)\n",
    "decoding_layers = model.layers[7](decoding_layers)\n",
    "\n",
    "decoder = Model(encoded_input, decoding_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "decoder.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
