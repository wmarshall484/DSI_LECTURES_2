{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essence of the hidden layer\n",
    "\n",
    "We know two things:\n",
    "\n",
    "* Neural networks are basically logistic regression.\n",
    "* Gradient descent can get us real far.\n",
    "\n",
    "Here's an idea just crazy enough it might work:\n",
    "\n",
    "* What if we do logistic regression in order to classify our samples as some _latent things_.\n",
    "* And then we do logistic regression using the _latent things_ as features.\n",
    "\n",
    "For example:\n",
    "\n",
    "* Take the Fantasy Uber Churn dataset\n",
    "* Predict if each sample is\n",
    " * A business traveler\n",
    " * A casual traveler\n",
    " * A real jerk\n",
    " * A late night party animal\n",
    " * Any number of other personas\n",
    "* Once we've established which categories they belong in, use that to predict if they'll churn.\n",
    "\n",
    "To discover this \"hidden layer\" of personas, we'll just initalize feature->persona and persona->churn connections at random, and use the gradient of the loss to slowly move the weights to improve the final loss.\n",
    "\n",
    "Hopefull, this will result in the discovery of the intermediate hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get that data\n",
    "\n",
    "Let's grab the Fantasy Uber dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"churn.csv\", parse_dates=[\"last_trip_date\",\"signup_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_surge</th>\n",
       "      <th>city</th>\n",
       "      <th>last_trip_date</th>\n",
       "      <th>phone</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>surge_pct</th>\n",
       "      <th>trips_in_first_30_days</th>\n",
       "      <th>luxury_car_user</th>\n",
       "      <th>weekday_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.67</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>King's Landing</td>\n",
       "      <td>2014-06-17</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>15.4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Astapor</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>Android</td>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.77</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Astapor</td>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.36</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.14</td>\n",
       "      <td>King's Landing</td>\n",
       "      <td>2014-06-29</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.13</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.19</td>\n",
       "      <td>Winterfell</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>Android</td>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>11.8</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>82.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_surge  \\\n",
       "0      3.67                   5.0                   4.7       1.10   \n",
       "1      8.26                   5.0                   5.0       1.00   \n",
       "2      0.77                   5.0                   4.3       1.00   \n",
       "3      2.36                   4.9                   4.6       1.14   \n",
       "4      3.13                   4.9                   4.4       1.19   \n",
       "\n",
       "             city last_trip_date    phone signup_date  surge_pct  \\\n",
       "0  King's Landing     2014-06-17   iPhone  2014-01-25       15.4   \n",
       "1         Astapor     2014-05-05  Android  2014-01-29        0.0   \n",
       "2         Astapor     2014-01-07   iPhone  2014-01-06        0.0   \n",
       "3  King's Landing     2014-06-29   iPhone  2014-01-10       20.0   \n",
       "4      Winterfell     2014-03-15  Android  2014-01-27       11.8   \n",
       "\n",
       "   trips_in_first_30_days  luxury_car_user  weekday_pct  \n",
       "0                       4             True         46.2  \n",
       "1                       0            False         50.0  \n",
       "2                       3            False        100.0  \n",
       "3                       9             True         80.0  \n",
       "4                      14            False         82.4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = df.last_trip_date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dummies = pd.get_dummies(df.pop(\"city\"))\n",
    "phone_dummies = pd.get_dummies(df.pop(\"phone\"))\n",
    "df[\"signup_dow\"] = df.signup_date.dt.weekday\n",
    "df[\"age\"] = (today - df.signup_date).dt.days\n",
    "df[\"pass_rating_blank\"] = df.avg_rating_by_driver.isna()\n",
    "df[\"driver_rating_blank\"] = df.avg_rating_of_driver.isna()\n",
    "df[\"avg_rating_by_driver\"] = df.avg_rating_by_driver.fillna(df.avg_rating_by_driver.mean())\n",
    "df[\"avg_rating_of_driver\"] = df.avg_rating_of_driver.fillna(df.avg_rating_of_driver.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer the target, a binary variable for churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df.last_trip_date <= today-timedelta(days=30)).astype(float).values\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop non-numeric columns and get the raw matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"last_trip_date\",\"signup_date\"], axis=1)\n",
    "X = df.values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model proceeds in two steps. In the first step, we make a random guess about the linear mapping of the 12 input features to 6 output features, and then apply a matrix to get the output features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#parameters for hidden layer\n",
    "W_hidden = tf.Variable(np.random.normal(size=(12,6)))\n",
    "b_hidden = tf.Variable(np.zeros(shape=(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(50000, 6), dtype=float64, numpy=\n",
       "array([[ 157.51801023,  123.42623012, -224.01717846,  -76.56557536,\n",
       "          14.25859132,  192.57573125],\n",
       "       [ 164.94845967,  115.62099873, -205.83461763,  -71.8160556 ,\n",
       "          17.04969972,  199.22173678],\n",
       "       [ 176.49691555,  133.44306734, -303.39512446, -128.33701125,\n",
       "           7.92108939,  271.16321516],\n",
       "       ...,\n",
       "       [ 155.8146044 ,   93.8258662 , -281.61212978, -114.46447596,\n",
       "          -6.22986826,  252.55168131],\n",
       "       [ 168.40335502,  123.18683328, -290.92950499, -119.47080611,\n",
       "           4.89119767,  264.58938538],\n",
       "       [ 180.56497915,  138.0565267 , -142.20821978,  -48.06636208,\n",
       "          43.57663036,  161.46131166]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ W_hidden + b_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use a sigmoid function to squish the linear output features to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.      , 1.      , 0.      , 0.      , 1.      , 1.      ],\n",
       "       [1.      , 1.      , 0.      , 0.      , 1.      , 1.      ],\n",
       "       [1.      , 1.      , 0.      , 0.      , 0.9995  , 1.      ],\n",
       "       ...,\n",
       "       [1.      , 1.      , 0.      , 0.      , 0.001966, 1.      ],\n",
       "       [1.      , 1.      , 0.      , 0.      , 0.9927  , 1.      ],\n",
       "       [1.      , 1.      , 0.      , 0.      , 1.      , 1.      ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_activation = tf.sigmoid( X @ W_hidden + b_hidden ).numpy().astype(np.float16)\n",
    "unseen_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row of the output corresponds to an input sample, and every column responds to an unseen \"feature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 1.], dtype=float16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_activation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second stage, we take the unseen layer, and apply an arbirary set of weights and biases to calculate a sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for final logistic regression\n",
    "W_final = tf.Variable(np.ones(shape=(6,1)))\n",
    "b_final = tf.Variable(0.0,dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=124, shape=(50000, 1), dtype=float64, numpy=\n",
       "array([[0.98201379],\n",
       "       [0.98201379],\n",
       "       [0.98200516],\n",
       "       ...,\n",
       "       [0.95266289],\n",
       "       [0.98188397],\n",
       "       [0.98201379]])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = tf.sigmoid( unseen_activation @ W_final + b_final )\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final prediction. It's wrong, but it's a start. It's essential that we can come up with a differentiable function that describes _how_ wrong it is. Here's binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=168, shape=(), dtype=float64, numpy=71160.3055911408>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum( tf.keras.backend.binary_crossentropy(y, yhat) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying things up a bit\n",
    "\n",
    "Same code as above, but packaged up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#parameters for hidden layer\n",
    "W_hidden = tf.Variable(np.random.normal(size=(12,6)))\n",
    "b_hidden = tf.Variable(np.zeros(shape=(6)))\n",
    "\n",
    "#parameters for final logistic regression\n",
    "W_final = tf.Variable(np.ones(shape=(6,1)))\n",
    "b_final = tf.Variable(0.0,dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W_hidden, b_hidden, W_final, b_final):\n",
    "    #find activation of hidden layer\n",
    "    hidden_activation = tf.math.sigmoid( X @ W_hidden + b_hidden )\n",
    "\n",
    "    #find prediction\n",
    "    return tf.math.sigmoid( hidden_activation@W_final + b_final )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getloss(y, yhat):\n",
    "    return tf.reduce_sum( tf.keras.backend.binary_crossentropy(y, yhat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=229, shape=(), dtype=float64, numpy=71160.2542384141>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(X, W_hidden, b_hidden, W_final, b_final)\n",
    "getloss(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a gradient tape to record the process of W_hidden being processed into a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(W_hidden)\n",
    "    \n",
    "    yhat = model(X, W_hidden, b_hidden, W_final, b_final)\n",
    "    loss = getloss(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the gradient tape has recorded a single forward pass of the computation graph, which it can use to do a backwards pass and get the gradient of the hidden layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_dWhidden = t.gradient(loss, W_hidden).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.42173391e+01,  9.14538269e+00,  8.48521514e+00,  3.04829406e+00,\n",
       "        1.52748636e+02, -1.60464601e-01,  7.96605488e-01, -5.62412772e-01,\n",
       "        6.50498509e+00,  2.98758180e+02, -3.09842817e-03,  4.19603912e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloss_dWhidden[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start those weights back at the beginning\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#parameters for hidden layer\n",
    "W_hidden = tf.Variable(np.random.normal(size=(12,6)))\n",
    "b_hidden = tf.Variable(np.zeros(shape=(6)))\n",
    "\n",
    "#parameters for final logistic regression\n",
    "W_final = tf.Variable(np.ones(shape=(6,1)))\n",
    "b_final = tf.Variable(0.0,dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need a function with no arguments that makes the loss tensor\n",
    "def foo():\n",
    "    return getloss(y, model(X, W_hidden, b_hidden, W_final, b_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=430, shape=(), dtype=float64, numpy=71160.2542384141>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = [] #for keeping track of the loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31562.868568259677\n",
      "1 31560.144759007966\n",
      "2 31558.905043342114\n",
      "3 31558.171335575127\n",
      "4 31557.513940005985\n",
      "5 31556.904434297092\n",
      "6 31556.33076669971\n",
      "7 31555.786677538395\n",
      "8 31555.26694260023\n",
      "9 31554.767520750847\n",
      "10 31554.284773034924\n",
      "11 31553.81549427451\n",
      "12 31553.356645012194\n",
      "13 31552.905342349994\n",
      "14 31552.458700697294\n",
      "15 31552.013806161212\n",
      "16 31551.567574375462\n",
      "17 31551.116718734687\n",
      "18 31550.657579426465\n",
      "19 31550.186089612078\n",
      "20 31549.697488327423\n",
      "21 31549.18631252781\n",
      "22 31548.646100426755\n",
      "23 31548.072162116303\n",
      "24 31547.477297114747\n",
      "25 31546.959229772023\n",
      "26 31546.43311269777\n",
      "27 31545.51375716831\n",
      "28 31544.259992616695\n",
      "29 31543.15248563041\n",
      "30 31542.038739625685\n",
      "31 31540.941884138505\n",
      "32 31539.850978990933\n",
      "33 31538.768827856453\n",
      "34 31537.68687353217\n",
      "35 31536.599579746286\n",
      "36 31535.49825285555\n",
      "37 31534.377171783766\n",
      "38 31533.22974329904\n",
      "39 31532.059737175456\n",
      "40 31530.84759242747\n",
      "41 31529.590639544957\n",
      "42 31528.165623556182\n",
      "43 31526.512261681295\n",
      "44 31524.169144879103\n",
      "45 31521.106243041504\n",
      "46 31517.58540824135\n",
      "47 31514.534305121077\n",
      "48 31512.29714947498\n",
      "49 31510.777259537048\n",
      "50 31509.693366179956\n",
      "51 31508.846141064732\n",
      "52 31508.120134714998\n",
      "53 31507.473158797882\n",
      "54 31506.868397612172\n",
      "55 31506.295582393694\n",
      "56 31505.66701881772\n",
      "57 31505.048802830333\n",
      "58 31504.413384270065\n",
      "59 31503.821140824453\n",
      "60 31503.242102481676\n",
      "61 31502.6897251048\n",
      "62 31502.149642576805\n",
      "63 31501.62868547697\n",
      "64 31501.118748391324\n",
      "65 31500.627557633532\n",
      "66 31500.146265229363\n",
      "67 31499.68600976149\n",
      "68 31499.226299766866\n",
      "69 31498.783332181632\n",
      "70 31498.31875478807\n",
      "71 31497.869104967867\n",
      "72 31497.399331490647\n",
      "73 31496.951016925064\n",
      "74 31496.495200188758\n",
      "75 31496.05728187318\n",
      "76 31495.615201650886\n",
      "77 31495.18587230362\n",
      "78 31494.751172434542\n",
      "79 31494.32712476909\n",
      "80 31493.89414025857\n",
      "81 31493.471552268278\n",
      "82 31493.03279015112\n",
      "83 31492.60377641975\n",
      "84 31492.150232690456\n",
      "85 31491.7066230552\n",
      "86 31491.23722389919\n",
      "87 31490.778743846564\n",
      "88 31490.298285722736\n",
      "89 31489.827474475434\n",
      "90 31489.336901095503\n",
      "91 31488.853896987508\n",
      "92 31488.350640165096\n",
      "93 31487.854271711512\n",
      "94 31487.33511464381\n",
      "95 31486.82377359273\n",
      "96 31486.28574450212\n",
      "97 31485.75728047297\n",
      "98 31485.19950398869\n",
      "99 31484.653714954304\n",
      "100 31484.080468962315\n",
      "101 31483.52171938197\n",
      "102 31482.94035753437\n",
      "103 31482.375081798767\n",
      "104 31481.791521633655\n",
      "105 31481.225324417697\n",
      "106 31480.643284683953\n",
      "107 31480.08048994913\n",
      "108 31479.50234503166\n",
      "109 31478.94590810721\n",
      "110 31478.37329919522\n",
      "111 31477.8250318869\n",
      "112 31477.26020724139\n",
      "113 31476.722310927482\n",
      "114 31476.16935748898\n",
      "115 31475.645491392606\n",
      "116 31475.109238885467\n",
      "117 31474.60347491365\n",
      "118 31474.087490256472\n",
      "119 31473.60281408554\n",
      "120 31473.10871495193\n",
      "121 31472.646432727743\n",
      "122 31472.17432561614\n",
      "123 31471.73429840967\n",
      "124 31471.28367371185\n",
      "125 31470.86511635513\n",
      "126 31470.43563170547\n",
      "127 31470.037840127145\n",
      "128 31469.629318678708\n",
      "129 31469.25167416518\n",
      "130 31468.863482907516\n",
      "131 31468.504992478578\n",
      "132 31468.135623407426\n",
      "133 31467.794621780362\n",
      "134 31467.441793194666\n",
      "135 31467.11600022053\n",
      "136 31466.777068063566\n",
      "137 31466.463898700862\n",
      "138 31466.136306493663\n",
      "139 31465.833247714803\n",
      "140 31465.514770271828\n",
      "141 31465.219585853618\n",
      "142 31464.908236926327\n",
      "143 31464.618878746216\n",
      "144 31464.312612101603\n",
      "145 31464.026955516852\n",
      "146 31463.723405617224\n",
      "147 31463.439004866643\n",
      "148 31463.135384241832\n",
      "149 31462.849347004492\n",
      "150 31462.54246365928\n",
      "151 31462.251438138523\n",
      "152 31461.93773694515\n",
      "153 31461.637948131243\n",
      "154 31461.31350161432\n",
      "155 31461.000784717988\n",
      "156 31460.66126603211\n",
      "157 31460.33114279492\n",
      "158 31459.971941754673\n",
      "159 31459.619884586726\n",
      "160 31459.236548467972\n",
      "161 31458.858615891382\n",
      "162 31458.44778718054\n",
      "163 31458.04190935722\n",
      "164 31457.603024369586\n",
      "165 31457.17110340564\n",
      "166 31456.70858653216\n",
      "167 31456.25819017872\n",
      "168 31455.781946643445\n",
      "169 31455.3247477342\n",
      "170 31454.846720504174\n",
      "171 31454.393690584824\n",
      "172 31453.923267165446\n",
      "173 31453.48138164776\n",
      "174 31453.024055654903\n",
      "175 31452.596674862813\n",
      "176 31452.15503909763\n",
      "177 31451.743370269236\n",
      "178 31451.31794246367\n",
      "179 31450.921837583664\n",
      "180 31450.511588113735\n",
      "181 31450.129917637732\n",
      "182 31449.73288218707\n",
      "183 31449.36386711723\n",
      "184 31448.977850526033\n",
      "185 31448.619544494257\n",
      "186 31448.242850180563\n",
      "187 31447.893764234163\n",
      "188 31447.525644338755\n",
      "189 31447.18512046423\n",
      "190 31446.82558489736\n",
      "191 31446.49358738303\n",
      "192 31446.142773970954\n",
      "193 31445.819350014688\n",
      "194 31445.477033763123\n",
      "195 31445.16191305347\n",
      "196 31444.827434358893\n",
      "197 31444.519970247427\n",
      "198 31444.192487142765\n",
      "199 31443.891881637308\n",
      "200 31443.570705765298\n",
      "201 31443.27631158204\n",
      "202 31442.961072150883\n",
      "203 31442.672527480092\n",
      "204 31442.363058379306\n",
      "205 31442.080173500468\n",
      "206 31441.776263798354\n",
      "207 31441.49879889196\n",
      "208 31441.20002544765\n",
      "209 31440.927546200077\n",
      "210 31440.633285062715\n",
      "211 31440.365177339994\n",
      "212 31440.074741594406\n",
      "213 31439.810337788287\n",
      "214 31439.52310937037\n",
      "215 31439.261801279772\n",
      "216 31438.977243704994\n",
      "217 31438.718483680997\n",
      "218 31438.436035555307\n",
      "219 31438.179238357232\n",
      "220 31437.898188386986\n",
      "221 31437.64262408215\n",
      "222 31437.362060604723\n",
      "223 31437.106810748828\n",
      "224 31436.825658053014\n",
      "225 31436.56964059506\n",
      "226 31436.28672586783\n",
      "227 31436.028740265127\n",
      "228 31435.74281769563\n",
      "229 31435.481547735864\n",
      "230 31435.191238479834\n",
      "231 31434.92517812678\n",
      "232 31434.628837763546\n",
      "233 31434.356148956605\n",
      "234 31434.05170357925\n",
      "235 31433.770036814014\n",
      "236 31433.45481504289\n",
      "237 31433.161111154463\n",
      "238 31432.831661037795\n",
      "239 31432.52192188645\n",
      "240 31432.17376625797\n",
      "241 31431.842757918625\n",
      "242 31431.470033137633\n",
      "243 31431.110919263883\n",
      "244 31430.705947201084\n",
      "245 31430.31011759462\n",
      "246 31429.863491654163\n",
      "247 31429.421736290467\n",
      "248 31428.9248566734\n",
      "249 31428.43227866417\n",
      "250 31427.884641057797\n",
      "251 31427.348948911444\n",
      "252 31426.765054329608\n",
      "253 31426.20701922178\n",
      "254 31425.60879623421\n",
      "255 31425.047166933226\n",
      "256 31424.44756322812\n",
      "257 31423.887534895515\n",
      "258 31423.287657094123\n",
      "259 31422.724669480594\n",
      "260 31422.11927425797\n",
      "261 31421.54367027\n",
      "262 31420.92061221671\n",
      "263 31420.315511000856\n",
      "264 31419.651465358016\n",
      "265 31418.98764380545\n",
      "266 31418.242891243244\n",
      "267 31417.471281788257\n",
      "268 31416.581231658794\n",
      "269 31415.62151279233\n",
      "270 31414.48270812474\n",
      "271 31413.20541945462\n",
      "272 31411.6509783366\n",
      "273 31409.87197129076\n",
      "274 31407.74800566575\n",
      "275 31405.535485628665\n",
      "276 31403.30146402194\n",
      "277 31401.362132982857\n",
      "278 31399.582880586455\n",
      "279 31398.035499103768\n",
      "280 31396.519256217158\n",
      "281 31395.153153340172\n",
      "282 31393.75613319953\n",
      "283 31392.48344364544\n",
      "284 31391.186500181277\n",
      "285 31389.98885456161\n",
      "286 31388.774623653455\n",
      "287 31387.63093633425\n",
      "288 31386.461830108194\n",
      "289 31385.341145453887\n",
      "290 31384.172447111796\n",
      "291 31383.034390283923\n",
      "292 31381.81091769201\n",
      "293 31380.598317157932\n",
      "294 31379.25440438902\n",
      "295 31377.896506928853\n",
      "296 31376.366664005305\n",
      "297 31374.784600682233\n",
      "298 31372.979769859063\n",
      "299 31371.038783191107\n",
      "300 31368.73677801386\n",
      "301 31366.074012567184\n",
      "302 31362.65242739515\n",
      "303 31358.454664580648\n",
      "304 31353.210281761356\n",
      "305 31347.4880848509\n",
      "306 31341.4936301954\n",
      "307 31335.822044791814\n",
      "308 31330.52791970296\n",
      "309 31325.899451401077\n",
      "310 31321.70480243289\n",
      "311 31317.994440420214\n",
      "312 31314.470113010204\n",
      "313 31311.213352693496\n",
      "314 31308.015036084627\n",
      "315 31305.001665522905\n",
      "316 31302.012360000503\n",
      "317 31299.179362171497\n",
      "318 31296.35343825854\n",
      "319 31293.67220006347\n",
      "320 31290.979609414575\n",
      "321 31288.427047616275\n",
      "322 31285.843954772223\n",
      "323 31283.398144359486\n",
      "324 31280.909466729943\n",
      "325 31278.55485191614\n",
      "326 31276.156290840114\n",
      "327 31273.88730522702\n",
      "328 31271.57951532051\n",
      "329 31269.396001673642\n",
      "330 31267.17749625518\n",
      "331 31265.078797559432\n",
      "332 31262.94416003637\n",
      "333 31260.92687141745\n",
      "334 31258.86837827113\n",
      "335 31256.926613529766\n",
      "336 31254.936945133457\n",
      "337 31253.0643445236\n",
      "338 31251.139351455604\n",
      "339 31249.331768900778\n",
      "340 31247.470954843382\n",
      "341 31245.727224101007\n",
      "342 31243.931305209164\n",
      "343 31242.251602982407\n",
      "344 31240.51999513191\n",
      "345 31238.9039171919\n",
      "346 31237.2341503009\n",
      "347 31235.679882146633\n",
      "348 31234.068525438546\n",
      "349 31232.57318930573\n",
      "350 31231.017286893562\n",
      "351 31229.578034959926\n",
      "352 31228.076016414598\n",
      "353 31226.690933612972\n",
      "354 31225.242252260778\n",
      "355 31223.91027030664\n",
      "356 31222.514202948478\n",
      "357 31221.234267546773\n",
      "358 31219.889017873535\n",
      "359 31218.65932385934\n",
      "360 31217.361981194987\n",
      "361 31216.179757671103\n",
      "362 31214.92687467113\n",
      "363 31213.788710291294\n",
      "364 31212.576958930433\n",
      "365 31211.47933254106\n",
      "366 31210.305749709252\n",
      "367 31209.245332181832\n",
      "368 31208.107037159087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 31207.080582576113\n",
      "370 31205.97430719911\n",
      "371 31204.97833233361\n",
      "372 31203.9001946455\n",
      "373 31202.93076890097\n",
      "374 31201.876437644285\n",
      "375 31200.929254995706\n",
      "376 31199.89435557738\n",
      "377 31198.965045324527\n",
      "378 31197.945540562974\n",
      "379 31197.030028271372\n",
      "380 31196.022373493197\n",
      "381 31195.117095429596\n",
      "382 31194.118155830256\n",
      "383 31193.220059395302\n",
      "384 31192.226943207952\n",
      "385 31191.333360281395\n",
      "386 31190.343339588864\n",
      "387 31189.45188142325\n",
      "388 31188.462471149756\n",
      "389 31187.57104765719\n",
      "390 31186.580200887103\n",
      "391 31185.68717000434\n",
      "392 31184.69349115892\n",
      "393 31183.79785822955\n",
      "394 31182.80074458576\n",
      "395 31181.902300967577\n",
      "396 31180.90197081528\n",
      "397 31180.001293276968\n",
      "398 31178.998703283352\n",
      "399 31178.0970054526\n",
      "400 31177.093681149367\n",
      "401 31176.19257648788\n",
      "402 31175.190386386148\n",
      "403 31174.29164613507\n",
      "404 31173.292545923345\n",
      "405 31172.39789602878\n",
      "406 31171.40366236183\n",
      "407 31170.514607897527\n",
      "408 31169.526593658862\n",
      "409 31168.644254990322\n",
      "410 31167.663221375522\n",
      "411 31166.788204659082\n",
      "412 31165.814276474135\n",
      "413 31164.946618562455\n",
      "414 31163.97935614889\n",
      "415 31163.118554937286\n",
      "416 31162.15707842433\n",
      "417 31161.302172138956\n",
      "418 31160.345253600608\n",
      "419 31159.494884661413\n",
      "420 31158.540952265586\n",
      "421 31157.693361903534\n",
      "422 31156.74041509148\n",
      "423 31155.893354320833\n",
      "424 31154.93883199185\n",
      "425 31154.089414623617\n",
      "426 31153.13006601254\n",
      "427 31152.27461064753\n",
      "428 31151.30640286744\n",
      "429 31150.440320866015\n",
      "430 31149.45841067237\n",
      "431 31148.576218558963\n",
      "432 31147.57504042174\n",
      "433 31146.670607704284\n",
      "434 31145.64416806775\n",
      "435 31144.71129373146\n",
      "436 31143.65374659996\n",
      "437 31142.687008670047\n",
      "438 31141.59346335978\n",
      "439 31140.589079265577\n",
      "440 31139.45643010585\n",
      "441 31138.412845641986\n",
      "442 31137.240374326706\n",
      "443 31136.158477355366\n",
      "444 31134.948036905564\n",
      "445 31133.830961775086\n",
      "446 31132.58658169735\n",
      "447 31131.439015316726\n",
      "448 31130.165900778877\n",
      "449 31128.992953617064\n",
      "450 31127.696193889977\n",
      "451 31126.50223544573\n",
      "452 31125.185760730255\n",
      "453 31123.973693813616\n",
      "454 31122.639816093142\n",
      "455 31121.41094510541\n",
      "456 31120.060421791888\n",
      "457 31118.81472952969\n",
      "458 31117.44710478445\n",
      "459 31116.183686002136\n",
      "460 31114.79774590362\n",
      "461 31113.515281705157\n",
      "462 31112.109599510342\n",
      "463 31110.80689620881\n",
      "464 31109.38049648852\n",
      "465 31108.05714629405\n",
      "466 31106.61026313469\n",
      "467 31105.267389289766\n",
      "468 31103.80216242399\n",
      "469 31102.443022218104\n",
      "470 31100.96382562793\n",
      "471 31099.59388131443\n",
      "472 31098.106946814503\n",
      "473 31096.733087517787\n",
      "474 31095.245349188706\n",
      "475 31093.874401885318\n",
      "476 31092.39202628033\n",
      "477 31091.029291358143\n",
      "478 31089.556698881996\n",
      "479 31088.20533515435\n",
      "480 31086.745130041498\n",
      "481 31085.406513177088\n",
      "482 31083.959940215456\n",
      "483 31082.6343470668\n",
      "484 31081.201613623474\n",
      "485 31079.88857168938\n",
      "486 31078.46881736475\n",
      "487 31077.167018749533\n",
      "488 31075.758182871206\n",
      "489 31074.465264886258\n",
      "490 31073.064118117618\n",
      "491 31071.77658961428\n",
      "492 31070.37895003621\n",
      "493 31069.092321086995\n",
      "494 31067.6933003172\n",
      "495 31066.402304364157\n",
      "496 31064.996443913486\n",
      "497 31063.695218135166\n",
      "498 31062.276508628023\n",
      "499 31060.958674005728\n",
      "500 31059.520538220502\n",
      "501 31058.17917432049\n",
      "502 31056.71443288841\n",
      "503 31055.341902576773\n",
      "504 31053.842588405016\n",
      "505 31052.43009255036\n",
      "506 31050.886898732664\n",
      "507 31049.423645670056\n",
      "508 31047.82488448334\n",
      "509 31046.29689111674\n",
      "510 31044.627083595282\n",
      "511 31043.015869246054\n",
      "512 31041.254437210446\n",
      "513 31039.536233897445\n",
      "514 31037.657181993272\n",
      "515 31035.803528824435\n",
      "516 31033.77686501574\n",
      "517 31031.75646562732\n",
      "518 31029.549849771192\n",
      "519 31027.328097603833\n",
      "520 31024.902809338637\n",
      "521 31022.431829426947\n",
      "522 31019.72569472945\n",
      "523 31016.919618722328\n",
      "524 31013.814473284212\n",
      "525 31010.512491498746\n",
      "526 31006.798797073214\n",
      "527 31002.752738071493\n",
      "528 30998.181258175748\n",
      "529 30993.221651906264\n",
      "530 30987.815418923583\n",
      "531 30982.219578521235\n",
      "532 30976.479841800425\n",
      "533 30970.835804001057\n",
      "534 30965.277962888613\n",
      "535 30959.956968559585\n",
      "536 30954.793347338462\n",
      "537 30949.825433812992\n",
      "538 30944.90242243227\n",
      "539 30940.10413295235\n",
      "540 30935.306750966873\n",
      "541 30930.702799186878\n",
      "542 30926.119440598173\n",
      "543 30921.715582533106\n",
      "544 30917.333848570255\n",
      "545 30913.091595115788\n",
      "546 30908.856526129257\n",
      "547 30904.734663837786\n",
      "548 30900.59075991907\n",
      "549 30896.544722953135\n",
      "550 30892.42887568965\n",
      "551 30888.39764107298\n",
      "552 30884.246825958093\n",
      "553 30880.17201555943\n",
      "554 30875.964014356945\n",
      "555 30871.82548652588\n",
      "556 30867.5715285388\n",
      "557 30863.374330274146\n",
      "558 30859.08168328008\n",
      "559 30854.835682585952\n",
      "560 30850.50833860521\n",
      "561 30846.229057928365\n",
      "562 30841.877457820152\n",
      "563 30837.585330987138\n",
      "564 30833.22452192953\n",
      "565 30828.940113772693\n",
      "566 30824.591283771733\n",
      "567 30820.34137146775\n",
      "568 30816.04139448073\n",
      "569 30811.864672757598\n",
      "570 30807.657590739786\n",
      "571 30803.591524095355\n",
      "572 30799.50781705427\n",
      "573 30795.574802677555\n",
      "574 30791.62440349989\n",
      "575 30787.829612724378\n",
      "576 30784.006533361135\n",
      "577 30780.341846546708\n",
      "578 30776.63203609317\n",
      "579 30773.08235542981\n",
      "580 30769.471641380835\n",
      "581 30766.02149150442\n",
      "582 30762.49949687226\n",
      "583 30759.136352939335\n",
      "584 30755.694720302086\n",
      "585 30752.408328982012\n",
      "586 30749.0376054192\n",
      "587 30745.818172435313\n",
      "588 30742.507093027958\n",
      "589 30739.34439455191\n",
      "590 30736.08124174567\n",
      "591 30732.96493193224\n",
      "592 30729.739630066448\n",
      "593 30726.66055038091\n",
      "594 30723.46638147807\n",
      "595 30720.418070073716\n",
      "596 30717.251859881373\n",
      "597 30714.23098358149\n",
      "598 30711.091654132637\n",
      "599 30708.097051614874\n",
      "600 30704.98389125347\n",
      "601 30702.015172419677\n",
      "602 30698.92703908098\n",
      "603 30695.98367666835\n",
      "604 30692.919216464637\n",
      "605 30690.00039138549\n",
      "606 30686.958712110598\n",
      "607 30684.063702257936\n",
      "608 30681.04486682335\n",
      "609 30678.17350560869\n",
      "610 30675.178412860467\n",
      "611 30672.331141320054\n",
      "612 30669.36083993644\n",
      "613 30666.538263235077\n",
      "614 30663.59318582931\n",
      "615 30660.795468116834\n",
      "616 30657.875051282586\n",
      "617 30655.101536411647\n",
      "618 30652.204331583576\n",
      "619 30649.453546760305\n",
      "620 30646.577622135923\n",
      "621 30643.84757691945\n",
      "622 30640.990914409216\n",
      "623 30638.27946958995\n",
      "624 30635.440141401363\n",
      "625 30632.745232144087\n",
      "626 30629.9213340025\n",
      "627 30627.240978744958\n",
      "628 30624.43047190025\n",
      "629 30621.762664971444\n",
      "630 30618.963310620777\n",
      "631 30616.305954626263\n",
      "632 30613.515422485245\n",
      "633 30610.866396485384\n",
      "634 30608.082503995884\n",
      "635 30605.439860632025\n",
      "636 30602.66083350209\n",
      "637 30600.023041779514\n",
      "638 30597.24769727055\n",
      "639 30594.613831587012\n",
      "640 30591.84164710535\n",
      "641 30589.21146420011\n",
      "642 30586.442556361922\n",
      "643 30583.816463015963\n",
      "644 30581.051525629486\n",
      "645 30578.43047909912\n",
      "646 30575.670710352788\n",
      "647 30573.0560961276\n",
      "648 30570.30311426897\n",
      "649 30567.69662462423\n",
      "650 30564.952354558365\n",
      "651 30562.35586720866\n",
      "652 30559.622386258758\n",
      "653 30557.037831230817\n",
      "654 30554.317183458497\n",
      "655 30551.746393703164\n",
      "656 30549.040403130297\n",
      "657 30546.484962612478\n",
      "658 30543.795082243247\n",
      "659 30541.256198962074\n",
      "660 30538.583421208197\n",
      "661 30536.061851300088\n",
      "662 30533.406684471724\n",
      "663 30530.902718249905\n",
      "664 30528.265215152358\n",
      "665 30525.778716076165\n",
      "666 30523.158532313486\n",
      "667 30520.6890062754\n",
      "668 30518.08546861082\n",
      "669 30515.632139642854\n",
      "670 30513.04431397533\n",
      "671 30510.606191842973\n",
      "672 30508.03294912827\n",
      "673 30505.608886880378\n",
      "674 30503.048969076146\n",
      "675 30500.637716899106\n",
      "676 30498.08980377423\n",
      "677 30495.690066235704\n",
      "678 30493.152840772294\n",
      "679 30490.763339427423\n",
      "680 30488.235547623957\n",
      "681 30485.855084312665\n",
      "682 30483.335585425786\n",
      "683 30480.963097352767\n",
      "684 30478.45090174138\n",
      "685 30476.085499022734\n",
      "686 30473.57979087893\n",
      "687 30471.22076963445\n",
      "688 30468.720910611857\n",
      "689 30466.36773832417\n",
      "690 30463.873247392898\n",
      "691 30461.525521664207\n",
      "692 30459.03602919395\n",
      "693 30456.693416393966\n",
      "694 30454.20859737027\n",
      "695 30451.87076341934\n",
      "696 30449.390263550624\n",
      "697 30447.056813410716\n",
      "698 30444.580188481094\n",
      "699 30442.250631755145\n",
      "700 30439.777321302303\n",
      "701 30437.451077610316\n",
      "702 30434.980427738312\n",
      "703 30432.656877739093\n",
      "704 30430.18821626409\n",
      "705 30427.86679398519\n",
      "706 30425.399547514273\n",
      "707 30423.07985924337\n",
      "708 30420.613689873546\n",
      "709 30418.29563652428\n",
      "710 30415.83056861077\n",
      "711 30413.51444434039\n",
      "712 30411.05095114659\n",
      "713 30408.73749280341\n",
      "714 30406.276519487325\n",
      "715 30403.9668899456\n",
      "716 30401.509802349236\n",
      "717 30399.205504815567\n",
      "718 30396.753970011712\n",
      "719 30394.4567064246\n",
      "720 30392.012526330767\n",
      "721 30389.72402584759\n",
      "722 30387.288954586536\n",
      "723 30385.010803313817\n",
      "724 30382.58637960591\n",
      "725 30380.319878894246\n",
      "726 30377.907297539714\n",
      "727 30375.653368234518\n",
      "728 30373.253403794683\n",
      "729 30371.012543012635\n",
      "730 30368.62552883317\n",
      "731 30366.397818251397\n",
      "732 30364.023677606478\n",
      "733 30361.808838729652\n",
      "734 30359.447161827353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735 30357.244652261128\n",
      "736 30354.894811226848\n",
      "737 30352.703954128636\n",
      "738 30350.365246156536\n",
      "739 30348.185381711926\n",
      "740 30345.85718783609\n",
      "741 30343.687831628897\n",
      "742 30341.36977562575\n",
      "743 30339.210765880205\n",
      "744 30336.902855772052\n",
      "745 30334.75447108721\n",
      "746 30332.457205598657\n",
      "747 30330.320237405576\n",
      "748 30328.03466212745\n",
      "749 30325.910431144413\n",
      "750 30323.638134153705\n",
      "751 30321.528446655167\n",
      "752 30319.271490541305\n",
      "753 30317.178537550968\n",
      "754 30314.939333468727\n",
      "755 30312.865543541782\n",
      "756 30310.64668177397\n",
      "757 30308.594544968702\n",
      "758 30306.398603562884\n",
      "759 30304.370488812106\n",
      "760 30302.199844994204\n",
      "761 30300.197833573293\n",
      "762 30298.054500761467\n",
      "763 30296.08025445671\n",
      "764 30293.96576167366\n",
      "765 30292.020437118234\n",
      "766 30289.93575973895\n",
      "767 30288.01997272687\n",
      "768 30285.965516054384\n",
      "769 30284.079353376743\n",
      "770 30282.054984654114\n",
      "771 30280.198056689635\n",
      "772 30278.203176983185\n",
      "773 30276.37470160652\n",
      "774 30274.40834638225\n",
      "775 30272.607253330312\n",
      "776 30270.66820943167\n",
      "777 30268.893253925595\n",
      "778 30266.980180946866\n",
      "779 30265.23005599672\n",
      "780 30263.341601370346\n",
      "781 30261.615039535052\n",
      "782 30259.749938478\n",
      "783 30258.045795576523\n",
      "784 30256.202948960432\n",
      "785 30254.520264128325\n",
      "786 30252.698789320646\n",
      "787 30251.036817929824\n",
      "788 30249.236069958664\n",
      "789 30247.594288438195\n",
      "790 30245.813851672036\n",
      "791 30244.191936169977\n",
      "792 30242.431589911885\n",
      "793 30240.829373640976\n",
      "794 30239.089038010396\n",
      "795 30237.506454376675\n",
      "796 30235.7861248498\n",
      "797 30234.223144516985\n",
      "798 30232.522824045147\n",
      "799 30230.97939379768\n",
      "800 30229.299030545186\n",
      "801 30227.775020469147\n",
      "802 30226.114457330757\n",
      "803 30224.609620920935\n",
      "804 30222.968560646375\n",
      "805 30221.48251039079\n",
      "806 30219.86049783938\n",
      "807 30218.392696915886\n",
      "808 30216.78911804335\n",
      "809 30215.33888720598\n",
      "810 30213.75298307842\n",
      "811 30212.31952078793\n",
      "812 30210.75041429475\n",
      "813 30209.332827712482\n",
      "814 30207.77956055427\n",
      "815 30206.37690500263\n",
      "816 30204.838482604748\n",
      "817 30203.449807148005\n",
      "818 30201.925249008134\n",
      "819 30200.549645567495\n",
      "820 30199.038037969978\n",
      "821 30197.67469071826\n",
      "822 30196.17523783303\n",
      "823 30194.82346870605\n",
      "824 30193.335537198232\n",
      "825 30191.994842653803\n",
      "826 30190.517994549835\n",
      "827 30189.188068707357\n",
      "828 30187.72207780426\n",
      "829 30186.402818150033\n",
      "830 30184.947666894346\n",
      "831 30183.639160851668\n",
      "832 30182.195017191894\n",
      "833 30180.89751065072\n",
      "834 30179.46468737977\n",
      "835 30178.178539088083\n",
      "836 30176.75744092645\n",
      "837 30175.48306874444\n",
      "838 30174.07413418871\n",
      "839 30172.811960117222\n",
      "840 30171.415605501683\n",
      "841 30170.16600603731\n",
      "842 30168.782578343467\n",
      "843 30167.5458453592\n",
      "844 30166.175588405385\n",
      "845 30164.951903824902\n",
      "846 30163.594940072973\n",
      "847 30162.384365481343\n",
      "848 30161.040693295537\n",
      "849 30159.84317360149\n",
      "850 30158.512677785846\n",
      "851 30157.328056370847\n",
      "852 30156.01052847135\n",
      "853 30154.83857014932\n",
      "854 30153.53373444928\n",
      "855 30152.37415216086\n",
      "856 30151.08169344903\n",
      "857 30149.93417489653\n",
      "858 30148.653764697083\n",
      "859 30147.517995879716\n",
      "860 30146.2493146109\n",
      "861 30145.124998149113\n",
      "862 30143.867751415637\n",
      "863 30142.75461843446\n",
      "864 30141.50854633521\n",
      "865 30140.406361448422\n",
      "866 30139.17124050259\n",
      "867 30138.07980016931\n",
      "868 30136.855438332313\n",
      "869 30135.774563616345\n",
      "870 30134.560789833144\n",
      "871 30133.49031534281\n",
      "872 30132.286965966065\n",
      "873 30131.226727323305\n",
      "874 30130.033632229486\n",
      "875 30128.98345460274\n",
      "876 30127.800425770904\n",
      "877 30126.760115688452\n",
      "878 30125.58694035277\n",
      "879 30124.55628223949\n",
      "880 30123.392721668224\n",
      "881 30122.371479517235\n",
      "882 30121.217273295057\n",
      "883 30120.205196862178\n",
      "884 30119.060071540225\n",
      "885 30118.056905657926\n",
      "886 30116.920585961314\n",
      "887 30115.926081136895\n",
      "888 30114.79830165122\n",
      "889 30113.812224055106\n",
      "890 30112.692739422157\n",
      "891 30111.714878637435\n",
      "892 30110.603470698705\n",
      "893 30109.633644094367\n",
      "894 30108.530125032263\n",
      "895 30107.56817824255\n",
      "896 30106.472389467406\n",
      "897 30105.51819309768\n",
      "898 30104.430000300978\n",
      "899 30103.483443536832\n",
      "900 30102.402728864636\n",
      "901 30101.463711049833\n",
      "902 30100.39036367041\n",
      "903 30099.458785092495\n",
      "904 30098.39269150581\n",
      "905 30097.46844458166\n",
      "906 30096.40947987031\n",
      "907 30095.492441703005\n",
      "908 30094.440462614726\n",
      "909 30093.530489561253\n",
      "910 30092.485329925294\n",
      "911 30091.582254450946\n",
      "912 30090.543723038005\n",
      "913 30089.647352797554\n",
      "914 30088.615233402365\n",
      "915 30087.725352227408\n",
      "916 30086.699405521154\n",
      "917 30085.81577583226\n",
      "918 30084.795742420887\n",
      "919 30083.918108529608\n",
      "920 30082.903712656276\n",
      "921 30082.03180446304\n",
      "922 30081.02275788472\n",
      "923 30080.156294586424\n",
      "924 30079.15230029483\n",
      "925 30078.29099385621\n",
      "926 30077.291749463126\n",
      "927 30076.43530773957\n",
      "928 30075.44050847127\n",
      "929 30074.58863797735\n",
      "930 30073.597979310194\n",
      "931 30072.7503876926\n",
      "932 30071.763567709575\n",
      "933 30070.91996601265\n",
      "934 30069.936687577618\n",
      "935 30069.096792392535\n",
      "936 30068.116765235376\n",
      "937 30067.280300808914\n",
      "938 30066.30324359644\n",
      "939 30065.469943948643\n",
      "940 30064.49558638562\n",
      "941 30063.66519745174\n",
      "942 30062.693282414453\n",
      "943 30061.865564183354\n",
      "944 30060.895849839373\n",
      "945 30060.07057841448\n",
      "946 30059.102840231313\n",
      "947 30058.279809696087\n",
      "948 30057.313842196138\n",
      "949 30056.492866128887\n",
      "950 30055.52848421482\n",
      "951 30054.70939667726\n",
      "952 30053.74643633703\n",
      "953 30052.92909215761\n",
      "954 30051.96741036467\n",
      "955 30051.151684558776\n",
      "956 30050.19115821467\n",
      "957 30049.37694442748\n",
      "958 30048.41746825038\n",
      "959 30047.604676174662\n",
      "960 30046.646159518932\n",
      "961 30045.834711326184\n",
      "962 30044.877074016513\n",
      "963 30044.0668999362\n",
      "964 30043.11006730645\n",
      "965 30042.301100584707\n",
      "966 30041.34499801069\n",
      "967 30040.537169560495\n",
      "968 30039.581716853838\n",
      "969 30038.77494996362\n",
      "970 30037.82005603626\n",
      "971 30037.014261521537\n",
      "972 30036.059819730202\n",
      "973 30035.25489188975\n",
      "974 30034.30077642668\n",
      "975 30033.496590106894\n",
      "976 30032.542653720247\n",
      "977 30031.739062701872\n",
      "978 30030.78513592096\n",
      "979 30029.981972739548\n",
      "980 30029.027864657422\n",
      "981 30028.22494185993\n",
      "982 30027.2704424096\n",
      "983 30026.467555152718\n",
      "984 30025.51243871248\n",
      "985 30024.709368531487\n",
      "986 30023.75339862673\n",
      "987 30022.949918154496\n",
      "988 30021.992852988085\n",
      "989 30021.188731385002\n",
      "990 30020.230329928287\n",
      "991 30019.4253387923\n",
      "992 30018.465367194694\n",
      "993 30017.65928674547\n",
      "994 30016.697524861247\n",
      "995 30015.89015022792\n",
      "996 30014.92639810146\n",
      "997 30014.11754557136\n",
      "998 30013.15162975237\n",
      "999 30012.341142854522\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.RMSprop()\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.minimize(foo, [W_hidden, b_hidden, W_final, b_final])\n",
    "    \n",
    "    currloss = foo().numpy()\n",
    "    print( i, currloss )\n",
    "    trace.append(currloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f853437e550>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(X, W_hidden, b_hidden, W_final, b_final).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931322276718463"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y.flatten(), yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = tf.math.sigmoid( X @ W_hidden + b_hidden ).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only columns with index 3 and 4 have any variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_1 = (activations[:,3]>0.5)\n",
    "neuron_2 = (activations[:,4]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_dist                    7.123171\n",
       "avg_rating_by_driver        4.806855\n",
       "avg_rating_of_driver        4.616371\n",
       "avg_surge                   1.111759\n",
       "surge_pct                  12.857591\n",
       "trips_in_first_30_days      1.012661\n",
       "luxury_car_user             0.310924\n",
       "weekday_pct                92.431417\n",
       "signup_dow                  2.889453\n",
       "age                       164.737941\n",
       "pass_rating_blank           0.006644\n",
       "driver_rating_blank         0.252276\n",
       "dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[neuron_1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_dist                    6.116270\n",
       "avg_rating_by_driver        4.771641\n",
       "avg_rating_of_driver        4.593224\n",
       "avg_surge                   1.000000\n",
       "surge_pct                   0.000000\n",
       "trips_in_first_30_days      0.907870\n",
       "luxury_car_user             0.247033\n",
       "weekday_pct                 0.000625\n",
       "signup_dow                  4.210493\n",
       "age                       165.807152\n",
       "pass_rating_blank           0.007964\n",
       "driver_rating_blank         0.331355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[neuron_2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_dist                   1.006901\n",
       "avg_rating_by_driver       0.035214\n",
       "avg_rating_of_driver       0.023147\n",
       "avg_surge                  0.111759\n",
       "surge_pct                 12.857591\n",
       "trips_in_first_30_days     0.104791\n",
       "luxury_car_user            0.063891\n",
       "weekday_pct               92.430792\n",
       "signup_dow                -1.321041\n",
       "age                       -1.069211\n",
       "pass_rating_blank         -0.001319\n",
       "driver_rating_blank       -0.079080\n",
       "dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[neuron_1].mean() - df[neuron_2].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron 1 activates when a passenger takes surge ride and rides on the weekdays. Neuron 2 activates when the passenger rides less and only on the weekends. For about half of passengers, neither neuron activates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 674.7974\n",
      "Epoch 2/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 659.0413\n",
      "Epoch 3/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 654.5189\n",
      "Epoch 4/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 652.6453\n",
      "Epoch 5/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 651.3531\n",
      "Epoch 6/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 649.8944\n",
      "Epoch 7/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 643.0210\n",
      "Epoch 8/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 627.2972\n",
      "Epoch 9/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 622.4288\n",
      "Epoch 10/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 618.7809\n",
      "Epoch 11/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 616.0574\n",
      "Epoch 12/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 613.7610\n",
      "Epoch 13/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 611.7053\n",
      "Epoch 14/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 609.8502\n",
      "Epoch 15/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 608.3182\n",
      "Epoch 16/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 606.7781\n",
      "Epoch 17/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 605.6514\n",
      "Epoch 18/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 604.4179\n",
      "Epoch 19/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 603.5707\n",
      "Epoch 20/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 602.4467\n",
      "Epoch 21/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 601.6111\n",
      "Epoch 22/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 601.0336\n",
      "Epoch 23/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 600.2611\n",
      "Epoch 24/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 599.5577\n",
      "Epoch 25/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 598.8873\n",
      "Epoch 26/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 598.3942\n",
      "Epoch 27/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 598.0503\n",
      "Epoch 28/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 597.3762\n",
      "Epoch 29/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 596.4869\n",
      "Epoch 30/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 593.3747\n",
      "Epoch 31/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 592.8438\n",
      "Epoch 32/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 592.3418\n",
      "Epoch 33/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 591.8260\n",
      "Epoch 34/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 591.3654\n",
      "Epoch 35/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 590.8682\n",
      "Epoch 36/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 590.5403\n",
      "Epoch 37/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 589.9257\n",
      "Epoch 38/1000\n",
      "50000/50000 [==============================] - 0s 7us/sample - loss: 589.5626\n",
      "Epoch 39/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 589.0711\n",
      "Epoch 40/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 589.2565\n",
      "Epoch 41/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 588.7725\n",
      "Epoch 42/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 588.3213\n",
      "Epoch 43/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 588.0071\n",
      "Epoch 44/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 587.9594\n",
      "Epoch 45/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 587.7704\n",
      "Epoch 46/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 587.4570\n",
      "Epoch 47/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 587.2414\n",
      "Epoch 48/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 586.9243\n",
      "Epoch 49/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 586.8587\n",
      "Epoch 50/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 586.7066\n",
      "Epoch 51/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 586.6152\n",
      "Epoch 52/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 586.4391\n",
      "Epoch 53/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 586.2368\n",
      "Epoch 54/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 586.3136\n",
      "Epoch 55/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 586.0765\n",
      "Epoch 56/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 585.7361\n",
      "Epoch 57/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 585.6576\n",
      "Epoch 58/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 585.3704\n",
      "Epoch 59/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 585.4009\n",
      "Epoch 60/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 585.4544\n",
      "Epoch 61/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 585.2563\n",
      "Epoch 62/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 585.3088\n",
      "Epoch 63/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 585.2064\n",
      "Epoch 64/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 584.9693\n",
      "Epoch 65/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 584.7392\n",
      "Epoch 66/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 584.5368\n",
      "Epoch 67/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 584.4495\n",
      "Epoch 68/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 584.4702\n",
      "Epoch 69/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 584.3242\n",
      "Epoch 70/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 584.1166\n",
      "Epoch 71/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 583.7071\n",
      "Epoch 72/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 583.6393\n",
      "Epoch 73/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 583.5307\n",
      "Epoch 74/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 582.8296\n",
      "Epoch 75/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 582.4335\n",
      "Epoch 76/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 581.7865\n",
      "Epoch 77/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 580.5422\n",
      "Epoch 78/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 578.2096\n",
      "Epoch 79/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 575.3143\n",
      "Epoch 80/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 573.7180\n",
      "Epoch 81/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 573.3819\n",
      "Epoch 82/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 572.9080\n",
      "Epoch 83/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 572.8181\n",
      "Epoch 84/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 572.5223\n",
      "Epoch 85/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 572.2530\n",
      "Epoch 86/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 572.1680\n",
      "Epoch 87/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 571.8541\n",
      "Epoch 88/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 571.7392\n",
      "Epoch 89/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 571.6430\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 0s 4us/sample - loss: 571.7342\n",
      "Epoch 91/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 571.2355\n",
      "Epoch 92/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 571.0434\n",
      "Epoch 93/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 570.8764\n",
      "Epoch 94/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 571.0284\n",
      "Epoch 95/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 570.7388\n",
      "Epoch 96/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 570.5876\n",
      "Epoch 97/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 570.0987\n",
      "Epoch 98/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 569.9246\n",
      "Epoch 99/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 569.8619\n",
      "Epoch 100/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 569.9987\n",
      "Epoch 101/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 569.7953\n",
      "Epoch 102/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 569.2685\n",
      "Epoch 103/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 569.4760\n",
      "Epoch 104/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 569.4416\n",
      "Epoch 105/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 568.7683\n",
      "Epoch 106/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 568.7270\n",
      "Epoch 107/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 568.4760\n",
      "Epoch 108/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 568.2212\n",
      "Epoch 109/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 568.2078\n",
      "Epoch 110/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 567.7763\n",
      "Epoch 111/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 567.9652\n",
      "Epoch 112/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 567.3708\n",
      "Epoch 113/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 567.6326\n",
      "Epoch 114/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 567.2325\n",
      "Epoch 115/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 567.1361\n",
      "Epoch 116/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 566.7072\n",
      "Epoch 117/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 566.5793\n",
      "Epoch 118/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 566.7676\n",
      "Epoch 119/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 565.8229\n",
      "Epoch 120/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 566.5118\n",
      "Epoch 121/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 565.7838\n",
      "Epoch 122/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 565.6984\n",
      "Epoch 123/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 565.5746\n",
      "Epoch 124/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 564.9406\n",
      "Epoch 125/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 564.5631\n",
      "Epoch 126/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 564.3115\n",
      "Epoch 127/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 563.7949\n",
      "Epoch 128/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 563.3828\n",
      "Epoch 129/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 563.6664\n",
      "Epoch 130/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 563.2868\n",
      "Epoch 131/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 562.5817\n",
      "Epoch 132/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 562.1779\n",
      "Epoch 133/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 562.0269\n",
      "Epoch 134/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 561.9286\n",
      "Epoch 135/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 562.3693\n",
      "Epoch 136/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 561.4632\n",
      "Epoch 137/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 561.2121\n",
      "Epoch 138/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 561.0260\n",
      "Epoch 139/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 561.0222\n",
      "Epoch 140/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 560.6726\n",
      "Epoch 141/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 560.8047\n",
      "Epoch 142/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 560.6684\n",
      "Epoch 143/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 560.7311\n",
      "Epoch 144/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 560.6857\n",
      "Epoch 145/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 560.1400\n",
      "Epoch 146/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 560.7038\n",
      "Epoch 147/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 560.2113\n",
      "Epoch 148/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 559.9803\n",
      "Epoch 149/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 560.0169\n",
      "Epoch 150/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.5169\n",
      "Epoch 151/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.6343\n",
      "Epoch 152/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 560.0544\n",
      "Epoch 153/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 559.7289\n",
      "Epoch 154/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.4071\n",
      "Epoch 155/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.4221\n",
      "Epoch 156/1000\n",
      "50000/50000 [==============================] - 0s 7us/sample - loss: 559.3750\n",
      "Epoch 157/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.3638\n",
      "Epoch 158/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.4609\n",
      "Epoch 159/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 559.0683\n",
      "Epoch 160/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 559.3546\n",
      "Epoch 161/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 559.7733\n",
      "Epoch 162/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 559.2138\n",
      "Epoch 163/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 559.1228\n",
      "Epoch 164/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.8532\n",
      "Epoch 165/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 558.7904\n",
      "Epoch 166/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.9293\n",
      "Epoch 167/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.9660\n",
      "Epoch 168/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.6845\n",
      "Epoch 169/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5878\n",
      "Epoch 170/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.9839\n",
      "Epoch 171/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.7590\n",
      "Epoch 172/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.5349\n",
      "Epoch 173/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5808\n",
      "Epoch 174/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.7376\n",
      "Epoch 175/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5407\n",
      "Epoch 176/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.7603\n",
      "Epoch 177/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5216\n",
      "Epoch 178/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.4937\n",
      "Epoch 179/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.7276\n",
      "Epoch 180/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5943\n",
      "Epoch 181/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5137\n",
      "Epoch 182/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.6946\n",
      "Epoch 183/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5688\n",
      "Epoch 184/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.3077\n",
      "Epoch 185/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.4084\n",
      "Epoch 186/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.3411\n",
      "Epoch 187/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.4471\n",
      "Epoch 188/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5847\n",
      "Epoch 189/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.2532\n",
      "Epoch 190/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.1525\n",
      "Epoch 191/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.5513\n",
      "Epoch 192/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.3749\n",
      "Epoch 193/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.4486\n",
      "Epoch 194/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 559.0208\n",
      "Epoch 195/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 557.9988\n",
      "Epoch 196/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 557.9464\n",
      "Epoch 197/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.3502\n",
      "Epoch 198/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.1296\n",
      "Epoch 199/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 558.0104\n",
      "Epoch 200/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.9653\n",
      "Epoch 201/1000\n",
      "50000/50000 [==============================] - 0s 7us/sample - loss: 557.9189\n",
      "Epoch 202/1000\n",
      "50000/50000 [==============================] - 0s 3us/sample - loss: 557.6450\n",
      "Epoch 203/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 558.2217\n",
      "Epoch 204/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.9627\n",
      "Epoch 205/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.8014\n",
      "Epoch 206/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.8272\n",
      "Epoch 207/1000\n",
      "50000/50000 [==============================] - 0s 7us/sample - loss: 557.5891\n",
      "Epoch 208/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 558.0553\n",
      "Epoch 209/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.8268\n",
      "Epoch 210/1000\n",
      "50000/50000 [==============================] - 0s 7us/sample - loss: 557.8770\n",
      "Epoch 211/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.9730\n",
      "Epoch 212/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 558.1840\n",
      "Epoch 213/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.8582\n",
      "Epoch 214/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.7877\n",
      "Epoch 215/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.9670\n",
      "Epoch 216/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.4406\n",
      "Epoch 217/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.6802\n",
      "Epoch 218/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.8305\n",
      "Epoch 219/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.5877\n",
      "Epoch 220/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.7887\n",
      "Epoch 221/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.7321\n",
      "Epoch 222/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.7143 0s - loss: 558.\n",
      "Epoch 223/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.2681\n",
      "Epoch 224/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.4966\n",
      "Epoch 225/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.6113\n",
      "Epoch 226/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.5147\n",
      "Epoch 227/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.3086\n",
      "Epoch 228/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.5893\n",
      "Epoch 229/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.4253\n",
      "Epoch 230/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.7275\n",
      "Epoch 231/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.6806\n",
      "Epoch 232/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.2294\n",
      "Epoch 233/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.5850\n",
      "Epoch 234/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.1617\n",
      "Epoch 235/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.6915\n",
      "Epoch 236/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.1799\n",
      "Epoch 237/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.4070\n",
      "Epoch 238/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.5094\n",
      "Epoch 239/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.3873\n",
      "Epoch 240/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 558.1133\n",
      "Epoch 241/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 558.1823\n",
      "Epoch 242/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.1630\n",
      "Epoch 243/1000\n",
      "50000/50000 [==============================] - ETA: 0s - loss: 558.444 - 0s 4us/sample - loss: 557.2623\n",
      "Epoch 244/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.3038\n",
      "Epoch 245/1000\n",
      "50000/50000 [==============================] - 0s 6us/sample - loss: 557.6289\n",
      "Epoch 246/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.2023\n",
      "Epoch 247/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.0637\n",
      "Epoch 248/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 556.8399\n",
      "Epoch 249/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.5287\n",
      "Epoch 250/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.6123\n",
      "Epoch 251/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.2420\n",
      "Epoch 252/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.1483\n",
      "Epoch 253/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 556.9840\n",
      "Epoch 254/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.0103\n",
      "Epoch 255/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.1821\n",
      "Epoch 256/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.2193\n",
      "Epoch 257/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.0802\n",
      "Epoch 258/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.1390\n",
      "Epoch 259/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.0990\n",
      "Epoch 260/1000\n",
      "50000/50000 [==============================] - 0s 5us/sample - loss: 557.8601\n",
      "Epoch 261/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.0356\n",
      "Epoch 262/1000\n",
      "50000/50000 [==============================] - 0s 4us/sample - loss: 557.0492\n",
      "Epoch 263/1000\n",
      "21504/50000 [===========>..................] - ETA: 0s - loss: 555.4697"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-50e18b7b9485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m               loss=cross_entropy)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# Callbacks batch_begin.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(6, activation=\"sigmoid\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def cross_entropy(y, yhat):\n",
    "    return tf.reduce_sum(-(y*tf.math.log(yhat)+(1-y)*tf.math.log(1-yhat)))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=cross_entropy)\n",
    "\n",
    "hist = model.fit(X,y, epochs=1000, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = (model.predict(X)>0.5).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6819549934088062"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activations = (tf.sigmoid( X @ W + b )>0.5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_dist                    5.808791\n",
       "avg_rating_by_driver        4.777735\n",
       "avg_rating_of_driver        4.601857\n",
       "avg_surge                   1.070598\n",
       "surge_pct                   8.356494\n",
       "trips_in_first_30_days      2.290342\n",
       "luxury_car_user             0.377933\n",
       "weekday_pct                61.130299\n",
       "signup_dow                  3.303955\n",
       "age                       165.482375\n",
       "pass_rating_blank           0.003901\n",
       "driver_rating_blank         0.160785\n",
       "dtype: float64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ hidden_activations.T[0] ].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and so forth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
