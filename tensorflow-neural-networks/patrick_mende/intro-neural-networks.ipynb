{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow and Neural Networks\n",
    "\n",
    "## Introductory Neural Netwoks\n",
    "\n",
    "### Patrick Mende\n",
    "#### (Modified notebook from Jack Bennetto)\n",
    "\n",
    "#### August 16, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Use TensorFlow to do calculations.\n",
    "* Know the advantages and disadvantages of neural networks.\n",
    "* Explain the basic neural-network algorithms.\n",
    "* Build a simple neural network in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Neural Networks?\n",
    "\n",
    "Neural Networks were originally developed in the 1950's using the neurons in the brain as inspiration.\n",
    "\n",
    "In the brain, we have neurons connected together by dendrons of different strengths, and these strength change as we learn. The connections are non-linear; a neural doesn't activate at all until it has sufficient input.\n",
    "\n",
    "But don't get too caught up in the analogy. Brains are an inspiration, not a model, and trying to fit to closely to them hasn't always helped researchers.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "I'm using the term 'neural network' throughout the lecture. The term 'deep learning' has become popular as well in recent years, in part to move away from the idea that these are based on the brain, and in part to emphasize the depth that's become possible. Sometimes people talk about 'artificial neural networks' to distinguish them from the biological ones. And a couple decades ago the terms 'connectionism' and 'parallel distributed processing' (https://mitpress.mit.edu/books/parallel-distributed-processing) were popular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Neural Networks?\n",
    "\n",
    "In general, neural networks perform well with high-dimensional data such as images, audio, and text.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    " * Hard to design/tune\n",
    " * Slow to train\n",
    " * Uninterpretable\n",
    " * Easy to overfit (need a lot of data)\n",
    " \n",
    "Advantange\n",
    "\n",
    " * Works well with high-dimensional data\n",
    " * Can find almost anything, when designed correctly\n",
    " * Online training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Stage 1: ('40s-60s) Early understanding of the brain and development of computers ('40s-60s)\n",
    "\n",
    "Stage 2: ('80s-'90s) Understanding of backpropagation, recognition that neural networks could be used for associative memory, first recurrent neural networks\n",
    "\n",
    "Stage 3: (2006-) Growth of GPUs, better algorithms for training deep networks, more designs, more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Let's start with a really simple neural network. We have 4 input features and 4 outputs. We can draw it like this:\n",
    "\n",
    "![Base neural network with no hidden layer](./img/base_nn_no_hidden.png)\n",
    "\n",
    "In principle, the arrows will carry along with them some sort of \"weight\" of that connection, and each node will be some function of all the incoming arrows/weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "#### Logisitic/sigmoid function\n",
    "\n",
    "When doing classification, the \"traditional\" function for each node is the logistic function:\n",
    "\n",
    "\\\\[\n",
    "\\LARGE y_i = f(\\vec{x}; W_{i,:}, b) = \\frac{1}{1+e^{-(b+W_{i,:}\\vec{x})}}\n",
    "\\\\]\n",
    "\n",
    "Here, \\\\(W_{i,:}\\\\) is the \\\\(i\\\\)th row of a matrix, \\\\(W\\\\), that contains all of the weights of each individual arrow pointing to \\\\(y_i\\\\). \\\\(b\\\\) is what is often called a \"bias node\". You can think of it as an intercept term.\n",
    "\n",
    "The logistic function saw wide use early in the history of neural networks because it has what people considered \"nice\" properties. Specifically that it:\n",
    "\n",
    "* Behaved like a real neuron\n",
    "* Was differentiable everywhere\n",
    "\n",
    "It turns out neither of these properties are necessary for good model performance.\n",
    "\n",
    "#### ReLU\n",
    "\n",
    "In more modern implementations of neural networks, the most widely used activation functions are some version of what is known as a *rectified linear unit* (ReLU). The simplest version of a ReLU looks like:\n",
    "\n",
    "\\\\[\n",
    "\\LARGE y_i = max(0, b + W \\vec{x})\n",
    "\\\\]\n",
    "\n",
    "Another commonly used flavor is ReLU6:\n",
    "\n",
    "\\\\[\n",
    "\\LARGE y_i = min(max(0, b + W \\vec{x}), 6)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we adjust \\\\(W\\\\) and \\\\(b\\\\), fitting them to the training data using stocastic gradiant descent or some other algorithm. If we're using TensorFlow, we use an optimizer as we did this morning.\n",
    "\n",
    "Nodes are sometimes called a **perceptrons** or **neurons**. We're just going to call them nodes. In general, the value of each node (other than the bias node) is some function of a linear combination of other nodes in the previous layer.\n",
    "\n",
    "Nodes are grouped into **layers**. For a **fully connected** layer, every node in the layer is connected to every node in the previous one. We'll focus on fully connected networks today, but there are many other architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hidden layers\n",
    "\n",
    "The power of neural networks comes when we add additional layers. The ofllowing is the same as the neural network above, except we've added a *single* hidden layer:\n",
    "\n",
    "![Fully connected NN w/ 1 hidden layer](./img/base_neural_network.png)\n",
    "\n",
    "The initial layer is the **input layer**; the last is the **output layer**. Any intermediate layers are called **hidden layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The need for a non-linear activation function with hidden layers\n",
    "\n",
    "Question: what happens if we don't have an activation function? That is, what if \\\\((h_1, h_2, \\dots, h_7)\\\\) are all simple linear combinations of our inputs, and our outputs are simple linear combinations of each \\\\(h\\\\)?\n",
    "\n",
    "\\\\[\n",
    "\\vec h =\n",
    "\\begin{bmatrix}\n",
    "h_0 \\\\ h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4 \\\\ h_5 \\\\ h_6 \\\\ h_7\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1       & 0       & 0       & 0       & 0       \\\\\n",
    "w_{1,0} & w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \\\\\n",
    "w_{2,0} & w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4} \\\\\n",
    "w_{3,0} & w_{3,1} & w_{3,2} & w_{3,3} & w_{3,4} \\\\\n",
    "w_{4,0} & w_{4,1} & w_{4,2} & w_{4,3} & w_{4,4} \\\\\n",
    "w_{5,0} & w_{5,1} & w_{5,2} & w_{5,3} & w_{5,4} \\\\\n",
    "w_{6,0} & w_{6,1} & w_{6,2} & w_{6,3} & w_{6,4} \\\\\n",
    "w_{7,0} & w_{7,1} & w_{7,2} & w_{7,3} & w_{7,4} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "\\end{bmatrix}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "How about \\\\(\\vec{y}\\\\)?\n",
    "\\\\[\n",
    "\\begin{aligned}\n",
    "\\vec y =\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "& \\begin{bmatrix}\n",
    "v_{1,0} & v_{1,1} & v_{1,2} & v_{1,3} & v_{1,4} & v_{1,5} & v_{1,6} & v_{1,7} \\\\\n",
    "v_{2,0} & v_{2,1} & v_{2,2} & v_{2,3} & v_{2,4} & v_{2,5} & v_{2,6} & v_{2,7} \\\\\n",
    "v_{3,0} & v_{3,1} & v_{3,2} & v_{3,3} & v_{3,4} & v_{3,5} & v_{3,6} & v_{3,7} \\\\\n",
    "v_{4,0} & v_{4,1} & v_{4,2} & v_{4,3} & v_{4,4} & v_{4,5} & v_{4,6} & v_{4,7}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h_0 \\\\ h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4 \\\\ h_5 \\\\ h_6 \\\\ h_7\n",
    "\\end{bmatrix} \\\\\n",
    "=\n",
    "& \\underbrace{\n",
    "\\begin{bmatrix}\n",
    "v_{1,0} & v_{1,1} & v_{1,2} & v_{1,3} & v_{1,4} & v_{1,5} & v_{1,6} & v_{1,7} \\\\\n",
    "v_{2,0} & v_{2,1} & v_{2,2} & v_{2,3} & v_{2,4} & v_{2,5} & v_{2,6} & v_{2,7} \\\\\n",
    "v_{3,0} & v_{3,1} & v_{3,2} & v_{3,3} & v_{3,4} & v_{3,5} & v_{3,6} & v_{3,7} \\\\\n",
    "v_{4,0} & v_{4,1} & v_{4,2} & v_{4,3} & v_{4,4} & v_{4,5} & v_{4,6} & v_{4,7}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1       & 0       & 0       & 0       & 0       \\\\\n",
    "w_{1,0} & w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \\\\\n",
    "w_{2,0} & w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4} \\\\\n",
    "w_{3,0} & w_{3,1} & w_{3,2} & w_{3,3} & w_{3,4} \\\\\n",
    "w_{4,0} & w_{4,1} & w_{4,2} & w_{4,3} & w_{4,4} \\\\\n",
    "w_{5,0} & w_{5,1} & w_{5,2} & w_{5,3} & w_{5,4} \\\\\n",
    "w_{6,0} & w_{6,1} & w_{6,2} & w_{6,3} & w_{6,4} \\\\\n",
    "w_{7,0} & w_{7,1} & w_{7,2} & w_{7,3} & w_{7,4} \n",
    "\\end{bmatrix}\n",
    "}_{\\text{What is this?}}\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Calculating the output based on the inputs is often called *forward propagation*, in which the signal is propagated from one layer to the next. To update the weights in the model, we use an algorithm called *backpropagation*. In this, we compare the predicted output with the expected value for a set of inputs to find the output error, and propagate the error backwards, from one layer to the previous one, based on the gradient of the intervening functions and weights. Once we have the error at each node we can use gradient descent or some related algorithm to adjust the weights at that node.\n",
    "\n",
    "We aren't going to do that manually, but if you're interested, you can check out the [Wikipedia entry](https://en.wikipedia.org/wiki/Backpropagation) for backpropagation. For a fuller, more formal treatment, read the subsection on backpropagation in Chapter 11 (Neural Networks) of Elements of Statistical learning.\n",
    "\n",
    "TensorFlow (or any other NN framework) takes care of performing backpropagation automatically. That said, it's an important algorithm that was critical to the development of NN and at some point it's worth studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our first neural network\n",
    "\n",
    "Although neural networks are typically used for classification, they can be used to fit other problems. Let's try to fit a sinusoidal curve to some noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "c_map = plt.get_cmap('Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npts = 10000\n",
    "x_train = scs.uniform(-5, 10).rvs(npts)\n",
    "y_train = np.sin(x_train) + scs.norm(0, 0.3).rvs(npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "ax.scatter(x_train, y_train, s=10, alpha=0.35, color=c_map(0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create placeholders where we train the data. The first dimension of each is `None` because we'll train it with a bunch of points at once. The second dimension is `1` because there's x and y are each one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a network with one hidden layer with 5 units. `W_0` and `W_1` are the strengths of the connections between the input and hidden layer, and the hidden layer and the output. We initialize these randomly.\n",
    "\n",
    "The biases `b_0` and `b_1` are the intercept terms. We typically start them wiht some non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 5\n",
    "\n",
    "W_0 = tf.Variable(tf.truncated_normal([1, n_hidden]), name='W_0')\n",
    "b_0 = tf.Variable(tf.constant(0.1, shape=[n_hidden]), name='b_0')\n",
    "h_0 = tf.matmul(x, W_0) + b_0\n",
    "W_1 = tf.Variable(tf.truncated_normal([n_hidden, 1]), name='W_1')\n",
    "b_1 = tf.Variable(tf.constant(0.1, shape=[1]), name='b_1')\n",
    "\n",
    "yhat = tf.matmul(h_0, W_1) + b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify a loss function. We'll use the residual sum of squares. We could have used cross entropy instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(yhat - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an optimizer function and use that to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the `train` tensor, TensorFlow will update the variables in the graph in the direction of minimizing the `loss`.\n",
    "\n",
    "Before we can do this, we need to create an session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate the global variables initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the train step a bunch of times, feeding in the data each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    sess.run(train, feed_dict={x:np.array(x_train).reshape(-1,1), y:y_train.reshape(-1,1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the results. Note we're only fitting the curve between -5 and 5 as show by the vertical lines. Red is predicted; blue is the actual curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_actual = np.linspace(-7,7,500)\n",
    "y_actual = np.sin(x_actual)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "y_pred = sess.run(yhat, {x:x_actual.reshape(-1,1)})\n",
    "ax.plot(x_actual, y_pred, color=c_map(0), lw=3)\n",
    "ax.plot(x_actual, y_actual, color=c_map(1), lw=1, ls='--')\n",
    "\n",
    "ax.axvline(-5, color='k')\n",
    "ax.axvline(5, color='k')\n",
    "ax.set_xlim((-7,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing non-linearity\n",
    "\n",
    "The problem is that our network is completely linear, so no matter what the weights the output is a linear function of the input. In order to fix this, we need to provide some sort of non-linear function at each of the layers.\n",
    "\n",
    "There are a number of different functions we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts = np.linspace(-8, 8, 200)\n",
    "\n",
    "functions = [tf.nn.sigmoid,\n",
    "             tf.nn.tanh,\n",
    "             tf.nn.softsign,\n",
    "             tf.nn.relu6,\n",
    "             tf.nn.relu,\n",
    "             tf.nn.softplus,\n",
    "             tf.nn.elu]\n",
    "\n",
    "fig,axes = plt.subplots(len(functions), figsize=(12,12))\n",
    "\n",
    "for i, (ax, func) in enumerate(zip(axes, functions)):\n",
    "    ax.plot(xpts, func(xpts).eval(), color=c_map(i), label=func.__name__)\n",
    "    ax.legend(loc='upper left', fontsize=16)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xlim(xpts.min(), xpts.max())\n",
    "\n",
    "fig.suptitle('Various Activation Functions', fontsize=22, y=1.015)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8\n",
    "\n",
    "W_0 = tf.Variable(tf.random_normal([1, n_hidden]), name='W_0')\n",
    "b_0 = tf.Variable(tf.constant(0.1, shape=[n_hidden]), name='b_0')\n",
    "#h_0 = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "h_0 = tf.nn.sigmoid(tf.matmul(x, W_0) + b_0)\n",
    "W_1 = tf.Variable(tf.random_normal([n_hidden, 1]), name='W_1')\n",
    "b_1 = tf.Variable(tf.constant(0.1, shape=[1]), name='b_1')\n",
    "\n",
    "yhat = tf.matmul(h_0, W_1) + b_1\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(yhat - y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train, feed_dict={x:np.array(x_train).reshape(-1,1), y:y_train.reshape(-1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_actual = np.linspace(-7,7,500)\n",
    "y_actual = np.sin(x_actual)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,5))\n",
    "y_pred = sess.run(yhat, {x:x_actual.reshape(-1,1)})\n",
    "ax.plot(x_actual,y_pred, color=c_map(0), lw=3)\n",
    "ax.plot(x_actual,y_actual, color=c_map(1), ls='--', lw=1)\n",
    "\n",
    "ax.axvline(-5, color='k')\n",
    "ax.axvline(5, color='k')\n",
    "ax.set_xlim((-7,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Since neural networks have a large number of parameters, they are fairly easy to overfit (partucularly if there aren't all that many data points). To avoid this most neural networks include some sort of regularization. There are a number of different approaches.\n",
    "\n",
    "One particularly popular approach is dropout. This involves removing connections between nodes - effectively setting entries in our connection matrix equal to zero:\n",
    "\n",
    "![Dropout neural network](./img/dropout_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is the same sort of L1 or L2 regularlization on the connection weights; these have similar effect to a linear model. TensorFlow has special operations to compute L2 loss (`tensorflow.nn.l1_loss`) but it's not hard to compute otherwise. We'll use that in an example tomorrow.\n",
    "\n",
    "Another approach is tying some of the weights together, by imposing a penalty based on the differences between certain parameters. The extreme example of this is *parameter sharing*. For this we require many of the parameters to be the same. We'll explore one example of this tomorrow with *convolutional neural networks* which are used to process images.\n",
    "\n",
    "In CNNs, the pixels of an image are mapped to multiple channels of the same size, with weights connecting the images in the input to the pixels of each of the output channels. However, pixels are only connected to nearby pixels in the output (typically, in a 5x5 region) and the parameters corresponding to the same offset (e.g., (+1, -2)) are force to be identical. This allows translational invariance.\n",
    "\n",
    "Other networks that use parameter sharing are *recurrent neural networks*, which are used for time-series data. Unlike ordinary neural networks which have a single feedforward step, RNNs are executed in an arbitrary number of steps, with output or hidden units feeding back into earlier units in the network, and the input added sequentially at each step. Training these involves unwinding them in time, so parameters are shared across time rather than space.\n",
    "\n",
    "![Recurrent neural network](./img/recurrent_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
