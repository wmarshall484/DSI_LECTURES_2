{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***The Multi-Armed Bandit***\n",
    "##### (Moses Marsh, Jack Bennetto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    " * Explain how multi-armed bandit addresses the trade-off between exploitation and exploration\n",
    " * Implement the multi-armed bandit algorithm.\n",
    " * Measure the regret of a strategy.\n",
    "\n",
    "### Agenda\n",
    "\n",
    " * What is a multi-armed bandit?\n",
    " * How do we use this to do smarter A/B tests?\n",
    " * Common strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "probabilities = np.arange(.58,.2,-.16)\n",
    "random.shuffle(probabilities)\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: treatments for flatulence\n",
    "\n",
    "Suppose you're a doctor who has developed a new treatment for flatulence. You aren't sure it will actually help, so you want to compare it to a control group.\n",
    "\n",
    "How would you do that?\n",
    "\n",
    "What if you were a Bayesian?\n",
    "\n",
    "The problem is that these are real people, suffering a real problem, and you help as many as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probability = {}\n",
    "probability['control'] = probabilities[0]\n",
    "probability['drug'] = probabilities[1]\n",
    "results = dict(control=[], drug=[], total=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_test(treatment):\n",
    "    result = stats.bernoulli(probability[treatment]).rvs()\n",
    "    results[treatment].append(result)\n",
    "    results['total'].append(result)\n",
    "    if result:\n",
    "        print(\"The patient got better!\")\n",
    "    else:\n",
    "        print(\"The patient is still sick :(\")\n",
    "    print(\"        got better   didn't\")\n",
    "    for treatment in ['control', 'drug', 'total']:\n",
    "        print(\"{:10} {:5}  {:5}\".format(treatment, results[treatment].count(1), results[treatment].count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some patients, giving some the drug and some not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient got better!\n",
      "        got better   didn't\n",
      "control        1      0\n",
      "drug           0      0\n",
      "total          1      0\n"
     ]
    }
   ],
   "source": [
    "run_test('control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is still sick :(\n",
      "        got better   didn't\n",
      "control        1      0\n",
      "drug           0      1\n",
      "total          1      1\n"
     ]
    }
   ],
   "source": [
    "run_test('drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of getting better:\n",
      "  control     0.58\n",
      "  drug        0.42\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities of getting better:')\n",
    "for treatment in ['control', 'drug']:\n",
    "    print(\"  {:10} {:5.2f}\".format(treatment, probability[treatment]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With traditional A/B testing there are two phases. First we do a test to determine the best choice, known as **exploration**. Once we're done testing, we use our results; this is **exploitation**. Sometimes that's necessary if the people doing the test are different from the ones using the result, but what if they aren't? Then we can combine the two phases, starting out exploring possible results and gradually concentrating on the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Armed Bandit\n",
    "\n",
    "The multi-armed bandit is a mathematical problem. Suppose we have two or more slot machines, and each slot machine (a.k.a. one-armed bandit) has a different (unknown!) chance of winning. What strategy should we follow to maximize our payoff after a finite number of plays.\n",
    "\n",
    "In this case we're assuming they all have the same payoff (\"binary bandits\") but there are many version of this problem.\n",
    "\n",
    "In reality these might be drugs, or web-site designs, or ad campaigns, or job-search strategies, or dating profiles, or anything where we want to exploit the \"winner\" of our hypothesis testing.\n",
    "\n",
    "To understand this we talk about minimizing **regret**, the expected difference in winnings between our strategy and the optimal one. (What does that mean?)\n",
    "\n",
    "How would you solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common strategies\n",
    "\n",
    "There are a number of common strategies that you'll implement in the assignment. Some are better than others, although a \"best\" strategy would require knowledge of the distribution of the payoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Algorithm\n",
    "\n",
    "The simplest model is a \"greedy\" algorithm, where we always choose the bandit that's been the most successful so far. Since we want to be able to explore at least a little, we might assume that each bandit has already had a single success.\n",
    "\n",
    "What are the limitations of this?\n",
    "\n",
    "\n",
    "### Epsilon-Greedy Algorithm\n",
    "\n",
    "With epsilon-greedy we choose the best algorithm most of the time, but sometimes (with probability $\\epsilon$) we choose one randomly.\n",
    "\n",
    "Again, there isn't a \"best\" value, but $\\epsilon = 0.1$ is typical.\n",
    "\n",
    "What are the limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We choose a bandit randomly in proportion to the softmax function of the payouts, e.g.\n",
    "\n",
    "If there are three bandits, A, B, and C, the probability of choosing A is\n",
    "\n",
    "$$ \\frac{ e^{p_A/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } + \\frac{ e^{p_B/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } + \\frac{ e^{p_C/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } $$\n",
    "where\n",
    "\n",
    "* $p_A$ is the average payoff of bandit A so far (assume 1.0 to start).\n",
    "* $\\tau$ is the \"temperature\" (generally constant).\n",
    "\n",
    "How does this behave in the extremes?\n",
    "\n",
    "\n",
    "* As $\\tau \\to \\infty$, the algorithm will choose bandits equally.\n",
    "* As $\\tau \\to 0$, it will choose the most successful so far.\n",
    "\n",
    "What are the limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1 Algorithm\n",
    "\n",
    "Another approach is to balance the choose bandits based on a combination of expected payoff and uncertainty. The UCB1 algorithm scores each bandit based on the upper confidence bound\n",
    "\n",
    "The UCB1 algorithm choosing the bandit for whom the Upper Confidence Bound is the highest, favoring bandits with a high expected payout, but also those with high uncertainty.\n",
    "\n",
    "Choose a bandit to maximize\n",
    "\n",
    "$$p_A + \\sqrt{\\frac{2 \\ln{N}}{n_A}} $$\n",
    "\n",
    "where\n",
    "\n",
    " * $p_A$ is the expected payout of bandit $A$.\n",
    " * $n_A$ is the number of times bandit $A$ has played.\n",
    " * N is the total number of trials so far.\n",
    "\n",
    "This chooses the bandit for whom the Upper Confidence Bound is the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Bandit\n",
    "\n",
    "Use Bayesian statistics:\n",
    "\n",
    "* Find probability distribution of payout of each bandit thus far. (how?)\n",
    "* For each bandit, sample from distribution.\n",
    "* Choose bandit for whom the sample has highest expected payout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
