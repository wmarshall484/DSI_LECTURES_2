{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***The Multi-Armed Bandit***\n",
    "##### (Moses Marsh, Jack Bennetto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives: answer the following\n",
    "\n",
    " * What are ***exploitation***, ***exploration***, and ***regret*** in this context?\n",
    " * How is this framework related to traditional A/B testing?\n",
    " * Whatâ€™s your favorite strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "probabilities = np.arange(.58,.2,-.16)\n",
    "random.shuffle(probabilities)\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: treatments for tongue psoriasis\n",
    "\n",
    "Suppose you're a doctor who has developed a new treatment for tongue psoriasis. You aren't sure it will actually help, so you want to compare it to a control group.\n",
    "\n",
    "How would you do that?\n",
    "\n",
    "What if you were a Bayesian?\n",
    "\n",
    "The problem is that these are real people, suffering a real problem, and you help as many as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = {}\n",
    "probability['control'] = probabilities[0]\n",
    "probability['drug'] = probabilities[1]\n",
    "results = dict(control=[], drug=[], total=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(treatment):\n",
    "    result = stats.bernoulli(probability[treatment]).rvs()\n",
    "    results[treatment].append(result)\n",
    "    results['total'].append(result)\n",
    "    if result:\n",
    "        print(\"The patient got better!\")\n",
    "    else:\n",
    "        print(\"The patient is still sick :(\")\n",
    "    print(\"        got better   didn't\")\n",
    "    for treatment in ['control', 'drug', 'total']:\n",
    "        print(\"{:10} {:5}  {:5}\".format(treatment, results[treatment].count(1), results[treatment].count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some patients, giving some the drug and some not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is still sick :(\n",
      "        got better   didn't\n",
      "control        0      3\n",
      "drug           1      1\n",
      "total          1      4\n"
     ]
    }
   ],
   "source": [
    "run_test('control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is still sick :(\n",
      "        got better   didn't\n",
      "control        0      3\n",
      "drug           1      2\n",
      "total          1      5\n"
     ]
    }
   ],
   "source": [
    "run_test('drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'control': [0, 0, 0], 'drug': [1, 0, 0], 'total': [0, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental success rates:\n",
      "  control     0.00\n",
      "  drug        0.33\n"
     ]
    }
   ],
   "source": [
    "print('Experimental success rates:')\n",
    "for treatment in ['control', 'drug']:\n",
    "    print(\"  {:10} {:5.2f}\".format(treatment, np.mean(results[treatment])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual probabilities of getting better:\n",
      "  control     0.26\n",
      "  drug        0.42\n"
     ]
    }
   ],
   "source": [
    "print('Actual probabilities of getting better:')\n",
    "for treatment in ['control', 'drug']:\n",
    "    print(\"  {:10} {:5.2f}\".format(treatment, probability[treatment]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With traditional A/B testing there are two phases. First we collect data to determine the best choice, known as **exploration**. Once we're done testing, we make a decision between our options and stick with it; this is **exploitation**. Sometimes it's necessary to keep the testing phase strictly before the deployment phase, but what if there is no such restriction? Then we can combine the two phases, starting out exploring possible results and gradually concentrating on the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Armed Bandit\n",
    "\n",
    "The multi-armed bandit is a mathematical problem. Suppose we have two or more slot machines, and each slot machine (a.k.a. one-armed bandit) has a different (unknown!) chance of winning. What strategy should we follow to maximize our payoff after a finite number of plays?\n",
    "- bandits: $\\{B_i\\}$\n",
    "- bandit payout probabilities: $\\{p_i\\}$\n",
    "\n",
    "In this case we're assuming they all have the same payoff amount (\"binary bandits\"), but different payout probabilities (there are many extensions of this problem, but this is a sufficient starting point).\n",
    "\n",
    "In reality these \"bandits\" might be drugs, or web-site designs, or ad campaigns, or job-search strategies, or dating profiles, or anything where we want to exploit the \"winner\" of our hypothesis testing.\n",
    "\n",
    "- exploration: collecting more data for each bandit to get a better estimate of the true payout probabilities\n",
    "- eploitation: using whichever bandit has performed the best so far\n",
    "\n",
    "Every strategy for optimization will have to balance exploration and exploitation.\n",
    "\n",
    "Each strategy will also have to track the performance of each bandit:\n",
    "- $n_i$: number of visits (or rounds, or pulls) to bandit $B_i$\n",
    "- $w_i$: number of successes at banding $B_i$\n",
    "- $\\hat{p}_i = w_i / n_i $: observed success rate of bandit $B_i$\n",
    "  - if $n_i = 0$, this is undefined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common strategies\n",
    "\n",
    "There are a number of common strategies that you'll implement in the assignment. Some are better than others, although a \"best\" strategy would require knowledge of the distribution of the payoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify a strategy's performance, we run simulations where we know the true payout probabilities and calculate **regret**: the expected difference in winnings between our strategy and the optimal one.\n",
    "- let $p^*$ be the max of $\\{p_1, p_2, p_3, \\ldots, p_k\\}$ \n",
    "- let $p(t)$ be the true success probability of the bandit chosed at round $t$\n",
    "- then our regret after $T$ rounds is\n",
    "$$ r = Tp^* - \\sum_{t=1}^T p(t) $$\n",
    "- and our expected average regret is \n",
    "$$ E[r] = \\lim_{T \\rightarrow \\infty} r / T = p^* - \\frac{1}{T}\\sum_{t=1}^Tp(t)$$\n",
    "\n",
    "We want a strategy that minimizes regret\n",
    "- A ***zero-regret strategy*** is defined as one with $E[r] = 0$\n",
    "- The interesting thing is that a zero-regret strategy does not guarantee that you will never choose a suboptimal outcome, instead it guarantees that as you continue to play you will tend to choose the optimal outcome.\n",
    "- Note again that actually calculating regret requires knowing the true bandit probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Algorithm\n",
    "\n",
    "The simplest model is a \"greedy\" algorithm, where we always choose the bandit that's been the most successful so far. Since we want to be able to explore at least a little, we might assume that each bandit has already had a single success.\n",
    "\n",
    "What are the limitations of this?\n",
    "\n",
    "\n",
    "### Epsilon-Greedy Algorithm\n",
    "\n",
    "With epsilon-greedy we choose the best algorithm most of the time, but sometimes (with probability $\\epsilon$) we choose one randomly.\n",
    "\n",
    "Again, there isn't a \"best\" value, but $\\epsilon = 0.1$ is typical.\n",
    "\n",
    "- ***explore*** with some fixed probability $\\epsilon$\n",
    "  - generate a random number between 0 and 1. If it is less than $\\epsilon$, choose a random bandit\n",
    "- ***exploit*** at all other times: choose the bandit with the highest $\\hat{p}_i$ \n",
    "\n",
    "\n",
    "Is this a zero-regret strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We choose a bandit randomly in proportion to the softmax function of the payouts, e.g.\n",
    "\n",
    "If there are three bandits, A, B, and C, the probability of choosing A is\n",
    "\n",
    "$$ \\frac{ e^{p_A/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } + \\frac{ e^{p_B/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } + \\frac{ e^{p_C/\\tau} }{  e^{p_A /\\tau} +  e^{p_B /\\tau} + e^{p_C /\\tau  } } $$\n",
    "where\n",
    "\n",
    "* $p_A$ is the average payoff of bandit A so far (assume 1.0 to start).\n",
    "* $\\tau$ is the \"temperature\" (generally constant).\n",
    "\n",
    "How does this behave in the extremes?\n",
    "\n",
    "\n",
    "* As $\\tau \\to \\infty$, the algorithm will choose bandits equally.\n",
    "* As $\\tau \\to 0$, it will choose the most successful so far.\n",
    "\n",
    "What are the limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1 Algorithm\n",
    "\n",
    "Another approach is to balance the choose bandits based on a combination of expected payoff and uncertainty. The UCB1 algorithm scores each bandit based on the upper confidence bound\n",
    "\n",
    "The UCB1 algorithm choosing the bandit for whom the Upper Confidence Bound is the highest, favoring bandits with a high expected payout, but also those with high uncertainty.\n",
    "\n",
    "Choose a bandit to maximize\n",
    "\n",
    "$$p_A + \\sqrt{\\frac{2 \\ln{N}}{n_A}} $$\n",
    "\n",
    "where\n",
    "\n",
    " * $p_A$ is the expected payout of bandit $A$.\n",
    " * $n_A$ is the number of times bandit $A$ has played.\n",
    " * N is the total number of trials so far.\n",
    "\n",
    "This chooses the bandit for whom the Upper Confidence Bound is the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Bandit\n",
    "\n",
    "Use Bayesian statistics:\n",
    "\n",
    "* Find probability distribution of payout of each bandit thus far. (how?)\n",
    "* For each bandit, sample from distribution.\n",
    "* Choose bandit for whom the sample has highest expected payout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
