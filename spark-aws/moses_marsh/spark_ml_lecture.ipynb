{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook was started with the script `jupyspark.sh`,  the variables `spark` and `sc` will already defined\n",
    "\n",
    "If not, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = (\n",
    "        ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "        .appName(\"lecture\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6f74d472e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-ML Objectives\n",
    "\n",
    "At the end of this lecture you should be able to:\n",
    "\n",
    "1. Be able to describe the Spark-ML API, and recognize differences to sk-learn.\n",
    "2. Chain spark-ml Transformers and Estimators together to compose ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n",
    "Spark's machine learning pipeline is organized three main components:\n",
    "\n",
    "- Spark Dataframes: data with a schema\n",
    "- ***transformer*** objects: anything that has a `.transform()` method. It takes a Spark Dataframe as input and returns a new Spark Dataframe\n",
    "- ***estimator*** objects: anything that has a `.fit()` method. It takes a Spark Dataframe as input and returns a ***transformer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's machine learning pipeline doesn't quite operate on a simple dataframe where each column contains a feature. Instead, you must explicitly construct a `features` column that contains a **vector** of the relevant values. \n",
    "\n",
    "To do so, spark has a **transformer** called `VectorAssembler`. You make a new instance of the `VectorAssembler`, specifying the list of columns to concatenate and the name of the output column to store the vectors in (`features` is a good choice). Then pass your dataframe to the `transform()` method, which will return a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+--------------------+\n",
      "|      Open|      High|       Low|     Close|            features|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "|117.949997|118.360001|117.309998|    118.25|[117.949997,118.3...|\n",
      "|117.099998|117.739998|     117.0|117.650002|[117.099998,117.7...|\n",
      "|116.809998|116.910004|116.279999|116.599998|[116.809998,116.9...|\n",
      "|116.860001|117.379997|116.330002|117.059998|[116.860001,117.3...|\n",
      "|    117.25|117.760002|113.800003|117.120003|[117.25,117.76000...|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[117.949997,118.3...|\n",
      "|[117.099998,117.7...|\n",
      "|[116.809998,116.9...|\n",
      "|[116.860001,117.3...|\n",
      "|[117.25,117.76000...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([117.95, 118.36, 117.31, 118.25])),\n",
       " Row(features=DenseVector([117.1, 117.74, 117.0, 117.65])),\n",
       " Row(features=DenseVector([116.81, 116.91, 116.28, 116.6])),\n",
       " Row(features=DenseVector([116.86, 117.38, 116.33, 117.06])),\n",
       " Row(features=DenseVector([117.25, 117.76, 113.8, 117.12]))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vec_assembler = VectorAssembler(inputCols=[\"Open\",\"High\", \"Low\",\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vec_assembler.transform(df_aapl)\n",
    "\n",
    "df_vector.select(['Open', 'High', 'Low', 'Close', 'features']).show(5)\n",
    "\n",
    "print(\"*\"*75)\n",
    "\n",
    "df_vector.select('features').show(5)\n",
    "\n",
    "print(\"*\"*75)\n",
    "\n",
    "df_vector.select('features').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of an **estimator**: `MinMaxScaler`. `fit()` takes a dataframe, stores the min & max value for each feature, and returns a **transformer** object that has the `.transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[117.949997,118.3...|[0.84364622791846...|\n",
      "|[117.099998,117.7...|[0.81798975110079...|\n",
      "|[116.809998,116.9...|[0.80923635459429...|\n",
      "|[116.860001,117.3...|[0.81074565144089...|\n",
      "|[117.25,117.76000...|[0.82251743035171...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866])),\n",
       " Row(scaledfeatures=DenseVector([0.818, 0.8109, 0.8563, 0.8473])),\n",
       " Row(scaledfeatures=DenseVector([0.8092, 0.7851, 0.8339, 0.8148])),\n",
       " Row(scaledfeatures=DenseVector([0.8107, 0.7997, 0.8355, 0.829])),\n",
       " Row(scaledfeatures=DenseVector([0.8225, 0.8115, 0.7568, 0.8309]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scaler_transformer = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scaler_transformer.transform(df_vector)\n",
    "\n",
    "\n",
    "scaled_data.select(\"features\", \"scaledfeatures\").show(5)\n",
    "\n",
    "print(\"*\"*75)\n",
    "\n",
    "scaled_data.select(\"scaledfeatures\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([117.95, 118.36, 117.31, 118.25]), scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.select(\"features\", \"scaledfeatures\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([117.95, 118.36, 117.31, 118.25])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.select(\"features\", \"scaledfeatures\").first()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.8436, 0.8302, 0.8659, 0.866])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.select(\"features\", \"scaledfeatures\").first()['scaledfeatures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers & Estimators\n",
    "\n",
    "Spark has so many.\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Many Data Science workflows can be described as sequential application of various `Transformers` and `Estimators`.\n",
    "\n",
    "![](images/ml-Pipeline.png)\n",
    "\n",
    "\n",
    "source: http://spark.apache.org/docs/latest/img/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see two ways to implement the above flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a a a b b c a d spark\", 1.0),\n",
    "    (1, \"b c c c d c c a\", 0.0),\n",
    "    (2, \"spark spark a a c spam\", 1.0),\n",
    "    (3, \"c d d b d spam\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|a a a b b c a d s...|  1.0|\n",
      "|  1|     b c c c d c c a|  0.0|\n",
      "|  2|spark spark a a c...|  1.0|\n",
      "|  3|      c d d b d spam|  0.0|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|               words|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0|a a a b b c a d s...|  1.0|[a, a, a, b, b, c...|\n",
      "|  1|     b c c c d c c a|  0.0|[b, c, c, c, d, c...|\n",
      "|  2|spark spark a a c...|  1.0|[spark, spark, a,...|\n",
      "|  3|      c d d b d spam|  0.0|[c, d, d, b, d, s...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokens = tokenizer.transform(training)\n",
    "tokens.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "| id|                text|label|               words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0|a a a b b c a d s...|  1.0|[a, a, a, b, b, c...|(6,[0,1,2,3,4],[1...|\n",
      "|  1|     b c c c d c c a|  0.0|[b, c, c, c, d, c...|(6,[0,1,2,3],[5.0...|\n",
      "|  2|spark spark a a c...|  1.0|[spark, spark, a,...|(6,[0,1,4,5],[1.0...|\n",
      "|  3|      c d d b d spam|  0.0|[c, d, d, b, d, s...|(6,[0,2,3,5],[1.0...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "cv_model = cv.fit(tokens)\n",
    "cv_df = cv_model.transform(tokens)\n",
    "cv_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(6,[0,1,2,3,4],[1...|\n",
      "|(6,[0,1,2,3],[5.0...|\n",
      "|(6,[0,1,4,5],[1.0...|\n",
      "|(6,[0,2,3,5],[1.0...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_df.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(6, {0: 1.0, 1: 4.0, 2: 1.0, 3: 2.0, 4: 1.0})),\n",
       " Row(features=SparseVector(6, {0: 5.0, 1: 1.0, 2: 1.0, 3: 1.0})),\n",
       " Row(features=SparseVector(6, {0: 1.0, 1: 2.0, 4: 2.0, 5: 1.0})),\n",
       " Row(features=SparseVector(6, {0: 1.0, 2: 3.0, 3: 1.0, 5: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.select('features').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,[0,1,2,3,4],[1.0,4.0,1.0,2.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "print(cv_df.select('features').take(5)[0]['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, \n",
    "                        regParam=0.001, \n",
    "                        featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        predictionCol='prediction',\n",
    "                        probabilityCol='probability')\n",
    "# These last four keywords are the defaults!\n",
    "# I've written them out here for clarity\n",
    "\n",
    "logistic_model = lr.fit(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions = logistic_model.transform(cv_df)\n",
    "\n",
    "train_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|                text|label|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0|a a a b b c a d s...|  1.0|[a, a, a, b, b, c...|(6,[0,1,2,3,4],[1...|[-5.6771333894961...|[0.00341167842863...|       1.0|\n",
      "|  1|     b c c c d c c a|  0.0|[b, c, c, c, d, c...|(6,[0,1,2,3],[5.0...|[5.48572083680497...|[0.99587156886007...|       0.0|\n",
      "|  2|spark spark a a c...|  1.0|[spark, spark, a,...|(6,[0,1,4,5],[1.0...|[-6.0274989763248...|[0.00240571627821...|       1.0|\n",
      "|  3|      c d d b d spam|  0.0|[c, d, d, b, d, s...|(6,[0,2,3,5],[1.0...|[5.79796408356077...|[0.99697545077755...|       0.0|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|                text|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|a a a b b c a d s...|  1.0|[-5.6771333894961...|[0.00341167842863...|       1.0|\n",
      "|     b c c c d c c a|  0.0|[5.48572083680497...|[0.99587156886007...|       0.0|\n",
      "|spark spark a a c...|  1.0|[-6.0274989763248...|[0.00240571627821...|       1.0|\n",
      "|      c d d b d spam|  0.0|[5.79796408356077...|[0.99697545077755...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions[['text', 'label', 'rawPrediction', 'probability', 'prediction']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0034, 0.9966])),\n",
       " Row(probability=DenseVector([0.9959, 0.0041])),\n",
       " Row(probability=DenseVector([0.0024, 0.9976])),\n",
       " Row(probability=DenseVector([0.997, 0.003]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions[['probability']].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawPrediction=DenseVector([-5.6771, 5.6771])),\n",
       " Row(rawPrediction=DenseVector([5.4857, -5.4857])),\n",
       " Row(rawPrediction=DenseVector([-6.0275, 6.0275])),\n",
       " Row(rawPrediction=DenseVector([5.798, -5.798]))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions[['rawPrediction']].take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark a a a a\"),\n",
    "    (5, \"c c c p\"),\n",
    "    (6, \"spark spam spark a\"),\n",
    "    (7, \"a a a c c c\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# What do we need to do to this to get a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't this work?\n",
    "\n",
    "#logistic_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "We need to transform all our test data with the same pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(test)\n",
    "test_vectors = cv_model.transform(test_tokens)\n",
    "test_output = logistic_model.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|     spark a a a a| [spark, a, a, a, a]| (6,[1,4],[4.0,1.0])|[-7.7612663456679...|[4.25735553078517...|       1.0|\n",
      "|  5|           c c c p|        [c, c, c, p]|       (6,[0],[3.0])|[3.51073257971980...|[0.97099160578993...|       0.0|\n",
      "|  6|spark spam spark a|[spark, spam, spa...|(6,[1,4,5],[1.0,2...|[-5.6987077106472...|[0.00333910522987...|       1.0|\n",
      "|  7|       a a a c c c|  [a, a, a, c, c, c]| (6,[0,1],[3.0,3.0])|[-0.6949249969721...|[0.33293838014924...|       1.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+----------+\n",
      "|              text|       rawPrediction|         probability|prediction|\n",
      "+------------------+--------------------+--------------------+----------+\n",
      "|     spark a a a a|[-7.7612663456679...|[4.25735553078517...|       1.0|\n",
      "|           c c c p|[3.51073257971980...|[0.97099160578993...|       0.0|\n",
      "|spark spam spark a|[-5.6987077106472...|[0.00333910522987...|       1.0|\n",
      "|       a a a c c c|[-0.6949249969721...|[0.33293838014924...|       1.0|\n",
      "+------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.select('text','rawPrediction','probability','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------+\n",
      "|              text|         probability|prediction|\n",
      "+------------------+--------------------+----------+\n",
      "|     spark a a a a|[4.25735553078517...|       1.0|\n",
      "|           c c c p|[0.97099160578993...|       0.0|\n",
      "|spark spam spark a|[0.00333910522987...|       1.0|\n",
      "|       a a a c c c|[0.33293838014924...|       1.0|\n",
      "+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.select('text', 'probability','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0004, 0.9996])),\n",
       " Row(probability=DenseVector([0.971, 0.029])),\n",
       " Row(probability=DenseVector([0.0033, 0.9967])),\n",
       " Row(probability=DenseVector([0.3329, 0.6671]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('probability').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawPrediction=DenseVector([-7.7613, 7.7613])),\n",
       " Row(rawPrediction=DenseVector([3.5107, -3.5107])),\n",
       " Row(rawPrediction=DenseVector([-5.6987, 5.6987])),\n",
       " Row(rawPrediction=DenseVector([-0.6949, 0.6949]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('rawPrediction').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively: put all these steps in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "| (6,[1,4],[4.0,1.0])|       1.0|[4.25735553078517...|\n",
      "|       (6,[0],[3.0])|       0.0|[0.97099160578993...|\n",
      "|(6,[1,4,5],[1.0,2...|       1.0|[0.00333910522987...|\n",
      "| (6,[0,1],[3.0,3.0])|       1.0|[0.33293838014924...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How can we test this against our training data?\n",
    "prediction = model.transform(test)\n",
    "prediction.select(['features', 'prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon: Spinning up a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this the long way, see this walkthrough: https://github.com/gSchool/dsi-spark-aws/blob/master/pair_part1.md\n",
    "\n",
    "To do this the short way, we've written a script that uses the AWS CLI to start up a cluster:\n",
    "https://github.com/gSchool/dsi-spark-aws/blob/master/scripts/launch_cluster.sh\n",
    "\n",
    "The script requires you to have:\n",
    "- AWS CLI set up\n",
    "- an S3 bucket\n",
    "- a PEM key pair, with the PEM file stored in `~/.ssh/` (if you need to create one, go [here](https://console.aws.amazon.com/ec2/v2/home#KeyPairs))\n",
    "- the accompanying file [`bootstrap-emr.sh`](https://github.com/gSchool/dsi-spark-aws/blob/master/scripts/bootstrap-emr.sh) in the same folder as `launch_cluster.sh`\n",
    "\n",
    "When running the script, you specify the name of the bucket, the name of the PEM key, and the number of worker nodes to have in your cluster. e.g.,\n",
    "```bash\n",
    "bash launch_cluster.sh mybucket mypem 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Command Line interface\n",
    "\n",
    "``` pip install awscli ```\n",
    "\n",
    "``` aws configure ```\n",
    "\n",
    " - leave `AWS Access Key ID` and `AWS Secret Access Key` as `None`, since you should have already put them in your  `~/.bash_profile` (`~/.bashrc` on Linux)\n",
    " - make sure `Default region name` matches the location of your cluster. [This page](https://www.npmjs.com/package/aws-regions) lists region codes.\n",
    " - leave `Default output format` as whatever it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 buckets with AWS CLI\n",
    "- Create a bucket: \n",
    "  - `aws s3 mb s3://mynewbucketname`\n",
    "- List files in a bucket: \n",
    "  - `aws s3 ls s3://bucketname`\n",
    "- Copy local file to bucket: \n",
    "  - `aws s3 cp path/to/localfile s3://mybucketname`\n",
    "- Copy from bucket to local current directory: \n",
    "  - `aws s3 cp s3://mybucket/path/to/file .`\n",
    "- [AWS CLI S3 management reference](https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC2: setting up ssh\n",
    "- example `~/.ssh/config` entry:\n",
    "\n",
    "```bash\n",
    "Host host_alias\n",
    "    HostName ec2-54-219-176-90.us-west-1.compute.amazonaws.com # see AWS console for public DNS\n",
    "    User hadoop # depending on your machine, 'user' may be 'ubuntu' or 'ec2-user' instead\n",
    "    IdentityFile ~/.ssh/key_file.pem # make sure this is the same key you chose when you set up the instance\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logging in to remote terminal:\n",
    " - `ssh host_alias`\n",
    "   - (This is shorthand for `ssh -i ~/.ssh/key_file.pem <User>@<HostName>`)\n",
    "- copying a file to a remote machine's home directory (note the colon!)\n",
    " - `scp path/to/local/file host_alias:`\n",
    "   - (This is shorthand for `scp -i ~/.ssh/key_file.pem path/to/local/file <User>@<HostName>:`)\n",
    "- copying a file to a remote machine\n",
    " - `scp path/to/local/file host_alias:path/to/target/directory`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tmux: set it and forget it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many processes are tied to your terminal. If you `ssh` into your remote machine and run a process in that terminal, that process will break if you suddenly lose your connection.\n",
    "\n",
    "`tmux` is a tool for \"terminal multiplexing\". It's great for managing many processes in many terminals. Here's the process for starting a process in a terminal that is detached from your `ssh`ed terminal using `tmux`:\n",
    "\n",
    "- `ssh` into your remote machine\n",
    "- start a tmux session with `tmux new -s some_name`\n",
    "- start your process (notebook or script or whatever)\n",
    "- type `<ctrl>-b d` to detach (now you are back in your `ssh`ed terminal)\n",
    "- exit or shut down or go to sleep or whatever\n",
    "- `ssh` back in to your remote machine\n",
    "- type `tmux a -t some_name` to check on that process\n",
    "- [handy tmux reference](https://gist.github.com/MohamedAlaa/2961058)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
