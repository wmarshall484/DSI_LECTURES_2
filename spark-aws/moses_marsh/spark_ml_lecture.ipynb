{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook was started with the script `jupyspark.sh`,  the variables `spark` and `sc` will already defined\n",
    "\n",
    "If not, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = (\n",
    "        ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "        .appName(\"lecture\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcdafb5c390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-ML Objectives\n",
    "\n",
    "At the end of this lecture you should be able to:\n",
    "\n",
    "1. Be able to describe the Spark-ML API, and recognize differences to sk-learn.\n",
    "2. Chain spark-ml Transformers and Estimators together to compose ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on DataFrames\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+--------------------+\n",
      "|      Open|      High|       Low|     Close|            features|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "|117.949997|118.360001|117.309998|    118.25|[117.949997,118.3...|\n",
      "|117.099998|117.739998|     117.0|117.650002|[117.099998,117.7...|\n",
      "|116.809998|116.910004|116.279999|116.599998|[116.809998,116.9...|\n",
      "|116.860001|117.379997|116.330002|117.059998|[116.860001,117.3...|\n",
      "|    117.25|117.760002|113.800003|117.120003|[117.25,117.76000...|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[117.949997,118.3...|\n",
      "|[117.099998,117.7...|\n",
      "|[116.809998,116.9...|\n",
      "|[116.860001,117.3...|\n",
      "|[117.25,117.76000...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([117.95, 118.36, 117.31, 118.25])),\n",
       " Row(features=DenseVector([117.1, 117.74, 117.0, 117.65])),\n",
       " Row(features=DenseVector([116.81, 116.91, 116.28, 116.6])),\n",
       " Row(features=DenseVector([116.86, 117.38, 116.33, 117.06])),\n",
       " Row(features=DenseVector([117.25, 117.76, 113.8, 117.12]))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Open\",\"High\", \"Low\",\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)\n",
    "df_vector.select(['Open', 'High', 'Low', 'Close', 'features']).show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "df_vector.select('features').show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "df_vector.select('features').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[117.949997,118.3...|[0.84364622791846...|\n",
      "|[117.099998,117.7...|[0.81798975110079...|\n",
      "|[116.809998,116.9...|[0.80923635459429...|\n",
      "|[116.860001,117.3...|[0.81074565144089...|\n",
      "|[117.25,117.76000...|[0.82251743035171...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866])),\n",
       " Row(scaledfeatures=DenseVector([0.818, 0.8109, 0.8563, 0.8473])),\n",
       " Row(scaledfeatures=DenseVector([0.8092, 0.7851, 0.8339, 0.8148])),\n",
       " Row(scaledfeatures=DenseVector([0.8107, 0.7997, 0.8355, 0.829])),\n",
       " Row(scaledfeatures=DenseVector([0.8225, 0.8115, 0.7568, 0.8309]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "scaledData.select(\"features\", \"scaledfeatures\").show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "scaledData.select(\"scaledfeatures\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([117.95, 118.36, 117.31, 118.25]), scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([117.95, 118.36, 117.31, 118.25])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.8436, 0.8302, 0.8659, 0.866])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()['scaledfeatures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The `VectorAssembler` class above is an example of a generic type in Spark, called a Transformer. Important things to know about this type:\n",
    "\n",
    "* They implement a `transform` method.\n",
    "* They convert one `DataFrame` into another, usually by adding columns.\n",
    "\n",
    "Examples of Transformers: `VectorAssembler`, `Tokenizer`, `StopWordsRemover`, `StandardScaler`, and many more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "According to the docs: An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Important things to know about this type:\n",
    "\n",
    "* They implement a `fit` method whose argument is a `DataFrame`.\n",
    "* The output of `fit` is another type called `Model`, which is a `Transformer`.\n",
    "\n",
    "Examples of Estimators: `LogisticRegression`, `DecisionTreeRegressor`, and many more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Many Data Science workflows can be described as sequential application of various `Transformers` and `Estimators`.\n",
    "\n",
    "![http://spark.apache.org/docs/latest/img/ml-Pipeline.png](http://spark.apache.org/docs/latest/img/ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see two ways to implement the above flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a a a b b c a d spark\", 1.0),\n",
    "    (1, \"b c c c d c c a\", 0.0),\n",
    "    (2, \"spark spark a a c spam\", 1.0),\n",
    "    (3, \"c d d b d spam\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|a a a b b c a d s...|  1.0|\n",
      "|  1|     b c c c d c c a|  0.0|\n",
      "|  2|spark spark a a c...|  1.0|\n",
      "|  3|      c d d b d spam|  0.0|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|               words|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0|a a a b b c a d s...|  1.0|[a, a, a, b, b, c...|\n",
      "|  1|     b c c c d c c a|  0.0|[b, c, c, c, d, c...|\n",
      "|  2|spark spark a a c...|  1.0|[spark, spark, a,...|\n",
      "|  3|      c d d b d spam|  0.0|[c, d, d, b, d, s...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokens = tokenizer.transform(training)\n",
    "tokens.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "| id|                text|label|               words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0|a a a b b c a d s...|  1.0|[a, a, a, b, b, c...|(262144,[27526,28...|\n",
      "|  1|     b c c c d c c a|  0.0|[b, c, c, c, d, c...|(262144,[27526,28...|\n",
      "|  2|spark spark a a c...|  1.0|[spark, spark, a,...|(262144,[28698,19...|\n",
      "|  3|      c d d b d spam|  0.0|[c, d, d, b, d, s...|(262144,[27526,28...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "hashes = hashingTF.transform(tokens)\n",
    "hashes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[27526,28...|\n",
      "|(262144,[27526,28...|\n",
      "|(262144,[28698,19...|\n",
      "|(262144,[27526,28...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashes.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(262144, {27526: 1.0, 28698: 1.0, 30913: 2.0, 227410: 4.0, 234657: 1.0})),\n",
       " Row(features=SparseVector(262144, {27526: 1.0, 28698: 5.0, 30913: 1.0, 227410: 1.0})),\n",
       " Row(features=SparseVector(262144, {28698: 1.0, 197793: 1.0, 227410: 2.0, 234657: 2.0})),\n",
       " Row(features=SparseVector(262144, {27526: 3.0, 28698: 1.0, 30913: 1.0, 197793: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes.select('features').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144,[27526,28698,30913,227410,234657],[1.0,1.0,2.0,4.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "print(hashes.select('features').first()['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, \n",
    "                        regParam=0.001, \n",
    "                        featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        predictionCol='prediction',\n",
    "                        probabilityCol='probability')\n",
    "# These last four keywords are the defaults!\n",
    "# I've written them out here for clarity\n",
    "\n",
    "logistic_model = lr.fit(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark a a a a\"),\n",
    "    (5, \"c c c p\"),\n",
    "    (6, \"spark spam spark a\"),\n",
    "    (7, \"a a a c c c\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# What do we need to do to this to get a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't this work?\n",
    "\n",
    "#logistic_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "We need to transform all our test data with the same pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(test)\n",
    "test_vectors = hashingTF.transform(test_tokens)\n",
    "test_output = logistic_model.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|     spark a a a a| [spark, a, a, a, a]|(262144,[227410,2...|[-7.7612663456679...|[4.25735553078516...|       1.0|\n",
      "|  5|           c c c p|        [c, c, c, p]|(262144,[28698,21...|[3.51073257971980...|[0.97099160578993...|       0.0|\n",
      "|  6|spark spam spark a|[spark, spam, spa...|(262144,[197793,2...|[-5.6987077106472...|[0.00333910522987...|       1.0|\n",
      "|  7|       a a a c c c|  [a, a, a, c, c, c]|(262144,[28698,22...|[-0.6949249969721...|[0.33293838014924...|       1.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+----------+\n",
      "|              text|       rawPrediction|         probability|prediction|\n",
      "+------------------+--------------------+--------------------+----------+\n",
      "|     spark a a a a|[-7.7612663456679...|[4.25735553078516...|       1.0|\n",
      "|           c c c p|[3.51073257971980...|[0.97099160578993...|       0.0|\n",
      "|spark spam spark a|[-5.6987077106472...|[0.00333910522987...|       1.0|\n",
      "|       a a a c c c|[-0.6949249969721...|[0.33293838014924...|       1.0|\n",
      "+------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.select('text','rawPrediction','probability','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------+\n",
      "|              text|         probability|prediction|\n",
      "+------------------+--------------------+----------+\n",
      "|     spark a a a a|[4.25735553078516...|       1.0|\n",
      "|           c c c p|[0.97099160578993...|       0.0|\n",
      "|spark spam spark a|[0.00333910522987...|       1.0|\n",
      "|       a a a c c c|[0.33293838014924...|       1.0|\n",
      "+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.select('text', 'probability','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0004, 0.9996])),\n",
       " Row(probability=DenseVector([0.971, 0.029])),\n",
       " Row(probability=DenseVector([0.0033, 0.9967])),\n",
       " Row(probability=DenseVector([0.3329, 0.6671]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('probability').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawPrediction=DenseVector([-7.7613, 7.7613])),\n",
       " Row(rawPrediction=DenseVector([3.5107, -3.5107])),\n",
       " Row(rawPrediction=DenseVector([-5.6987, 5.6987])),\n",
       " Row(rawPrediction=DenseVector([-0.6949, 0.6949]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('rawPrediction').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively: put all these steps in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|(262144,[227410,2...|       1.0|[4.25735553078518...|\n",
      "|(262144,[28698,21...|       0.0|[0.97099160578993...|\n",
      "|(262144,[197793,2...|       1.0|[0.00333910522987...|\n",
      "|(262144,[28698,22...|       1.0|[0.33293838014925...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How can we test this against our training data?\n",
    "prediction = model.transform(test)\n",
    "prediction.select(['features', 'prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
