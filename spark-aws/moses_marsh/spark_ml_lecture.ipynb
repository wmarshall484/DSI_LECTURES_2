{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was started with the script `jupyspark.sh`, so the variables `spark` and `sc` are already defined.\n",
    "\n",
    "If it weren't, I would run the following cell uncommented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark as ps\n",
    "\n",
    "# spark = ps.sql.SparkSession.builder \\\n",
    "#         .master(\"local[4]\") \\\n",
    "#         .appName(\"df lecture\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>df lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc326897b38>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.1.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>df lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=df lecture>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-ML Objectives\n",
    "\n",
    "At the end of this lecture you should be able to:\n",
    "\n",
    "1. Be able to describe the Spark-ML API, and recognize differences to sk-learn.\n",
    "2. Chain spark-ml Transformers and Estimators together to compose ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on DataFrames\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+--------------------+\n",
      "|      Open|      High|       Low|     Close|            features|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "|117.949997|118.360001|117.309998|    118.25|[117.949997,118.3...|\n",
      "|117.099998|117.739998|     117.0|117.650002|[117.099998,117.7...|\n",
      "|116.809998|116.910004|116.279999|116.599998|[116.809998,116.9...|\n",
      "|116.860001|117.379997|116.330002|117.059998|[116.860001,117.3...|\n",
      "|    117.25|117.760002|113.800003|117.120003|[117.25,117.76000...|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Open\",\"High\", \"Low\",\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)\n",
    "df_vector.select(['Open', 'High', 'Low', 'Close', 'features']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[117.949997,118.3...|[0.84364622791846...|\n",
      "|[117.099998,117.7...|[0.81798975110079...|\n",
      "|[116.809998,116.9...|[0.80923635459429...|\n",
      "|[116.860001,117.3...|[0.81074565144089...|\n",
      "|[117.25,117.76000...|[0.82251743035171...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "scaledData.select(\"features\", \"scaledfeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([117.95, 118.36, 117.31, 118.25]), scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([117.95, 118.36, 117.31, 118.25])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.8436, 0.8302, 0.8659, 0.866])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"features\", \"scaledfeatures\").first()['scaledfeatures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The `VectorAssembler` class above is an example of a generic type in Spark, called a Transformer. Important things to know about this type:\n",
    "\n",
    "* They implement a `transform` method.\n",
    "* They convert one `DataFrame` into another, usually by adding columns.\n",
    "\n",
    "Examples of Transformers: `VectorAssembler`, `Tokenizer`, `StopWordsRemover`, `StandardScaler`, and many more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "According to the docs: An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Important things to know about this type:\n",
    "\n",
    "* They implement a `fit` method whose argument is a `DataFrame`.\n",
    "* The output of `fit` is another type called `Model`, which is a `Transformer`.\n",
    "\n",
    "Examples of Estimators: `LogisticRegression`, `DecisionTreeRegressor`, and many more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Many Data Science workflows can be described as sequential application of various `Transformers` and `Estimators`.\n",
    "\n",
    "![http://spark.apache.org/docs/latest/img/ml-Pipeline.png](http://spark.apache.org/docs/latest/img/ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see two ways to implement the above flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c c d d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g g h\", 1.0),\n",
    "    (3, \"hadoop a mapreduce g\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0| a b c c d d e spark|  1.0|\n",
      "|  1|                 b d|  0.0|\n",
      "|  2|       spark f g g h|  1.0|\n",
      "|  3|hadoop a mapreduce g|  0.0|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|               words|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0| a b c c d d e spark|  1.0|[a, b, c, c, d, d...|\n",
      "|  1|                 b d|  0.0|              [b, d]|\n",
      "|  2|       spark f g g h|  1.0| [spark, f, g, g, h]|\n",
      "|  3|hadoop a mapreduce g|  0.0|[hadoop, a, mapre...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokens = tokenizer.transform(training)\n",
    "tokens.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "| id|                text|label|               words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0| a b c c d d e spark|  1.0|[a, b, c, c, d, d...|(262144,[17222,27...|\n",
      "|  1|                 b d|  0.0|              [b, d]|(262144,[27526,30...|\n",
      "|  2|       spark f g g h|  1.0| [spark, f, g, g, h]|(262144,[15554,24...|\n",
      "|  3|hadoop a mapreduce g|  0.0|[hadoop, a, mapre...|(262144,[42633,51...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "hashes = hashingTF.transform(tokens)\n",
    "hashes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[17222,27...|\n",
      "|(262144,[27526,30...|\n",
      "|(262144,[15554,24...|\n",
      "|(262144,[42633,51...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashes.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(262144, {17222: 1.0, 27526: 2.0, 28698: 2.0, 30913: 1.0, 227410: 1.0, 234657: 1.0})),\n",
       " Row(features=SparseVector(262144, {27526: 1.0, 30913: 1.0})),\n",
       " Row(features=SparseVector(262144, {15554: 1.0, 24152: 1.0, 51505: 2.0, 234657: 1.0})),\n",
       " Row(features=SparseVector(262144, {42633: 1.0, 51505: 1.0, 155117: 1.0, 227410: 1.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes.select('features').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144,[17222,27526,28698,30913,227410,234657],[1.0,2.0,2.0,1.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "print(hashes.select('features').first()['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, \n",
    "                        regParam=0.001, \n",
    "                        featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        predictionCol='prediction',\n",
    "                        probabilityCol='probability')\n",
    "# These last four keywords are the defaults!\n",
    "# I've written them out here for clarity\n",
    "\n",
    "logistic_model = lr.fit(hashes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# What do we need to do to this to get a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't this work?\n",
    "\n",
    "#logistic_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "We need to transform all our test data with the same pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.transform(test)\n",
    "test_vectors = hashingTF.transform(test_tokens)\n",
    "test_output = logistic_model.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[20197,24...|[-1.5983128691464...|[0.16821754677494...|       1.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[18910,10...|[2.46049217146948...|[0.92132534525405...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[155117,2...|[-3.1687538922198...|[0.04035864933082...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[66695,15...|[4.94885618901197...|[0.99295841983053...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------+\n",
      "|              text|         probability|prediction|\n",
      "+------------------+--------------------+----------+\n",
      "|       spark i j k|[0.16821754677494...|       1.0|\n",
      "|             l m n|[0.92132534525405...|       0.0|\n",
      "|spark hadoop spark|[0.04035864933082...|       1.0|\n",
      "|     apache hadoop|[0.99295841983053...|       0.0|\n",
      "+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output.select('text', 'probability','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.1682, 0.8318])),\n",
       " Row(probability=DenseVector([0.9213, 0.0787])),\n",
       " Row(probability=DenseVector([0.0404, 0.9596])),\n",
       " Row(probability=DenseVector([0.993, 0.007]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('probability').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawPrediction=DenseVector([-1.5983, 1.5983])),\n",
       " Row(rawPrediction=DenseVector([2.4605, -2.4605])),\n",
       " Row(rawPrediction=DenseVector([-3.1688, 3.1688])),\n",
       " Row(rawPrediction=DenseVector([4.9489, -4.9489]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.select('rawPrediction').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|(262144,[20197,24...|       1.0|[0.16821754677494...|\n",
      "|(262144,[18910,10...|       0.0|[0.92132534525405...|\n",
      "|(262144,[155117,2...|       1.0|[0.04035864933082...|\n",
      "|(262144,[66695,15...|       0.0|[0.99295841983053...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How can we test this against our training data?\n",
    "prediction = model.transform(test)\n",
    "prediction.select(['features', 'prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
