{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:05.765137",
     "start_time": "2017-03-01T08:30:05.749884"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consistent with Spark 2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "- Interact with Spark RDDs using SQL\n",
    "\n",
    "\n",
    "### Why?\n",
    "- SQL is familiar\n",
    "- Easy to use\n",
    "- Portable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Hive Context to bridge the gap between an RDD and a \"Hive table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:05.783723",
     "start_time": "2017-03-01T08:30:05.766658"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"SQL Lecture\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A SparkSession is like a HiveContext, which is like an sqlContext.\n",
    "\n",
    "- In Spark 2, use SparkSession.\n",
    "\n",
    "Q: What is the difference between SparkContext and SparkSession (or HiveContext)?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data\n",
    "  sources.\n",
    "\n",
    "\n",
    "Spark SQL Using CSV\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:05.799538",
     "start_time": "2017-03-01T08:30:05.785100"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read this data as a Spark DataFrame\n",
    "- Deal with header\n",
    "- Split into fields\n",
    "- Assign column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:06.250058",
     "start_time": "2017-03-01T08:30:05.800815"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load RDD\n",
    "rdd = sc.textFile('sales.csv')\n",
    "\n",
    "# Deal with header\n",
    "rdd = rdd.filter(lambda line: not line.startswith('#'))\n",
    "\n",
    "# Split into fields\n",
    "rdd = rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# Assign variable types\n",
    "rdd = rdd.map(lambda (id, date, store, state, product, amount):\n",
    "              (int(id), date, int(store), state, int(product), float(amount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:06.271968",
     "start_time": "2017-03-01T08:30:06.251370"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a schema\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "        StructField('id', IntegerType(), True),\n",
    "        StructField('date', StringType(), True),\n",
    "        StructField('store', IntegerType(), True),\n",
    "        StructField('state', StringType(), True),\n",
    "        StructField('product', IntegerType(), True),\n",
    "        StructField('amount', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:08.123319",
     "start_time": "2017-03-01T08:30:06.274192"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:09.619618",
     "start_time": "2017-03-01T08:30:08.125060"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.059593",
     "start_time": "2017-03-01T08:30:09.621273"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL With DataFrames\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the table so it can be seen in 'SQL World'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.246077",
     "start_time": "2017-03-01T08:30:10.061082"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run queries on the registered table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.268251",
     "start_time": "2017-03-01T08:30:10.247555"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT * FROM sales \n",
    "    WHERE amount > 300\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.406100",
     "start_time": "2017-03-01T08:30:10.269684"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.431046",
     "start_time": "2017-03-01T08:30:10.407697"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.474024",
     "start_time": "2017-03-01T08:30:10.433015"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:10.634482",
     "start_time": "2017-03-01T08:30:10.475358"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember, you have to call the .show() method\n",
    "# of a Spark DF to see its contents.\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Results\n",
    "--------------\n",
    "\n",
    "- Saving to JSON is preferred (for ease of use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:30.782107",
     "start_time": "2017-03-01T08:30:30.411831"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json\n",
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:30.919484",
     "start_time": "2017-03-01T08:30:30.783712"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat high-sales.json/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Using JSON Data\n",
    "-------------------------\n",
    "\n",
    "Q: What is JSON-formatted data?\n",
    "\n",
    "- In Spark the JSON format means that each line is a JSON document.\n",
    "\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and\n",
    "  read using `textFile()`.\n",
    "\n",
    "- JSON works well with Spark SQL because the data has an embedded\n",
    "  schema.\n",
    "\n",
    "Q: What other formats are supported by Spark SQL?\n",
    "\n",
    "- Spark SQL also supports Parquet, which is a compact binary format\n",
    "  for big data.\n",
    "\n",
    "- If your data is in CSV then you have to add the schema\n",
    "  programmatically after you load the data.\n",
    "\n",
    "Parsing JSON Data\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:30.947749",
     "start_time": "2017-03-01T08:30:30.921123"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.102010",
     "start_time": "2017-03-01T08:30:30.949398"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the hive context reads a json file, it automatically gathers the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.268954",
     "start_time": "2017-03-01T08:30:31.103483"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a 50% sample of the DataFrame (without replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.363070",
     "start_time": "2017-03-01T08:30:31.270402"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(False, .5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.390147",
     "start_time": "2017-03-01T08:30:31.364654"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute SQL statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.520848",
     "start_time": "2017-03-01T08:30:31.391635"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')\n",
    "result = spark.sql('select state, store from sales')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.613426",
     "start_time": "2017-03-01T08:30:31.522773"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Methods\n",
    "-----------------\n",
    "\n",
    "Slice the DataFrame by column and by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.722039",
     "start_time": "2017-03-01T08:30:31.615500"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(['state','store']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.839485",
     "start_time": "2017-03-01T08:30:31.724097"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.filter(df.amount > 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:31.959865",
     "start_time": "2017-03-01T08:30:31.841968"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('state','store', df.amount + 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:32.055515",
     "start_time": "2017-03-01T08:30:31.961704"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('state','store',df['amount'] + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:32.176892",
     "start_time": "2017-03-01T08:30:32.057646"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('state', df.amount > 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:33.118887",
     "start_time": "2017-03-01T08:30:32.178568"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select('state','amount').groupby('state').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:33.660840",
     "start_time": "2017-03-01T08:30:33.120463"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT state, AVG(amount) as avg_amount\n",
    "    FROM sales\n",
    "    GROUP BY state\n",
    "    '''\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write user-defined functions in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:33.717447",
     "start_time": "2017-03-01T08:30:33.662858"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_one(x):\n",
    "    return x + 1\n",
    "spark.udf.register(\n",
    "    'add_one',\n",
    "    add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:33.852576",
     "start_time": "2017-03-01T08:30:33.718717"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, add_one(amount) AS\n",
    "    FROM sales\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames are also RDDs.\n",
    "- Each row is an RDD object\n",
    "- Field names are attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:33.949108",
     "start_time": "2017-03-01T08:30:33.854566"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rdd.map(lambda row: row.amount**2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you are ready for 'big math' convert your Dataframe to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:34.011414",
     "start_time": "2017-03-01T08:30:33.950942"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complicated schemas\n",
    "- Nested json\n",
    "- Field of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:34.041287",
     "start_time": "2017-03-01T08:30:34.013287"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile people.json\n",
    "{\"name\":\"Yin\", \"address\":{\"city\":\"SF\",\"state\":\"CA\"}, \"hobbies\" : [\"fishing\",\"tennis\"]}\n",
    "{\"name\":\"Mike\", \"address\":{\"city\":\"SE\", \"state\":\"WA\"}, \"hobbies\":[\"coding\", \"fishing\"]}\n",
    "{\"name\":\"Mary\", \"address\":{\"city\":\"SE\", \"state\":\"WA\"}, \"hobbies\":[\"playing chess\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read people.json using the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:34.374689",
     "start_time": "2017-03-01T08:30:34.043103"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people = spark.read.json('people.json')\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get name and city for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:34.475696",
     "start_time": "2017-03-01T08:30:34.375977"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people.registerTempTable('people')\n",
    "result = spark.sql('''\n",
    "    SELECT name, address.city\n",
    "    FROM people\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lists are hard to deal with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T08:30:34.628644",
     "start_time": "2017-03-01T08:30:34.477209"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT * FROM people\n",
    "    LATERAL VIEW explode(hobbies) h as hobby\n",
    "    ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
