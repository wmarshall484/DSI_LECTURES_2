{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML\n",
    "\n",
    "## Morning Objectives\n",
    "\n",
    "At the end of this lecture you should be able to:\n",
    "\n",
    "1. Be able to describe the Spark ML API, and recognize differences with sklearn\n",
    "1. Chain Spark `Dataframe` methods together to do data munging\n",
    "1. Chain Spark ML `Transformers` and `Estimators` together to compose ML `Pipeline`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, RegexTokenizer, HashingTF, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Let's design chains of transformations together!\n",
    "\n",
    "### A.1. Computing sales per state\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df = sqlContext.read.csv('data/sales.csv',\n",
    "    header=True,      # use headers\n",
    "    quote='\"',        # use \" for quoting\n",
    "    sep=',',          # use , for separating fields\n",
    "    inferSchema=True) # infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "You want to obtain a sorted `DataFrame` of the states in which you have most sales done (`Amount`).  (i.e., by decreasing order of sales)\n",
    "\n",
    "1. What transformations do you need to apply?\n",
    "2. What if you had to draw a workflow of the transformations to apply?\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Find the date on which AAPL's closing stock price was the highest\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl_df = sqlContext.read.csv('data/aapl.csv',\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    sep=',',\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Now, design a pipeline that will:\n",
    "\n",
    "1. Keep only fields for Date and Close\n",
    "2. Order by Close in descending order\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Supervised Machine Learning on DataFrames\n",
    "\n",
    "- (http://spark.apache.org/docs/latest/ml-features.html)\n",
    "\n",
    "### Question: What is the difference between `aapl_df` and `vector_df` after running the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Close'], outputCol='features')\n",
    "\n",
    "vector_df = assembler.transform(aapl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(aapl_df)\n",
    "print\n",
    "print aapl_df.schema\n",
    "print\n",
    "aapl_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(vector_df)\n",
    "print\n",
    "print vector_df.schema\n",
    "print\n",
    "vector_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up: Why does this difference matter?\n",
    "\n",
    "Let's try to run one of Spark ML's built-in transformers on some of our data.  Let's min-max scale the `Close` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='Close', outputCol='Scaled Close').fit(aapl_df)\n",
    "\n",
    "scaled_close_df = scaler.transform(aapl_df)\n",
    "\n",
    "scaled_close_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print aapl_df.schema['Close']\n",
    "print vector_df.schema['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway: Gotta have the column as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features').fit(vector_df)\n",
    "\n",
    "scaled_features_df = scaler.transform(vector_df)\n",
    "\n",
    "scaled_features_df.select('features', 'scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "The `VectorAssembler` class above is an example of a generic type in Spark, called a [Transformer](http://spark.apache.org/docs/latest/ml-pipeline.html#transformers).  Important things to know about this type:\n",
    "\n",
    "- They implement a `transform` method\n",
    "- They convert one `DataFrame` into another, usually by adding columns\n",
    "\n",
    "Examples of transformers: [`VectorAssembler`](http://spark.apache.org/docs/latest/ml-features.html#vectorassembler), [`Tokenizer`](http://spark.apache.org/docs/latest/ml-features.html#tokenizer), [`StopWordsRemover`](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover), and [many more](http://spark.apache.org/docs/latest/ml-features.html)\n",
    "\n",
    "## Estimators\n",
    "\n",
    "According to the documentation: \"An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data\".  Important things to know about them:\n",
    "\n",
    "- They implement a `fit` method whose argument is a `DataFrame`\n",
    "- The output of `fit` is another type called `Model`, which is a `Transformer`\n",
    "\n",
    "Examples of estimators: [`LogisticRegression`](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression), [`DecisionTreeRegressor`](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression), and [many more](http://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "Many Data Science workflows can be described as sequential application of various `Transforms` and `Estimators`.\n",
    "\n",
    "![http://spark.apache.org/docs/latest/img/ml-Pipeline.png](http://spark.apache.org/docs/latest/img/ml-Pipeline.png)\n",
    "\n",
    "Let's see two ways to implement the above flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training set from a list of (id, text, label) tuples\n",
    "\n",
    "training_df = spark.createDataFrame([(0, 'spark is like hadoop mapreduce', 1.0),\n",
    "        (1, 'sparks light fire!!!', 0.0),\n",
    "        (2, 'elephants like simba', 0.0),\n",
    "        (3, 'hadoop is an elephant', 1.0),\n",
    "        (4, 'hadoop mapreduce', 1.0)],\n",
    "    ['id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='tokens', pattern='\\\\W')\n",
    "tokens_df = tokenizer.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = HashingTF(inputCol='tokens', outputCol='features')\n",
    "tf_df = tf.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(maxIter=10, regParam=.001).fit(tf_df)\n",
    "# (uses columns named features/label by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare test set, which are unlabeled (id, text) tuples\n",
    "\n",
    "test_df = spark.createDataFrame([(5, 'simba has a spark'),\n",
    "        (6, 'hadoop'),\n",
    "        (7, 'mapreduce in spark'),\n",
    "        (8, 'apache hadoop')],\n",
    "    ['id', 'text'])\n",
    "\n",
    "# What do we need to do to this to get prediction on our test set?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, configure a ML pipeline, which consists of three stages: tokenizer, tf, and lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='tokens', pattern='\\\\W')\n",
    "tf = HashingTF(inputCol='tokens', outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, tf, lr])\n",
    "\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What do we need to do to this to get prediction on our test set?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Unsupervised Machine Learning on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read csv\n",
    "iris_df = sqlContext.read.csv('data/iris.csv',\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    sep=',',\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "pipeline = (Pipeline(stages=[\n",
    "        VectorAssembler(inputCols=col_names, outputCol='features'),\n",
    "        PCA(k=3, inputCol='features', outputCol='pca_features')\n",
    "    ]).fit(iris_df))\n",
    "\n",
    "pipeline.transform(iris_df).select('pca_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.stages[-1].explainedVariance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
