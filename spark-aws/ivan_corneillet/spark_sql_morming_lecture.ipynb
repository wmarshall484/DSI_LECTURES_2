{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Spark SQL?\n",
    "\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "\n",
    "- Schemas enable using column names instead of column positions\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "- Schemas make your data more structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "What are the cons of schemas?\n",
    "</summary>\n",
    "1. Schemas make your data more structured.\n",
    "<br>\n",
    "2. They make things less flexible.\n",
    "<br>\n",
    "3. Y2K was a schema-problem.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark SQL\n",
    "\n",
    "How can I start using Spark SQL?\n",
    "\n",
    "- Create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10adfb910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.HiveContext at 0x10ae09990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = pyspark.HiveContext(sc)\n",
    "\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between SparkContext and HiveContext?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs, DataFrames, and schemas\n",
    "\n",
    "What is a DataFrame?\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as RDDs with schema. \n",
    "\n",
    "What is a schema?\n",
    "\n",
    "- Schemas are metadata about your data.\n",
    "- Schemas define table names, column names, and column types over your data.\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs, instead of using column positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using CSV\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID,Date,Store,State,Product,Amount\r\n",
      "101,11/13/2014,100,WA,331,300.00\r\n",
      "104,11/18/2014,700,OR,329,450.00\r\n",
      "102,11/15/2014,203,CA,321,200.00\r\n",
      "106,11/19/2014,202,CA,331,330.00\r\n",
      "103,11/17/2014,101,WA,373,750.00\r\n",
      "105,11/19/2014,202,CA,321,200.00"
     ]
    }
   ],
   "source": [
    "!cat sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda (id, date, store, state, product, amount):\\\n",
    "        (int(id), date, int(store), state, int(product), float(amount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('store', IntegerType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('product', IntegerType(), True),\n",
    "    StructField('amount', FloatType(), True)\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: string, store: int, state: string, product: int, amount: float]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split()`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL With DataFrames\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sqlContext.sql('''\n",
    "SELECT state, amount\n",
    "    FROM sales\n",
    "    WHERE amount > 100\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the results using `show()` or `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'WA', amount=300.0),\n",
       " Row(state=u'OR', amount=450.0),\n",
       " Row(state=u'CA', amount=200.0),\n",
       " Row(state=u'CA', amount=330.0),\n",
       " Row(state=u'WA', amount=750.0),\n",
       " Row(state=u'CA', amount=200.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "If I run `result.collect()` twice how many times will the data be read from disk?\n",
    "</summary>\n",
    "1. RDDs are lazy.<br>\n",
    "2. Therefore the data will be read twice.<br>\n",
    "3. Unless you cache the RDD, all transformations in the RDD will execute on each action.<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Tables\n",
    "\n",
    "How can I cache the RDD for a table to avoid roundtrips to disk on each action?\n",
    "\n",
    "- Use `cacheTable()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is particularly useful if you are using Spark SQL to explore data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results\n",
    "\n",
    "How can I save the results back out to the file system?\n",
    "\n",
    "(first, make sure the files don't exist...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json high-sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either write them out using the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or you can save them as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.write.parquet('high-sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 ivan  staff  233 Sep 22 09:24 sales.csv\n",
      "sales.csv\n",
      "#ID,Date,Store,State,Product,Amount\n",
      "101,11/13/2014,100,WA,331,300.00\n",
      "104,11/18/2014,700,OR,329,450.00\n",
      "102,11/15/2014,203,CA,321,200.00\n",
      "106,11/19/2014,202,CA,331,330.00\n",
      "103,11/17/2014,101,WA,373,750.00\n",
      "105,11/19/2014,202,CA,321,200.00"
     ]
    }
   ],
   "source": [
    "!ls -l sales.csv\n",
    "!echo sales.csv\n",
    "!cat sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "-rwxrwxrwx  1 ivan  staff   0 Sep 22 09:24 \u001b[31m_SUCCESS\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  90 Sep 22 09:24 \u001b[31mpart-00000\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  90 Sep 22 09:24 \u001b[31mpart-00001\u001b[m\u001b[m\n",
      "high-sales.json/part-00000\n",
      "{\"state\":\"WA\",\"amount\":300.0}\n",
      "{\"state\":\"OR\",\"amount\":450.0}\n",
      "{\"state\":\"CA\",\"amount\":200.0}\n",
      "high-sales.json/part-00001\n",
      "{\"state\":\"CA\",\"amount\":330.0}\n",
      "{\"state\":\"WA\",\"amount\":750.0}\n",
      "{\"state\":\"CA\",\"amount\":200.0}\n"
     ]
    }
   ],
   "source": [
    "!ls -l high-sales.json\n",
    "!for i in high-sales.json/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\n",
      "-rwxrwxrwx  1 ivan  staff    0 Sep 22 09:24 \u001b[31m_SUCCESS\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  286 Sep 22 09:24 \u001b[31m_common_metadata\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  736 Sep 22 09:24 \u001b[31m_metadata\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  523 Sep 22 09:24 \u001b[31mpart-r-00000-21d3b2ea-6ea5-4265-9178-0925b6ad2b47.gz.parquet\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 ivan  staff  559 Sep 22 09:24 \u001b[31mpart-r-00001-21d3b2ea-6ea5-4265-9178-0925b6ad2b47.gz.parquet\u001b[m\u001b[m\n",
      "high-sales.parquet/part-r-00000-21d3b2ea-6ea5-4265-9178-0925b6ad2b47.gz.parquet\n",
      "PAR1\u0015\u0000\u00150\u0015L,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg\u0002�� �?\bD:;\u0002\u0000���\u0018\u0000\u0000\u0000\u0015\u0000\u0015$\u0015H,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg`�����\u0010�=�\u0001�I]\u0012\u0000\u0000\u0000\u0015\u0002\u0019<H\u0004root\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\u0006\u0000\b\u0019\u0018\u0005state\u0015\u0004\u0016\u0006\u0016j\u0016�\u0001&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\b\u00195\u0006\u0000\b\u0019\u0018\u0006amount\u0015\u0004\u0016\u0006\u0016f\u0016�\u0001&�\u0001<\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018\u0018parquet-mr version 1.6.0\u0000w\u0001\u0000\u0000PAR1high-sales.parquet/part-r-00001-21d3b2ea-6ea5-4265-9178-0925b6ad2b47.gz.parquet\n",
      "PAR1\u0015\u0004\u0015\u0018\u0015<L\u0015\u0004\u0015\u0004\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb``pvd\u0002��\u0000��ik\f",
      "\u0000\u0000\u0000\u0015\u0000\u0015\u0012\u0015:,\u0015\u0006\u0015\u0004\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fgdf\u0002\u0000�&i�\t\u0000\u0000\u0000\u0015\u0000\u0015$\u0015L,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg`X���`�����\f",
      "\u0000(O\u001e",
      "W\u0012\u0000\u0000\u0000\u0015\u0002\u0019<H\u0004root\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\u0004\u0006\b\u0019\u0018\u0005state\u0015\u0004\u0016\u0006\u0016~\u0016�\u0001&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\b\u00195\u0006\u0000\b\u0019\u0018\u0006amount\u0015\u0004\u0016\u0006\u0016f\u0016�\u0001&�\u0001<\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018\u0018parquet-mr version 1.6.0\u0000w\u0001\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "!ls -l high-sales.parquet \n",
    "!for i in high-sales.parquet/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using JSON Data\n",
    "\n",
    "What is JSON-formatted data?\n",
    "\n",
    "- In Spark the JSON format means that each line is a JSON document.\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and read using `textFile()`.\n",
    "- JSON works well with Spark SQL because the data has an embedded schema.\n",
    "\n",
    "What other formats are supported by Spark SQL?\n",
    "\n",
    "- Spark SQL also supports Parquet, which is a compact binary format for big data.\n",
    "- If your data is in CSV then you have to add the schema programmatically after you load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing JSON Data\n",
    "\n",
    "How can I read JSON input and put it into a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\r\n",
      "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\r\n",
      "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\r\n",
      "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\r\n",
      "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\r\n",
      "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
     ]
    }
   ],
   "source": [
    "!cat sales.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json('sales.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[amount: double, date: string, id: bigint, product: bigint, state: string, store: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON is self-describing and does not require defining a schema.\n",
    "\n",
    "How can inspect my DataFrame?\n",
    "\n",
    "- Use `show()` to look at the first 20 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to look at a 50% sample of the DataFrame (without\n",
    "  replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount,DoubleType,true),StructField(date,StringType,true),StructField(id,LongType,true),StructField(product,LongType,true),StructField(state,StringType,true),StructField(store,LongType,true)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(amount,DoubleType,true),\n",
       " StructField(date,StringType,true),\n",
       " StructField(id,LongType,true),\n",
       " StructField(product,LongType,true),\n",
       " StructField(state,StringType,true),\n",
       " StructField(store,LongType,true)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, amount: string, id: string, product: string, store: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Methods\n",
    "\n",
    "How can I slice the DataFrame by column and by row?\n",
    "\n",
    "- DataFrames provide a *Pandas*-like API for manipulating data.\n",
    "- To select specific columns use `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', 'amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', df.amount + 100).show()\n",
    "\n",
    "# (or)\n",
    "df.select('state', df['amount'] + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount < 300)|\n",
      "+-----+--------------+\n",
      "|   WA|         false|\n",
      "|   OR|         false|\n",
      "|   CA|          true|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|          true|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount = 300)|\n",
      "+-----+--------------+\n",
      "|   WA|          true|\n",
      "|   OR|         false|\n",
      "|   CA|         false|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|         false|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', df.amount < 300).show()\n",
    "\n",
    "df.select('state', df.amount == 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   OR|    1|\n",
      "|   CA|    3|\n",
      "|   WA|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', 'amount').groupBy('state').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|102|\n",
      "|106|\n",
      "|105|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state == 'CA').select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|104|\n",
      "|106|\n",
      "|103|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('sales')\n",
    "\n",
    "sqlContext.sql('''\n",
    "SELECT id\n",
    "    FROM sales\n",
    "    WHERE amount > 300\n",
    "''')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert DataFrames to regular RDDs?\n",
    "\n",
    "- DataFrames are also RDDs.\n",
    "- You can use `map()` to iterate over the rows of the DataFrame.\n",
    "- You can access the values in a row using field names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300.0, 450.0, 200.0, 330.0, 750.0, 200.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.map(lambda row: row.amount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use `collect()` or `take()` to pull DataFrame rows into\n",
    "  the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date=u'11/13/2014', id=101, product=331, state=u'WA', store=100),\n",
       " Row(amount=450.0, date=u'11/18/2014', id=104, product=329, state=u'OR', store=700),\n",
       " Row(amount=200.0, date=u'11/15/2014', id=102, product=321, state=u'CA', store=203),\n",
       " Row(amount=330.0, date=u'11/19/2014', id=106, product=331, state=u'CA', store=202),\n",
       " Row(amount=750.0, date=u'11/17/2014', id=103, product=373, state=u'WA', store=101),\n",
       " Row(amount=200.0, date=u'11/19/2014', id=105, product=321, state=u'CA', store=202)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert Spark DataFrames to Pandas data frames?\n",
    "\n",
    "- Use `toPandas()` to convert Spark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>product</th>\n",
       "      <th>state</th>\n",
       "      <th>store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>331</td>\n",
       "      <td>WA</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450.0</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>104</td>\n",
       "      <td>329</td>\n",
       "      <td>OR</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>102</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>330.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>106</td>\n",
       "      <td>331</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750.0</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>103</td>\n",
       "      <td>373</td>\n",
       "      <td>WA</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>105</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount        date   id  product state  store\n",
       "0   300.0  11/13/2014  101      331    WA    100\n",
       "1   450.0  11/18/2014  104      329    OR    700\n",
       "2   200.0  11/15/2014  102      321    CA    203\n",
       "3   330.0  11/19/2014  106      331    CA    202\n",
       "4   750.0  11/17/2014  103      373    WA    101\n",
       "5   200.0  11/19/2014  105      321    CA    202"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "\n",
    "print type(pandas_df)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON vs CSV vs Parquet \n",
    "\n",
    "What are the pros and cons of JSON vs CSV vs Parquet?\n",
    "\n",
    "Feature | JSON | CSV | Parquet\n",
    "---|---|---|---\n",
    "Human-Readable | Yes | Yes | No\n",
    "Compact | No | Moderately | Highly\n",
    "Columnar | No | No | Yes\n",
    "Self-Describing | Yes | No | Yes\n",
    "Requires Schema | No | Yes | No\n",
    "Splittable | Yes | Yes | Yes\n",
    "Popular | No | Yes | Not yet\n",
    "\n",
    "What are columnar data formats?\n",
    "\n",
    "- Columnar data formats store data column-wise.\n",
    "- This allows them to do run-length encoding (RLE).\n",
    "- Instead of storing `San Francisco` 100 times, they will just store it once and the count of how many times it occurs.\n",
    "- When the data is repetitive and redundant as unstructured big data tends to be, columnar data formats use up a fraction of the disk space of non-columnar formats.\n",
    "\n",
    "What are splittable data formats?\n",
    "\n",
    "- On big data systems data is stored in blocks.\n",
    "- For example, on HDFS data is stored in 64/128 MB blocks.\n",
    "- Splittable data formats enable records in a block to be processed without looking at the entire file.\n",
    "\n",
    "What are some examples of a non-splittable data format?\n",
    "\n",
    "- Gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "\n",
    "How can I create my own User-Defined Functions?\n",
    "\n",
    "- Import the types (e.g. StringType, IntegerType, FloatType) that we are returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a UDF to calculate sales tax of 10% on the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_tax(amount):\n",
    "    return amount * 1.10\n",
    "\n",
    "sqlContext.registerFunction('add_tax', add_tax, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+--------+\n",
      "|amount|      date| id|product|state|store|with_tax|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|   330.0|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|   495.0|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|   220.0|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|   363.0|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|   825.0|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|   220.0|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT *, add_tax(amount) AS with_tax FROM sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optional last argument of `registerFunction` is function return\n",
    "  type; default is `StringType`.\n",
    "\n",
    "- UDFs can single or multiple arguments. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
