{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML\n",
    "\n",
    "## Morning Objectives\n",
    "\n",
    "At the end of this lecture you should be able to:\n",
    "\n",
    "1. Be able to describe the Spark ML API, and recognize differences with sklearn\n",
    "1. Chain Spark `Dataframe` methods together to do data munging\n",
    "1. Chain Spark ML `Transformers` and `Estimators` together to compose ML `Pipeline`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, RegexTokenizer, HashingTF, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Let's design chains of transformations together!\n",
    "\n",
    "### A.1. Computing sales per state\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df = sqlContext.read.csv('data/sales.csv',\n",
    "    header=True,      # use headers\n",
    "    quote='\"',        # use \" for quoting\n",
    "    sep=',',          # use , for separating fields\n",
    "    inferSchema=True) # infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(#ID,IntegerType,true),StructField(Date,StringType,true),StructField(Store,IntegerType,true),StructField(State,StringType,true),StructField(Product,IntegerType,true),StructField(Amount,DoubleType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "You want to obtain a sorted `DataFrame` of the states in which you have most sales done (`Amount`).  (i.e., by decreasing order of sales)\n",
    "\n",
    "1. What transformations do you need to apply?\n",
    "2. What if you had to draw a workflow of the transformations to apply?\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|State| Money|\n",
      "+-----+------+\n",
      "|   WA|1050.0|\n",
      "|   CA| 730.0|\n",
      "|   OR| 450.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(sales_df.groupBy(sales_df.State)\n",
    "    .agg(F.sum(sales_df.Amount).alias('Money'))\n",
    "    .orderBy('Money', ascending=False)\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Find the date on which AAPL's closing stock price was the highest\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl_df = sqlContext.read.csv('data/aapl.csv',\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    sep=',',\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aapl_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Now, design a pipeline that will:\n",
    "\n",
    "1. Keep only fields for Date and Close\n",
    "2. Order by Close in descending order\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Date|     Close|\n",
      "+--------------------+----------+\n",
      "|2015-11-03 00:00:...|    122.57|\n",
      "|2015-11-04 00:00:...|     122.0|\n",
      "|2015-11-02 00:00:...|    121.18|\n",
      "|2015-11-06 00:00:...|121.059998|\n",
      "|2015-11-05 00:00:...|120.919998|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(aapl_df.select('Date', 'Close')\n",
    "    .orderBy('Close', ascending=False)\n",
    "    .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Supervised Machine Learning on DataFrames\n",
    "\n",
    "- (http://spark.apache.org/docs/latest/ml-features.html)\n",
    "\n",
    "### Question: What is the difference between `aapl_df` and `vector_df` after running the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Close'], outputCol='features')\n",
    "\n",
    "vector_df = assembler.transform(aapl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "StructType(List(StructField(Date,TimestampType,true),StructField(Open,DoubleType,true),StructField(High,DoubleType,true),StructField(Low,DoubleType,true),StructField(Close,DoubleType,true),StructField(Volume,IntegerType,true),StructField(Adj Close,DoubleType,true)))\n",
      "\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print type(aapl_df)\n",
    "print\n",
    "print aapl_df.schema\n",
    "print\n",
    "aapl_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "StructType(List(StructField(Date,TimestampType,true),StructField(Open,DoubleType,true),StructField(High,DoubleType,true),StructField(Low,DoubleType,true),StructField(Close,DoubleType,true),StructField(Volume,IntegerType,true),StructField(Adj Close,DoubleType,true),StructField(features,VectorUDT,true)))\n",
      "\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------------+\n",
      "|                Date|      Open|      High|       Low|     Close|  Volume| Adj Close|    features|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------------+\n",
      "|2016-10-25 00:00:...|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|    [118.25]|\n",
      "|2016-10-24 00:00:...|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|[117.650002]|\n",
      "|2016-10-21 00:00:...|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|[116.599998]|\n",
      "|2016-10-20 00:00:...|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|[117.059998]|\n",
      "|2016-10-19 00:00:...|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|[117.120003]|\n",
      "+--------------------+----------+----------+----------+----------+--------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print type(vector_df)\n",
    "print\n",
    "print vector_df.schema\n",
    "print\n",
    "vector_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up: Why does this difference matter?\n",
    "\n",
    "Let's try to run one of Spark ML's built-in transformers on some of our data.  Let's min-max scale the `Close` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Column Close must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c6ce07ae55a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Scaled Close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maapl_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaled_close_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maapl_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaled_close_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'requirement failed: Column Close must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol='Close', outputCol='Scaled Close').fit(aapl_df)\n",
    "\n",
    "scaled_close_df = scaler.transform(aapl_df)\n",
    "\n",
    "scaled_close_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(Close,DoubleType,true)\n",
      "StructField(features,VectorUDT,true)\n"
     ]
    }
   ],
   "source": [
    "print aapl_df.schema['Close']\n",
    "print vector_df.schema['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway: Gotta have the column as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|    features|     scaled_features|\n",
      "+------------+--------------------+\n",
      "|    [118.25]| [0.865963404782699]|\n",
      "|[117.650002]|[0.8473472730564975]|\n",
      "|[116.599998]|[0.8147688098332226]|\n",
      "|[117.059998]|[0.8290412250646944]|\n",
      "|[117.120003]|[0.8309029995776607]|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features').fit(vector_df)\n",
    "\n",
    "scaled_features_df = scaler.transform(vector_df)\n",
    "\n",
    "scaled_features_df.select('features', 'scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "The `VectorAssembler` class above is an example of a generic type in Spark, called a [Transformer](http://spark.apache.org/docs/latest/ml-pipeline.html#transformers).  Important things to know about this type:\n",
    "\n",
    "- They implement a `transform` method\n",
    "- They convert one `DataFrame` into another, usually by adding columns\n",
    "\n",
    "Examples of transformers: [`VectorAssembler`](http://spark.apache.org/docs/latest/ml-features.html#vectorassembler), [`Tokenizer`](http://spark.apache.org/docs/latest/ml-features.html#tokenizer), [`StopWordsRemover`](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover), and [many more](http://spark.apache.org/docs/latest/ml-features.html)\n",
    "\n",
    "## Estimators\n",
    "\n",
    "According to the documentation: \"An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data\".  Important things to know about them:\n",
    "\n",
    "- They implement a `fit` method whose argument is a `DataFrame`\n",
    "- The output of `fit` is another type called `Model`, which is a `Transformer`\n",
    "\n",
    "Examples of estimators: [`LogisticRegression`](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression), [`DecisionTreeRegressor`](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression), and [many more](http://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "Many Data Science workflows can be described as sequential application of various `Transforms` and `Estimators`.\n",
    "\n",
    "![http://spark.apache.org/docs/latest/img/ml-Pipeline.png](http://spark.apache.org/docs/latest/img/ml-Pipeline.png)\n",
    "\n",
    "Let's see two ways to implement the above flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training set from a list of (id, text, label) tuples\n",
    "\n",
    "training_df = spark.createDataFrame([(0, 'spark is like hadoop mapreduce', 1.0),\n",
    "        (1, 'sparks light fire!!!', 0.0),\n",
    "        (2, 'elephants like simba', 0.0),\n",
    "        (3, 'hadoop is an elephant', 1.0),\n",
    "        (4, 'hadoop mapreduce', 1.0)],\n",
    "    ['id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='tokens', pattern='\\\\W')\n",
    "tokens_df = tokenizer.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|              tokens|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0|spark is like had...|  1.0|[spark, is, like,...|\n",
      "|  1|sparks light fire!!!|  0.0|[sparks, light, f...|\n",
      "|  2|elephants like simba|  0.0|[elephants, like,...|\n",
      "|  3|hadoop is an elep...|  1.0|[hadoop, is, an, ...|\n",
      "|  4|    hadoop mapreduce|  1.0| [hadoop, mapreduce]|\n",
      "+---+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = HashingTF(inputCol='tokens', outputCol='features')\n",
    "tf_df = tf.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "| id|                text|label|              tokens|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0|spark is like had...|  1.0|[spark, is, like,...|(262144,[15889,42...|\n",
      "|  1|sparks light fire!!!|  0.0|[sparks, light, f...|(262144,[34036,91...|\n",
      "|  2|elephants like simba|  0.0|[elephants, like,...|(262144,[23518,54...|\n",
      "|  3|hadoop is an elep...|  1.0|[hadoop, is, an, ...|(262144,[15889,15...|\n",
      "|  4|    hadoop mapreduce|  1.0| [hadoop, mapreduce]|(262144,[42633,15...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(maxIter=10, regParam=.001).fit(tf_df)\n",
    "# (uses columns named features/label by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------------------+\n",
      "|              text|prediction|         probability|\n",
      "+------------------+----------+--------------------+\n",
      "| simba has a spark|       0.0|[0.78779795057740...|\n",
      "|            hadoop|       1.0|[0.02996000405249...|\n",
      "|mapreduce in spark|       1.0|[0.02396543994089...|\n",
      "|     apache hadoop|       1.0|[0.02996000405249...|\n",
      "+------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare test set, which are unlabeled (id, text) tuples\n",
    "\n",
    "test_df = spark.createDataFrame([(5, 'simba has a spark'),\n",
    "        (6, 'hadoop'),\n",
    "        (7, 'mapreduce in spark'),\n",
    "        (8, 'apache hadoop')],\n",
    "    ['id', 'text'])\n",
    "\n",
    "# What do we need to do to this to get prediction on our test set?\n",
    "\n",
    "predictions_df = model.transform(tf.transform(tokenizer.transform(test_df)))\n",
    "predictions_df.select('text', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, configure a ML pipeline, which consists of three stages: tokenizer, tf, and lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='tokens', pattern='\\\\W')\n",
    "tf = HashingTF(inputCol='tokens', outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, tf, lr])\n",
    "\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------------------+\n",
      "|              text|prediction|         probability|\n",
      "+------------------+----------+--------------------+\n",
      "| simba has a spark|       0.0|[0.78779795057740...|\n",
      "|            hadoop|       1.0|[0.02996000405249...|\n",
      "|mapreduce in spark|       1.0|[0.02396543994089...|\n",
      "|     apache hadoop|       1.0|[0.02996000405249...|\n",
      "+------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What do we need to do to this to get prediction on our test set?\n",
    "\n",
    "predictions_df = model.transform(test_df)\n",
    "predictions_df.select('text', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Unsupervised Machine Learning on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read csv\n",
    "iris_df = sqlContext.read.csv('data/iris.csv',\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    sep=',',\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "|              5.1|             3.5|              1.4|             0.2|\n",
      "|              4.9|             3.0|              1.4|             0.2|\n",
      "|              4.7|             3.2|              1.3|             0.2|\n",
      "|              4.6|             3.1|              1.5|             0.2|\n",
      "|              5.0|             3.6|              1.4|             0.2|\n",
      "|              5.4|             3.9|              1.7|             0.4|\n",
      "|              4.6|             3.4|              1.4|             0.3|\n",
      "|              5.0|             3.4|              1.5|             0.2|\n",
      "|              4.4|             2.9|              1.4|             0.2|\n",
      "|              4.9|             3.1|              1.5|             0.1|\n",
      "|              5.4|             3.7|              1.5|             0.2|\n",
      "|              4.8|             3.4|              1.6|             0.2|\n",
      "|              4.8|             3.0|              1.4|             0.1|\n",
      "|              4.3|             3.0|              1.1|             0.1|\n",
      "|              5.8|             4.0|              1.2|             0.2|\n",
      "|              5.7|             4.4|              1.5|             0.4|\n",
      "|              5.4|             3.9|              1.3|             0.4|\n",
      "|              5.1|             3.5|              1.4|             0.3|\n",
      "|              5.7|             3.8|              1.7|             0.3|\n",
      "|              5.1|             3.8|              1.5|             0.3|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        pca_features|\n",
      "+--------------------+\n",
      "|[-2.8271359726790...|\n",
      "|[-2.7959524821488...|\n",
      "|[-2.6215235581650...|\n",
      "|[-2.7649059004742...|\n",
      "|[-2.7827501159516...|\n",
      "|[-3.2314457367733...|\n",
      "|[-2.6904524156023...|\n",
      "|[-2.8848611044591...|\n",
      "|[-2.6233845324473...|\n",
      "|[-2.8374984110638...|\n",
      "|[-3.0048163084440...|\n",
      "|[-2.8982003795119...|\n",
      "|[-2.7239091217858...|\n",
      "|[-2.2861426515079...|\n",
      "|[-2.8677998808418...|\n",
      "|[-3.1274737739836...|\n",
      "|[-2.8888168946571...|\n",
      "|[-2.8630203653038...|\n",
      "|[-3.3122651363522...|\n",
      "|[-2.9239969088652...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "pipeline = (Pipeline(stages=[\n",
    "        VectorAssembler(inputCols=col_names, outputCol='features'),\n",
    "        PCA(k=3, inputCol='features', outputCol='pca_features')\n",
    "    ]).fit(iris_df))\n",
    "\n",
    "pipeline.transform(iris_df).select('pca_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.9246, 0.053, 0.0172])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.stages[-1].explainedVariance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
