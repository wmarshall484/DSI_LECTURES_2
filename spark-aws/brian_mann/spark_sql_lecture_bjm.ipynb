{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL\n",
    "=========\n",
    "\n",
    "Spark SQL\n",
    "---------\n",
    "\n",
    "What is Spark SQL?\n",
    "\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "\n",
    "- Schemas enable using column names instead of column positions\n",
    "\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "\n",
    "- Schemas make your data more structured.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What are the cons (pros?) of schemas?\n",
    "</summary>\n",
    "1. Schemas make your data more structured.\n",
    "<br>\n",
    "2. They make things more fragile.\n",
    "<br>\n",
    "3. Y2K was a schema-problem.\n",
    "</details>\n",
    "\n",
    "\n",
    "Start Spark SQL\n",
    "---------------\n",
    "\n",
    "How can I start using Spark SQL?\n",
    "\n",
    "- Create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x109a02b90>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.HiveContext object at 0x109a028d0>\n"
     ]
    }
   ],
   "source": [
    "sqlContext = pyspark.HiveContext(sc)\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of a HiveContext you can initialize `sqlContext` using\n",
    "  `pyspark.SqlContext(sc)`\n",
    "\n",
    "- However, this is less preferred.\n",
    "\n",
    "What is the difference between SparkContext and HiveContext?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data\n",
    "  sources.\n",
    "\n",
    "DataFrame, Schema, SchemaRDD\n",
    "----------------------------\n",
    "\n",
    "What is a DataFrame?\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "\n",
    "- Think of a DataFrames as RDDs with schema. \n",
    "\n",
    "What is a schema?\n",
    "\n",
    "- Schemas are metadata about your data.\n",
    "\n",
    "- Schemas define table names, column names, and column types over your\n",
    "  data.\n",
    "\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs,\n",
    "  instead of using column positions.\n",
    "\n",
    "What is a SchemaRDD?\n",
    "\n",
    "- Spark 1.3 introduced the concept of a DataFrame as the primary SQL\n",
    "  abstraction.\n",
    "\n",
    "- Before Spark 1.3 DataFrames were called SchemaRDD.\n",
    "\n",
    "- Some of the DataFrame syntax will require using Spark 1.3 or later.\n",
    "\n",
    "- Watch out for syntax changes.\n",
    "\n",
    "- We will use the term DataFrame to refer to both SchemaRDDs and\n",
    "  DataFrames.\n",
    "\n",
    "Spark SQL Using CSV\n",
    "-------------------\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?\n",
    "\n",
    "- Make sure the CSV exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda \\\n",
    "      (id,date,store,state,product,amount):\\\n",
    "      (int(id),date,int(store),state,int(product),float(amount)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the DataFrame object. Note: This will only work with Spark\n",
    "  1.3 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are\n",
    "processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split()`\n",
    "</details>\n",
    "\n",
    "## Why SQL?\n",
    "\n",
    "Suppose I wanted to get the states (and amounts) where the amount is > $300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 300.0), (u'OR', 450.0), (u'CA', 330.0), (u'WA', 750.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x[5] >= 300.0).map(lambda x: (x[3], x[5])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more readable to do something like\n",
    "(especially if you're familiar with SQL)\n",
    "\n",
    "```\n",
    "SELECT state, amount\n",
    "FROM sales\n",
    "WHERE amount > 300\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL With DataFrames\n",
    "-------------------------\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\n",
    "    'SELECT state, amount FROM sales WHERE amount >= 300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'sales']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the results using `show()` or `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(state=u'WA', amount=300.0),\n",
       " Row(state=u'OR', amount=450.0),\n",
       " Row(state=u'CA', amount=330.0),\n",
       " Row(state=u'WA', amount=750.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.show()\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "If I run `result.collect()` twice how many times will the data be read\n",
    "from disk?\n",
    "</summary>\n",
    "1. RDDs are lazy.<br>\n",
    "2. Therefore the data will be read twice.<br>\n",
    "3. Unless you cache the RDD, All transformations in the RDD will\n",
    "execute on each action.<br>\n",
    "</details>\n",
    "\n",
    "Caching Tables\n",
    "--------------\n",
    "\n",
    "How can I cache the RDD for a table to avoid roundtrips to disk on\n",
    "each action?\n",
    "\n",
    "- Use `cacheTable()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable('sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is particularly useful if you are using Spark SQL to explore\n",
    "  data.\n",
    "\n",
    "Saving Results\n",
    "--------------\n",
    "\n",
    "How can I save the results back out to the file system?\n",
    "\n",
    "- Make sure the files do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json high-sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either write them out using the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or you can save them as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.write.parquet('high-sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l sales.csv high-sales.json high-sales.parquet \n",
    "!for i in high-sales.json/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Using JSON Data\n",
    "-------------------------\n",
    "\n",
    "What is JSON-formatted data?\n",
    "\n",
    "- In Spark the JSON format means that each line is a JSON document.\n",
    "\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and\n",
    "  read using `textFile()`.\n",
    "\n",
    "- JSON works well with Spark SQL because the data has an embedded\n",
    "  schema.\n",
    "\n",
    "What other formats are supported by Spark SQL?\n",
    "\n",
    "- Spark SQL also supports Parquet, which is a compact binary format\n",
    "  for big data.\n",
    "\n",
    "- If your data is in CSV then you have to add the schema\n",
    "  programmatically after you load the data.\n",
    "\n",
    "Parsing JSON Data\n",
    "-----------------\n",
    "\n",
    "How can I read JSON input and put it into a DataFrame?\n",
    "\n",
    "- First make sure the file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales = sqlContext.read.json('sales.json')\n",
    "\n",
    "# OR\n",
    "\n",
    "#sales_json_rdd = sc.textFile('sales.json')\n",
    "#sales = sqlContext.jsonRDD(sales_json_rdd)\n",
    "#sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON is self-describing and does not require defining a schema.\n",
    "\n",
    "How can inspect my DataFrame?\n",
    "\n",
    "- Use `show()` to look at the first 20 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to look at a 50% sample of the DataFrame (without\n",
    "  replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selects each row with a probability of 0.5\n",
    "sales.sample(False, 0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(amount,DoubleType,true),StructField(date,StringType,true),StructField(id,LongType,true),StructField(product,LongType,true),StructField(state,StringType,true),StructField(store,LongType,true)))\n",
      "--\n",
      "[StructField(amount,DoubleType,true), StructField(date,StringType,true), StructField(id,LongType,true), StructField(product,LongType,true), StructField(state,StringType,true), StructField(store,LongType,true)]\n",
      "--\n",
      "DataFrame[summary: string, amount: string, id: string, product: string, store: string]\n",
      "--\n",
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print sales.schema\n",
    "print '--'\n",
    "print sales.schema.fields\n",
    "print '--'\n",
    "print sales.describe()\n",
    "print '--'\n",
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Methods\n",
    "-----------------\n",
    "\n",
    "How can I slice the DataFrame by column and by row?\n",
    "\n",
    "- DataFrames provide a *Pandas*-like API for manipulating data.\n",
    "\n",
    "- To select specific columns use `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state','amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state',sales.amount+100).show()\n",
    "sales.select('state',sales['amount']+100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|state|adjusted_amount|\n",
      "+-----+---------------+\n",
      "|   WA|          400.0|\n",
      "|   OR|          550.0|\n",
      "|   CA|          300.0|\n",
      "|   CA|          430.0|\n",
      "|   WA|          850.0|\n",
      "|   CA|          300.0|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state',sales.amount+100).withColumnRenamed('(amount + 100)', 'adjusted_amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount < 300)|\n",
      "+-----+--------------+\n",
      "|   WA|         false|\n",
      "|   OR|         false|\n",
      "|   CA|          true|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|          true|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount = 300)|\n",
      "+-----+--------------+\n",
      "|   WA|          true|\n",
      "|   OR|         false|\n",
      "|   CA|         false|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|         false|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state',sales.amount<300).show()\n",
    "sales.select('state',sales.amount == 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|state|       AVG(amount)|\n",
      "+-----+------------------+\n",
      "|   OR|             450.0|\n",
      "|   CA|243.33333333333334|\n",
      "|   WA|             525.0|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state','amount').groupBy('state').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.filter(sales.state == 'CA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n",
      "+-----+------------------+\n",
      "|state|               _c1|\n",
      "+-----+------------------+\n",
      "|   OR|             450.0|\n",
      "|   CA|243.33333333333334|\n",
      "|   WA|             525.0|\n",
      "+-----+------------------+\n",
      "\n",
      "+----------+-----+\n",
      "|      date|  _c1|\n",
      "+----------+-----+\n",
      "|11/15/2014|200.0|\n",
      "|11/19/2014|265.0|\n",
      "|11/18/2014|450.0|\n",
      "|11/13/2014|300.0|\n",
      "|11/17/2014|750.0|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.registerTempTable('sales')\n",
    "sqlContext.sql('select * from sales where amount > 300').show()\n",
    "sqlContext.sql('select state, AVG(amount) from sales GROUP BY state').show()\n",
    "sqlContext.sql('select date, AVG(amount) from sales GROUP BY date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert DataFrames to regular RDDs?\n",
    "\n",
    "- DataFrames are also RDDs.\n",
    "\n",
    "- You can use `map()` to iterate over the rows of the DataFrame.\n",
    "\n",
    "- You can access the values in a row using field names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[160] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300.0, 450.0, 200.0, 330.0, 750.0, 200.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.map(lambda row: row.amount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use `collect()` or `take()` to pull DataFrame rows into\n",
    "  the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date=u'11/13/2014', id=101, product=331, state=u'WA', store=100),\n",
       " Row(amount=450.0, date=u'11/18/2014', id=104, product=329, state=u'OR', store=700),\n",
       " Row(amount=200.0, date=u'11/15/2014', id=102, product=321, state=u'CA', store=203),\n",
       " Row(amount=330.0, date=u'11/19/2014', id=106, product=331, state=u'CA', store=202),\n",
       " Row(amount=750.0, date=u'11/17/2014', id=103, product=373, state=u'WA', store=101),\n",
       " Row(amount=200.0, date=u'11/19/2014', id=105, product=321, state=u'CA', store=202)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert Spark DataFrames to Pandas data frames?\n",
    "\n",
    "- Use `toPandas()` to convert Spark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   amount        date   id  product state  store\n",
      "0     300  11/13/2014  101      331    WA    100\n",
      "1     450  11/18/2014  104      329    OR    700\n",
      "2     200  11/15/2014  102      321    CA    203\n",
      "3     330  11/19/2014  106      331    CA    202\n",
      "4     750  11/17/2014  103      373    WA    101\n",
      "5     200  11/19/2014  105      321    CA    202\n"
     ]
    }
   ],
   "source": [
    "x = sales.toPandas()\n",
    "print type(x)\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON vs CSV vs Parquet \n",
    "----------------------\n",
    "\n",
    "What are the pros and cons of JSON vs CSV vs Parquet?\n",
    "\n",
    "Feature            |JSON               |CSV            |Parquet\n",
    "-------            |----               |---            |-------\n",
    "Human-Readable     |Yes                |Yes            |No\n",
    "Compact            |No                 |Moderately     |Highly\n",
    "Columnar           |No                 |No             |Yes\n",
    "Self-Describing    |Yes                |No             |Yes\n",
    "Requires Schema    |No                 |Yes            |No\n",
    "Splittable         |Yes                |Yes            |Yes\n",
    "Popular            |No                 |Yes            |Not yet\n",
    "\n",
    "What are columnar data formats?\n",
    "\n",
    "- Columnar data formats store data column-wise.\n",
    "\n",
    "- This allows them to do RLE or run-length encoding.\n",
    "\n",
    "- Instead of storing `San Francisco` 100 times, they will just store\n",
    "  it once and the count of how many times it occurs.\n",
    "\n",
    "- When the data is repetitive and redundant as unstructured big data\n",
    "  tends to be, columnar data formats use up a fraction of the disk\n",
    "  space of non-columnar formats.\n",
    "\n",
    "What are splittable data formats?\n",
    "\n",
    "- On big data systems data is stored in blocks.\n",
    "\n",
    "- For example, on HDFS data is stored in 128 MB blocks.\n",
    "\n",
    "- Splittable data formats enable records in a block to be processed\n",
    "  without looking at the entire file.\n",
    "\n",
    "What are some examples of a non-splittable data format?\n",
    "\n",
    "- Gzip\n",
    "\n",
    "User Defined Functions\n",
    "----------------------\n",
    "\n",
    "How can I create my own User-Defined Functions?\n",
    "\n",
    "- Import the types (e.g. StringType, IntegerType, FloatType) that we\n",
    "  are returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a UDF to calculate sales tax of 10% on the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_tax(amount):\n",
    "    return amount * 1.10\n",
    "\n",
    "sqlContext.registerFunction(\"add_tax\", add_tax, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+--------+\n",
      "|amount|      date| id|product|state|store|with_tax|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|   330.0|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|   495.0|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|   220.0|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|   363.0|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|   825.0|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|   220.0|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT *, add_tax(amount) AS with_tax FROM sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optional last argument of `registerFunction` is function return\n",
    "  type; default is `StringType`.\n",
    "\n",
    "- UDFs can single or multiple arguments. \n",
    "\n",
    "SQL Types\n",
    "---------\n",
    "\n",
    "How can I find out all the types that are available for SQL schemas\n",
    "and UDF?\n",
    "\n",
    "- In the IPython REPL type `import pyspark.sql.types`. \n",
    "\n",
    "- Then type `pyspark.sql.types.[TAB]`\n",
    "\n",
    "- Autocomplete will show you all the available types.\n",
    "\n",
    "Types          |Meaning\n",
    "-----          |-------\n",
    "StringType     |String\n",
    "IntegerType    |Int\n",
    "FloatType      |Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark.sql.types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
