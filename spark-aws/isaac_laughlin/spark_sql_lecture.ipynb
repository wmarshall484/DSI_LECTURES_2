{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "<h1 class=\"tocheading\">Spark SQL</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Spark SQL\n",
    "=========\n",
    "\n",
    "Spark SQL\n",
    "---------\n",
    "\n",
    "What is Spark SQL?\n",
    "\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "\n",
    "- Schemas enable using column names instead of column positions\n",
    "\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "\n",
    "- Schemas make your data more structured.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What are the cons of schemas?\n",
    "</summary>\n",
    "1. Schemas make your data more structured.\n",
    "<br>\n",
    "2. They make things more fragile.\n",
    "<br>\n",
    "3. Y2K was a schema-problem.\n",
    "</details>\n",
    "\n",
    "\n",
    "Start Spark SQL\n",
    "---------------\n",
    "\n",
    "How can I start using Spark SQL?\n",
    "\n",
    "- Create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x103917dd0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.HiveContext object at 0x1063ea050>\n"
     ]
    }
   ],
   "source": [
    "sqlContext = pyspark.HiveContext(sc)\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of a HiveContext you can initialize `sqlContext` using\n",
    "  `pyspark.SqlContext(sc)`\n",
    "\n",
    "- However, this is less preferred.\n",
    "\n",
    "What is the difference between SparkContext and HiveContext?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data\n",
    "  sources.\n",
    "\n",
    "DataFrame, Schema, SchemaRDD\n",
    "----------------------------\n",
    "\n",
    "What is a DataFrame?\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "\n",
    "- Think of a DataFrames as RDDs with schema. \n",
    "\n",
    "What is a schema?\n",
    "\n",
    "- Schemas are metadata about your data.\n",
    "\n",
    "- Schemas define table names, column names, and column types over your\n",
    "  data.\n",
    "\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs,\n",
    "  instead of using column positions.\n",
    "\n",
    "What is a SchemaRDD?\n",
    "\n",
    "- Spark 1.3 introduced the concept of a DataFrame as the primary SQL\n",
    "  abstraction.\n",
    "\n",
    "- Before Spark 1.3 DataFrames were called SchemaRDD.\n",
    "\n",
    "- Some of the DataFrame syntax will require using Spark 1.3 or later.\n",
    "\n",
    "- Watch out for syntax changes.\n",
    "\n",
    "- We will use the term DataFrame to refer to both SchemaRDDs and\n",
    "  DataFrames.\n",
    "\n",
    "Spark SQL Using CSV\n",
    "-------------------\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?\n",
    "\n",
    "- Make sure the CSV exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda \\\n",
    "      (id,date,store,state,product,amount):\\\n",
    "      (int(id),date,int(store),state,int(product),float(amount)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark.sql.types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the DataFrame object. Note: This will only work with Spark\n",
    "  1.3 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rdd,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If your version of Spark is earlier than 1.3 use the following\n",
    "  syntax instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:298: UserWarning: applySchema is deprecated, please use createDataFrame instead\n",
      "  warnings.warn(\"applySchema is deprecated, please use createDataFrame instead\")\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.applySchema(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The older syntax will work in Spark 1.3 and later as well, but it\n",
    "  will give you deprecation warnings.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are\n",
    "processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split()`\n",
    "</details>\n",
    "\n",
    "Using SQL With DataFrames\n",
    "-------------------------\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: string, store: int, state: string, product: int, amount: float]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10a77a150>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nix4Akvs/AyM40joNXJsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106790b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.toPandas().amount.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\n",
    "    'SELECT state,amount from sales where amount > 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the results using `show()` or `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(state=u'WA', amount=300.0),\n",
       " Row(state=u'OR', amount=450.0),\n",
       " Row(state=u'CA', amount=200.0),\n",
       " Row(state=u'CA', amount=330.0),\n",
       " Row(state=u'WA', amount=750.0),\n",
       " Row(state=u'CA', amount=200.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.show()\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "If I run `result.collect()` twice how many times will the data be read\n",
    "from disk?\n",
    "</summary>\n",
    "1. RDDs are lazy.<br>\n",
    "2. Therefore the data will be read twice.<br>\n",
    "3. Unless you cache the RDD, All transformations in the RDD will\n",
    "execute on each action.<br>\n",
    "</details>\n",
    "\n",
    "Caching Tables\n",
    "--------------\n",
    "\n",
    "How can I cache the RDD for a table to avoid roundtrips to disk on\n",
    "each action?\n",
    "\n",
    "- Use `cacheTable()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable('sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is particularly useful if you are using Spark SQL to explore\n",
    "  data.\n",
    "\n",
    "Saving Results\n",
    "--------------\n",
    "\n",
    "How can I save the results back out to the file system?\n",
    "\n",
    "- Make sure the files do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json high-sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either write them out using the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or you can save them as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.write.parquet('high-sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"state\":\"WA\",\"amount\":300.0}\r\n",
      "{\"state\":\"OR\",\"amount\":450.0}\r\n",
      "{\"state\":\"CA\",\"amount\":200.0}\r\n",
      "{\"state\":\"CA\",\"amount\":330.0}\r\n",
      "{\"state\":\"WA\",\"amount\":750.0}\r\n",
      "{\"state\":\"CA\",\"amount\":200.0}\r\n"
     ]
    }
   ],
   "source": [
    "!cat high-sales.json/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 isaac  staff  233 Jun  9 09:14 sales.csv\n",
      "\n",
      "high-sales.json:\n",
      "total 16\n",
      "-rwxrwxrwx  1 isaac  staff   0 Jun  9 09:31 \u001b[31m_SUCCESS\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  90 Jun  9 09:31 \u001b[31mpart-00000\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  90 Jun  9 09:31 \u001b[31mpart-00001\u001b[m\u001b[m\n",
      "\n",
      "high-sales.parquet:\n",
      "total 32\n",
      "-rwxrwxrwx  1 isaac  staff    0 Jun  9 09:31 \u001b[31m_SUCCESS\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  294 Jun  9 09:31 \u001b[31m_common_metadata\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  744 Jun  9 09:31 \u001b[31m_metadata\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  531 Jun  9 09:31 \u001b[31mpart-r-00000-76dd3d11-f1ee-4770-bb6e-9cac6b809aef.gz.parquet\u001b[m\u001b[m\n",
      "-rwxrwxrwx  1 isaac  staff  567 Jun  9 09:31 \u001b[31mpart-r-00001-76dd3d11-f1ee-4770-bb6e-9cac6b809aef.gz.parquet\u001b[m\u001b[m\n",
      "high-sales.parquet/part-r-00000-76dd3d11-f1ee-4770-bb6e-9cac6b809aef.gz.parquet\n",
      "PAR1\u0015\u0000\u00150\u0015L,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg\u0002�� �?\bD:;\u0002\u0000���\u0018\u0000\u0000\u0000\u0015\u0000\u0015$\u0015H,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg`�����\u0010�=�\u0001�I]\u0012\u0000\u0000\u0000\u0015\u0002\u0019<H\f",
      "spark_schema\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\u0006\u0000\b\u0019\u0018\u0005state\u0015\u0004\u0016\u0006\u0016j\u0016�\u0001&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\b\u00195\u0006\u0000\b\u0019\u0018\u0006amount\u0015\u0004\u0016\u0006\u0016f\u0016�\u0001&�\u0001<\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018\u0018parquet-mr version 1.6.0\u0000\u0001\u0000\u0000PAR1high-sales.parquet/part-r-00001-76dd3d11-f1ee-4770-bb6e-9cac6b809aef.gz.parquet\n",
      "PAR1\u0015\u0004\u0015\u0018\u0015<L\u0015\u0004\u0015\u0004\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb``pvd\u0002��\u0000��ik\f",
      "\u0000\u0000\u0000\u0015\u0000\u0015\u0012\u0015:,\u0015\u0006\u0015\u0004\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fgdf\u0002\u0000�&i�\t\u0000\u0000\u0000\u0015\u0000\u0015$\u0015L,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000cb```fg`X���`�����\f",
      "\u0000(O\u001e",
      "W\u0012\u0000\u0000\u0000\u0015\u0002\u0019<H\f",
      "spark_schema\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\u0004\u0006\b\u0019\u0018\u0005state\u0015\u0004\u0016\u0006\u0016~\u0016�\u0001&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\b\u00195\u0006\u0000\b\u0019\u0018\u0006amount\u0015\u0004\u0016\u0006\u0016f\u0016�\u0001&�\u0001<\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018\u0018parquet-mr version 1.6.0\u0000\u0001\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "!ls -l sales.csv high-sales.json high-sales.parquet \n",
    "!for i in high-sales.parquet/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Using JSON Data\n",
    "-------------------------\n",
    "\n",
    "What is JSON-formatted data?\n",
    "\n",
    "- In Spark the JSON format means that each line is a JSON document.\n",
    "\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and\n",
    "  read using `textFile()`.\n",
    "\n",
    "- JSON works well with Spark SQL because the data has an embedded\n",
    "  schema.\n",
    "\n",
    "What other formats are supported by Spark SQL?\n",
    "\n",
    "- Spark SQL also supports Parquet, which is a compact binary format\n",
    "  for big data.\n",
    "\n",
    "- If your data is in CSV then you have to add the schema\n",
    "  programmatically after you load the data.\n",
    "\n",
    "Parsing JSON Data\n",
    "-----------------\n",
    "\n",
    "How can I read JSON input and put it into a DataFrame?\n",
    "\n",
    "- First make sure the file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sqlContext.read.json('sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON is self-describing and does not require defining a schema.\n",
    "\n",
    "How can inspect my DataFrame?\n",
    "\n",
    "- Use `show()` to look at the first 20 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to look at a 50% sample of the DataFrame (without\n",
    "  replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount,DoubleType,true),StructField(date,StringType,true),StructField(id,LongType,true),StructField(product,LongType,true),StructField(state,StringType,true),StructField(store,LongType,true)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.sample(False,0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(amount,DoubleType,true),StructField(date,StringType,true),StructField(id,LongType,true),StructField(product,LongType,true),StructField(state,StringType,true),StructField(store,LongType,true)))\n",
      "--\n",
      "[StructField(amount,DoubleType,true), StructField(date,StringType,true), StructField(id,LongType,true), StructField(product,LongType,true), StructField(state,StringType,true), StructField(store,LongType,true)]\n",
      "--\n",
      "DataFrame[summary: string, amount: string, id: string, product: string, store: string]\n",
      "--\n",
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print sales.schema\n",
    "print '--'\n",
    "print sales.schema.fields\n",
    "print '--'\n",
    "print sales.describe()\n",
    "print '--'\n",
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Methods\n",
    "-----------------\n",
    "\n",
    "How can I slice the DataFrame by column and by row?\n",
    "\n",
    "- DataFrames provide a *Pandas*-like API for manipulating data.\n",
    "\n",
    "- To select specific columns use `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state','amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state',sales.amount+100).show()\n",
    "sales.select('state',sales['amount']+100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount < 300)|\n",
      "+-----+--------------+\n",
      "|   WA|         false|\n",
      "|   OR|         false|\n",
      "|   CA|          true|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|          true|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount = 300)|\n",
      "+-----+--------------+\n",
      "|   WA|          true|\n",
      "|   OR|         false|\n",
      "|   CA|         false|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|         false|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state',sales.amount<300).show()\n",
    "sales.select('state',sales.amount == 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   OR|    1|\n",
      "|   CA|    3|\n",
      "|   WA|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select('state','amount').groupBy('state').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|102|\n",
      "|106|\n",
      "|105|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.filter(sales.state == 'CA').select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|104|\n",
      "|106|\n",
      "|103|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.registerTempTable('sales')\n",
    "sqlContext.sql('select id from sales where amount > 300').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert DataFrames to regular RDDs?\n",
    "\n",
    "- DataFrames are also RDDs.\n",
    "\n",
    "- You can use `map()` to iterate over the rows of the DataFrame.\n",
    "\n",
    "- You can access the values in a row using field names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300.0, 450.0, 200.0, 330.0, 750.0, 200.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.map(lambda row: row.amount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use `collect()` or `take()` to pull DataFrame rows into\n",
    "  the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date=u'11/13/2014', id=101, product=331, state=u'WA', store=100),\n",
       " Row(amount=450.0, date=u'11/18/2014', id=104, product=329, state=u'OR', store=700),\n",
       " Row(amount=200.0, date=u'11/15/2014', id=102, product=321, state=u'CA', store=203),\n",
       " Row(amount=330.0, date=u'11/19/2014', id=106, product=331, state=u'CA', store=202),\n",
       " Row(amount=750.0, date=u'11/17/2014', id=103, product=373, state=u'WA', store=101),\n",
       " Row(amount=200.0, date=u'11/19/2014', id=105, product=321, state=u'CA', store=202)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert Spark DataFrames to Pandas data frames?\n",
    "\n",
    "- Use `toPandas()` to convert Spark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   amount        date   id  product state  store\n",
      "0   300.0  11/13/2014  101      331    WA    100\n",
      "1   450.0  11/18/2014  104      329    OR    700\n",
      "2   200.0  11/15/2014  102      321    CA    203\n",
      "3   330.0  11/19/2014  106      331    CA    202\n",
      "4   750.0  11/17/2014  103      373    WA    101\n",
      "5   200.0  11/19/2014  105      321    CA    202\n"
     ]
    }
   ],
   "source": [
    "x = sales.toPandas()\n",
    "print type(x)\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON vs CSV vs Parquet \n",
    "----------------------\n",
    "\n",
    "What are the pros and cons of JSON vs CSV vs Parquet?\n",
    "\n",
    "Feature            |JSON               |CSV            |Parquet\n",
    "-------            |----               |---            |-------\n",
    "Human-Readable     |Yes                |Yes            |No\n",
    "Compact            |No                 |Moderately     |Highly\n",
    "Columnar           |No                 |No             |Yes\n",
    "Self-Describing    |Yes                |No             |Yes\n",
    "Requires Schema    |No                 |Yes            |No\n",
    "Splittable         |Yes                |Yes            |Yes\n",
    "Popular            |No                 |Yes            |Not yet\n",
    "\n",
    "What are columnar data formats?\n",
    "\n",
    "- Columnar data formats store data column-wise.\n",
    "\n",
    "- This allows them to do RLE or run-length encoding.\n",
    "\n",
    "- Instead of storing `San Francisco` 100 times, they will just store\n",
    "  it once and the count of how many times it occurs.\n",
    "\n",
    "- When the data is repetitive and redundant as unstructured big data\n",
    "  tends to be, columnar data formats use up a fraction of the disk\n",
    "  space of non-columnar formats.\n",
    "\n",
    "What are splittable data formats?\n",
    "\n",
    "- On big data systems data is stored in blocks.\n",
    "\n",
    "- For example, on HDFS data is stored in 128 MB blocks.\n",
    "\n",
    "- Splittable data formats enable records in a block to be processed\n",
    "  without looking at the entire file.\n",
    "\n",
    "What are some examples of a non-splittable data format?\n",
    "\n",
    "- Gzip\n",
    "\n",
    "User Defined Functions\n",
    "----------------------\n",
    "\n",
    "How can I create my own User-Defined Functions?\n",
    "\n",
    "- Import the types (e.g. StringType, IntegerType, FloatType) that we\n",
    "  are returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a UDF to calculate sales tax of 10% on the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_tax(amount):\n",
    "    return amount * 1.10\n",
    "\n",
    "sqlContext.registerFunction(\"add_tax\", add_tax, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+--------+\n",
      "|amount|      date| id|product|state|store|with_tax|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|   330.0|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|   495.0|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|   220.0|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|   363.0|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|   825.0|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|   220.0|\n",
      "+------+----------+---+-------+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT *, add_tax(amount) AS with_tax FROM sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optional last argument of `registerFunction` is function return\n",
    "  type; default is `StringType`.\n",
    "\n",
    "- UDFs can single or multiple arguments. \n",
    "\n",
    "SQL Types\n",
    "---------\n",
    "\n",
    "How can I find out all the types that are available for SQL schemas\n",
    "and UDF?\n",
    "\n",
    "- In the IPython REPL type `import pyspark.sql.types`. \n",
    "\n",
    "- Then type `pyspark.sql.types.[TAB]`\n",
    "\n",
    "- Autocomplete will show you all the available types.\n",
    "\n",
    "Types          |Meaning\n",
    "-----          |-------\n",
    "StringType     |String\n",
    "IntegerType    |Int\n",
    "FloatType      |Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPT Announcement about Individual Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark.sql.types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
