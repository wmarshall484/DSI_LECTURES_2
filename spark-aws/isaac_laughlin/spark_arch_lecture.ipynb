{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "<h1 class=\"tocheading\">Spark Architecture</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Spark Architecture\n",
    "==================\n",
    "\n",
    "Starting Spark\n",
    "--------------\n",
    "\n",
    "Start Spark if you do not have it running already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x104034890>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Components\n",
    "----------------\n",
    "\n",
    "What are the major components of Spark?\n",
    "\n",
    "Component       |Defines         |Concept                      |Use Case\n",
    "---------       |-------         |-------                      |--------\n",
    "Spark           |RDD             |Record Sequences             |Batch Processing\n",
    "Spark SQL       |DataFrame       |Record Sequences with Schema |SQL queries on data\n",
    "Spark Streaming |DStream         |Micro-Batches                |Near-Realtime Processing\n",
    "MLlib           |ML Dataset      |Transformer Pipelines        |ML Algorithms\n",
    "GraphX          |Edge/Vertex RDD |Graphs                       |Graph Algorithms\n",
    "SparkR          |DataFrame       |Spark from R                 |Scale up R \n",
    "\n",
    "Spark Deployment\n",
    "----------------\n",
    "\n",
    "What are the different ways to deploy Spark?\n",
    "\n",
    "Deployment         |Scenario           |Use Case\n",
    "----------         |--------           |--------\n",
    "Local              |Single machine     |For testing or small datasets\n",
    "Spark Standalone   |Cluster            |Spark-dedicated cluster\n",
    "YARN               |Cluster            |Shared cluster with HDFS, Map-Reduce, Hive\n",
    "Mesos              |Cluster            |Shared cluster with web servers, YARN\n",
    "EC2                |Cluster            |Cloud-based cluster, uses Spark Standalone\n",
    "\n",
    "Deployment Notes\n",
    "----------------\n",
    "\n",
    "- YARN is the most popular configuration in production environments.\n",
    "\n",
    "- YARN is used for on-prem while Spark Standalone is used for\n",
    "  cloud hosting.\n",
    "\n",
    "- Spark Standalone is good for demo, proof-of-concept, or testing.\n",
    "\n",
    "- Spark Standalone is also used on EC2 for cloud-based clusters.\n",
    "\n",
    "Metrics Through Application UI\n",
    "------------------------------\n",
    "\n",
    "How can I find out:\n",
    "\n",
    "- How many worker nodes do I have?\n",
    "\n",
    "- How many executors do I have?\n",
    "\n",
    "- How many partitions is my data split into?\n",
    "\n",
    "- Why is it taking so long?\n",
    "\n",
    "The *Application UI* can tell you all this.\n",
    "\n",
    "- The application UI serves up metrics through a web UI on port 4040\n",
    "  on the machine your driver is running on.\n",
    "\n",
    "- Try it now: <http://localhost:4040>.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What if you have multiple Spark applications running? Which ports\n",
    "are they going to be on?\n",
    "</summary>\n",
    "1. If port 4040 is busy the application UI uses 4041, 4042, and so\n",
    "on.<br>\n",
    "2. Each Spark context has its own application UI.<br>\n",
    "</details>\n",
    "\n",
    "Gathering Custom Metrics\n",
    "------------------------\n",
    "\n",
    "Suppose I am processing sales transactions and some of the data is\n",
    "bad. How should I deal with this?\n",
    "\n",
    "- I could filter out the bad data.\n",
    "\n",
    "- But what if most of the data is bad?\n",
    "\n",
    "- Or what if my logic for detecting badness is broken?\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Here are two solutions. What are the pros and cons?\n",
    "</summary>\n",
    "1. Printing is dangerous because log files can get as big as the data.<br>\n",
    "2. Log messages are hard to track down on a cluster.<br>\n",
    "3. Counting is better.<br>\n",
    "4. It produces a quick actionable number.<br>\n",
    "</details>\n",
    "\n",
    "Accumulators\n",
    "------------\n",
    "\n",
    "What are *Accumulators*?\n",
    "\n",
    "- Accumulators are like counters in MapReduce.\n",
    "\n",
    "- They let you gather execution metrics.\n",
    "\n",
    "- They should be used for counting small sets of events.\n",
    "\n",
    "- Accumulators send data from executors to the driver during\n",
    "  execution.\n",
    "\n",
    "Accumulator Example: Counting Partitions\n",
    "----------------------------------------\n",
    "\n",
    "How can we find out how many partitions are being used for\n",
    "processing our data?\n",
    "\n",
    "- Use an accumulator to count each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Accumulator<id=1, value=4>\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def inc_acc(x, acc):\n",
    "    acc+= 1\n",
    "    return x\n",
    "\n",
    "part_count = sc.accumulator(0)\n",
    "\n",
    "record_count = sc.parallelize(xrange(100))\\\n",
    "  .mapPartitions(lambda iterable: inc_acc(iterable,part_count))\\\n",
    "  .count()\n",
    "\n",
    "print record_count\n",
    "print repr(part_count)\n",
    "print part_count.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `mapPartitions()` is called once per partition and gets an iterator\n",
    "  that can walk through the records in the partition.\n",
    "\n",
    "Parallelizing More Partitions\n",
    "-----------------------------\n",
    "\n",
    "Test the code by passing a second argument to `parallelize` with\n",
    "the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Accumulator<id=2, value=10>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "part_count = sc.accumulator(0)\n",
    "\n",
    "record_count = sc.parallelize(xrange(100),10)\\\n",
    "  .mapPartitions(lambda iter: inc_acc(iter,part_count))\\\n",
    "  .count()\n",
    "\n",
    "print record_count\n",
    "print repr(part_count)\n",
    "print part_count.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning\n",
    "--------------\n",
    "\n",
    "Test the code by adding a `repartition` function call before\n",
    "`mapPartitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Accumulator<id=3, value=20>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "part_count = sc.accumulator(0)\n",
    "\n",
    "record_count = sc.parallelize(xrange(100),10)\\\n",
    "  .repartition(20)\\\n",
    "  .mapPartitions(lambda iter: inc_acc(iter,part_count))\\\n",
    "  .count()\n",
    "\n",
    "print record_count\n",
    "print repr(part_count)\n",
    "print part_count.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Where is the accumulator getting created? Where is it getting\n",
    "incremented? How is it getting passed?\n",
    "</summary>\n",
    "1. The accumulator is created on the driver.<br>\n",
    "2. It is incremented on the executors.<br>\n",
    "3. The variable `part_acc` contains a reference to the accumulator.<br>\n",
    "4. This is passed as part of the `lambda` to the executors.<br>\n",
    "</details>\n",
    "\n",
    "Speeding Up Joins\n",
    "=================\n",
    "\n",
    "Joining Datasets\n",
    "----------------\n",
    "\n",
    "Given this sales data how can we find out which customers produced\n",
    "the most revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customers_data = [\n",
    "  (101, \"Alice\"),\n",
    "  (102, \"Bob\"),\n",
    "  (103, \"Cat\"),\n",
    "]\n",
    "sales_data = [\n",
    "  (103, 7.0, \"Milk\"), \n",
    "  (101, 2.0, \"Parsley\"), \n",
    "  (101, 70.0, \"Clothes\"), \n",
    "  (103, 11.0, \"Coffee\"),\n",
    "  (102, 5.0, \"Eggs\"), \n",
    "  (101, 5.0, \"Coffee\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join the RDDs and then `reduceByKey` on customer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, ('Alice', 77.0)), (102, ('Bob', 5.0)), (103, ('Cat', 18.0))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelize and clean up the data.\n",
    "customers = sc.parallelize(customers_data)\n",
    "sales = sc.parallelize(sales_data)\\\n",
    "  .map(lambda (customer_id,amount,product): (customer_id,amount))\n",
    "\n",
    "# Join and add up all amounts per customer.\n",
    "customers.join(sales)\\\n",
    "  .reduceByKey(lambda (name1,amount1),(name2,amount2): (name1,amount1+amount2)) \\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with Join\n",
    "------------------\n",
    "\n",
    "What is the big problem with joins?\n",
    "\n",
    "- Joins are a common way of combining datasets.\n",
    "\n",
    "- They are also very expensive.\n",
    "\n",
    "- `customers.join(sales)` shuffles `customers` and `sales` records by\n",
    "  `customer_id` across different machines.\n",
    "\n",
    "- The network is the bottleneck.\n",
    "\n",
    "How can we speed this up?\n",
    "\n",
    "- If one table was smaller you could pass it to all the executors\n",
    "  first.\n",
    "\n",
    "Optimizing Joins\n",
    "----------------\n",
    "\n",
    "How can we avoid shuffling data in joins?\n",
    "\n",
    "- What if all the sales transactions for Alice were on the same\n",
    "  machine as her customer record.\n",
    "  \n",
    "- If Spark knows that the records to be joined are all on the same\n",
    "  machine it won't shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(102, ('Bob', 5.0)), (101, ('Alice', 77.0)), (103, ('Cat', 18.0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelize and clean up the data.\n",
    "customers = sc.parallelize(customers_data)\\\n",
    "  .partitionBy(2)\n",
    "sales = sc.parallelize(sales_data)\\\n",
    "  .map(lambda (customer_id,amount,product): (customer_id,amount))\\\n",
    "  .partitionBy(2)\n",
    "\n",
    "# Join and add up all amounts per customer.\n",
    "customers.join(sales)\\\n",
    "  .reduceByKey(lambda (name1,amount1),(name2,amount2): (name1,amount1+amount2)) \\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.partitionBy(n)` creates `n` partitions based on keys.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "If you apply `map` or `flatMap` to an RDD after `partitionBy` will\n",
    "Spark still avoid shuffling the data?\n",
    "</summary>\n",
    "1. In that case the keys might have changed, so Spark will reshuffle\n",
    "the data<br>\n",
    "2. To preserve the partitioned-by-key state if you have to transform\n",
    "data further after `partitionBy` use `mapValues` or `flatMapValues`\n",
    "instead of `map` and `flatMap`.<br>\n",
    "</details>\n",
    "\n",
    "Sharing Data With Executors\n",
    "---------------------------\n",
    "\n",
    "How can we share data with all the executors?\n",
    "\n",
    "- One way of sharing data is through closures. \n",
    "\n",
    "- When you reference a variable in the `lambda` or function in a\n",
    "  transformation it gets sent with the function to the executors.\n",
    "\n",
    "- Between `customers` and `sales` which one is likely to be smaller?\n",
    "  Lets send the smaller table in through closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 77.0), ('Bob', 5.0), ('Cat', 18.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn customers into a dictionary.\n",
    "customer_dict = dict(customers_data)\n",
    "\n",
    "# Now process sales data and pass customers to the executors.\n",
    "sc.parallelize(sales_data)\\\n",
    "  .map(lambda (customer_id,amount,product): (customer_id,amount))\\\n",
    "  .reduceByKey(lambda amount1,amount2: amount1 + amount2)\\\n",
    "  .map(lambda (customer_id,amount): (customer_dict[customer_id],amount))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closures\n",
    "--------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "How do closures work?\n",
    "\n",
    "- The data is serialized with the lambdas or functions passed to the\n",
    "  transformations.\n",
    "\n",
    "- A copy of it is then sent to every single task.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What is a downside of sending the data to every single task? Could\n",
    "we do better?\n",
    "</summary>\n",
    "1. It would be nice if we could just send one copy of the data to each\n",
    "executor<br>\n",
    "2. This is exactly what *Broadcast Variables* do.<br>\n",
    "</details>\n",
    "\n",
    "Broadcast Variables\n",
    "-------------------\n",
    "\n",
    "What are *Broadcast Variables*?\n",
    "\n",
    "- Broadcast variables let the driver send data to the tasks efficiently.\n",
    "\n",
    "- Only one copy of the data is sent to each executor.\n",
    "\n",
    "- The data is transferred through a peer-to-peer algorithm called\n",
    "  Cornet.\n",
    "\n",
    "- This speeds up the transfer by 1.7x compared to sending the data\n",
    "  from the driver to each executor directly according to \n",
    "  [research][cornet-paper] at Berkeley.\n",
    "  \n",
    "[cornet-paper]: http://www.cs.berkeley.edu/~jordan/papers/chowdhury-etal-sigcomm11.pdf\n",
    "\n",
    "Using Broadcast Variables\n",
    "-------------------------\n",
    "\n",
    "Lets use broadcast variables to transfer the customer dictionary to\n",
    "the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 77.0), ('Bob', 5.0), ('Cat', 18.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn customers into a broadcasted dictionary.\n",
    "customer_dict = sc.broadcast(dict(customers_data))\n",
    "\n",
    "# Now process sales data and pass customers to the executors.\n",
    "sc.parallelize(sales_data)\\\n",
    "  .map(lambda (customer_id,amount,product): (customer_id,amount))\\\n",
    "  .reduceByKey(lambda amount1,amount2: amount1 + amount2)\\\n",
    "  .map(lambda (customer_id,amount): (customer_dict.value[customer_id],amount))\\\n",
    "  .collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key differences are:\n",
    "\n",
    "- To broadcast your data you wrap it into a `sc.broadcast()` call.\n",
    "\n",
    "- To access it in a task you use `customer_dict.value` instead of\n",
    "  `customer_dict`. \n",
    "\n",
    "- Calling `.value` accesses the underlying shared broadcast object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
