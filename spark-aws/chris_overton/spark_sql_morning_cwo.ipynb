{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark SQL\n",
    "Revised for Spark 2.0  \n",
    "<br>\n",
    "Chris Overton   \n",
    "2016.11.03  \n",
    "Adapted most recently from Ivan Corneillet,\n",
    "with advice from Jean-Francois Omhover and Miles Erickson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Objectives\n",
    "- Basic facility with SQL within Spark, as revised in version 2.0\n",
    "- Mix and match RDD and SQL approaches to data manipulation\n",
    "- Understand current status of Spark SQL technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction: flow of ideas leading to Spark (and esp. to its SQL dialect)\n",
    "1) 'Old SQL' - successively larger db's  \n",
    "2a) Rebellion: other ways to deal with structured data  \n",
    "    - for easier analysis, force data into single rectangular table \n",
    "    (e.g. star schema, R data frame, Pandas)  \n",
    "    - programmatic manipulation of 'flat files'  \n",
    "    - object-oriented db's  \n",
    "    - NoSQL  \n",
    "    - you only get to see the data once: streamed data  \n",
    "    - graph db's  \n",
    "2b) Scalability, especially through map-reduce  \n",
    "3) Spark - combines learning from 2a and 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction: paradigm shift\n",
    "- Old way: move data to centralized, fast executor of code\n",
    "- Hadoop: forced parallelism/concurrency through map and reduce steps\n",
    "    - Try to iron the cycles out of data flow -> immutability, fewer side-effects\n",
    "- Spark:  \n",
    "    1) **Move code to data**  \n",
    "    2) Smarter optimization of process flow  \n",
    "    3) Do more in memory, rather than through transits through slower storage (e.g. disk) and across machines  \n",
    "    3) Maintain 'small data feel' with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction: paradigm shift\n",
    "Where this doesn't work well: highly 'cyclic', mutable data  \n",
    "<br>\n",
    "<details><summary>\n",
    "Q: What's a good example of this?\n",
    "</summary>\n",
    "Real-time transactional db, like for banking\n",
    "<br>\n",
    "...but Spark would be fine for analyzing transaction logs\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is Spark SQL?\n",
    "\n",
    "- Schema-enabled RDD subclasses + SQL DML (Data Manipulation Language)\n",
    "\n",
    "What are schemas?\n",
    "\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "\n",
    "- Can use column names instead of column positions\n",
    "- Can query using SQL and DataFrame syntax\n",
    "- Make your data more structured\n",
    "    - Type safety can be verified before piping data through algorithms\n",
    "    - Possible storage economy and improved access speed, such as through columnar db's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The state of Spark with release of version 2.0 \n",
    "A lot has changed in the recent version upgrade, especially for Spark SQL, where fundamental classes have been overhauled.\n",
    "\n",
    "In fact, Spark has continued to evolve rapidly. That means knowledge (and software) need to be kept up to date!  \n",
    "This presentation emphasizes the new version: if someone is enough of an 'early adopter' to use Spark, they're likely to upgrade quickly to version 2.0  \n",
    "\n",
    "Many texts and online resources cover Spark 1.* --> you may want to focus on more recent documentation  \n",
    "\n",
    "We cover only the python interface to Spark, using pyspark. Besides R and Java, it is useful to learn to talk to Spark from scala, since this is the language of its implementation, and fits with its api paradigms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "#sc = pyspark.SparkContext() #Don't need to instantiate - is done by pyspark\n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Start Spark SQL\n",
    "\n",
    "**In Spark >= 2.0**, create a SparkSession from the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#left out of following chain:     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The old way for Spark 1.* **, create a HiveContext from the SparkContext.  \n",
    "**Not used further in this presentation:**   \n",
    "Alternately, create queries from a SqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.HiveContext at 0x10ae09990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not run: remains available for integration with Hadoop and backwards compatibility\n",
    "#sqlContext = pyspark.HiveContext(sc)\n",
    "\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the difference between SparkContext and HiveContext?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DataFrames in 2.0\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as RDDs with schema. \n",
    "\n",
    "For strongly-typed languages (Scala, Java), a DataFrame is a DataSet(Row). This means an instance of class DataSet, parametrized by what it is a set of, namely an instance of class Row.  \n",
    "<br>\n",
    "For weakly-typed languages (Python, R), DataFrame is called by a similar api, but is a different subclass of RDD.  \n",
    "This sacrifices some execution speed and type safety, but requires less code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL using CSV\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID,Date,Store,State,Product,Amount\r\n",
      "101,11/13/2014,100,WA,331,300.00\r\n",
      "104,11/18/2014,700,OR,329,450.00\r\n",
      "102,11/15/2014,203,CA,321,200.00\r\n",
      "106,11/19/2014,202,CA,331,330.00\r\n",
      "103,11/17/2014,101,WA,373,750.00\r\n",
      "105,11/19/2014,202,CA,321,200.00"
     ]
    }
   ],
   "source": [
    "!cat sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda (id, date, store, state, product, amount):\\\n",
    "        (int(id), date, int(store), state, int(product), float(amount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hard-coding a schema\n",
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('store', IntegerType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('product', IntegerType(), True),\n",
    "    StructField('amount', FloatType(), True)\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Define the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[amount: double, date: string, id: bigint, product: bigint, state: string, store: bigint]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split()` (or `line.split('\\t')`)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using SQL With DataFrames\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SparkSession (formerly SqlContext!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql('''\n",
    "SELECT state, amount\n",
    "    FROM sales\n",
    "    WHERE amount > 100\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "Wait a minute?! What just got executed?\n",
    "</summary>\n",
    "<br>\n",
    "This records a possible transformation that is not run until required by an action\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- View the results using `show()` or `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state=u'WA', amount=300.0),\n",
       " Row(state=u'OR', amount=450.0),\n",
       " Row(state=u'CA', amount=200.0),\n",
       " Row(state=u'CA', amount=330.0),\n",
       " Row(state=u'WA', amount=750.0),\n",
       " Row(state=u'CA', amount=200.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>\n",
    "If I run `result.collect()` twice how many times will the data be read from disk?\n",
    "</summary>\n",
    "1. RDDs are lazy.<br>\n",
    "2. Therefore the data will be read twice.<br>\n",
    "3. Unless you cache the RDD, all transformations in the RDD will execute on each action.<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Caching Tables\n",
    "\n",
    "How can I cache the RDD for a table to avoid roundtrips to disk on each action?\n",
    "\n",
    "- Use `cacheTable()` - EXCEPT this api hasn't been moved to SparkSession in v 2.0. Now you have to use 'catalog' to get at the legacy SqlContext method...\n",
    "\n",
    "Note: this also lets you see the execution time, and gets it out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Old v 1.* code: sqlContext.cacheTable('sales')\n",
    "spark.catalog.cacheTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is particularly useful if you are using Spark SQL to explore data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Saving Results - csv, json, and parquet file formats\n",
    "\n",
    "How can I save the results back out to the file system?\n",
    "\n",
    "(first, make sure the files don't exist...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json high-sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can write them out using the JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Or you can save them as Parquet...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result.write.parquet('high-sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 christopher.overton  staff  233 Nov  3 08:16 sales.csv\n",
      "sales.csv\n",
      "#ID,Date,Store,State,Product,Amount\n",
      "101,11/13/2014,100,WA,331,300.00\n",
      "104,11/18/2014,700,OR,329,450.00\n",
      "102,11/15/2014,203,CA,321,200.00\n",
      "106,11/19/2014,202,CA,331,330.00\n",
      "103,11/17/2014,101,WA,373,750.00\n",
      "105,11/19/2014,202,CA,321,200.00"
     ]
    }
   ],
   "source": [
    "!ls -l sales.csv\n",
    "!echo sales.csv\n",
    "!cat sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "-rw-r--r--  1 christopher.overton  staff   0 Nov  3 08:26 _SUCCESS\n",
      "-rw-r--r--  1 christopher.overton  staff  90 Nov  3 08:26 part-00000\n",
      "-rw-r--r--  1 christopher.overton  staff  90 Nov  3 08:26 part-00001\n",
      "high-sales.json/part-00000\n",
      "{\"state\":\"WA\",\"amount\":300.0}\n",
      "{\"state\":\"OR\",\"amount\":450.0}\n",
      "{\"state\":\"CA\",\"amount\":200.0}\n",
      "high-sales.json/part-00001\n",
      "{\"state\":\"CA\",\"amount\":330.0}\n",
      "{\"state\":\"WA\",\"amount\":750.0}\n",
      "{\"state\":\"CA\",\"amount\":200.0}\n"
     ]
    }
   ],
   "source": [
    "!ls -l high-sales.json\n",
    "!for i in high-sales.json/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "-rw-r--r--  1 christopher.overton  staff    0 Nov  3 08:27 _SUCCESS\n",
      "-rw-r--r--  1 christopher.overton  staff  533 Nov  3 08:27 part-r-00000-14cef377-3a09-4adc-ad38-a83d53049287.snappy.parquet\n",
      "-rw-r--r--  1 christopher.overton  staff  549 Nov  3 08:27 part-r-00001-14cef377-3a09-4adc-ad38-a83d53049287.snappy.parquet\n",
      "high-sales.parquet/part-r-00000-14cef377-3a09-4adc-ad38-a83d53049287.snappy.parquet\n",
      "PAR1\u0015\u0000\u00150\u00152,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\u0018\u0014\u0002\u0000\u0000\u0000\u0003\u0007\u0001\u00064WA\u0002\u0000\u0000\u0000OR\u0002\u0000\u0000\u0000CA\u0015\u0000\u0015$\u0015(,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0012D\u0002\u0000\u0000\u0000\u0003\u0007\u0000\u0000�C\u0000\u0000�C\u0000\u0000HC\u0015\u0002\u0019<H\f",
      "spark_schema\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\b\u0000\u0006\u0019\u0018\u0005state\u0015\u0002\u0016\u0006\u0016j\u0016l&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&t\u001c",
      "\u0015\b\u00195\b\u0000\u0006\u0019\u0018\u0006amount\u0015\u0002\u0016\u0006\u0016f\u0016j&t<\u0018\u0004\u0000\u0000�C\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018;parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)\u0000�\u0001\u0000\u0000PAR1high-sales.parquet/part-r-00001-14cef377-3a09-4adc-ad38-a83d53049287.snappy.parquet\n",
      "PAR1\u0015\u0004\u0015\u0018\u0015\u001c",
      "L\u0015\u0004\u0015\u0004\u0000\u0000\f",
      ",\u0002\u0000\u0000\u0000CA\u0002\u0000\u0000\u0000WA\u0015\u0000\u0015\u0012\u0015\u0016,\u0015\u0006\u0015\u0004\u0015\u0006\u0015\b\u001c",
      "\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000\t \u0002\u0000\u0000\u0000\u0003\u0007\u0001\u0003\u0002\u0015\u0000\u0015$\u0015(,\u0015\u0006\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0012D\u0002\u0000\u0000\u0000\u0003\u0007\u0000\u0000�C\u0000�;D\u0000\u0000HC\u0015\u0002\u0019<H\f",
      "spark_schema\u0015\u0004\u0000\u0015\f",
      "%\u0002\u0018\u0005state%\u0000\u0000\u0015\b%\u0002\u0018\u0006amount\u0000\u0016\u0006\u0019\u001c",
      "\u0019,&\b\u001c",
      "\u0015\f",
      "\u00195\b\u0004\u0006\u0019\u0018\u0005state\u0015\u0002\u0016\u0006\u0016~\u0016�\u0001&\b<\u0018\u0002WA\u0018\u0002CA\u0016\u0000\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\b\u00195\b\u0000\u0006\u0019\u0018\u0006amount\u0015\u0002\u0016\u0006\u0016f\u0016j&�\u0001<\u0018\u0004\u0000�;D\u0018\u0004\u0000\u0000HC\u0016\u0000\u0000\u0000\u0000\u0016�\u0001\u0016\u0006\u0000\u0019\u001c",
      "\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"float\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018;parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d)\u0000�\u0001\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "!ls -l high-sales.parquet \n",
    "!for i in high-sales.parquet/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON vs CSV vs Parquet \n",
    "\n",
    "What are the pros and cons of JSON vs CSV vs Parquet?\n",
    "\n",
    "Feature | JSON | CSV | Parquet\n",
    "---|---|---|---\n",
    "Human-Readable | Yes | Yes | No\n",
    "Compact | No | Moderately | Highly\n",
    "Columnar | No | No | Yes\n",
    "Self-Describing | Yes | No | Yes\n",
    "Requires Schema | No | Yes | No\n",
    "Splittable | Yes | Yes | Yes\n",
    "Popular | Getting there | Yes | Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are columnar data formats?\n",
    "\n",
    "- Columnar data formats store data column-wise.\n",
    "- This allows them to do run-length encoding (RLE).\n",
    "- Instead of storing `San Francisco` 100 times, they will just store it once and the count of how many times it occurs.\n",
    "- When the data is repetitive and redundant as unstructured big data tends to be, columnar data formats use up a fraction of the disk space of non-columnar formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are splittable data formats?\n",
    "\n",
    "- On big data systems data is stored in blocks.\n",
    "- For example, on HDFS data is stored in 64/128 MB blocks.\n",
    "- Splittable data formats enable records in a block to be processed without looking at the entire file.\n",
    "\n",
    "What are some examples of a non-splittable data format?\n",
    "\n",
    "- Gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parsing text files\n",
    "\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and read using `textFile()`\n",
    "- JSON works well with Spark SQL because the data has an embedded schema  \n",
    "<br>\n",
    "- Parquet is similarly self-describing, and is more efficient for larger tables  \n",
    "<br>\n",
    "- If you load CSV, you still have to add the schema programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\r\n",
      "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\r\n",
      "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\r\n",
      "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\r\n",
      "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\r\n",
      "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
     ]
    }
   ],
   "source": [
    "!cat sales.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Now read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('sales.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[amount: double, date: string, id: bigint, product: bigint, state: string, store: bigint]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here is how to look at a 50% sample of the DataFrame (without\n",
    "  replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here is how to inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount,DoubleType,true),StructField(date,StringType,true),StructField(id,LongType,true),StructField(product,LongType,true),StructField(state,StringType,true),StructField(store,LongType,true)))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(amount,DoubleType,true),\n",
       " StructField(date,StringType,true),\n",
       " StructField(id,LongType,true),\n",
       " StructField(product,LongType,true),\n",
       " StructField(state,StringType,true),\n",
       " StructField(store,LongType,true)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, amount: string, id: string, product: string, store: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DataFrame Methods\n",
    "\n",
    "How can I slice the DataFrame by column and by row?\n",
    "\n",
    "- DataFrames provide a *Pandas*-like API for manipulating data.\n",
    "- To select specific columns use `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', 'amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can also modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount + 100)|\n",
      "+-----+--------------+\n",
      "|   WA|         400.0|\n",
      "|   OR|         550.0|\n",
      "|   CA|         300.0|\n",
      "|   CA|         430.0|\n",
      "|   WA|         850.0|\n",
      "|   CA|         300.0|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', df.amount + 100).show()\n",
    "\n",
    "# (or)\n",
    "df.select('state', df['amount'] + 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|state|(amount < 300)|\n",
      "+-----+--------------+\n",
      "|   WA|         false|\n",
      "|   OR|         false|\n",
      "|   CA|          true|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|          true|\n",
      "+-----+--------------+\n",
      "\n",
      "+-----+--------------+\n",
      "|state|(amount = 300)|\n",
      "+-----+--------------+\n",
      "|   WA|          true|\n",
      "|   OR|         false|\n",
      "|   CA|         false|\n",
      "|   CA|         false|\n",
      "|   WA|         false|\n",
      "|   CA|         false|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', df.amount < 300).show()\n",
    "\n",
    "df.select('state', df.amount == 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   OR|    1|\n",
      "|   CA|    3|\n",
      "|   WA|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', 'amount').groupBy('state').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|102|\n",
      "|106|\n",
      "|105|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state == 'CA').select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|104|\n",
      "|106|\n",
      "|103|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('sales')\n",
    "\n",
    "spark.sql('''\n",
    "SELECT id\n",
    "    FROM sales\n",
    "    WHERE amount > 300\n",
    "''')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How can I convert DataFrames to regular RDDs?\n",
    "\n",
    "Old way: Spark 1.*:  \n",
    "- DataFrames are also RDDs.\n",
    "- You can use `map()` to iterate over the rows of the DataFrame.\n",
    "- You can access the values in a row using field names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Old v 1.* code\n",
    "#df.map(lambda row: row.amount).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "New way: Spark 2.0: just call rdd from a df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame, pyspark.rdd.RDD)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(df), type(df.rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can also use `collect()` or `take()` to pull DataFrame rows into\n",
    "  the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date=u'11/13/2014', id=101, product=331, state=u'WA', store=100),\n",
       " Row(amount=450.0, date=u'11/18/2014', id=104, product=329, state=u'OR', store=700),\n",
       " Row(amount=200.0, date=u'11/15/2014', id=102, product=321, state=u'CA', store=203),\n",
       " Row(amount=330.0, date=u'11/19/2014', id=106, product=331, state=u'CA', store=202),\n",
       " Row(amount=750.0, date=u'11/17/2014', id=103, product=373, state=u'WA', store=101),\n",
       " Row(amount=200.0, date=u'11/19/2014', id=105, product=321, state=u'CA', store=202)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can I convert Spark DataFrames to Pandas data frames?\n",
    "\n",
    "- Use `toPandas()` to convert Spark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>product</th>\n",
       "      <th>state</th>\n",
       "      <th>store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300.0</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>331</td>\n",
       "      <td>WA</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450.0</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>104</td>\n",
       "      <td>329</td>\n",
       "      <td>OR</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>102</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>330.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>106</td>\n",
       "      <td>331</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750.0</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>103</td>\n",
       "      <td>373</td>\n",
       "      <td>WA</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.0</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>105</td>\n",
       "      <td>321</td>\n",
       "      <td>CA</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount        date   id  product state  store\n",
       "0   300.0  11/13/2014  101      331    WA    100\n",
       "1   450.0  11/18/2014  104      329    OR    700\n",
       "2   200.0  11/15/2014  102      321    CA    203\n",
       "3   330.0  11/19/2014  106      331    CA    202\n",
       "4   750.0  11/17/2014  103      373    WA    101\n",
       "5   200.0  11/19/2014  105      321    CA    202"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "\n",
    "print type(pandas_df)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## User Defined Functions\n",
    "\n",
    "How can I create my own User-Defined Functions (\"UDF\")?\n",
    "\n",
    "- Import the types (e.g. StringType, IntegerType, FloatType) that we are returning  \n",
    "- Register the new udf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|amount|amt_plus_tax|\n",
      "+------+------------+\n",
      "| 300.0|       330.0|\n",
      "| 450.0|       495.0|\n",
      "| 200.0|       220.0|\n",
      "| 330.0|       363.0|\n",
      "| 750.0|       825.0|\n",
      "| 200.0|       220.0|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The 1.* old way: \n",
    "def add_tax1(amount):\n",
    "    return amount * 1.10\n",
    "#sqlContext.registerFunction('add_tax', add_tax, FloatType())\n",
    "\n",
    "#The 2.0 new way:\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.udf.register('add_tax2', lambda x: x * 1.10, FloatType())\n",
    "#or use legacy call:\n",
    "#spark.catalog.registerFunction('add_tax2', lambda x: x * 1.10, FloatType())\n",
    "spark.sql(\"select amount, add_tax2(amount) as amt_plus_tax from sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Optional last argument of `registerFunction` is function return\n",
    "  type; default is `StringType`.\n",
    "\n",
    "- UDFs can use single or multiple arguments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core and Spark SQL Changes (from 2.0 release notes)\n",
    "\n",
    "One of the largest changes in Spark 2.0 is the new updated APIs:\n",
    "\n",
    "    - Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface.\n",
    "    - SparkSession: new entry point that replaces the old SQLContext and HiveContext for DataFrame and Dataset APIs. SQLContext and HiveContext are kept for backward compatibility.\n",
    "    - A new, streamlined configuration API for SparkSession\n",
    "    - Simpler, more performant accumulator API\n",
    "    - A new, improved Aggregator API for typed aggregation in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL (from 2.0 release notes)\n",
    "\n",
    "Spark 2.0 substantially improved SQL functionalities with SQL2003 support. Spark SQL can now run all 99 TPC-DS queries. More prominently, we have improved:\n",
    "\n",
    "    - A native SQL parser that supports both ANSI-SQL as well as Hive QL\n",
    "    - Native DDL command implementations\n",
    "    - Subquery support, including\n",
    "        - Uncorrelated Scalar Subqueries\n",
    "        - Correlated Scalar Subqueries\n",
    "        - NOT IN predicate Subqueries (in WHERE/HAVING clauses)\n",
    "        - IN predicate subqueries (in WHERE/HAVING clauses)\n",
    "        - (NOT) EXISTS predicate subqueries (in WHERE/HAVING clauses)\n",
    "    - View canonicalization support\n",
    "\n",
    "In addition, when building without Hive support, Spark SQL should have almost all the functionality as when building with Hive support, with the exception of Hive connectivity, Hive UDFs, and script transforms."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
