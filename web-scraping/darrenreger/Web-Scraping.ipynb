{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "# Most of this lecture is stolen from Cary, a little from Eric and some other scattered pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is web scraping?\n",
    "\n",
    "The practice of gathering data through any means other than a program interacting with an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we web scrape?\n",
    "\n",
    "Browsers are good for many things, but not so much for gathering information\n",
    "\n",
    "- Sometimes the data we want isn't available packaged all nicely\n",
    "- Sometimes the company doesn't want to make it easy for us to get all their data\n",
    "- Sometimes the data source we need isn't big enough to warrant that company creating an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we do it?\n",
    "\n",
    "If you can view it in your browser, you can grab it with a python script!\n",
    "\n",
    "We'll discuss what packages you need when we get there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is there an easier way?\n",
    "\n",
    "API's are a thing. Sometimes they give us what we need and that's nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the keys to a good scraper?\n",
    "\n",
    "Some things to keep in mind when building your scraper:\n",
    "\n",
    "- Save all the data you collect\n",
    "- Don't abuse try:excepts\n",
    "- Don't get banned from the site you're interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afternoon Objectives\n",
    "\n",
    "1. Understand the process of getting data from the web.\n",
    "2. Know the basics of HTML/CSS:\n",
    "    * Know how to pull desired data from web pages.\n",
    "3. Be able to use existing API's to get fetch pre-formatted data.\n",
    "\n",
    "### Internet vs. World Wide Web\n",
    "\n",
    "* The internet is commonly refered to as a network of networks. It is the infrastructure that allows networks all around the world to connect with one another. There are many different protocols to transfer information within this larger, meta-network.\n",
    "* The World Wide Web, or Web, provides one of the ways that data can be transfered over the internet. Uses a **U**niform **R**esource **L**ocator, URL, to specify the location, within the internet, of a document.\n",
    "* Documents on the web are generally written in **H**yper**T**ext **M**arkup **L**anguage, HTML, which can be natively viewed by browsers, the tool that we use to browse the web.\n",
    "\n",
    "### Communication on the Web\n",
    "\n",
    "Information is transmitted around the web through a number of protocols. The main one that you will see is the **H**yper**T**ext **T**ransfer **P**rotocol, HTTP. These transfers, called **requests**, are initiated in a number of ways, but always begin with the client, read: you at your browser.\n",
    "\n",
    "There are 4 main types of request that can be issued by your browser: get, post, put and delete. For web scraping purposes, you will almost always be using get requests. We will learn some more about the others in a couple of weeks during data products day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML & CSS\n",
    "HTML, or *HyperText Markup Language*, is the language that virtually all web pages are written in.  HTML allows us to describe how web pages should render themselves and display content.  They do this by using *HTML Tags*, where each tag is describing different document content.  For example we could specify the title of our page with the following tags: `<title>This is the Title!</title>`.  `<title>` is the opening tag and `</title>` is the corresponding closing tag.  Anything that falls in the middle will be interpreted as the title of the document.  For a more in-depth introduction to HTML, please refer to [this w3schools Tutorial](http://www.w3schools.com/html/html_intro.asp).\n",
    "\n",
    "We can also apply *attributes* to HTML tags to control how they will be stylized.  Let's say we would like to change the color of our super original title tag to red to make it pop.  We could easily achieve this with the following style attribute: `<title style=\"color:red;\">This is a red Title!</title>`.  But what if we have a very large website and want to change the look and feel of the entire thing?  Certainly there has to be a better way than manually manipulating individual tags?  \n",
    "\n",
    "This can easily be achieved with the use of *Cascading Style Sheets*, or CSS.  Typically CSS is defined in a separate style sheet file with a `.css` extension which controls how certain tags are stylized.  For example, the following CSS block would make any `<p>` tags have red text and be in the courier font:\n",
    "\n",
    "```css\n",
    "p  {\n",
    "    color: red;\n",
    "    font-family: courier;\n",
    "}\n",
    "```\n",
    "\n",
    "This is all well and good, but our goal isn't to build websites, it's to systematically extract information from them!  To this end, CSS selectors provide a succinct way of only selecting certain aspects from a potentially giant block of HTML.\n",
    "\n",
    "#### CSS Selectors\n",
    "To really see the power of CSS selectors we need to see them in action!  To that end, try getting through the first 10-15 levels of the [Game of Fruit](http://flukeout.github.io/).\n",
    "\n",
    "You can also refer to this [CSS Selector Cheatsheet](http://www.cheetyr.com/css-selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from a Web Page with Python\n",
    "\n",
    "Scraping a web site basically comes down to making a request from Python and parsing through the HTML that is returned from each page. For each of these tasks we have a Python library, `requests` and `bs4`, respectively.\n",
    "\n",
    "### Requests Library\n",
    "\n",
    "The [requests](http://docs.python-requests.org/en/latest/index.html) library is designed to simplify the process of making http requests within Python. The interface is mindbogglingly simple. Instantiate a requests object to the request, this will mostly be a `get`, with the URL and optional parameters you'd like passed through the request. That instance make the results of the request available via attributes/methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "fun_cheap = 'http://sf.funcheap.com'\n",
    "r = requests.get('http://sf.funcheap.com/2016/06/25/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.text[:1000] # First 1000 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Info from a Web Page with BeautifulSoup\n",
    "\n",
    "Now that we can gain easy access to the HMTL for a web page, we need some way to pull the desired content from it. Luckily there is already a system in place to do this. With a combination of HMTL and CSS selectors we can identify the information on a HMTL page that we wish to retrieve and grab it with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Beautiful Soup, so rich and green,\n",
    "\n",
    "Waiting in a hot tureen!\n",
    "\n",
    "Who for such dainties would not stoop?\n",
    "\n",
    "Soup of the evening, beautiful Soup!”\n",
    "\n",
    "--Lewis Carroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://www.espn.com/college-football/statistics/player/_/stat/passing/sort/passingYards/year/2015/qualified/false/count/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs_obj = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs_obj.findAll('tr',{'class':'evenrow','class':'oddrow'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs_obj.findAll('tr',{'class':re.compile('^(evenrow|oddrow)')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for obj in bs_obj.findAll('tr',{'class':re.compile('^(evenrow|oddrow)')}):\n",
    "    print obj.find('span')['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for obj in bs_obj.findAll('tr',{'class':re.compile('^(evenrow|oddrow)')}):\n",
    "    html = requests.get(obj.find('a')['href'])\n",
    "    bs_obj2 = BeautifulSoup(html.text, 'html.parser')\n",
    "    name = bs_obj2.h1.get_text()\n",
    "    born = bs_obj2.find('ul',{'class':'player-metadata'}).findAll('li')[0].get_text()\n",
    "    if born[0] == 'B':\n",
    "        print '{} Born: {}'.format(name,born[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "z = requests.get('http://galvanizesf.roomzilla.net')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "z = requests.get('http://galvanizesf.roomzilla.net', auth=('', 'gVIP543'))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(z.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes they hide the info from us, but we can find it being sneaky...\n",
    "http://m.mlb.com/scoreboard#date=11/1/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you go through a web site you should build up a dictionary for the documents that you want to store in Mongo. In the example above we may, for each post url, create a dictionary with the information:\n",
    "```python\n",
    "    { url: url_of_event,\n",
    "      date: date_event,\n",
    "      cost: cost_of_event }\n",
    "```\n",
    "\n",
    "We can then insert these dictionaries into a Mongo database via PyMongo, which we will learn about next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from an Existing API\n",
    "\n",
    "Let's take a look at the API for all the publically avaliable policing data in the [UK](https://data.police.uk/docs/). After taking a look at the documentation for the interface, let's experiment with what we get when we issue a request to this API. The process looks remarkable similar to the one we went through for scraping a web page, except this time the response we're looking for is avaliable via the `json()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://data.police.uk/api/crimes-street/all-crime?lat=52.629729&lng=-1.131592&date=2013-01')\n",
    "r.json()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Scraping and Mongo\n",
    "\n",
    "Many APIs will give you a choice of how it will return data to you, choosing json will make life easier since we will frequently be using Mongo for our storage unit during our scraping endeavors, and it plays very well with json. \n",
    "\n",
    "Interacting with Mongo from Python is done with the other Mongo client that we talked about earlier PyMongo. It is designed to have a similar interface as the Mongo shell does, this ends up being fairly intuitive since both Python and JavaScript are object oriented languages, and therefore store and refer to things in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.uk_police\n",
    "collection = db.all_crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_request = requests.get('https://data.police.uk/api/crimes-no-location?category=all-crime&force=warwickshire&date=2013-09')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_request.json()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Possible way to grab data for range of months and years\n",
    "for year in range(2001, 2016):\n",
    "    for month in range(1, 13):\n",
    "        r = requests.get('https://data.police.uk/api/crimes-no-location?category=all-crime&force=warwickshire&date={}-{}'.format(year, month))\n",
    "        collection.insert_many(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collection.insert_many(other_request.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "for item in collection.find({ 'category' : 'public-order' }):\n",
    "    pp.pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember to close the connection\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the Requests HTTP library\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# A User agent header required for the Wikipedia API.\n",
    "headers = {'user_agent': 'Web_Scraping/1.1 (darren.reger@galvanize.com; dsi example exercise)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experiment with fetching one or two pages and examining the result (fill in URL and payload)\n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "# parameters for the API request\n",
    "payload = { 'action' : 'parse' , 'format' : 'json','page' : 'Kevin Bacon' }\n",
    "\n",
    "# make the request\n",
    "r = requests.post(url, data=payload, headers=headers)\n",
    "\n",
    "# print out the result of the request as JSON\n",
    "print r.json()['parse']\n",
    "\n",
    "# this is the same as going to https://en.wikipedia.org/w/api.php?action=parse&format=json&page=Kevin%20Bacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading an image or file\n",
    "We use urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlretrieve('https://images.craigslist.org/00Y0Y_kho6IzfhxVn_600x450.jpg', filename='cltest.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftover from Cary's Lecture in case the ESPN live demo goes poorly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('a', rel=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.select('h2.title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = soup.find_all('h2', class_='title')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_clear_float = title.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for tag in good_clear_float.find_all('a', rel=True):\n",
    "    urls.append(tag.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
