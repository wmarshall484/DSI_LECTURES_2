{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web Scraping!!!!\n",
    "\n",
    "## Morning Objectives\n",
    "\n",
    "1. Understand motivation for web scraping:\n",
    "    * What does a web data pipeline look like?\n",
    "    * How should we store data from the web?\n",
    "2. Know high level differences between NoSQL and SQL.\n",
    "3. Perform basic operations using (Py)Mongo.\n",
    "\n",
    "<div style=\"text-align: center\"><h3>The Reality of Scraping</h3><img src=\"images/scraping_meme.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we scrape the web?\n",
    "\n",
    "* Realistically, data that you want to study won't always be avaliable to you in the form of a curated data set.\n",
    "* Need to go to the internets to find interesting data:\n",
    "    * From an existing company\n",
    "    * Text for NLP\n",
    "    * Images\n",
    "    <div style=\"text-align: center\"><h3>Web Data Pipeline</h3><img src=\"images/web_data_pipeline.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data from the web\n",
    "\n",
    "* We already know how to store data -> SQL (RBDMS).\n",
    "    * Why wouldn't SQL necessarily be the best tool for storing data that we retrieve from the web?\n",
    "        * Data are messy!\n",
    "* Enter No SQL. Stands for **N**ot **o**nly **SQL**. MongoDB is a flavor of NoSQL, like PosgreSQL is a flavor of SQL.\n",
    "    * A NoSQL paradigm may be preferable to SQL because it is schemaless.\n",
    "    * Great for storing unstructured data, as we may find on the web!\n",
    "    * MongoDB is a document-oriented DBMS:\n",
    "      <div style=\"text-align: center\"><h3>Centered around \"Documents\"</h3><img src=\"images/document_based_storage.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs. Mongo\n",
    "\n",
    "* SQL - want to prevent reduncancy in data by having tables with unique information and relations between them (normalized data).\n",
    "    * Creates a framework for querying with joins.\n",
    "    * Makes it easier to update database. Only ever have to change information in a single place.\n",
    "    * This can result in \"simple\" queries being slower, but more complex queries are often faster.\n",
    "* Mongo - document based storage system. Does not enforce normalized data. Can have data redundancies in documents (denormalized data).\n",
    "    * No joins.\n",
    "    * A change to database generally results in needing to change many documents.\n",
    "    * Since there is redundancy in the documents, simple queries are generally faster. But complex queries are often slower.\n",
    "    \n",
    "\n",
    "|         | SQL          | Mongo          |\n",
    "|---------|--------------|----------------|\n",
    "| Schema  | Yes => Joins | No => No Joins |\n",
    "| Storage | Table        | Collection     |\n",
    "|         | Row          | Document       |\n",
    "|         | Column       | Field          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Mongo\n",
    "\n",
    "In practice, there two main ways that you will be connecting with mongo:\n",
    "\n",
    "* From Python\n",
    "* From the console - shell\n",
    "    \n",
    "<div style=\"text-align: center\"><img src=\"images/mongo_clients.png\" style=\"width: 600px\"></div>\n",
    "\n",
    "Both of these clients require a Mongo server to be running. In practice this will require you to start a Mongo Daemon process. To do this exectute the command `mongod` at the terminal.\n",
    "\n",
    "* Note: The Mongo Daemon will need to occupy the terminal that you started it in for the life of the server session. Read: run `mongod` in a seperate terminal tab (or tmux).\n",
    "\n",
    "Now we're going to explore the interacting with the mongo shell. This is not the main way that you'll be interacting with mongo while scraping, we have Python for that. But it's good to know how to issue queries from the shell for various reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo Shell Demo Code\n",
    "\n",
    "## Using Mongo - General Commands for Inspecting Mongo\n",
    "\n",
    "```javascript\n",
    "help                        // List top level mongo commands\n",
    "\n",
    "db.help()                   // List database level mongo commands\n",
    "\n",
    "db.<collection name>.help() // List collection level mongo commands.\n",
    "\n",
    "show dbs                    // Get list of databases on your system\n",
    "\n",
    "use <database name>         // Change the database that you're current using\n",
    "\n",
    "show collections            // Get list of collections within the database that you're currently using\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting\n",
    "\n",
    "Once you're using a database you refer to it with the name **db**. Collections within databases are accessible through dot notation.\n",
    "\n",
    "```javascript\n",
    "db.users.insert({ name: 'Jon', age: '45', friends: [ 'Henry', 'Ashley']})\n",
    "\n",
    "db.getCollectionNames()  // Another way to get the names of collections in current database\n",
    "\n",
    "db.users.insert({ name: 'Ashley', age: '37', friends: [ 'Jon', 'Henry']})\n",
    "db.users.insert({ name: 'Frank', age: '17', friends: [ 'Billy'], car : 'Civic'})\n",
    "\n",
    "db.users.find()\n",
    "\n",
    "    { \"_id\" : ObjectId(\"573a39\"), \"name\" : \"Jon\", \"age\" : \"45\", \"friends\" : [ \"Henry\", \"Ashley\" ] }\n",
    "    { \"_id\" : ObjectId(\"573a3a\"), \"name\" : \"Ashley\", \"age\" : \"37\", \"friends\" : [ \"Jon\", \"Henry\" ] }\n",
    "    { \"_id\" : ObjectId(\"573a3b\"), \"name\" : \"Frank\", \"age\" : \"17\", \"friends\" : [ \"Billy\" ], \"car\" : \"Civic\" }\n",
    "```\n",
    "\n",
    "Things to note:\n",
    "* The three documents that we inserted into the above database didn't all have the same fields.\n",
    "* Mongo creates an ` _id` field for each document if one isn't provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "```javascript\n",
    "db.users.find({ name: 'Jon'})                       // find by single field\n",
    "\n",
    "db.users.find({ car: { $exists : true } })          // find by presence of field\n",
    "\n",
    "db.users.find({ friends: 'Henry' })                 // find by value in array\n",
    "\n",
    "db.users.find({}, { name: true })                   // field selection (only return name)\n",
    "```\n",
    "\n",
    "A quick way to figure out how to write a Mongo query is to think about how you would do it in SQL and check out a resource like this Mongo endorsed [conversion guide](https://docs.mongodb.com/manual/reference/sql-comparison/#create-and-alter), or use something like a [query translator](http://www.querymongo.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating\n",
    "\n",
    "```javascript\n",
    "db.users.update({name: \"Jon\"}, { $set: {friends: [\"Phil\"]}})            // replaces friends array\n",
    "\n",
    "db.users.update({name: \"Jon\"}, { $push: {friends: \"Susie\"}})            // adds to friends array\n",
    "\n",
    "db.users.update({name: \"Stevie\"}, { $push: {friends: \"Nicks\"}}, true)   // upsert\n",
    "\n",
    "db.users.update({}, { $set: { activated : false } }, false, true)       // multiple updates\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Cursors\n",
    "\n",
    "To import existing data into a mongo database one uses `mongoimport` at the command line. In this way mongo will accept a number of data types: JSON, CSV, and TSV.\n",
    "\n",
    "```\n",
    "mongoimport --db tweets --collection coffee --file coffee-tweets.json\n",
    "```\n",
    "\n",
    "Now that we have some larger data we can see that returns from queries are not always so small.\n",
    "\n",
    "```javascript\n",
    "use tweets\n",
    "db.coffee.find()\n",
    "```\n",
    "\n",
    "When the return from a query will display up to the first 20 documents, after that you will need to type `it` to get more. The cursor that it returns is actually an object that has many methods implemented on it and supports the command `it` to iterate through more return items.\n",
    "\n",
    "```javascript\n",
    "db.coffee.find().count()      // 122\n",
    "\n",
    "db.coffee.find().limit(2)     // Only two documents\n",
    "\n",
    "db.coffee.find().sort({ 'user.followers_count' : -1}).limit(3)  // Top three users by followers count\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "MongoDB also has a flexible shell/driver. This allows you take some action based on a query or update documents you can use an iterator on the cursor to go document by document. In the Javascript shell we can do this with Javascript's `forEach`. `forEach` is similar to Python's iteration with the `for` loop; however, Javascript actually has a more functional approach to this type of iteration and requires that you pass a callback, a function, similar to `map` and `reduce`.\n",
    "\n",
    "```javascript\n",
    "db.coffee.find().forEach(function(doc) {\n",
    "    doc.entities.urls.forEach(function(url) {\n",
    "        db.urls.insert({ 'url' : url, 'user': doc.user });\n",
    "    });\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Aggregations in Mongo end up being way less pretty than in SQL/Pandas. Let's just bite the bullet and take a look:\n",
    "\n",
    "```\n",
    "db.coffee.aggregate( [ { $group :\n",
    "    {\n",
    "        _id: \"$place.country_code\",\n",
    "        total: { $sum: \"$retweet_count\" }\n",
    "    }\n",
    "}])\n",
    "```\n",
    "\n",
    "Here we are first declaring that we're going to do some sort of grouping operation. Then, as Mongo desires everything to have an `_id` field, we specify that the `_id` is going to be the country code that is contained within the place field. And then we're going to perform a sum over the retweet counts within each country code; this information is going to be stored in a field called `total`. What do we get back?\n",
    "\n",
    "```\n",
    "db.coffee.find({ retweet_count : { $gt : 0 }  })\n",
    "```\n",
    "\n",
    "For a guide on how to convert from an SQL style aggregation to a Mongo style aggreagetion, check out this [aggreagtion conversion guide](https://docs.mongodb.com/manual/reference/sql-aggregation-comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afternoon Objectives\n",
    "\n",
    "1. Understand the process of getting data from the web.\n",
    "2. Know the basics of HTML/CSS:\n",
    "    * Know how to pull desired data from web pages.\n",
    "3. Be able to use existing API's to get fetch pre-formatted data.\n",
    "\n",
    "### Internet vs. World Wide Web\n",
    "\n",
    "* The internet is commonly refered to as a network of networks. It is the infrastructure that allow networks all around the world to connect with one another. There are many different protocols to transfer information within this larger, meta-network.\n",
    "* The World Wide Web, or Web, provides one of the ways that data can be transfered over the internet. Uses a **U**niform **R**esource **L**ocator, URL, to specify the location, within the internet, of a document.\n",
    "\n",
    "    <div style=\"text-align: center\"><h3>Anatomy of a URL</h3><img src=\"images/url.png\" style=\"width: 600px\"></div>\n",
    "    \n",
    "* Documents on the web are generally written in **H**yper**T**ext **M**arkup **L**anguage, HTML, which can be natively viewed by browsers, the tool that we use to browse the web.\n",
    "\n",
    "### Communication on the Web\n",
    "\n",
    "Information is transmitted around the web through a number of protocols. The main one that you will see is the **H**yper**T**ext **T**ransfer **P**rotocol, HTTP. These transfers, called **requests**, are initiated in a number of ways, but always begin with the client, read: you at your browser.\n",
    "\n",
    " <div style=\"text-align: center\"><h3>Requests in Action</h3><img src=\"images/requests.png\" style=\"width: 600px\"></div>\n",
    " \n",
    "There are 4 main types of request that can be issued by your browser: get, post, put and delete. For web scraping purposes, you will almost always be using get requests. We will learn some more about the others in a couple of weeks during data products day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from a Web Page with Python\n",
    "\n",
    "Scraping a web site basically comes down to making a request from Python and parsing through the HTML that is returned from each page. For each of these tasks we have a Python library, `requests` and `bs4`, respectively.\n",
    "\n",
    "### Requests Library\n",
    "\n",
    "The [requests](http://docs.python-requests.org/en/latest/index.html) library is designed to simplify the process of making http requests within Python. The interface is mindbogglingly simple. Instantiate a requests object to the request, this will mostly be a `get`, with the URL and optional parameters you'd like passed through the request. That instance make the results of the request available via attributes/methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "seven_by_seven = 'http://www.7x7.com'\n",
    "r = requests.get(seven_by_seven)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Info from a Web Page\n",
    "\n",
    "Now that we can gain easy access to the HMTL for a web page, we need some way to pull the desired content from it. Luckily there is already a system in place to do this. With a combination of HMTL and CSS selectors we can identify the information on a HMTL page that we wish to retrieve and grab it with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.select('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very cool resource for learning about CSS selectors: http://flukeout.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tag in soup.select('h2 a'):\n",
    "    url = tag['href']\n",
    "    print url if '7x7' in url else seven_by_seven + url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_current_pages = 2054\n",
    "seven_by_seven_base_url = seven_by_seven + '/?page=0%2C{}'\n",
    "for page in xrange(0, total_current_pages + 1):\n",
    "    page_r = requests.get(seven_by_seven_base_url.format(page))\n",
    "    soup = BeautifulSoup(page_r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you go through a web site you should build up a dictionary for the documents that you want to store in Mongo. IN the example above we may, for each post url, create a dictionary with the information:\n",
    "```python\n",
    "    { url: url_of_post,\n",
    "      date: date_of_post,\n",
    "      poster: name_of poster,\n",
    "      content: text_from_post }\n",
    "```\n",
    "\n",
    "We can then insert these dictionaries into a Mongo database via PyMongo, which we will learn about next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from an Existing API\n",
    "\n",
    "Let's take a look at the API for all the publically avaliable policing data in the [UK](https://data.police.uk/docs/). After taking a look at the documentation for the interface, let's experiment with what we get when we issue a request to this API. The process looks remarkable similar to the one we went through for scraping a web page, except this time the response we're looking for is avaliable via the `json()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://data.police.uk/api/crimes-street-dates')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Scraping and Mongo\n",
    "\n",
    "Many APIs will give you a choice of how it will return data to you, choosing json will make life easier since we will frequently be using Mongo for our storage unit during our scraping endeavors, and it plays very well with json. \n",
    "\n",
    "Interacting with Mongo from Python is done with the other Mongo client that we talked about earlier PyMongo. It is designed to have a similar interface as the Mongo shell does, this ends up being fairly intuitive since both Python and JavaScript are object oriented languages, and therefore store and refer to things in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client['uk_police']\n",
    "collection = db['all_crime']\n",
    "\n",
    "r = requests.get('https://data.police.uk/api/crimes-no-location?category=all-crime&force=warwickshire&date=2013-09')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.insert_many(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "for item in collection.find({ 'month' : '2013-09' }):\n",
    "    pp.pprint(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
