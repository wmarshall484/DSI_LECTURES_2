{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Statistics, Markov-Chain Monte Carlo, and PyMC\n",
    "\n",
    "#### Jack Bennetto\n",
    "#### November 22, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we talked a bit about Bayesian statistics. To review:\n",
    "\n",
    "Suppose we're considering some hypothesis $H$ and we've collected some data $\\mathbf{X}$.\n",
    "$$ P(H|\\mathbf{X}) = \\frac{P(\\mathbf{X}|H) P(H)}{P(\\mathbf{X})} $$\n",
    "\n",
    "Each term has a name.\n",
    "\n",
    "* $P(H)$ is the *prior probability*\n",
    "* $P(\\mathbf{X}|H)$ is the *likelihood*.\n",
    "* $P(\\mathbf{X})$ is the *normalizing constant*.\n",
    "* $P(H|\\mathbf{X})$ is the *posterior probability*.\n",
    "\n",
    "\n",
    "If there are a bunch of hypotheses $H_1, H_2, ... H_n$, we could write this as\n",
    "\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we see the normalizing constant is the likelihood times the prior summed over all possible hypothesis. In other words, it's the constant (independent of hypothesis) needed to be multiplied by all the numerators so that they all add up to one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you worked that out computationally you saw calculating the normalizing constant was difficult, particularly in cases with an infinite number of possible hypotheses. One approach we took was to used conjugate priors, but they aren't available in general. The other is to divide the hypothesis space into many slices, but that beocmes computationally impossible in high-dimensional space. In particular, that means giving the same attention to areas that are likely in the posterior as those that are almost impossible.\n",
    "\n",
    "A third choice is random sampling. We call this a *monte-carlo* approach. In particular we sample many points (hypothese) from the posterior distribution, transitioning from each point to the next, in what is called Markov-Chain Monte Carlo (MCMC). There are a number of different approaches to this, but a common one, called the [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) algorithm, works like this.\n",
    "\n",
    "First, we start at a given set of values. We then adjust them slightly (in such way that the probabilities are symmetric) and consider a new point. If it has a higher value for the pdf of the posterior, we accept it automatically. If it's lower, we accept it with a probability of the fraction of the ratio with the dpdf at the current point. Otherwise  we reject the new point. We repeat this many, mnay times.\n",
    "\n",
    "Today most MCMC approaches use more sophisticated algorithms (pymc3 used NUTS (No-U-Turn Sampling)), but the idea is often similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to install pymc3. Try using conda, like this:\n",
    "\n",
    "`conda install -c conda-forge pymc3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as scs\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the coin-flip/CTR example we've seem a couple times before. In this we have data on successes and failures and we want to determine the value of some unknown click-through rate. First, let's make some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_actual = 0.10\n",
    "data_ctr = scs.bernoulli(prob_actual).rvs(100)\n",
    "data_ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pymc interface uses the `with` statement in python to specify a model with which we'll be operating. The first time we interact with in we'll need to call the constructor and assign it to a variable with `as`; after that we just refer to the variable. Any variables created are added to that model object automatically, and any sampling or optimization is done on that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model_ctr:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are a couple different types of variables. First, we can create a variable that represents some prior distribution of a variable. Here we're just starting with a uniform distribution for a prior. Each object created in the model (usually) needs a name so we can identity it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model_ctr:\n",
    "    prob = pm.Uniform('prob', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other type of variable in the model is a likelihood, specifying the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model_ctr:\n",
    "    observed = pm.Bernoulli('observed', prob, observed=data_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do some sampling! The result of sampling is often called a *trace*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_ctr:\n",
    "    trace = pm.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `sample` will contain values for all of the variables, distributed (if we have enough) in the same manner as the posterior distribution of those variables. Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(trace['prob'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "Why doesn't it peak at the actual value for the probability (`prob_actual`)?\n",
    "\n",
    "Why isn't this very smooth?\n",
    "\n",
    "Is this a beta distribution?\n",
    "\n",
    "What else can we do with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A (slightly) harder example\n",
    "\n",
    "Bernoulli distributions aren't all that exciting. Let's sample a few points from a normal distribution and  try to recover the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_actual = 5\n",
    "sigma_actual = 2\n",
    "data = scs.norm(mu_actual, sigma_actual).rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean = {:.3f} sd = {:.3f}\".format(data.mean(), data.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's assume we know `sigma`, just to make it easier. We'll take a uniform prior for `mu` but assume it's between 0 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model_normal:\n",
    "    # prior\n",
    "    mu = pm.Uniform(\"mu\", 0, 10)\n",
    "    # likelihood\n",
    "    observed = pm.Normal(\"observed\", mu, 2, observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to start my calculating the maximum *a posteriori* (MAP) value for the `mu`.\n",
    "\n",
    "Question: what is MAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_normal:\n",
    "    estimate = pm.find_MAP()\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_normal:\n",
    "    trace = pm.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(trace['mu'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really we want to estimate both the `mu` and `sigma`. We just need to put both in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model_normal2:\n",
    "    # prior\n",
    "    mu = pm.Uniform('mu', 0, 10)\n",
    "    sigma = pm.Uniform('sigma', 0, 10)\n",
    "    # likelihood\n",
    "    observed = pm.Normal(\"observed\", mu, sigma, observed=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_normal2:\n",
    "    estimate = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these the right values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"                   mu   sigma\")\n",
    "print(\"MAP estimate      {:5.3f} {:5.3f}\".format(float(estimate['mu']), float(estimate['sigma'])))\n",
    "print(\"sample statistics {:5.3f} {:5.3f}\".format(data.mean(), data.std()))\n",
    "print(\"actual            {:5.3f} {:5.3f}\".format(mu_actual, sigma_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_normal2:\n",
    "    trace = pm.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2)\n",
    "axes[0].hist(trace['mu'], bins=50, normed=True)\n",
    "axes[1].hist(trace['sigma'], bins=50, normed=True)\n",
    "axes[0].set_title(\"mu\")\n",
    "axes[1].set_title(\"sigma\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a function to plot this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot will show us the how they are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(trace['mu'], trace['sigma'], '.', alpha=0.05)\n",
    "ax.set_xlabel('mu')\n",
    "ax.set_ylabel('sigma')\n",
    "ax.plot(data.mean(), data.std(), 'rx', ms=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these aren't independent. If sigma is small, then mu must be near the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once more, together\n",
    "\n",
    "So all that's great, but generally we're interested in doing more than taking the mean. Let's try doing a linear-regression problem together (with some fake data) before doing the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
