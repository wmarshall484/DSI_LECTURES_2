{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power / Bayes\n",
    "### Jack Bennetto\n",
    "#### May 4, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    " * Define power, significance, standard deviation, effect size and sample size, and explain their relationship\n",
    " * Compute the sample size needed for an experiment\n",
    " * Describe the difference between Frequentist and Bayesian statistics\n",
    " * Use Bayes' theorem to calculate posterior probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "In the morning we'll talk about power calculations when doing Frequentist A/B testing. In the afternoon we'll move on to Bayesian statistics.\n",
    "\n",
    "### Morning: Power Calculation\n",
    "\n",
    "* Review of A/B testing\n",
    "* Types of Errors and Power\n",
    "* Calculating sample size\n",
    "\n",
    "### Afternoon: Bayesian Statistics\n",
    "\n",
    "* Compare Frequentist and Bayesian approaches\n",
    "* Review Bayes' theorem\n",
    "* Calculate posterior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Lecture: Power Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B Testing\n",
    "\n",
    "Yesterday we talked about Frequentist A/B testing. In this, you set up some test to measure some number of values and decide how suprised you need to be to reject the null hypothesis. This significance level, called $\\alpha$, is often something like 0.05, i.e., if there's less than a 5% chance your results (or something even more extreme) could have happened by chance given the null hypothesis, then you assume that hypothesis is wrong.\n",
    "\n",
    "\n",
    "Of course, you might be wrong and reject the null hypothesis even though it's correct just because of the sample of data you took. That is called a **false positive** or **type I error**. It's positive because you have a positive result (rejecting null) and it's false because it's wrong. I rememeber that's a type I errors because false positive is much more common than false negative in casual conversation.\n",
    "\n",
    "The lower you set $\\alpha$, the less likely you'll get a type-I error.\n",
    "\n",
    "But there's a tradeoff. The opposite situation is a **false negative** or **type II error**, when the null hypothesis is wrong but you fail to reject it. The probability of a type-II error (assuming the null hypothesis is wrong) is $\\beta$.\n",
    "\n",
    "The lower you set $\\alpha$ the higher $\\beta$ will be, and vise versa.\n",
    "\n",
    "**Power**, defined as $1 - \\beta$, is the probability that you're correctly able the reject the null hypothesis assuming that it is in fact false.\n",
    "\n",
    "\n",
    "|               | Reject $H_0$            | Fail to reject $H_0$\n",
    "|---------------|-------------------------|---------------\n",
    "|**$H_0$ false**| Correct ($1-\\beta$)     | Type II error ($\\beta$)\n",
    "|**$H_0$ true** | Type I error ($\\alpha$) | Correct ($1-\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside on terminology\n",
    "\n",
    "Generally confusion matrices are discussed in the context of predictive statistics, but it's the same concept so I'm including this here to put the various terms in context.\n",
    "\n",
    "![confusion matrix](Confusion Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting the power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_power(n, sigma, effect_size, critical_value):\n",
    "    standard_error = sigma / n**0.5\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = np.linspace(-3, 8, 200)\n",
    "    xpos = x[x >= critical_value]\n",
    "    xneg = x[x <= critical_value]\n",
    "\n",
    "    h0 = scs.norm(0, standard_error)\n",
    "    ha = scs.norm(effect_size, standard_error)\n",
    "\n",
    "    ax.plot(x, h0.pdf(x), color='red', label='$H_0$')\n",
    "    ax.plot(x, ha.pdf(x), color='blue', label='$H_A$')\n",
    "    ax.fill_between(xpos, 0, h0.pdf(xpos), color='red', alpha=0.2, label=\"$\\\\alpha$\")\n",
    "    ax.fill_between(xneg, 0, ha.pdf(xneg), color='blue', alpha=0.2, label=\"$\\\\beta$\")\n",
    "    ax.fill_between(xpos, 0, ha.pdf(xpos), color='white', hatch='////', alpha=0.2, label=\"Power\")\n",
    "    ax.axvline(critical_value, color='black')\n",
    "    ax.set_xlabel(\"sample mean\")\n",
    "    ax.set_ylabel(\"pdf\")\n",
    "    ax.set_ylim(ymin=0.0)\n",
    "    ax.legend()\n",
    "\n",
    "    print \"standard deviation = {0:7.4f}\".format(sigma)\n",
    "    print \"sample size (n)    = {0:7d}\".format(n)\n",
    "    print \"   standard error  = {0:7.4f}\".format(standard_error)\n",
    "    print \"alpha              = {0:7.4f}\".format(1-h0.cdf(critical_value))\n",
    "    print \"effect size        = {0:7.4f}\".format(effect_size)\n",
    "    print \"power              = {0:7.4f}\".format(1-ha.cdf(critical_value))\n",
    "    print \"   beta            = {0:7.4f}\".format(ha.cdf(critical_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_power(n=100, sigma=10.0, effect_size=2., critical_value=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **effect size** is the amount of difference we hope to detect with our study. If we will only recommend a drug if it will lower blood pressure by at least 10 mm Hg, that's the effect size. In the picture above and calculations below we take the most conservative approach, that that actually is the effect size.\n",
    "\n",
    "We usually don't know the **standard deviation** $\\sigma$ beforehand, and we may need to do a pilot study to estimate this.\n",
    "Ëœ\n",
    "We've already talked about $\\alpha$ and power (or $1-\\beta$). When we create a study, we generally decide what values we expect for these. It's common to choose 0.05 for $\\alpha$ and 0.80 for power.\n",
    "\n",
    "The remaining factor is **sample size** n, the number of data in our sample. In general we are trying to calculate this from the other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating sample size\n",
    "\n",
    "If the samples are large enough the sample means falls in a normal distribution by the **central limit theorem**. The standard deviation of this distribution is called the **standard error** and is given by\n",
    "\n",
    "$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation of the original distribution and $n$ is the sample size.\n",
    "\n",
    "For the minimum value of $n$, the distance from the $\\mu_0$ to the critical value is $SE \\cdot Z_\\alpha$, and the distance of the critical value to $\\mu_A$ is $SE \\cdot Z_\\beta$. So\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_A - \\mu_0 & = SE \\cdot (Z_\\beta + Z_\\alpha) \\\\\n",
    "              & = \\frac{\\sigma}{\\sqrt{n}} (Z_\\beta + Z_\\alpha)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ n \\ge \\left( \\frac{\\sigma ( Z_\\beta + Z_\\alpha )}{\\mu_A - \\mu_0} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sample_size(sigma, effect_size, alpha=0.05, power=0.80):\n",
    "    beta = 1 - power\n",
    "    return ((sigma*(scs.norm(0,1).ppf(beta) + scs.norm(0,1).ppf(alpha)))/effect_size)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship of factors\n",
    "\n",
    "As a summary, here are how the factors relate. This should be read as, for example, \"Holding everything else constant, if **Effect size** is **Larger** then **Power** is **Larger**.\n",
    "\n",
    "| Factor             | Direction |\n",
    "|-------------------|------|----\n",
    "| Power             | Larger | Smaller\n",
    "| Effect size       | Larger | Smaller\n",
    "| Sample size       | Larger | Smaller\n",
    "| Standard deviation| Smaller| Larger\n",
    "| Significance level| Smaller| Larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "## Afternoon lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've discussed Frequentist statistics. Let's review this a bit.\n",
    "\n",
    "What is likelihood?\n",
    "\n",
    "What is a confidence interval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist vs Bayesians\n",
    "\n",
    "In general, Frequentist and Bayesians look at statistical problems in fundamentally ways that are almost inverse of each other.\n",
    "\n",
    "The Frequentist says \"There is one true hypothesis, though we don't know what it is. The observation (or data or sample) is one of many that could have been generated.\"\n",
    "\n",
    "The Bayesian says \"There is no single true hypothesis, but any number of possible hypotheses, each with a probability. The only thing we can know for certain is the observation.\"\n",
    "\n",
    "For a Frequentist, it's absurd to talk about the probability of some hypothesis being true; either it is true or it isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cookie problem\n",
    "\n",
    "Last week we talked about drawing a vanilla cookie from one of two bowls, and used that to calculate probability that we'd picked the first bowl. Imagine that we'd continued the experiment, drawing additional cookies (perhaps with replacement) to get a better and better idea of which bowl we took.\n",
    "\n",
    "This is Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem.\n",
    "\n",
    "Suppose we're considering some hypothesis $H$ and we've collected some data $\\mathbf{X}$.\n",
    "$$ P(H|\\mathbf{X}) = \\frac{P(\\mathbf{X}|H) P(H)}{P(\\mathbf{X})} $$\n",
    "\n",
    "Each term has a name.\n",
    "\n",
    "* $P(H)$ is the *prior probability*\n",
    "* $P(\\mathbf{X}|H)$ is the *likelihood*.\n",
    "* $P(\\mathbf{X})$ is the *normalizing constant*.\n",
    "* $P(H|\\mathbf{X})$ is the *posterior probability*.\n",
    "\n",
    "\n",
    "If there are a bunch of hypotheses $H_1, H_2, ... H_n$, we could write this as\n",
    "\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we see the normalizing constant is the likelihood times the prior summed over all possible hypothesis. In other words, it's the constant (independent of hypothesis) needed to be multiplied by all the numerators so that they all add up to one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baysian statistics to find a mean\n",
    "Let's assume you have a bunch of points drawn from a normal distribution. To make things easy, let's say you happen to know that the standard deviation is 3, and the mean $\\mu \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$.\n",
    "\n",
    "Humans are pretty bad at choosing random numbers, so someone will need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = scs.randint(0,10).rvs()\n",
    "print mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to choose a number from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd = 3\n",
    "scs.norm(mu, sd).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datum = ???enter number here???\n",
    "likelihood = []\n",
    "for i in range(0,10):\n",
    "    likelihood.append(scs.norm(i, sd).pdf(datum))\n",
    "    print(\"The likelihood of N({0}, {1}) generating {2:5.2f} is {3:6.4f}\"\n",
    "           .format(i, sd, datum, likelihood[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), likelihood)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Do these add to one?\n",
    "\n",
    "Which of these hypotheses has the maximum likelihood of producing the data?\n",
    "\n",
    "If we were a Frequentist, we'd go with that, and then we'd construct a confidence interval that giving a range that (had we sampled from the data many times) has a certain probability (maybe 95%) of including the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But today we're all going to be Bayesians, which means we're going to assign probabilities\n",
    "\n",
    "The tough part of being a Bayesian is we need to start out with a prior probabilities. For this, we'll assume that all the probabilities are equal. You choose them that way, that works out, but if I'd just asked you to choose a number that would have been weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = np.ones(10)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to multiple each of these by the likelihood..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    probs[i] *= scs.norm(i, sd).pdf(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then divide normalize them by dividing them each by the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs /= probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and print out the **probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {2:6.4f}\"\n",
    "           .format(i, sd, probs[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was great, but maybe we should get some more data. Generate another number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scs.norm(mu, sd).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datum = ???enter number here???\n",
    "\n",
    "for i in range(10):\n",
    "    probs[i] *= scs.norm(i, sd).pdf(datum)\n",
    "probs /= probs.sum()\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {3:6.4f}\"\n",
    "           .format(i, sd, datum, probs[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Priors\n",
    "\n",
    "The largest disagreement with Bayesian statistics is the nessecity of the prior. In the example about, and in the cookie problem, the prior had actual meaning and the math is beyond dispute. If that's the case (or at least somewhat the case), we have an **informed prior**.\n",
    "\n",
    "In the real world that's not always true and we have to choose an **uninformed prior**. If so we usually say something like \"every possibility is equally likely\" and leave it at that.\n",
    "\n",
    "But if you have enough data the choice of prior doesn't matter all that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
