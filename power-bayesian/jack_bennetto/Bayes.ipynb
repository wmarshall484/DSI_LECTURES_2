{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics\n",
    "### Jack Bennetto\n",
    "#### February 15, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    " * Describe the difference between Frequentist and Bayesian statistics\n",
    " * Use Bayes' theorem to calculate posterior probabilities \n",
    "\n",
    "## Agenda\n",
    "\n",
    "* Compare Frequentist and Bayesian approaches\n",
    "* Review Bayes' theorem\n",
    "* Calculate posterior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've discussed Frequentist statistics. Let's review this a bit.\n",
    "\n",
    "What is likelihood?\n",
    "\n",
    "What is MLE?\n",
    "\n",
    "What is a confidence interval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist vs Bayesians\n",
    "\n",
    "**Question:** What is statistics?\n",
    "\n",
    "When people talk about statistics they usually mean **Frequentist statistics**. All the talk of p-values and confidence intervals and NHST and MLE is all Frequentist terminology. But there's another way to think about statistics. Long ago **Bayesian statistics** was controversial; today it's popular enough that any data scientist should understand both.\n",
    "\n",
    "In general, Frequentist and Bayesians look at statistical problems in fundamentally different ways that are almost inverse of each other.\n",
    "\n",
    "The Frequentist says \"There is one true hypothesis, though we don't know what it is. The observation (or data or sample) is one of many that could have been generated.\"\n",
    "\n",
    "The Bayesian says \"There is no single true hypothesis, but any number of possible hypotheses, each with a probability. The only thing we can know for certain is the observation.\"\n",
    "\n",
    "For a Frequentist, it's absurd to talk about the probability of some hypothesis being true; either it is true or it isn't. This is sometimes taken as a criticism of Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cookie problem\n",
    "\n",
    "The other day we talked about drawing a vanilla cookie from one of two bowls, and used that to calculate probability that we'd picked the first bowl. Imagine that we'd continued the experiment, drawing additional cookies (perhaps with replacement) to get a better and better idea of which bowl we took.\n",
    "\n",
    "This is Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem.\n",
    "\n",
    "Suppose we're considering some hypothesis $H$ and we've collected some data $\\mathbf{X}$.\n",
    "$$ P(H|\\mathbf{X}) = \\frac{P(\\mathbf{X}|H) P(H)}{P(\\mathbf{X})} $$\n",
    "\n",
    "Each term has a name.\n",
    "\n",
    "* $P(H)$ is the *prior probability*\n",
    "* $P(\\mathbf{X}|H)$ is the *likelihood*.\n",
    "* $P(\\mathbf{X})$ is the *normalizing constant*.\n",
    "* $P(H|\\mathbf{X})$ is the *posterior probability*.\n",
    "\n",
    "\n",
    "If there are a bunch of hypotheses $H_1, H_2, ... H_n$, we could write this as\n",
    "\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we see the normalizing constant is the likelihood times the prior summed over all possible hypothesis (using the law of total probability).\n",
    "\n",
    "In other words, it's the constant (independent of hypothesis) needed to be multiplied by all the numerators so that they all add up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics to find a mean\n",
    "Let's assume you have a bunch of points drawn from a normal distribution. To make things easy, let's say you happen to know that the standard deviation is 3, and the mean $\\mu \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$. We're going to determine the probability that any of those are the correct mean based on data.\n",
    "\n",
    "Humans are pretty bad at choosing random numbers, so someone will need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = stats.randint(0,10).rvs()\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to choose a number from the distribution. What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = 3\n",
    "stats.norm(mu, sd).rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now using that number, I'm going to figure out the likelihood you would have gotten that from any of the possible hypotheses, by looking at the **pdf** of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = ???enter data here???\n",
    "likelihood = []\n",
    "for i in range(0,10):\n",
    "    likelihood.append(stats.norm(i, sd).pdf(datum))\n",
    "    print(\"The likelihood of N({0}, {1}) generating {2:5.2f} is {3:6.4f}\"\n",
    "           .format(i, sd, datum, likelihood[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), likelihood)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do these add to one?\n",
    "\n",
    "Which of these hypotheses has the maximum likelihood of producing the data?\n",
    "\n",
    "If we were a Frequentist, we'd go with that, and then we'd construct a confidence interval, giving a range that (had we sampled from the data many times) has a certain probability (maybe 95%) of including the actual value.\n",
    "\n",
    "But today we're all going to be Bayesians, which means we're going to assign probabilities of each hypothesis being true.\n",
    "\n",
    "The tough part of being a Bayesian is we need to start out with a prior probabilities. For this, we'll assume that all the probabilities are equal. You chose them that way using the computer, so that works out, but if you'd picked a number from your head, and you liked some numbers more than others, that might not be best.\n",
    "\n",
    "The arbitrary choice of priors is probably **the largest criticism** of Bayesian statistics. But if you have enough data it doesn't matter that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.ones(10)/10\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to multiple each of these by the likelihood..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    probs[i] *= stats.norm(i, sd).pdf(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then divide normalize them by dividing them each by the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs /= probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, what we've done is multiplied each of the prior probabilities by the likelihood of each hypothesis of generating the observed data, and divided these all by the normalizing constant, to get the posterior probabilities.\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Question:** what are those terms called again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got for the **probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {2:6.4f}\"\n",
    "           .format(i, sd, probs[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do these add up to one?\n",
    "\n",
    "Okay, that was great, but maybe we should get some more data. Generate another number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(mu, sd).rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we\n",
    " * calculate the likelihoods,\n",
    " * multiply these **by our old posterior probabilities** (which are the new priors),\n",
    " * normalize (divide the sum of the prior times likelihood, so they add to one), and\n",
    " * look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datum = ???enter data here???\n",
    "\n",
    "for i in range(10):\n",
    "    probs[i] *= stats.norm(i, sd).pdf(datum)\n",
    "probs /= probs.sum()\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {3:10.8f}\"\n",
    "           .format(i, sd, datum, probs[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing this iteratively, repeatedly getting another data point and updating our prior, but we could have done this all at once, calculating the likelihood of seeing the whole dataset.\n",
    "\n",
    "In a real problem we'd have *many* more possible hypotheses. In the case above, we might not know the number came from a discrete distribution so we'd need to consider every possible value. And we probably wouldn't know the standard deviation, so we'd need to consider every combination of a mean and standard deviation. We could follow the same approach, calculating the likelihood of seeing our data for each possible hypothesis and updating the posterior probabilities. Later we'll talk about how to solve this practically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
