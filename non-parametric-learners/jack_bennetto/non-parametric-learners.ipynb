{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Learners\n",
    "\n",
    "## kNN and Decision Trees\n",
    "\n",
    "### Jack Bennetto\n",
    "#### March 26, 2017\n",
    "\n",
    "(based on John Bourassa's lecture, based on my lecture in zipfian, based on the lecture from Jonathan Torrez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "#### Morning: kNN\n",
    "\n",
    " * Non-parametric models\n",
    " * Understanding kNN\n",
    " * Scaling\n",
    " * Distance metrics\n",
    " * Curse of Dimensionality\n",
    " \n",
    "#### Afternoon: Decision Trees\n",
    " * Introduction\n",
    " * Entropy and purity\n",
    " * Recursion\n",
    " * Decision-tree algorithm\n",
    " * Gini and regression\n",
    " * Pre- and post- pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Objectives - kNN\n",
    "\n",
    "* **Implement** the kNN algorithm\n",
    "* **State** common distance metrics used for kNN\n",
    "* **Describe** effect of varying k (num_neigbors)\n",
    "* **Explain** importance of scaling for kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Non-Parametric Models\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "\n",
    "$$\\hat y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + ... + \\hat\\beta_i X_i $$\n",
    "    \n",
    "How many parameters can we describe it with?\n",
    "\n",
    "Can we ever change the number of parameters?\n",
    "\n",
    "What does using this model mean about the structure of our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "* Linear regression\n",
    "* Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Non-Parametric Models\n",
    "\n",
    "Class discussion: What do you think they are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: contrast the pros and cons of these two types of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Non-parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as scs\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "%matplotlib inline\n",
    "# Make it pretty\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Seed random functions for reproducibility\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "#Reduce the data down to 2 classes and 2 predictor variables\n",
    "X = X[y != 0, :2] \n",
    "\n",
    "#Add some noise so data points aren't exactly duplicated\n",
    "X = X + np.random.normal(0, .05, size = X.shape)\n",
    "\n",
    "#Change Sepal Length from cm to mm to cause scaling issues\n",
    "X[:,0] = X[:,0] * 10\n",
    "y = y[y != 0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save some points for later\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flower_data(ax, X, y):\n",
    "    ax.scatter(X[y == 0,0], X[y == 0,1], c = \"orange\", label = \"versicolor\")\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], color = \"black\", label = \"virginica\")\n",
    "    ax.set_xlabel(\"Sepal Length (mm)\")\n",
    "    ax.set_ylabel(\"Sepal Width (cm)\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_flower_data(ax, Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we get new data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What type of flower are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new points\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the original data\n",
    "plot_flower_data(ax, Xtrain, ytrain)\n",
    "\n",
    "ax.scatter(Xtest[:,0], Xtest[:,1], color = \"red\", s = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN pseudocode\n",
    "\n",
    "Class discussion: What do you think this is? Describe both the training and prediction phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "ax.plot(a[0], a[1], 'ro')\n",
    "ax.plot(b[0], b[1], 'bo')\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "#ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we predict for different values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mult_decision_boundary(ax, X, y, k, scaled=True, \n",
    "                                title='Title', xlabel='xlabel', \n",
    "                                ylabel='ylabel', hard_class = True):\n",
    "    \n",
    "    \"\"\"Plot the decision boundary of a kNN classifier.\n",
    "    \n",
    "    Builds and fits a sklearn kNN classifier internally.\n",
    "\n",
    "    X must contain only 2 continuous features.\n",
    "\n",
    "    Function modeled on sci-kit learn example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: Matplotlib axes object\n",
    "        The plot to draw the data and boundary on\n",
    "        \n",
    "    X: numpy array\n",
    "        Training data\n",
    "    \n",
    "    y: numpy array\n",
    "        Target labels\n",
    "    \n",
    "    k: int\n",
    "        The number of neighbors that get a vote.\n",
    "        \n",
    "    scaled: boolean, optional (default=True)\n",
    "        If true scales the features, else uses features in original units\n",
    "    \n",
    "    title: string, optional (default = 'Title')\n",
    "        A string for the title of the plot\n",
    "    \n",
    "    xlabel: string, optional (default = 'xlabel')\n",
    "        A string for the label on the x-axis of the plot\n",
    "    \n",
    "    ylabel: string, optional (default = 'ylabel')\n",
    "        A string for the label on the y-axis of the plot\n",
    "    \n",
    "    hard_class: boolean, optional (default = True)\n",
    "        Use hard (deterministic) boundaries vs. soft (probabilistic) boundaries\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    x_mesh_step_size = 0.1\n",
    "    y_mesh_step_size = 0.01\n",
    "    \n",
    "    #Hard code in colors for classes, one class in red, one in blue\n",
    "    bg_colors = np.array([np.array([255, 150, 150])/255, np.array([150, 150, 255])/255])\n",
    "    cmap_light = ListedColormap(bg_colors)\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    #Build a kNN classifier\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    \n",
    "    if scaled:\n",
    "        #Build pipeline to scale features\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X, y)\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = 45, 85\n",
    "    y_min, y_max = 2, 4\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, x_mesh_step_size),\n",
    "                         np.arange(y_min, y_max, y_mesh_step_size))\n",
    "    if hard_class:\n",
    "        dec_boundary = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        ax.pcolormesh(xx, yy, dec_boundary, cmap=cmap_light)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    else:\n",
    "        dec_boundary = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        colors = dec_boundary.dot(bg_colors)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "        ax.imshow(colors.reshape(200, 400, 3), origin = \"lower\", aspect = \"auto\", extent = (x_min, x_max, y_min, y_max))\n",
    "\n",
    "    ax.set_title(title + \", k={0}, scaled={1}\".format(k, scaled))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((y_min, y_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_ks = [1, 3, 5, 10, 25, 75]\n",
    "\n",
    "title=\"Iris Decision Boundary\"\n",
    "xlabel=\"Sepal Length (mm)\"\n",
    "ylabel=\"Sepal Width (cm)\"\n",
    "\n",
    "# Building my plot, 3 rows x 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 14))\n",
    " \n",
    "# Loop through the possible ks, put each decision boundary a separate axes\n",
    "for k, ax in zip(some_ks, axs.flatten()):\n",
    "    plot_mult_decision_boundary(ax, X, y, k=k, title = title, xlabel = xlabel, ylabel = ylabel)\n",
    "\n",
    "# Keep things from overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft kNN\n",
    "\n",
    "But a good classifier will return probabilities, not just classes. With kNN we use the number of neighbors of each class as the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building my plot, 3 rows x 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 14))\n",
    "#ax.imshow()\n",
    "# Loop through the possible ks, put each decision boundary a separate axes\n",
    "for k, ax in zip(some_ks, axs.flatten()):\n",
    "    plot_mult_decision_boundary(ax, X, y, k=k, title = title, xlabel = xlabel, ylabel = ylabel, hard_class = False)\n",
    "\n",
    "# Keep things from overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k\n",
    "\n",
    "How do we figure out the best value for k?\n",
    "\n",
    "In general, a good starting point for k is $\\sqrt{n}$\n",
    "\n",
    "Let's investigate for various values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sample(X, y, k, seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=seed)\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return log_loss(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 50\n",
    "step = 2\n",
    "results = []\n",
    "for k in range(1, max_k, step):\n",
    "    kresults = []\n",
    "    for seed in range(100):\n",
    "        kresults.append(predict_sample(X, y, k, seed))\n",
    "    results.append(kresults)\n",
    "results = pd.DataFrame(results)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "data = np.array(results.T)\n",
    "ax.boxplot(data, labels=range(1, max_k, step))\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"log loss on additional data\")\n",
    "ax.set_title(\"Log Loss of kNN on Iris for Various k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "What is bias and variance again?\n",
    "\n",
    "Class discussion: How does changing k affect each? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create samples from a multivariate normal distribution\n",
    "# which approximates the input data\n",
    "def gen_similar_samples(X, y, n_samples = 4, sample_size = 50):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for _ in range(n_samples):\n",
    "        Xdata = np.zeros_like(X)\n",
    "        ydata = np.zeros_like(y)\n",
    "        for i, value in enumerate(np.unique(y)):\n",
    "            Xdata[i*sample_size:(i+1)*sample_size] = np.random.multivariate_normal(X[y == value].mean(axis = 0), \n",
    "                                                                                   np.cov(X[y == value].T), \n",
    "                                                                                   sample_size)\n",
    "            ydata[i*sample_size:(i+1)*sample_size] = value\n",
    "        Xs.append(Xdata)\n",
    "        ys.append(ydata)\n",
    "    \n",
    "    return list(zip(Xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "test_data = gen_similar_samples(X, y)\n",
    "for sample, ax in zip(test_data, axs.flatten()):\n",
    "    X_sample = sample[0]\n",
    "    y_sample = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X_sample, y_sample, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Offset the figure title to make it look nice\n",
    "plt.subplots_adjust(top=0.9)\n",
    "# Set a title for the entire figure\n",
    "plt.suptitle('Low Bias, High Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for sample, ax in zip(test_data, axs.flatten()):\n",
    "    X = sample[0]\n",
    "    y = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X, y, k=50, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('High Bias, Low Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Could Be Important\n",
    "\n",
    "What is going on in these plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 4))\n",
    "                        \n",
    "plot_mult_decision_boundary(axs[0], X, y, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "plot_mult_decision_boundary(axs[1], X, y, k=1, scaled=False, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule is that if changing the units before building the model will change the predictions, scale the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics\n",
    "\n",
    "So far we've used euclidean distance for a metric; there are other choices.\n",
    "\n",
    "### Manhattan Distance\n",
    "City blocks, L1\n",
    "\n",
    "*Manhattan* distance is the distance as measured along axes at right angles:\n",
    "\n",
    "$$\\sum_i |a_i - b_i|$$\n",
    "\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Straight line, L2\n",
    "\n",
    "*Euclidean* distance is the distance metric you're most familiar with:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = ||\\mathbf{a} - \\mathbf{b}|| \\ = \\sqrt{\\sum (a_i - b_i)^2} $$\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Angle\n",
    "\n",
    "*Cosine* similarity is another commonly used distance metric. It's measuring the angle between the two vectors:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}||  ||\\mathbf{b}||} $$\n",
    "\n",
    "\n",
    "#### sklearn default is Minkowski (with p=2 which is really just euclidean, p=1 is manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your understanding:\n",
    "\n",
    "Which of these people are the most similar?  Which would be the most similar by Euclidean or Manhattan distance?\n",
    "\n",
    "| age | salary |\n",
    "|--|--|\n",
    "| 25 | 20k |\n",
    "| 25 | 60k |\n",
    "| 25 | 65k |\n",
    "| 75 | 60k |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "### Perspective 1: Sample Density\n",
    "\n",
    "Sampling density is proportional to $$N^\\frac{1}{p}$$\n",
    "\n",
    "where N is the number of samples and p is the number of dimensions.\n",
    "\n",
    "Let's consider a data set with 100 samples that all have only one feature/predictor. But, you feel that one feature doesn't tell you enough to properly predict anything, so you set out to collect new data. The new data will have ten features/predictors for each sample. How many samples do we need to have an equally dense sample as our original dataset?\n",
    "\n",
    "#### Original Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          density & = N^\\frac{1}{p} \\\\\n",
    "          & = 100^\\frac{1}{1} \\\\\n",
    "          & = 100 \n",
    "\\end{align}$$\n",
    "\n",
    "#### New Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          N & = density^ p \\\\\n",
    "          &= 100^{10} \\\\\n",
    "          &= 100,000,000,000,000,000,000\n",
    "\\end{align}$$\n",
    "\n",
    "No big, just one hundred quadrillion samples...\n",
    "\n",
    "### Perspective 2: Loss of Locality\n",
    "\n",
    "As we increase dimensionality, we lose the concept of locality and things get infinitely far apart.\n",
    "\n",
    "More precisely, samples that are simliar no longer look similar and \"closeness\" becomes more arbitrary than meaningful. \n",
    "\n",
    "For example, in Euclidean distance we must be close in ***all*** dimensions to be considered close.\n",
    "\n",
    "Close in 2 of 3 dimensions, but far in the 3rd dimension makes the points far apart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points drawn from a standard normal distribution \n",
    "# get further from the origin as dimensionality increases\n",
    "n_points = 10000\n",
    "\n",
    "norm_data = np.random.normal(size = (n_points, 10))\n",
    "pct_close = []\n",
    "for i in range(1, 11):\n",
    "    value = np.mean(np.linalg.norm(norm_data[:,:i], axis = 1) < 1)\n",
    "    print(\"{} dim: {:.2f}%\".format(i, value * 100))\n",
    "    pct_close.append(value)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.plot(range(1, 11), pct_close)\n",
    "ax.set_ylabel(\"Percent of points with L2 norm > 1\")\n",
    "ax.set_xlabel(\"Number of Dimensions\")\n",
    "ax.set_title(\"Ratio of points within unit sphere at increasing dimensionality\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Variants\n",
    "\n",
    "Here are a few common variants on the kNN algorithm.\n",
    "\n",
    "### Weighted Voting\n",
    "\n",
    "<img src=\"images/weighted_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "The ith point votes with weight of $$\\frac{1}{d_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "asize = 50/((a - center)**2).sum(axis=0)\n",
    "bsize = 50/((b - center)**2).sum(axis=0)\n",
    "ax.scatter(a[0], a[1], c='r', s=asize)\n",
    "ax.scatter(b[0], b[1], c='b', s=bsize)\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regression kNN\n",
    "\n",
    "<img src=\"images/regression_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "Let the k nearest points have targets $$t_1, t_2,..., t_k$$\n",
    "\n",
    "Predict the mean value ***or*** predict a weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of kNN\n",
    "\n",
    "Class discussion: what do you think they are? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives  - Decision trees\n",
    "\n",
    "* **Describe** pros/cons of decision tree algorithm\n",
    "* **Implement** pseudocode for decision tree algorithm\n",
    "* **Describe** common measures for making splits in a decision tree\n",
    "* **Demonstrate** concept of recursion and **relate** it to decision trees\n",
    "* **State** pruning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in our data\n",
    "tennis_df = pd.read_table('data/tennis.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up a few things, based on my preferences and making calculating probabilities easier\n",
    "tennis_df.rename(columns={'playtennis': 'played'}, inplace=True)\n",
    "tennis_df['played'] = tennis_df['played'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "tennis_df.sort_values('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Would You Determine If You Played Tennis?\n",
    "\n",
    "Class discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Our Decisions\n",
    "\n",
    "### Shannon Entropy\n",
    "\n",
    "Common measure of impurity.  Shannon Entropy quantifies the amount of information gained by observing a random variable.  The value of receiving this new information is inversely proportional to the probability of it occurring.  For example, if you are told that you are going to be doing pair exercises this afternoon, that doesn't provide you any additional information.\n",
    "\n",
    "While the occurrence of very uncommon events does provide you the most information, they are also very unlikely.  Since Shannon Entropy calculates the expectation of the information provided by observing a random process, maximal entropy is achieved when all outcomes are equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "H(X) & = E[I(X)] \\\\\n",
    "     & = E[log_2(\\tfrac{1}{Pr(X)})] \\\\\n",
    "     & = -E[log_2(Pr(X))] \\\\\n",
    "     & = - \\sum_i Pr_i log_2(Pr_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = Discrete Random Variable\n",
    "\n",
    "I(X) = Information Content of X\n",
    "\n",
    "Pr$_i$ = Probability of X taking on value i\n",
    "\n",
    "log$_2$(Pr$_i$) = \"Information\" supplied by observing value i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlog2x(x):\n",
    "    '''return x * log2(x), returning 0 for x==0'''\n",
    "    return scipy.special.xlogy(x,x) / np.log(2)\n",
    "\n",
    "x = np.linspace(0, 1)\n",
    "entropy = - (xlog2x(x) + xlog2x(1-x))\n",
    "gini = 2 * (1 - x**2 - (1-x)**2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, entropy, label=\"Entropy\")\n",
    "ax.plot(x, gini, label=\"2 * Gini\")\n",
    "ax.set_ylabel(\"Entropy/Impurity\")\n",
    "ax.set_xlabel(\"Proportion of one class\")\n",
    "ax.set_title(\"Impurity of a Binary Decision-Tree Node\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "Probability of a mis-classification occurring if random samples from the distribution are classified according to the probability of the distribution.\n",
    "\n",
    "For example, say X represents the beverage your coworker drinks in the morning, and after watching them for a few weeks (_creepy_) you estimate the following:\n",
    "\n",
    "P(x = \"coffee\") = .2\n",
    "\n",
    "P(x = \"tea\") = .5\n",
    "\n",
    "P(x = \"water\") = .3\n",
    "\n",
    "If each morning you generated a random number and used that to predict your coworker's beverage choice of the day, how often would you be correct?\n",
    "\n",
    "P(x = \"coffee\", guess = \"coffee\") = P(x = \"coffee\") * P(guess = \"coffee\")\n",
    "\n",
    "P(x = \"coffee\") == P(guess = \"coffee\") == .2\n",
    "\n",
    "P(x == guess) = .2$^2$ + .5$^2$ + .3$^2$ = .38\n",
    "\n",
    "And the Gini Index is just 1 minus the probability of a correct guess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Gini}(S) = 1 - \\sum_{i \\in S} Pr_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(y):\n",
    "    \"\"\"Return the entropy of the array y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    total_samples = y.shape[0]\n",
    "    summation = 0\n",
    "    \n",
    "    for class_i in np.unique(y):\n",
    "        prob = sum(y == class_i) / float(total_samples)\n",
    "        summation += prob * np.log2(prob)\n",
    "    \n",
    "    return -summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tennis = tennis_df['played'].values\n",
    "\n",
    "print(\"Entropy of original data set is {}\".format(calc_entropy(y_tennis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for Entropy\n",
    "\n",
    "$$\\text{IG}(S, C) = H(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} H(C_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C = Candidate subsets of S\n",
    "\n",
    "|C|, |S| = Number of elements in C, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_info_gain(y, y1, y2, impurity_func):\n",
    "    \"\"\"Return the information gain of making the given split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "        Labels for parent node\n",
    "    \n",
    "    y1: 1d numpy array\n",
    "        Labels for potential child node 1\n",
    "    \n",
    "    y2: 1d numpy array\n",
    "        Labels for potential child node 2\n",
    "    \n",
    "    impurity_func: function\n",
    "        Function which calculates the impurity of the node \n",
    "        (e.g. Shannon Entropy)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    total_samples = float(y.shape[0])\n",
    "    child_imp = 0\n",
    "    y_impurity = impurity_func(y)\n",
    "    \n",
    "    for child_node in (y1, y2):\n",
    "        child_num = child_node.shape[0]\n",
    "        child_imp += (child_num / total_samples) * impurity_func(child_node)\n",
    "        \n",
    "    return y_impurity - child_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's split on the temperature.\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'hot']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'hot']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information Gain is {0}.\".format(calc_info_gain(y_tennis, y1, y2, calc_entropy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not much gain, maybe I really liked playing when the temperature was mild\n",
    "# Let's try that split\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'mild']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'mild']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information Gain is {0}.\".format(calc_info_gain(y_tennis, y1, y2, calc_entropy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still not much headway, let's just try every possible split and see what works\n",
    "# And also confirm what we used our common sense algorithm we started with\n",
    "\n",
    "possible_splits = {}\n",
    "# Get just my features from the dataframe\n",
    "feature_cols = tennis_df.drop('played', axis=1).columns\n",
    "\n",
    "# For a given column, find all the unique possible values\n",
    "for col in feature_cols:\n",
    "    col_splits = np.unique(tennis_df[col])\n",
    "    # For each possible value, split the dataset using that value\n",
    "    for pos_val in col_splits:\n",
    "        y1 = tennis_df[tennis_df[col] == pos_val]['played'].values\n",
    "        y2 = tennis_df[tennis_df[col] != pos_val]['played'].values\n",
    "        # Calculate the information gain, save it for later\n",
    "        ig = calc_info_gain(y_tennis, y1, y2, calc_entropy)\n",
    "        key = \"{0}: {1}\".format(col, pos_val)\n",
    "        possible_splits[key] = ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out our results in a pretty way\n",
    "colname1 = \"Col Name: Value\"\n",
    "colname2 = \"Information Gain\"\n",
    "\n",
    "# :20 is specifying a column width, https://docs.python.org/3/library/string.html#formatspec\n",
    "print(\"{0:20} || {1}\".format(colname1, colname2))\n",
    "print(\"-\"*40)\n",
    "# operator.itemgetter(1) allows us to sort by the second item of the tuple\n",
    "for k,v in sorted(possible_splits.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    print(\"{0:20} || {1}\".format(k, round(v, 4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting on overcast gave us the largest info gain\n",
    "# Let's look at what those child nodes look like and their respective entropies\n",
    "child_node_left = tennis_df[tennis_df['outlook'] == 'overcast']\n",
    "print(\"Entropy of left child node is {0}\".format(\n",
    "          calc_entropy(child_node_left['played'].values)))\n",
    "child_node_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, we got a pure node, that makes sense.\n",
    "# What about the other node?\n",
    "child_node_right = tennis_df[tennis_df['outlook'] != 'overcast']\n",
    "print(\"Entropy of right child node is {0}\".format(\n",
    "          calc_entropy(child_node_right['played'].values)))\n",
    "child_node_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: why in the world would we want a node that has maximum entropy?\n",
    "\n",
    "## How would you know how to stop?\n",
    "\n",
    "Class discussion: When would you stop? Potential problems?\n",
    "\n",
    "Will get to this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "### Splitting Continuous Values\n",
    "\n",
    "Class discussion: how would you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "\n",
    "The final predictions of a decision tree and a regression tree are implemented differently:\n",
    "\n",
    "### Classification\n",
    "\n",
    "* Majority class in leaf node\n",
    "* Better to predict probability of label (impure leaf nodes)\n",
    "\n",
    "### Regression\n",
    "\n",
    "* Mean of values in leaf node\n",
    "* Linear Regression (ooo, fancy)\n",
    "    * Also called *model trees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion\n",
    "\n",
    "Recursion uses the idea of \"divide and conquer\" to solve problems. It divides a complex problem you are working on into smaller sub-problems that are easily solved, rather than trying to solve the complex problem directly.\n",
    "\n",
    "Recursive functions split the problem into two cases: the ***base case*** and the ***recursive case***. The function continually loops, calling itself, until it reaches the base case.\n",
    "\n",
    "* Base case: Stopping criteria, the simplest case that can be solved directly.\n",
    "* Recursive case: Function that splits the problem into the smaller subproblems.\n",
    "    \n",
    "### Three Laws of Recursion\n",
    "\n",
    "1. A recursive algorithm must have a base case.\n",
    "2. A recursive algorithm must change its state and move toward the base case.\n",
    "3. A recursive algorithm must call itself, recursively.\n",
    "    \n",
    "### Example: Factorial\n",
    "\n",
    "Are the following functions the same?\n",
    "\n",
    "$$ f(x) = \\prod_{i=1}^xi $$\n",
    "\n",
    "$$f(x) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & \\mbox{if } x \\leq 1 \\\\\n",
    "\t\txf(x-1) & \\mbox otherwise\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(x):\n",
    "    \"\"\"Recursively calculate x!\"\"\"\n",
    "    # base case is when we get to x=0, which is 0! = 1\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    # otherwise, recursive case, notice how we are reducing x\n",
    "    else:\n",
    "        return x * factorial(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power(base, exp):\n",
    "    \"\"\"Recursively caclulate base ** exp\"\"\"\n",
    "    # base case is when exp = 0, base ** 0 = 1\n",
    "    if exp <= 0:\n",
    "        return 1\n",
    "    #  otherwise, recursive case, reduce exp\n",
    "    return base * power(base, exp - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "power(25, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "25 ** 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion not always the answer, but useful tool to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You do, fill in the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def len_str(s):\n",
    "    \"\"\"Recursively determine the length of a string\"\"\"\n",
    "    # base case, when should you stop?  \n",
    "    # recursive case, how can you reduce your problem?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test your function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Pseudocode\n",
    "\n",
    "Recursive partitioning algorithm\n",
    "\n",
    "```\n",
    "function BuildTree:\n",
    "    # base case, stop building tree\n",
    "    If every item in the dataset is in the same class\n",
    "    or there is not feature left to split the data:\n",
    "        return a leaf node with the class label\n",
    "    # recursive case, keep splitting stuff\n",
    "    Else:\n",
    "        find the best feature and value to split the data\n",
    "        split the dataset\n",
    "        create a node\n",
    "        for each split\n",
    "            call Build Tree and add the result as a child of the node\n",
    "        return node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Decision Trees\n",
    "\n",
    "Class discussion: what are some problems you forsee with this approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classification_tree(ax, X, y, model=None, fit = True):\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'r.', label='versicolor')\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'b.', label='virginica')\n",
    "    ax.set_title(\"Classifying Iris with Decision Trees\")\n",
    "    ax.set_xlabel(\"Sepal Length (mm)\")\n",
    "    ax.set_ylabel(\"Sepal Width (cm)\")\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    if model is None:\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "    if fit:\n",
    "        model.fit(X, y)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    plot_classification_thresholds(ax, X, y, 0, model.tree_, xlim, ylim)\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "def plot_classification_thresholds(ax, X, y, inode, tree, xlim, ylim):\n",
    "    threshold = tree.threshold[inode]\n",
    "    if tree.feature[inode] == -2:\n",
    "        color = ['r', 'b'][np.argmax(tree.value[inode])]\n",
    "        ax.add_patch(Rectangle((xlim[0], ylim[0]), xlim[1]-xlim[0], ylim[1]-ylim[0], fc=color, alpha=0.3))\n",
    "        return\n",
    "    if tree.feature[inode] == 0:\n",
    "        ax.plot((threshold, threshold), ylim, 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, (xlim[0], threshold), ylim)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, (threshold, xlim[1]), ylim)\n",
    "    else:\n",
    "        ax.plot(xlim, (threshold, threshold), 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, xlim, (ylim[0], threshold))\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, xlim, (threshold, ylim[1]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = gen_similar_samples(X, y, 1)[0]\n",
    "model = tree.DecisionTreeClassifier()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_classification_tree(ax, X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is some risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = gen_similar_samples(X, y, 1)[0]\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_classification_tree(ax, X, y, model, fit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't let it split so many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 10)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_classification_tree(ax, X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Tree\n",
    "\n",
    "* Cross validate!\n",
    "    * Train trees with different parameters, see which performs best on validation set\n",
    "    * No different than any other model\n",
    "\n",
    "## Pruning\n",
    "\n",
    "We call the idea of modifying a decision tree to improve its performance **pruning**.\n",
    "\n",
    "There are two approaches: **pre-pruning** (limiting the tree as we build it) and **post-pruning** (sometimes just called \"pruning\"; chopping back the tree after it is built.\n",
    "\n",
    "### Pre\n",
    "\n",
    "Prune as we build the tree, control hyper-parameters\n",
    "\n",
    "Class discussion: what can we control?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post\n",
    "\n",
    "Prune after we build the complete tree\n",
    "\n",
    "* Merge leaves if doing so decreases *test-set* error\n",
    "* If not leaves, consider each branch first (recursively)\n",
    "\n",
    "#### Pseudocode\n",
    "```\n",
    "function Prune:\n",
    "    if either left or right is not a leaf:\n",
    "        call Prune on those that aren't\n",
    "    if both left and right are (now) leaf nodes:\n",
    "        calculate error associated with merging two leaf nodes\n",
    "        calculate error associated without merging two leaf nodes\n",
    "        if merging results in lower error:\n",
    "            merge the leaf nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Decision Trees\n",
    "\n",
    "Class discussion: what do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision Tree Algorithms\n",
    "\n",
    "There are some famous variants of the decision tree algorithm:\n",
    "\n",
    "### ID3\n",
    "\n",
    "Short for Iterative Dichotomiser 3, the original Decision Tree algorithm developed by Ross Quinlan (who's responsible for a lot of proprietary decision tree algorithms) in the 1980's.\n",
    "\n",
    "* designed for only categorial features\n",
    "* splits categorical features completely\n",
    "* uses entropy and information gain to pick the best split\n",
    "\n",
    "### CART\n",
    "\n",
    "Short for Classification and Regression Tree was invented about the same time as ID3 by Breiman, Friedman, Olshen and Stone. The CART algorithm has the following properties:\n",
    "\n",
    "* handles both categorial and continuous data\n",
    "* always uses binary splits\n",
    "* uses gini impurity to pick the best split\n",
    "\n",
    "Algorithms will be called CART even if they don't follow all of the specifications of the original algorithm.\n",
    "\n",
    "### C4.5\n",
    "\n",
    "This is Quinlan's first improvement on the ID3 algorithm. The main improvements are:\n",
    "\n",
    "* handles continuous data\n",
    "* implements pruning to reduce overfitting\n",
    "\n",
    "### C5.0\n",
    "\n",
    "This is supposedly better, but it's proprietary so we don't have access to the specifics of the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
