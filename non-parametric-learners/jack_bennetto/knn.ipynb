{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest-Neighbors\n",
    "\n",
    "\n",
    "### Jack Bennetto\n",
    "#### January 29, 2019\n",
    "\n",
    "(based on John Bourassa's lecture, based on my lecture in zipfian, based on the lecture from Jonathan Torrez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "#### Morning: kNN\n",
    "\n",
    " * Non-parametric models\n",
    " * Understanding kNN\n",
    " * Scaling\n",
    " * Distance metrics\n",
    " * Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Objectives - kNN\n",
    "\n",
    "* **Implement** the kNN algorithm\n",
    "* **State** common distance metrics used for kNN\n",
    "* **Describe** effect of varying k (num_neigbors)\n",
    "* **Explain** importance of scaling for kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Non-Parametric Models\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "\n",
    "$$\\hat y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + ... + \\hat\\beta_i X_i $$\n",
    "    \n",
    "How many parameters can we describe it with?\n",
    "\n",
    "Can we ever change the number of parameters?\n",
    "\n",
    "What does using this model mean about the structure of our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "* Linear regression\n",
    "* Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Non-Parametric Models\n",
    "\n",
    "Class discussion: What do you think they are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: contrast the pros and cons of these two types of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "%matplotlib inline\n",
    "# Make it pretty\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Seed random functions for reproducibility\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "#Reduce the data down to 2 classes and 2 predictor variables\n",
    "X = X[y != 0, :2] \n",
    "\n",
    "#Add some noise so data points aren't exactly duplicated\n",
    "X = X + np.random.normal(0, .05, size = X.shape)\n",
    "\n",
    "#Change Sepal Length from cm to mm to cause scaling issues\n",
    "X[:,0] = X[:,0] * 10\n",
    "y = y[y != 0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save some points for later\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flower_data(ax, X, y):\n",
    "    ax.scatter(X[y == 0,0], X[y == 0,1], c = \"orange\", label = \"versicolor\")\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], color = \"black\", label = \"virginica\")\n",
    "    ax.set_xlabel(\"Sepal Length (mm)\")\n",
    "    ax.set_ylabel(\"Sepal Width (cm)\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_flower_data(ax, Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we get new data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What type of flower are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new points\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the original data\n",
    "plot_flower_data(ax, Xtrain, ytrain)\n",
    "\n",
    "ax.scatter(Xtest[:,0], Xtest[:,1], color = \"red\", s = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN pseudocode\n",
    "\n",
    "Class discussion: What do you think this is? Describe both the training and prediction phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "ax.plot(a[0], a[1], 'ro')\n",
    "ax.plot(b[0], b[1], 'bo')\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "#ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we predict for different values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mult_decision_boundary(ax, X, y, k, scaled=True, \n",
    "                                title='Title', xlabel='xlabel', \n",
    "                                ylabel='ylabel', hard_class = True):\n",
    "    \n",
    "    \"\"\"Plot the decision boundary of a kNN classifier.\n",
    "    \n",
    "    Builds and fits a sklearn kNN classifier internally.\n",
    "\n",
    "    X must contain only 2 continuous features.\n",
    "\n",
    "    Function modeled on sci-kit learn example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: Matplotlib axes object\n",
    "        The plot to draw the data and boundary on\n",
    "        \n",
    "    X: numpy array\n",
    "        Training data\n",
    "    \n",
    "    y: numpy array\n",
    "        Target labels\n",
    "    \n",
    "    k: int\n",
    "        The number of neighbors that get a vote.\n",
    "        \n",
    "    scaled: boolean, optional (default=True)\n",
    "        If true scales the features, else uses features in original units\n",
    "    \n",
    "    title: string, optional (default = 'Title')\n",
    "        A string for the title of the plot\n",
    "    \n",
    "    xlabel: string, optional (default = 'xlabel')\n",
    "        A string for the label on the x-axis of the plot\n",
    "    \n",
    "    ylabel: string, optional (default = 'ylabel')\n",
    "        A string for the label on the y-axis of the plot\n",
    "    \n",
    "    hard_class: boolean, optional (default = True)\n",
    "        Use hard (deterministic) boundaries vs. soft (probabilistic) boundaries\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    x_mesh_step_size = 0.1\n",
    "    y_mesh_step_size = 0.01\n",
    "    \n",
    "    #Hard code in colors for classes, one class in red, one in blue\n",
    "    bg_colors = np.array([np.array([255, 150, 150])/255, np.array([150, 150, 255])/255])\n",
    "    cmap_light = ListedColormap(bg_colors)\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    #Build a kNN classifier\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    \n",
    "    if scaled:\n",
    "        #Build pipeline to scale features\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X, y)\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = 45, 85\n",
    "    y_min, y_max = 2, 4\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, x_mesh_step_size),\n",
    "                         np.arange(y_min, y_max, y_mesh_step_size))\n",
    "    if hard_class:\n",
    "        dec_boundary = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        ax.pcolormesh(xx, yy, dec_boundary, cmap=cmap_light)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    else:\n",
    "        dec_boundary = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        colors = dec_boundary.dot(bg_colors)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "        ax.imshow(colors.reshape(200, 400, 3), origin = \"lower\", aspect = \"auto\", extent = (x_min, x_max, y_min, y_max))\n",
    "\n",
    "    ax.set_title(title + \", k={0}, scaled={1}\".format(k, scaled))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((y_min, y_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_ks = [1, 3, 5, 10, 25, 75]\n",
    "\n",
    "title=\"Iris Decision Boundary\"\n",
    "xlabel=\"Sepal Length (mm)\"\n",
    "ylabel=\"Sepal Width (cm)\"\n",
    "\n",
    "# Building my plot, 3 rows x 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 14))\n",
    " \n",
    "# Loop through the possible ks, put each decision boundary a separate axes\n",
    "for k, ax in zip(some_ks, axs.flatten()):\n",
    "    plot_mult_decision_boundary(ax, X, y, k=k, title = title, xlabel = xlabel, ylabel = ylabel)\n",
    "\n",
    "# Keep things from overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft kNN\n",
    "\n",
    "But a good classifier will return probabilities, not just classes. With kNN we use the number of neighbors of each class as the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building my plot, 3 rows x 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 14))\n",
    "#ax.imshow()\n",
    "# Loop through the possible ks, put each decision boundary a separate axes\n",
    "for k, ax in zip(some_ks, axs.flatten()):\n",
    "    plot_mult_decision_boundary(ax, X, y, k=k, title = title, xlabel = xlabel, ylabel = ylabel, hard_class = False)\n",
    "\n",
    "# Keep things from overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k\n",
    "\n",
    "How do we figure out the best value for k?\n",
    "\n",
    "In general, a good starting point for k is $\\sqrt{n}$\n",
    "\n",
    "Let's investigate for various values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(X, y, k, seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=seed)\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return log_loss(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_k = 50\n",
    "step = 2\n",
    "results = []\n",
    "for k in range(1, max_k, step):\n",
    "    kresults = []\n",
    "    for seed in range(100):\n",
    "        kresults.append(predict_sample(X, y, k, seed))\n",
    "    results.append(kresults)\n",
    "results = pd.DataFrame(results)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "data = np.array(results.T)\n",
    "ax.boxplot(data, labels=range(1, max_k, step))\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"log loss on additional data\")\n",
    "ax.set_title(\"Log Loss of kNN on Iris for Various k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "What is bias and variance again?\n",
    "\n",
    "Class discussion: How does changing k affect each? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples from a multivariate normal distribution\n",
    "# which approximates the input data\n",
    "def gen_similar_samples(X, y, n_samples = 4, sample_size = 50):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for _ in range(n_samples):\n",
    "        Xdata = np.zeros_like(X)\n",
    "        ydata = np.zeros_like(y)\n",
    "        for i, value in enumerate(np.unique(y)):\n",
    "            Xdata[i*sample_size:(i+1)*sample_size] = np.random.multivariate_normal(X[y == value].mean(axis = 0), \n",
    "                                                                                   np.cov(X[y == value].T), \n",
    "                                                                                   sample_size)\n",
    "            ydata[i*sample_size:(i+1)*sample_size] = value\n",
    "        Xs.append(Xdata)\n",
    "        ys.append(ydata)\n",
    "    \n",
    "    return list(zip(Xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "test_data = gen_similar_samples(X, y)\n",
    "for sample, ax in zip(test_data, axs.flatten()):\n",
    "    X_sample = sample[0]\n",
    "    y_sample = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X_sample, y_sample, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Offset the figure title to make it look nice\n",
    "plt.subplots_adjust(top=0.9)\n",
    "# Set a title for the entire figure\n",
    "plt.suptitle('Low Bias, High Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for sample, ax in zip(test_data, axs.flatten()):\n",
    "    X = sample[0]\n",
    "    y = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X, y, k=50, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('High Bias, Low Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Could Be Important\n",
    "\n",
    "What is going on in these plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 4))\n",
    "                        \n",
    "plot_mult_decision_boundary(axs[0], X, y, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "plot_mult_decision_boundary(axs[1], X, y, k=1, scaled=False, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule is that if changing the units before building the model will change the predictions, scale the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics\n",
    "\n",
    "So far we've used euclidean distance for a metric; there are other choices.\n",
    "\n",
    "### Manhattan Distance\n",
    "City blocks, L1\n",
    "\n",
    "*Manhattan* distance is the distance as measured along axes at right angles:\n",
    "\n",
    "$$\\sum_i |a_i - b_i|$$\n",
    "\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Straight line, L2\n",
    "\n",
    "*Euclidean* distance is the distance metric you're most familiar with:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = ||\\mathbf{a} - \\mathbf{b}|| \\ = \\sqrt{\\sum (a_i - b_i)^2} $$\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Angle\n",
    "\n",
    "*Cosine* similarity is another commonly used distance metric. It's measuring the angle between the two vectors:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}||  ||\\mathbf{b}||} $$\n",
    "\n",
    "\n",
    "#### sklearn default is Minkowski (with p=2 which is really just euclidean, p=1 is manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your understanding:\n",
    "\n",
    "Which of these people are the most similar?  Which would be the most similar by Euclidean or Manhattan distance?\n",
    "\n",
    "| age | salary |\n",
    "|--|--|\n",
    "| 25 | 20k |\n",
    "| 25 | 60k |\n",
    "| 25 | 65k |\n",
    "| 75 | 60k |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "### Perspective 1: Sample Density\n",
    "\n",
    "Sampling density is proportional to $$N^\\frac{1}{p}$$\n",
    "\n",
    "where N is the number of samples and p is the number of dimensions.\n",
    "\n",
    "Let's consider a data set with 100 samples that all have only one feature/predictor. But, you feel that one feature doesn't tell you enough to properly predict anything, so you set out to collect new data. The new data will have ten features/predictors for each sample. How many samples do we need to have an equally dense sample as our original dataset?\n",
    "\n",
    "#### Original Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          density & = N^\\frac{1}{p} \\\\\n",
    "          & = 100^\\frac{1}{1} \\\\\n",
    "          & = 100 \n",
    "\\end{align}$$\n",
    "\n",
    "#### New Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          N & = density^ p \\\\\n",
    "          &= 100^{10} \\\\\n",
    "          &= 100,000,000,000,000,000,000\n",
    "\\end{align}$$\n",
    "\n",
    "No big, just one hundred quadrillion samples...\n",
    "\n",
    "### Perspective 2: Loss of Locality\n",
    "\n",
    "As we increase dimensionality, we lose the concept of locality and things get infinitely far apart. More precisely, samples that are similar no longer look similar and \"closeness\" becomes more arbitrary than meaningful. \n",
    "\n",
    "For example, in Euclidean distance we must be close in ***all*** dimensions to be considered close. If points are close in 2 of 3 dimensions, but far in the 3rd dimension then all points will be far apart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points drawn from a standard normal distribution \n",
    "# get further from the origin as dimensionality increases\n",
    "n_points = 10000\n",
    "\n",
    "norm_data = np.random.normal(size = (n_points, 10))\n",
    "pct_close = []\n",
    "for i in range(1, 11):\n",
    "    value = np.mean(np.linalg.norm(norm_data[:,:i], axis = 1) < 1)\n",
    "    print(\"{} dim: {:.2f}%\".format(i, value * 100))\n",
    "    pct_close.append(value)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.plot(range(1, 11), pct_close)\n",
    "ax.set_ylabel(\"Percent of points with L2 norm > 1\")\n",
    "ax.set_xlabel(\"Number of Dimensions\")\n",
    "ax.set_title(\"Ratio of points within unit sphere at increasing dimensionality\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Variants\n",
    "\n",
    "Here are a few common variants on the kNN algorithm.\n",
    "\n",
    "### Weighted Voting\n",
    "\n",
    "<img src=\"images/weighted_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "The ith point votes with weight of $$\\frac{1}{d_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "asize = 50/((a - center)**2).sum(axis=0)\n",
    "bsize = 50/((b - center)**2).sum(axis=0)\n",
    "ax.scatter(a[0], a[1], c='r', s=asize)\n",
    "ax.scatter(b[0], b[1], c='b', s=bsize)\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regression kNN\n",
    "\n",
    "<img src=\"images/regression_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "Let the k nearest points have targets $$t_1, t_2,..., t_k$$\n",
    "\n",
    "Predict the mean value ***or*** predict a weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of kNN\n",
    "\n",
    "Class discussion: what do you think they are? why?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
