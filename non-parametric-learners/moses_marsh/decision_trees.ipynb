{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "# Make it pretty\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Seed random functions for reproducibility\n",
    "#np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "### Moses Marsh\n",
    "\n",
    "(based heavily on the notebooks from Jack Bennetto & Jonathan Torrez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives: Answer the following\n",
    " * What are some differences between parametric and non-parametric models?\n",
    " * When splitting a data set, how do you evaluate the quality of a split?\n",
    "   * entropy, Gini impurity, and information gain\n",
    "   * classification vs regression\n",
    " * How do you build a decision tree?\n",
    "   * recursion\n",
    " * How can you tune a decision tree to prevent overfitting?\n",
    "   * pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Non-Parametric Models\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "\n",
    "$$\\hat y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + ... + \\hat\\beta_i X_i $$\n",
    "    \n",
    "How many parameters can we describe it with?\n",
    "\n",
    "Can we ever change the number of parameters?\n",
    "\n",
    "What does using this model mean about the structure of our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/prop_param.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "* Linear regression\n",
    "* Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Non-Parametric Models\n",
    "\n",
    "Class discussion: What do you think they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/prop_non_param.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: contrast the pros and cons of these two types of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_param.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Non-parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_nonparam.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Play Tennis\" data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in our data\n",
    "tennis_df = pd.read_table('data/tennis.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up a few things, based on my preferences and making calculating probabilities easier\n",
    "tennis_df.rename(columns={'playtennis': 'played'}, inplace=True)\n",
    "tennis_df['played'] = tennis_df['played'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "tennis_df.sort_values('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Would You Determine If You Played Tennis?\n",
    "\n",
    "Class discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Our Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does windiness do a good job of predicting \"played\"? Let's split our dataset into two subsets: points with `wind == True` and points with `wind == False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tennis_df['wind'] == True\n",
    "print(tennis_df[mask])\n",
    "print('--'*25)\n",
    "print(tennis_df[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much did this help us? A perfect decision would have separated all the `played == 0` points from the `played == 1` points, and a perfectly useless decision would have given us two subsets of the data that each had equal amounts of `0`s and `1`s. Can we make a metric for how \"mixed up\" or \"impure\" the set of labels is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "\n",
    "One common measure of impurity is _entropy_. Let $X$ be a discrete random variable with probability mass function $P(X)$.\n",
    "\n",
    "For today's purposes, the values of $X$ are _class labels_, and $P(X=i)=p_i$ is the _probability of drawing a point with class $i$_\n",
    "\n",
    "For $m$ classes, the entropy is defined as:\n",
    "$$\\begin{align}\n",
    "H(X) & = E[log_2(\\tfrac{1}{P(X)})] \\\\\n",
    "     & = -E[log_2(P(X))] \\\\\n",
    "     & = - \\sum_{i=1}^m p_i log_2(p_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Graph of Bernoulli Random Variable\n",
    "\n",
    "<img src=\"images/entropy_graph.png\" width=500 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(y):\n",
    "    \"\"\"Return the entropy of the array y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    summation = 0\n",
    "    \n",
    "    for class_i in np.unique(y):\n",
    "        prob = np.mean(y == class_i)\n",
    "        summation += prob * np.log2(prob)\n",
    "    \n",
    "    return -summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tennis_df['played'].values\n",
    "\n",
    "print(\"Entropy of original data set is {}\".format(calc_entropy(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain \n",
    "Information gain is the difference between the _impurity of the parent node $S$_ and the _weighted impurities of the child nodes$\\{C_i\\}$_. A higher IG for a split means that the split did a better job of separating (or un-mixing) the class labels. \n",
    "\n",
    "$$\\text{IG}(S, C) = H(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} H(C_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_info_gain(y, y1, y2, impurity_func):\n",
    "    \"\"\"Return the information gain of making the given split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "        Labels for parent node\n",
    "    \n",
    "    y1: 1d numpy array\n",
    "        Labels for potential child node 1\n",
    "    \n",
    "    y2: 1d numpy array\n",
    "        Labels for potential child node 2\n",
    "    \n",
    "    impurity_func: function\n",
    "        Function which calculates the impurity of the node \n",
    "        (e.g. Shannon Entropy)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    total_samples = float(y.shape[0])\n",
    "    child_imp = 0\n",
    "    y_impurity = impurity_func(y)\n",
    "    \n",
    "    for child_node in (y1, y2):\n",
    "        child_num = child_node.shape[0]\n",
    "        child_imp += (child_num / total_samples) * impurity_func(child_node)\n",
    "        \n",
    "    return y_impurity - child_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's split on the temperature.\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'hot']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'hot']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(y1)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information Gain is {0}.\".format(calc_info_gain(y, y1, y2, calc_entropy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not much gain, maybe I really liked playing when the temperature was mild\n",
    "# Let's try that split\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'mild']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'mild']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information Gain is {0}.\".format(calc_info_gain(y, y1, y2, calc_entropy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still not much headway, let's just try every possible split and see what works\n",
    "\n",
    "possible_splits = {}\n",
    "# Get just my features from the dataframe\n",
    "feature_cols = tennis_df.drop('played', axis=1).columns\n",
    "\n",
    "# For a given column, find all the unique possible values\n",
    "for col in feature_cols:\n",
    "    col_splits = np.unique(tennis_df[col])\n",
    "    # For each possible value, split the dataset using that value\n",
    "    for pos_val in col_splits:\n",
    "        y1 = tennis_df[tennis_df[col] == pos_val]['played'].values\n",
    "        y2 = tennis_df[tennis_df[col] != pos_val]['played'].values\n",
    "        # Calculate the information gain, save it for later\n",
    "        ig = calc_info_gain(y, y1, y2, calc_entropy)\n",
    "        key = \"{0}: {1}\".format(col, pos_val)\n",
    "        possible_splits[key] = ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out our results in a pretty way\n",
    "colname1 = \"Col Name: Value\"\n",
    "colname2 = \"Information Gain\"\n",
    "\n",
    "# :20 is specifying a column width, https://docs.python.org/3/library/string.html#formatspec\n",
    "print(\"{0:20} || {1}\".format(colname1, colname2))\n",
    "print(\"-----------------------------------------\")\n",
    "for k,v in sorted(possible_splits.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(\"{0:20} || {1}\".format(k, v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting on overcast gave us the largest info gain\n",
    "# Let's look at what those child nodes look like and their respective entropies\n",
    "child_node_left = tennis_df[tennis_df['outlook'] == 'overcast']\n",
    "print(\"Entropy of left child node is {0}\".format(\n",
    "          calc_entropy(child_node_left['played'].values)))\n",
    "child_node_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, we got a pure node, that makes sense.\n",
    "# What about the other node?\n",
    "child_node_right = tennis_df[tennis_df['outlook'] != 'overcast']\n",
    "print(\"Entropy of right child node is {0}\".format(\n",
    "          calc_entropy(child_node_right['played'].values)))\n",
    "child_node_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you know how to stop?\n",
    "\n",
    "Class discussion: When would you stop? Potential problems?\n",
    "\n",
    "Will get to this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ways to Calculate Impurity\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "This is the default for sci-kit learn trees. Again it involves knowing the class frequencies $p_i$ within a set of points.\n",
    "\n",
    "The Gini impurity is the _probability of misclassification_ under the following procedure:\n",
    "* Take a random element from the set.\n",
    "* Label it randomly according to the distribution of labels in the set.\n",
    "\n",
    "The probability of picking a point of class $i$ is $p_i$, and the probability of then giving it the wrong label is $(1-p_i)$, so the total misclassification probability for a set containing $m$ classes is\n",
    "\n",
    "$$\\text{Gini}(S) = \\sum_{i=1}^m p_i(1-p_i) = 1 - \\sum_{i=1}^m p_i^2$$\n",
    "\n",
    "### Information Gain with Gini Impurity\n",
    "\n",
    "$$\\text{IG}(S, C) = \\text{Gini}(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} \\text{Gini}(C_i)$$\n",
    "\n",
    "## Regression Trees\n",
    "\n",
    "### Splitting Continuous Values\n",
    "\n",
    "Class discussion: how would you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance! For a node $S$ with $N$ data points:\n",
    "$$ \\text{Var}(S) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\bar{y})^2$$\n",
    "$$ \\bar{y} = \\frac{1}{N}\\sum_{i=1}^N y_i $$\n",
    "\n",
    "$$\\text{IG}(S, C) = \\text{Var}(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} \\text{Var}(C_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "\n",
    "The final predictions of a decision tree and a regression tree are implemented differently:\n",
    "\n",
    "### Classification\n",
    "\n",
    "* Majority class in leaf node\n",
    "* Better to predict probability of label (impure leaf nodes)\n",
    "\n",
    "### Regression\n",
    "\n",
    "* Mean of values in leaf node\n",
    "* Linear Regression (ooo, fancy)\n",
    "    * Also called *model trees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion\n",
    "\n",
    "Recursion uses the idea of \"divide and conquer\" to solve problems. It divides a complex problem you are working on into smaller sub-problems that are easily solved, rather than trying to solve the complex problem directly.\n",
    "\n",
    "Recursive functions split the problem into two cases: the ***base case*** and the ***recursive case***. The function continually loops, calling itself, until it reaches the base case.\n",
    "\n",
    "* Base case: Stopping criteria, the simplest case that can be solved directly.\n",
    "* Recursive case: Function that splits the problem into the smaller subproblems.\n",
    "    \n",
    "### Three Laws of Recursion\n",
    "\n",
    "1. A recursive algorithm must have a base case.\n",
    "2. A recursive algorithm must change its state and move toward the base case.\n",
    "3. A recursive algorithm must call itself, recursively.\n",
    "    \n",
    "### Example: Factorial\n",
    "\n",
    "Are the following functions the same?\n",
    "\n",
    "$$ f(x) = \\prod_{i=1}^xi $$\n",
    "\n",
    "$$f(x) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & \\mbox{if } x \\leq 1 \\\\\n",
    "\t\txf(x-1) & \\mbox otherwise\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(x):\n",
    "    \"\"\"Recursively calculate x!\"\"\"\n",
    "    # base case is when we get to x=0, which is 0! = 1\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    # otherwise, recursive case, notice how we are reducing x\n",
    "    else:\n",
    "        return x * factorial(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power(base, exp):\n",
    "    \"\"\"Recursively caclulate base ** exp\"\"\"\n",
    "    # base case is when exp = 0, base ** 0 = 1\n",
    "    if exp <= 0:\n",
    "        return 1\n",
    "    #  otherwise, recursive case, reduce exp\n",
    "    return base * power(base, exp - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power(2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "power(25, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "25 ** 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion not always the answer, but useful tool to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You do, fill in the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def len_str(s):\n",
    "    \"\"\"Recursively determine the length of a string\"\"\"\n",
    "    # base case, when should you stop?\n",
    "    if s == '':\n",
    "        return 0\n",
    "    # recursive case, how can you reduce your problem?\n",
    "    return len_str(s[1:]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function\n",
    "len_str('pineapple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Pseudocode\n",
    "\n",
    "Recursive partitioning algorithm\n",
    "\n",
    "```\n",
    "function BuildTree:\n",
    "    # base case, stop building tree\n",
    "    If every item in the dataset is in the same class\n",
    "    or there is not feature left to split the data:\n",
    "        return a leaf node with the class label\n",
    "    # recursive case, keep splitting stuff\n",
    "    Else:\n",
    "        find the best feature and value to split the data\n",
    "        split the dataset into two subsets\n",
    "        create a node\n",
    "        for each subset\n",
    "            call Build Tree and add the result as a child of the node\n",
    "        return node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Decision Trees\n",
    "\n",
    "Class discussion: what are some problems you forsee with this approach?\n",
    "\n",
    "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "Let's go back to the dogs and horses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_horse_dog_data(n_horses, n_dogs, messiness=1):\n",
    "    \"\"\"Return sample of horse dog data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_horses: int\n",
    "        The number of horses\n",
    "    n_dogs: numpy array\n",
    "        The number of dogs\n",
    "    messiness: float\n",
    "        How much to multiple by the standard deviations of the logs (default 1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    horse_weight: numpy array\n",
    "        The weights of the horses\n",
    "    horse_height: numpy array\n",
    "        The height of the horses\n",
    "    dog_weight: numpy array\n",
    "        The weight of the dogs\n",
    "    dog_height: numpy array\n",
    "        The height of the dogs\n",
    "    \"\"\"\n",
    "\n",
    "    horse_weight = scs.norm(3,0.15*messiness).rvs(n_horses)\n",
    "    horse_height = horse_weight * 0.4 + scs.norm(-0.5,0.05*messiness).rvs(n_horses)\n",
    "    horse_weight = 10**horse_weight\n",
    "    horse_height = 10**horse_height\n",
    "    dog_weight = np.zeros(n_dogs)\n",
    "    dog_height = np.zeros(n_dogs)\n",
    "\n",
    "    dog_weight[:n_dogs//2] = scs.norm(1.7,0.20*messiness).rvs(n_dogs//2)\n",
    "    dog_weight[n_dogs//2:] = scs.norm(2.5,0.25*messiness).rvs(n_dogs//2)\n",
    "    dog_height[:n_dogs//2] = dog_weight[:n_dogs//2]*0.50 + scs.norm(-0.5,0.2*messiness).rvs(n_dogs//2)\n",
    "    dog_height[n_dogs//2:] = dog_weight[n_dogs//2:]*0.30 + scs.norm(-0.5,0.1*messiness).rvs(n_dogs//2)\n",
    "    dog_weight = 10**dog_weight\n",
    "    dog_height = 10**dog_height\n",
    "    return horse_weight, horse_height, dog_weight, dog_height\n",
    "\n",
    "\n",
    "def prep_data_for_model(horse_weight, horse_height, dog_weight, dog_height):\n",
    "    \"\"\"Return horse dog data in format for kNN classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    horse_weight: numpy array\n",
    "        The weights of the horses\n",
    "    horse_height: numpy array\n",
    "        The height of the horses\n",
    "    dog_weight: numpy array\n",
    "        The weight of the dogs\n",
    "    dog_height: numpy array\n",
    "        The height of the dogs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: numpy array, shape = [n_samples, 2]\n",
    "        The features (weight, height) of the data\n",
    "    y: numpy array, shape = [n_samples,]\n",
    "        The labels for our classification (horse=0, dog=1)\n",
    "    \"\"\"\n",
    "    X = np.column_stack((np.concatenate((horse_weight, dog_weight)),\n",
    "                         np.concatenate((horse_height, dog_height))))\n",
    "    y = np.concatenate((np.zeros(horse_weight.shape),\n",
    "                        np.ones(dog_weight.shape)))\n",
    "    return X, y\n",
    "\n",
    "# Function to make getting new samples for model input easier, laziness always wins\n",
    "def get_new_ugly_sample():\n",
    "    \"\"\"Return new sample ready for modeling.\n",
    "    \n",
    "    Combines make_ugly_data and prep_data_for_model, see the docs for those functions\n",
    "    \"\"\"\n",
    "    horse_weight, horse_height, dog_weight, dog_height = make_horse_dog_data(100, 60, messiness=1.5)\n",
    "    X, y = prep_data_for_model(horse_weight, horse_height, dog_weight, dog_height)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_classification_tree(ax, X, y, model=None):\n",
    "    \"\"\"Plot the horse dog data\"\"\"\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'r.', label='Horse')\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'b.', label='Dog')\n",
    "    ax.set_title(\"Classifying Horses and Dogs with Decision Trees\")\n",
    "    ax.set_xlabel(\"Weight (lbs)\")\n",
    "    ax.set_ylabel(\"Height (ft)\")\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    if model is None:\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    # preserve upper limits\n",
    "    xlim = (0, ax.get_xlim()[1])\n",
    "    ylim = (0, ax.get_ylim()[1])\n",
    "    plot_classification_thresholds(ax, X, y, 0, model.tree_, xlim, ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "def plot_classification_thresholds(ax, X, y, inode, tree, xlim, ylim):\n",
    "    threshold = tree.threshold[inode]\n",
    "    if tree.feature[inode] == -2:\n",
    "        color = ['r', 'b'][np.argmax(tree.value[inode])]\n",
    "        ax.add_patch(Rectangle((xlim[0], ylim[0]), xlim[1]-xlim[0], ylim[1]-ylim[0], fc=color, alpha=0.3))\n",
    "        return\n",
    "    if tree.feature[inode] == 0:\n",
    "        ax.plot((threshold, threshold), ylim, 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, (xlim[0], threshold), ylim)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, (threshold, xlim[1]), ylim)\n",
    "    else:\n",
    "        ax.plot(xlim, (threshold, threshold), 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, xlim, (ylim[0], threshold))\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, xlim, (threshold, ylim[1]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_new_ugly_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_classification_tree(ax, X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is some risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Tree\n",
    "\n",
    "* Cross validate!\n",
    "    * Train trees with different parameters, see which performs best on validation set\n",
    "    * No different than any other model\n",
    "\n",
    "## Pruning\n",
    "\n",
    "We call the idea of modifying a decision tree to improve its performance ***pruning***.\n",
    "\n",
    "There are two approaches: ***pre-pruning*** and ***post-pruning***.\n",
    "\n",
    "### Pre\n",
    "\n",
    "Prune as we build the tree, control hyper-parameters\n",
    "\n",
    "Class discussion: what can we control?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers/preprune.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post\n",
    "\n",
    "Prune after we build the complete tree\n",
    "\n",
    "* Merge leaves if doing so decreases test-set error\n",
    "\n",
    "#### Pseudocode\n",
    "```\n",
    "function Prune:\n",
    "    if either left or right is not a leaf:\n",
    "        call Prune on those that aren't\n",
    "    if both left and right are (now) leaf nodes:\n",
    "        calculate error associated with merging two leaf nodes\n",
    "        calculate error associated without merging two leaf nodes\n",
    "        if merging results in lower error:\n",
    "            merge the leaf nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Decision Trees\n",
    "\n",
    "Class discussion: what do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_dt.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision Tree Algorithms\n",
    "\n",
    "There are some famous variants of the decision tree algorithm:\n",
    "\n",
    "### ID3\n",
    "\n",
    "Short for Iterative Dichotomiser 3, the original Decision Tree algorithm developed by Ross Quinlan (who's responsible for a lot of proprietary decision tree algorithms) in the 1980's.\n",
    "\n",
    "* designed for only categorial features\n",
    "* splits categorical features completely\n",
    "* uses entropy and information gain to pick the best split\n",
    "\n",
    "### CART\n",
    "\n",
    "Short for Classification and Regression Tree was invented about the same time as ID3 by Breiman, Friedman, Olshen and Stone. The CART algorithm has the following properties:\n",
    "\n",
    "* handles both categorial and continuous data\n",
    "* always uses binary splits\n",
    "* uses gini impurity to pick the best split\n",
    "\n",
    "Algorithms will be called CART even if they don't follow all of the specifications of the original algorithm.\n",
    "\n",
    "### C4.5\n",
    "\n",
    "This is Quinlan's first improvement on the ID3 algorithm. The main improvements are:\n",
    "\n",
    "* handles continuous data\n",
    "* implements pruning to reduce overfitting\n",
    "\n",
    "### C5.0\n",
    "\n",
    "This is supposedly better, but it's proprietary so we don't have access to the specifics of the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
