{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric-Learners\n",
    "\n",
    "## kNN and Decision Trees\n",
    "\n",
    "### Moses Marsh\n",
    "#### May 24, 2017\n",
    "\n",
    "(based heavily on the notebooks from Jack Bennetto & Jonathan Torrez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "#### Morning: kNN\n",
    "\n",
    " * Non-parametric models\n",
    " * Understanding kNN\n",
    " * Scaling\n",
    " * Distance metrics\n",
    " * Curse of Dimensionality\n",
    " \n",
    "#### Afternoon: Decision Trees\n",
    " * Entropy and purity\n",
    " * Recursion\n",
    " * Decision-tree algorithm\n",
    " * Gini and regression\n",
    " * Pre- and post- pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Objectives [morning-kNN]\n",
    "\n",
    "* **Implement** pseudocode for kNN algorithm\n",
    "* **State** common distance metrics used for kNN\n",
    "* **Describe** effect of varying k (num_neigbors)\n",
    "* **Explain** importance of scaling for kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs. Non-Parametric Models\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "\n",
    "$$\\hat y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + ... + \\hat\\beta_i X_i $$\n",
    "    \n",
    "How many parameters can we describe it with?\n",
    "\n",
    "Can we ever change the number of parameters?\n",
    "\n",
    "What does using this model mean about the structure of our data?\n",
    "\n",
    "**Hint:** alternate way to write linear regression equation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/prop_param.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "* Linear regression\n",
    "* Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Non-Parametric Models\n",
    "\n",
    "Class discussion: What do you think they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/prop_non_param.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: contrast the pros and cons of these two types of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_param.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros-Cons of Non-parametric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_nonparam.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "# Make it pretty\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Seed random functions for reproducibility\n",
    "#np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_horse_dog_data(n_horses, n_dogs, messiness=1):\n",
    "    \"\"\"Return sample of horse dog data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_horses: int\n",
    "        The number of horses\n",
    "    n_dogs: numpy array\n",
    "        The number of dogs\n",
    "    messiness: float\n",
    "        How much to multiple by the standard deviations of the logs (default 1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    horse_weight: numpy array\n",
    "        The weights of the horses\n",
    "    horse_height: numpy array\n",
    "        The height of the horses\n",
    "    dog_weight: numpy array\n",
    "        The weight of the dogs\n",
    "    dog_height: numpy array\n",
    "        The height of the dogs\n",
    "    \"\"\"\n",
    "\n",
    "    horse_weight = scs.norm(3,0.15*messiness).rvs(n_horses)\n",
    "    horse_height = horse_weight * 0.4 + scs.norm(-0.5,0.05*messiness).rvs(n_horses)\n",
    "    horse_weight = 10**horse_weight\n",
    "    horse_height = 10**horse_height\n",
    "    dog_weight = np.zeros(n_dogs)\n",
    "    dog_height = np.zeros(n_dogs)\n",
    "\n",
    "    dog_weight[:n_dogs//2] = scs.norm(1.7,0.20*messiness).rvs(n_dogs//2)\n",
    "    dog_weight[n_dogs//2:] = scs.norm(2.5,0.25*messiness).rvs(n_dogs//2)\n",
    "    dog_height[:n_dogs//2] = dog_weight[:n_dogs//2]*0.50 + scs.norm(-0.5,0.2*messiness).rvs(n_dogs//2)\n",
    "    dog_height[n_dogs//2:] = dog_weight[n_dogs//2:]*0.30 + scs.norm(-0.5,0.1*messiness).rvs(n_dogs//2)\n",
    "    dog_weight = 10**dog_weight\n",
    "    dog_height = 10**dog_height\n",
    "    return horse_weight, horse_height, dog_weight, dog_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "\n",
    "horse_weight, horse_height, dog_weight, dog_height = make_horse_dog_data(100, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_horse_dog_data(ax, horse_weight, horse_height, dog_weight, dog_height):\n",
    "    \"\"\"Plot the horse dog data\"\"\"\n",
    "    ax.plot(horse_weight, horse_height, 'ro', label='Horse')\n",
    "    ax.plot(dog_weight, dog_height, 'bo', label='Dog')\n",
    "    ax.set_title(\"Horse vs Dog\")\n",
    "    ax.set_xlabel(\"Weight (lbs)\")\n",
    "    ax.set_ylabel(\"Height (ft)\")\n",
    "    ax.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_horse_dog_data(ax, horse_weight, horse_height, dog_weight, dog_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we get new data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_weights = [200, 500, 1200]\n",
    "new_heights = [0.7, 2.5, 5.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are they horses or dogs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the new points\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(new_weights, new_heights, marker='o', markersize=10, color=\"yellow\", linestyle='None')\n",
    "\n",
    "\n",
    "# Plot the original data\n",
    "plot_horse_dog_data(ax, horse_weight, horse_height, dog_weight, dog_height)\n",
    "\n",
    "# Draw arrows highlighting the new points\n",
    "for new_x, new_y in zip(new_weights, new_heights):\n",
    "    ax.annotate(\"\",\n",
    "                xy=(new_x, new_y), xycoords='data',\n",
    "                xytext=(100, -7), textcoords='offset points',\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What algorithm did you use?\n",
    "\n",
    "Class discussion\n",
    "\n",
    "## kNN pseudocode\n",
    "\n",
    "Class discussion: What do you think this is? Describe both the training and prediction phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/knn_pseudocode.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "ax.plot(a[0], a[1], 'ro')\n",
    "ax.plot(b[0], b[1], 'bo')\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "#ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "#ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we predict for different values of k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some data that isn't quite as nice\n",
    "horse_weight, horse_height, dog_weight, dog_height = make_horse_dog_data(50, 30, messiness=1.5)\n",
    "fig, ax = plt.subplots()\n",
    "plot_horse_dog_data(ax, horse_weight, horse_height, dog_weight, dog_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data_for_model(horse_weight, horse_height, dog_weight, dog_height):\n",
    "    \"\"\"Return horse dog data in format for kNN classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    horse_weight: numpy array\n",
    "        The weights of the horses\n",
    "    horse_height: numpy array\n",
    "        The height of the horses\n",
    "    dog_weight: numpy array\n",
    "        The weight of the dogs\n",
    "    dog_height: numpy array\n",
    "        The height of the dogs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: numpy array, shape = [n_samples, 2]\n",
    "        The features (weight, height) of the data\n",
    "    y: numpy array, shape = [n_samples,]\n",
    "        The labels for our classification (horse=0, dog=1)\n",
    "    \"\"\"\n",
    "    X = np.column_stack((np.concatenate((horse_weight, dog_weight)),\n",
    "                         np.concatenate((horse_height, dog_height))))\n",
    "    y = np.concatenate((np.zeros(horse_weight.shape),\n",
    "                        np.ones(dog_weight.shape)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to make getting new samples for model input easier, laziness always wins\n",
    "def get_new_ugly_sample():\n",
    "    \"\"\"Return new sample ready for modeling.\n",
    "    \n",
    "    Combines make_ugly_data and prep_data_for_model, see the docs for those functions\n",
    "    \"\"\"\n",
    "    horse_weight, horse_height, dog_weight, dog_height = make_horse_dog_data(100, 60, messiness=1.5)\n",
    "    X, y = prep_data_for_model(horse_weight, horse_height, dog_weight, dog_height)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mult_decision_boundary(ax, X, y, k, scaled=True, title='Title', xlabel='xlabel', ylabel='ylabel'):\n",
    "    \"\"\"Plot the decision boundary of a kNN classifier.\n",
    "    \n",
    "    Builds and fits a sklearn kNN classifier internally.\n",
    "\n",
    "    X must contain only 2 continuous features.\n",
    "\n",
    "    Function modeled on sci-kit learn example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: Matplotlib axes object\n",
    "        The plot to draw the data and boundary on\n",
    "        \n",
    "    X: numpy array\n",
    "        Training data\n",
    "    \n",
    "    y: numpy array\n",
    "        Target labels\n",
    "    \n",
    "    k: int\n",
    "        The number of neighbors that get a vote.\n",
    "        \n",
    "    scaled: boolean, optional (default=True)\n",
    "        If true scales the features, else uses features in original units\n",
    "    \n",
    "    title: string, optional (default = 'Title')\n",
    "        A string for the title of the plot\n",
    "    \n",
    "    xlabel: string, optional (default = 'xlabel')\n",
    "        A string for the label on the x-axis of the plot\n",
    "    \n",
    "    ylabel: string, optional (default = 'ylabel')\n",
    "        A string for the label on the y-axis of the plot\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    x_mesh_step_size = 5.0\n",
    "    y_mesh_step_size = 0.05\n",
    "    \n",
    "    #Hard code in colors for classes, one class in red, one in blue\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    #Build a kNN classifier\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "    \n",
    "    if scaled:\n",
    "        #Build pipeline to scale features\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X, y)\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = 0, 3000\n",
    "    y_min, y_max = 0, 10\n",
    "    #x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    #y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, x_mesh_step_size),\n",
    "                         np.arange(y_min, y_max, y_mesh_step_size))\n",
    "    dec_boundary = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    dec_boundary = dec_boundary.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, dec_boundary, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "\n",
    "    ax.set_title(title + \", k={0}, scaled={1}\".format(k, scaled))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    ax.set_xlim(xmin=x_min, xmax=x_max)\n",
    "    ax.set_ylim(ymin=y_min, ymax=y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a sample to plot\n",
    "X, y = get_new_ugly_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save some titles and labels to make our plots pretty\n",
    "title = 'Horse vs. Dog'\n",
    "xlabel = 'Weight (lbs)'\n",
    "ylabel = 'Height (ft)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_ks = [1, 3, 5, 10, 25, 50]\n",
    "\n",
    "# Building my plot, 3 rows x 2 columns\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 14))\n",
    " \n",
    "# Loop through the possible ks, put each decision boundary a separate axes\n",
    "for k, ax in zip(some_ks, axs.flatten()):\n",
    "    plot_mult_decision_boundary(ax, X, y, k=k, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "# Keep things from overlapping\n",
    "plt.tight_layout()\n",
    "# Show me the money\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k start\n",
    "\n",
    "In general, a good starting point for k is $\\sqrt{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "Class discussion: How does changing k affect each? Why?\n",
    "\n",
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/bias_knn.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/variance_knn.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take 4 different samples from the same population and save them in a list\n",
    "samples_from_same_pop = []\n",
    "\n",
    "for _ in xrange(4):\n",
    "    sample = get_new_ugly_sample()\n",
    "    samples_from_same_pop.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for sample, ax in zip(samples_from_same_pop, axs.flatten()):\n",
    "    X = sample[0]\n",
    "    y = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X, y, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Offset the figure title to make it look nice\n",
    "plt.subplots_adjust(top=0.9)\n",
    "# Set a title for the entire figure\n",
    "plt.suptitle('Low Bias, High Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for sample, ax in zip(samples_from_same_pop, axs.flatten()):\n",
    "    X = sample[0]\n",
    "    y = sample[1]\n",
    "    plot_mult_decision_boundary(ax, X, y, k=50, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('High Bias, Low Variance', fontsize=20, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Could Be Important\n",
    "\n",
    "So far we've been scaling the features by default. What if we didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 4))\n",
    "                        \n",
    "plot_mult_decision_boundary(axs[0], X, y, k=1, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "plot_mult_decision_boundary(axs[1], X, y, k=1, scaled=False, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is important for any algorithms that use distance in feature space.  If changing the units will change the predictions, scale the data first.\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "A few common distance metrics, but there are many more.\n",
    "\n",
    "### Manhattan Distance\n",
    "City blocks, L1\n",
    "\n",
    "*Manhattan* distance is the distance as measured along axes at right angles:\n",
    "\n",
    "$$\\sum_i |a_i - b_i|$$\n",
    "\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Straight line, L2\n",
    "\n",
    "*Euclidean* distance is the distance metric you've probably heard of before:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = ||\\mathbf{a} - \\mathbf{b}|| \\ = \\sqrt{\\sum (a_i - b_i)^2} $$\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Angle\n",
    "\n",
    "*Cosine* similarity is another commonly used distance metric. It's measuring the angle between the two vectors:\n",
    "\n",
    "$$ d(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}||  ||\\mathbf{b}||} $$\n",
    "\n",
    "\n",
    "#### sklearn default is Minkowski (with p=2 which is really just euclidean, p=1 is manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "### Perspective 1: Sample Density\n",
    "\n",
    "Sampling density is proportional to $$N^\\frac{1}{p}$$\n",
    "\n",
    "where N is the number of samples and p is the number of dimensions.\n",
    "\n",
    "Let's consider a data set with 100 samples that all have only one feature/predictor. But, you feel that one feature doesn't tell you enough to properly predict anything, so you set out to collect new data. The new data will have ten features/predictors for each sample. How many samples do we need to have an equally dense sample as our original dataset?\n",
    "\n",
    "#### Original Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          density & = N^\\frac{1}{p} \\\\\n",
    "          & = 100^\\frac{1}{1} \\\\\n",
    "          & = 100 \n",
    "\\end{align}$$\n",
    "\n",
    "#### New Sample\n",
    "\n",
    "$$\\begin{align}\n",
    "          N & = density^ p \\\\\n",
    "          &= 100^{10} \\\\\n",
    "          &= 100,000,000,000,000,000,000\n",
    "\\end{align}$$\n",
    "\n",
    "No big, just one hundred quadrillion samples...\n",
    "\n",
    "### Perspective 2: Loss of Locality\n",
    "\n",
    "As we increase dimensionality, we lose the concept of locality and things get infinitely far apart.\n",
    "\n",
    "More precisely, samples that are similar no longer look similar and \"closeness\" becomes more arbitrary than meaningful. \n",
    "\n",
    "For example, in Euclidean distance we must be close in ***all*** dimensions to be considered close.\n",
    "\n",
    "Close in 2 of 3 dimensions, but far in the 3rd dimension makes the points far apart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of kNN\n",
    "\n",
    "Class discussion: what do you think they are? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_knn.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Variants\n",
    "\n",
    "Here are a few common variants on the kNN algorithm.\n",
    "\n",
    "### Weighted Voting\n",
    "\n",
    "<img src=\"images/weighted_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "The ith point votes with weight of $$\\frac{1}{d_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "a = np.array([[2.1, 3.2, 2.0, 2.9, 4.1, 3.0, 4.1, 6.2, 8.4, 4.1, 4.9, 5.8, 8.1, 5.0, 7.2],\n",
    "              [3.1, 3.2, 4.0, 3.9, 4.0, 5.1, 4.9, 5.0, 5.1, 6.2, 5.9, 6.1, 6.0, 7.2, 6.8]])\n",
    "b = np.array([[1.2, 1.8, 3.0, 5.1, 6.1, 1.3, 2.9, 3.7, 7.1, 6.2, 6.9, 8.0, 2.7, 0.3, 1.1, 6.9, 8.1, 0.0, 2.1, 1.2, 3.3, 2.1, 2.9],\n",
    "              [1.1, 1.8, 1.0, 0.9, 1.1, 2.3, 1.9, 2.3, 1.9, 3.1, 3.2, 3.0, 3.7, 4.3, 4.1, 3.9, 4.1, 5.1, 5.2, 6.3, 5.8, 7.1, 6.7]])\n",
    "center = np.array([[2.5], [3.5]])\n",
    "asize = 50/((a - center)**2).sum(axis=0)\n",
    "bsize = 50/((b - center)**2).sum(axis=0)\n",
    "ax.scatter(a[0], a[1], c='r', s=asize)\n",
    "ax.scatter(b[0], b[1], c='b', s=bsize)\n",
    "ax.plot(center[0], center[1], 'kx', ms=10) \n",
    "ax.add_artist(plt.Circle(center, 0.4, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 1.0, fc='None', ec='k', lw=0.5))\n",
    "ax.add_artist(plt.Circle(center, 2.0, fc='None', ec='k', lw=0.5))\n",
    "ax.set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regression kNN\n",
    "\n",
    "<img src=\"images/regression_knn.png\" width=500 align=\"left\"/>\n",
    "\n",
    "Let the k nearest points have distances $$d_1, d_2,..., d_k$$\n",
    "\n",
    "Let the k nearest points have targets $$t_1, t_2,..., t_k$$\n",
    "\n",
    "Predict the mean value ***or*** predict a weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Objectives [morning-kNN]\n",
    "Reinforces the following objectives\n",
    "\n",
    "* **Implement** pseudocode for kNN algorithm\n",
    "    * Writing a class that implements the kNN algorithm\n",
    "* **State** common distance metrics used for kNN\n",
    "    * Writing code that calculates two most common\n",
    "* **Describe** effect of varying k (num_neigbors)\n",
    "    * Cross validating to choose k\n",
    "    * Plotting decision boundary can visualize the effect\n",
    "* **Explain** importance of scaling for kNN\n",
    "    * Plotting decision boundary can visualize the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Afternoon Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives [afternoon-DT]\n",
    "\n",
    "* **Describe** pros/cons of decision tree algorithm\n",
    "* **Implement** pseudocode for decision tree algorithm\n",
    "* **Describe** common measures for making splits in a decision tree\n",
    "* **Demonstrate** concept of recursion and **relate** it to decision trees\n",
    "* **State** pruning techniques and pros/cons of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in our data\n",
    "tennis_df = pd.read_table('data/tennis.txt', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up a few things, based on my preferences and making calculating probabilities easier\n",
    "tennis_df.rename(columns={'playtennis': 'played'}, inplace=True)\n",
    "tennis_df['played'] = tennis_df['played'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "tennis_df.sort_values('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Would You Determine If You Played Tennis?\n",
    "\n",
    "Class discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Our Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does windiness do a good job of predicting \"played\"? Let's split our dataset into two subsets: points with `wind == True` and points with `wind == False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = tennis_df['wind'] == True\n",
    "print tennis_df[mask]\n",
    "print '--'*25\n",
    "print tennis_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much did this help us? A perfect decision would have separated all the `played == 0` points from the `played == 1` points, and a perfectly useless decision would have given us two subsets of the data that each had equal amounts of `0`s and `1`s. Can we make a metric for how \"mixed up\" or \"impure\" the set of labels is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "\n",
    "One common measure of impurity. Let $X$ be a discrete random variable with probability mass function $P(X)$.\n",
    "\n",
    "For today's purposes, the values of $X$ are _class labels_, and $P(X=i)=p_i$ is the _probability of drawing a point with class $i$_\n",
    "\n",
    "The entropy is defined as:\n",
    "$$\\begin{align}\n",
    "H(X) & = E[log_2(\\tfrac{1}{P(X)})] \\\\\n",
    "     & = -E[log_2(P(X))] \\\\\n",
    "     & = - \\sum_i p_i log_2(p_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Graph of Bernoulli Random Variable\n",
    "\n",
    "<img src=\"images/entropy_graph.png\" width=500 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(y):\n",
    "    \"\"\"Return the entropy of the array y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    total_samples = y.shape[0]\n",
    "    summation = 0\n",
    "    \n",
    "    for class_i in np.unique(y):\n",
    "        prob = sum(y == class_i) / float(total_samples)\n",
    "        summation += prob * np.log2(prob)\n",
    "    \n",
    "    return -summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tennis_df['played'].values\n",
    "\n",
    "print \"Entropy of original data set is {}\".format(calc_entropy(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain \n",
    "Information gain is the difference between the _impurity of the parent node_ and the _weighted impurities of the child nodes_. A higher IG for a split means that the split did a better job of separating (or un-mixing) the class labels. \n",
    "\n",
    "$$\\text{IG}(S, C) = H(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} H(C_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_info_gain(y, y1, y2, impurity_func):\n",
    "    \"\"\"Return the information gain of making the given split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1d numpy array\n",
    "        Labels for parent node\n",
    "    \n",
    "    y1: 1d numpy array\n",
    "        Labels for potential child node 1\n",
    "    \n",
    "    y2: 1d numpy array\n",
    "        Labels for potential child node 2\n",
    "    \n",
    "    impurity_func: function\n",
    "        Function which calculates the impurity of the node \n",
    "        (e.g. Shannon Entropy)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    total_samples = float(y.shape[0])\n",
    "    child_imp = 0\n",
    "    y_impurity = impurity_func(y)\n",
    "    \n",
    "    for child_node in (y1, y2):\n",
    "        child_num = child_node.shape[0]\n",
    "        child_imp += (child_num / total_samples) * impurity_func(child_node)\n",
    "        \n",
    "    return y_impurity - child_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's split on the temperature.\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'hot']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'hot']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Information Gain is {0}.\".format(calc_info_gain(y, y1, y2, calc_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not much gain, maybe I really liked playing when the temperature was mild\n",
    "# Let's try that split\n",
    "\n",
    "y1 = tennis_df[tennis_df['temperature'] == 'mild']['played'].values\n",
    "y2 = tennis_df[tennis_df['temperature'] != 'mild']['played'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Information Gain is {0}.\".format(calc_info_gain(y, y1, y2, calc_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Still not much headway, let's just try every possible split and see what works\n",
    "# And also confirm what we used our common sense algorithm we started with\n",
    "\n",
    "possible_splits = {}\n",
    "# Get just my features from the dataframe\n",
    "feature_cols = tennis_df.drop('played', axis=1).columns\n",
    "\n",
    "# For a given column, find all the unique possible values\n",
    "for col in feature_cols:\n",
    "    col_splits = np.unique(tennis_df[col])\n",
    "    # For each possible value, split the dataset using that value\n",
    "    for pos_val in col_splits:\n",
    "        y1 = tennis_df[tennis_df[col] == pos_val]['played'].values\n",
    "        y2 = tennis_df[tennis_df[col] != pos_val]['played'].values\n",
    "        # Calculate the information gain, save it for later\n",
    "        ig = calc_info_gain(y, y1, y2, calc_entropy)\n",
    "        key = \"{0}: {1}\".format(col, pos_val)\n",
    "        possible_splits[key] = ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out our results in a pretty way\n",
    "colname1 = \"Col Name: Value\"\n",
    "colname2 = \"Information Gain\"\n",
    "\n",
    "# :20 is specifying a column width, https://docs.python.org/3/library/string.html#formatspec\n",
    "print \"{0:20} || {1}\".format(colname1, colname2)\n",
    "print \"-----------------------------------------\"\n",
    "# operator.itemgetter(1) allows us to sort by the second item of the tuple\n",
    "for k,v in sorted(possible_splits.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    print \"{0:20} || {1}\".format(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting on overcast gave us the largest info gain\n",
    "# Let's look at what those child nodes look like and their respective entropies\n",
    "child_node_left = tennis_df[tennis_df['outlook'] == 'overcast']\n",
    "print \"Entropy of left child node is {0}\".format(\n",
    "          calc_entropy(child_node_left['played'].values))\n",
    "child_node_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, we got a pure node, that makes sense.\n",
    "# What about the other node?\n",
    "child_node_right = tennis_df[tennis_df['outlook'] != 'overcast']\n",
    "print \"Entropy of right child node is {0}\".format(\n",
    "          calc_entropy(child_node_right['played'].values))\n",
    "child_node_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class discussion: why in the world would we want a node that has maximum entropy?\n",
    "\n",
    "## How would you know how to stop?\n",
    "\n",
    "Class discussion: When would you stop? Potential problems?\n",
    "\n",
    "Will get to this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ways to Calculate Impurity\n",
    "\n",
    "### Gini Index\n",
    "\n",
    "This is the default for sci-kit learn trees.\n",
    "\n",
    "It is a measure of this probability:\n",
    "\n",
    "* Take a random element from the set.\n",
    "* Label it randomly according to the distribution of labels in the set.\n",
    "* What is the probability that it is labeled incorrectly?\n",
    "\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{i \\in S} p_i^2$$\n",
    "\n",
    "### Information Gain with Gini Index\n",
    "\n",
    "$$\\text{IG}(S, C) = \\text{Gini}(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} \\text{Gini}(C_i)$$\n",
    "\n",
    "## Regression Trees\n",
    "\n",
    "### Splitting Continuous Values\n",
    "\n",
    "Class discussion: how would you do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/regtree.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impurity for regression trees\n",
    "Use the _variance_ of values within a node.\n",
    "\n",
    "$$\\text{Var}(S) = \\sum_{y \\in S} (y - \\bar{y})^2$$\n",
    "$$\\bar{y} = \\text{Mean}(S) = \\frac{1}{N_S}\\sum_{y \\in S} y $$\n",
    "\n",
    "Then the information gain is\n",
    "$$\\text{IG}(S, C) = Var(S) - \\sum_{C_i \\in C} \\frac{|C_i|}{|S|} Var(C_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting\n",
    "\n",
    "The final predictions of a decision tree and a regression tree are implemented differently:\n",
    "\n",
    "### Classification\n",
    "\n",
    "* Majority class in leaf node\n",
    "* Better to predict probability of label (impure leaf nodes)\n",
    "\n",
    "### Regression\n",
    "\n",
    "* Mean of values in leaf node\n",
    "* Linear Regression (ooo, fancy)\n",
    "    * Also called *model trees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion\n",
    "\n",
    "Recursion uses the idea of \"divide and conquer\" to solve problems. It divides a complex problem you are working on into smaller sub-problems that are easily solved, rather than trying to solve the complex problem directly.\n",
    "\n",
    "Recursive functions split the problem into two cases: the ***base case*** and the ***recursive case***. The function continually loops, calling itself, until it reaches the base case.\n",
    "\n",
    "* Base case: Stopping criteria, the simplest case that can be solved directly.\n",
    "* Recursive case: Function that splits the problem into the smaller subproblems.\n",
    "    \n",
    "### Three Laws of Recursion\n",
    "\n",
    "1. A recursive algorithm must have a base case.\n",
    "2. A recursive algorithm must change its state and move toward the base case.\n",
    "3. A recursive algorithm must call itself, recursively.\n",
    "    \n",
    "### Example: Factorial\n",
    "\n",
    "Are the following functions the same?\n",
    "\n",
    "$$ f(x) = \\prod_{i=1}^xi $$\n",
    "\n",
    "$$f(x) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & \\mbox{if } x \\leq 1 \\\\\n",
    "\t\txf(x-1) & \\mbox otherwise\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(x):\n",
    "    \"\"\"Recursively calculate x!\"\"\"\n",
    "    # base case is when we get to x=0, which is 0! = 1\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    # otherwise, recursive case, notice how we are reducing x\n",
    "    else:\n",
    "        return x * factorial(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factorial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factorial(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power(base, exp):\n",
    "    \"\"\"Recursively caclulate base ** exp\"\"\"\n",
    "    # base case is when exp = 0, base ** 0 = 1\n",
    "    if exp <= 0:\n",
    "        return 1\n",
    "    #  otherwise, recursive case, reduce exp\n",
    "    return base * power(base, exp - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "power(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Map out on whiteboard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "power(25, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 20\n",
    "25 ** 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion not always the answer, but useful tool to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You do, fill in the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def len_str(s):\n",
    "    \"\"\"Recursively determine the length of a string\"\"\"\n",
    "    # base case, when should you stop?  \n",
    "    # recursive case, how can you reduce your problem?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test your function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Pseudocode\n",
    "\n",
    "Recursive partitioning algorithm\n",
    "\n",
    "```\n",
    "function BuildTree:\n",
    "    # base case, stop building tree\n",
    "    If every item in the dataset is in the same class\n",
    "    or there is not feature left to split the data:\n",
    "        return a leaf node with the class label\n",
    "    # recursive case, keep splitting stuff\n",
    "    Else:\n",
    "        find the best feature and value to split the data\n",
    "        split the dataset into two subsets\n",
    "        create a node\n",
    "        for each subset\n",
    "            call Build Tree and add the result as a child of the node\n",
    "        return node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Decision Trees\n",
    "\n",
    "Class discussion: what are some problems you forsee with this approach?\n",
    "\n",
    "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "Let's go back to the dogs and horses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_classification_tree(ax, X, y, model=None):\n",
    "    \"\"\"Plot the horse dog data\"\"\"\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'r.', label='Horse')\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'b.', label='Dog')\n",
    "    ax.set_title(\"Classifying Horses and Dogs with Decision Trees\")\n",
    "    ax.set_xlabel(\"Weight (lbs)\")\n",
    "    ax.set_ylabel(\"Height (ft)\")\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    if model is None:\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    # preserve upper limits\n",
    "    xlim = (0, ax.get_xlim()[1])\n",
    "    ylim = (0, ax.get_ylim()[1])\n",
    "    plot_classification_thresholds(ax, X, y, 0, model.tree_, xlim, ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "def plot_classification_thresholds(ax, X, y, inode, tree, xlim, ylim):\n",
    "    threshold = tree.threshold[inode]\n",
    "    if tree.feature[inode] == -2:\n",
    "        color = ['r', 'b'][np.argmax(tree.value[inode])]\n",
    "        ax.add_patch(Rectangle((xlim[0], ylim[0]), xlim[1]-xlim[0], ylim[1]-ylim[0], fc=color, alpha=0.3))\n",
    "        return\n",
    "    if tree.feature[inode] == 0:\n",
    "        ax.plot((threshold, threshold), ylim, 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, (xlim[0], threshold), ylim)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, (threshold, xlim[1]), ylim)\n",
    "    else:\n",
    "        ax.plot(xlim, (threshold, threshold), 'k', lw=0.5)\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_left[inode],  tree, xlim, (ylim[0], threshold))\n",
    "        plot_classification_thresholds(ax, X, y, tree.children_right[inode], tree, xlim, (threshold, ylim[1]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = get_new_ugly_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_classification_tree(ax, X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is some risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Tree\n",
    "\n",
    "* Cross validate!\n",
    "    * Train trees with different parameters, see which performs best on validation set\n",
    "    * No different than any other model\n",
    "\n",
    "## Pruning\n",
    "\n",
    "We call the idea of modifying a decision tree to improve its performance ***pruning***.\n",
    "\n",
    "There are two approaches: ***pre-pruning*** and ***post-pruning***.\n",
    "\n",
    "### Pre\n",
    "\n",
    "Prune as we build the tree, control hyper-parameters\n",
    "\n",
    "Class discussion: what can we control?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('answers/preprune.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post\n",
    "\n",
    "Prune after we build the complete tree\n",
    "\n",
    "* Merge leaves if doing so decreases test-set error\n",
    "\n",
    "#### Pseudocode\n",
    "```\n",
    "function Prune:\n",
    "    if either left or right is not a leaf:\n",
    "        call Prune on those that aren't\n",
    "    if both left and right are (now) leaf nodes:\n",
    "        calculate error associated with merging two leaf nodes\n",
    "        calculate error associated without merging two leaf nodes\n",
    "        if merging results in lower error:\n",
    "            merge the leaf nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Decision Trees\n",
    "\n",
    "Class discussion: what do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load answers after class discussion\n",
    "with open('answers/pc_dt.txt') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision Tree Algorithms\n",
    "\n",
    "There are some famous variants of the decision tree algorithm:\n",
    "\n",
    "### ID3\n",
    "\n",
    "Short for Iterative Dichotomiser 3, the original Decision Tree algorithm developed by Ross Quinlan (who's responsible for a lot of proprietary decision tree algorithms) in the 1980's.\n",
    "\n",
    "* designed for only categorial features\n",
    "* splits categorical features completely\n",
    "* uses entropy and information gain to pick the best split\n",
    "\n",
    "### CART\n",
    "\n",
    "Short for Classification and Regression Tree was invented about the same time as ID3 by Breiman, Friedman, Olshen and Stone. The CART algorithm has the following properties:\n",
    "\n",
    "* handles both categorial and continuous data\n",
    "* always uses binary splits\n",
    "* uses gini impurity to pick the best split\n",
    "\n",
    "Algorithms will be called CART even if they don't follow all of the specifications of the original algorithm.\n",
    "\n",
    "### C4.5\n",
    "\n",
    "This is Quinlan's first improvement on the ID3 algorithm. The main improvements are:\n",
    "\n",
    "* handles continuous data\n",
    "* implements pruning to reduce overfitting\n",
    "\n",
    "### C5.0\n",
    "\n",
    "This is supposedly better, but it's proprietary so we don't have access to the specifics of the improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives [afternoon-DT]\n",
    "Reinforces the following objectives\n",
    "\n",
    "* **Describe** pros/cons of decision tree algorithm\n",
    "* **Implement** pseudocode for decision tree algorithm\n",
    "    * Writing code that implements CART algorithm\n",
    "* **Describe** common measures for making splits in a decision tree\n",
    "    * Classification\n",
    "        * Writing code to calculate entropy and Gini\n",
    "* **Demonstrate** concept of recursion and **relate** it to decision trees\n",
    "    * * Writing code that implements CART algorithm\n",
    "    \n",
    "#### Extra Credit 1\n",
    "* **State** pruning techniques and pros/cons of each\n",
    "    * Pre-pruning\n",
    "        * Writing code to implement pre-pruning\n",
    "    * Post-pruning\n",
    "        * Writing code to implement post-pruning\n",
    "        \n",
    "#### Extra Credit 2\n",
    "* **Describe** common measures for making splits in a decision tree\n",
    "    * Regression\n",
    "        * Writing code to implement regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
