{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)\n",
    "\n",
    "EDA is a key step in the **Data Preparation** phase of [CRISP-DM](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining).  Approaching EDA systematically is important to ensure that you don't forget any steps and avoid making errors which could compromise your analysis/modeling later.  Before starting EDA, you should have defined the business question and understand what data sources are available.  During EDA, you want to refine your intuition about the data and develop hypotheses which we could model or test later.\n",
    "\n",
    "Note how at each step, I keep checking that everything worked as expected.\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "*  Load data\n",
    "*  Sanity check data\n",
    "*  Summary statistics\n",
    "*  Plots\n",
    "*  Identify and examine problem data\n",
    "   *  Outliers\n",
    "   *  Missing data\n",
    "*  Clean data\n",
    "*  First crappy model\n",
    "\n",
    "After EDA, it is time to peform cleaning for more intensive modeling, engineer features, and refine the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "To learn more about EDA:\n",
    "*  John Tukey's [Exploratory Data Analysis](http://www.amazon.com/Exploratory-Data-Analysis-John-Tukey/dp/0201076160/ref=sr_1_1?ie=UTF8&qid=1455242410&sr=8-1&keywords=tukey+exploratory+data+analysis)\n",
    "*  Ben Hamner's [Python Data Visualizations](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) on Kaggle\n",
    "*  Appendix A in [Fundamentals of Machine Learning for Predictive Data Analytics](http://machinelearningbook.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Question\n",
    "\n",
    "In this case, we have a research question, not a business question.  We want to use data on irises to determine the species of a flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "The second step in CRISP-DM is understanding what data is available.  We are fortunate to have access to R.A. Fisher's famous iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Now we load and prepare the data for modeling.  This consists of:\n",
    "\n",
    "* Loading the data\n",
    "* Data hygiene/cleaning\n",
    "* EDA\n",
    "* Feature engineering (building features for modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Everything starts with loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                      names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checking\n",
    "\n",
    "After loading the data, you need to check that the data loaded correctly.  If you are working with a large dataset, create a subset so that you can quickly get everything working.  Later, rerun with the full data or larger subsamples.  Make sure that the subsamples are representative by trying multiple subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that data types are correct and if there any data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the `species` to a categorical variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.species = df.species.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check conversion was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  Everything is now the correct data type.  And, we are fortunate not to have any **missing values**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to tabulate (1-way and 2-way) categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the data is ready for the next step: summary statistics.  Note:  there is no class imbalance.  If there were, we would have to think about up/down sampling or weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "Next, I look at summary statistics. I want to understand the nature of the variation and whether there are any problems, such as outliers or missing values. You may decide to plot at this stage as well -- histograms and box plots are particularly useful.  You may also decide to compute statistics for subsets of the data based on some categorical field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous variables\n",
    "\n",
    "Look at standard moments to understand *central tendency* and *variation* in data and to identify potential outliers:\n",
    "\n",
    "*  min\n",
    "*  q25: 25-th percentile\n",
    "*  median\n",
    "*  q75: 75-th percentile\n",
    "*  mean\n",
    "*  max\n",
    "*  standard deviation\n",
    "*  iqr: interquartile range (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete (categorical) variables\n",
    "\n",
    "Check frequency counts and proportions via histograms.  Cross-tabulation is often helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the classic statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is very *skewed* or has *kurtosis*, you might want to check other percentiles or statistics.  Or, check for departures from normality.  Big data, such as Amazon data, often has a long tail, so you may need to transform data during feature engineering, such as by taking `log`s of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that statistics look different for each species so that we have a chance of predicting species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('species').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Summary statistics can be misleading, as Anscomb showed with his [Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).  Consequently, you should always plot (a subset) of your data to develop intuition and hypotheses about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common plots Ã  la Tukey\n",
    "\n",
    "There a many different types of plots, but the standard ones to consider are:\n",
    "\n",
    "*  scatter (matrix) plot\n",
    "*  bar plot\n",
    "*  histogram\n",
    "*  box plot\n",
    "\n",
    "For time series, you will want to look at:\n",
    "\n",
    "*  $\\{y_t\\}$ vs. time\n",
    "*  autocorrelation function (ACF)\n",
    "*  partial autocorrelation function (PACF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to start by plotting the features against each other to better understand the variation in the data.  If you have a lot of features, you will need to chose the sensible subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(df, alpha=0.3, c=df.species.cat.codes, figsize=(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots make us hopeful that the variation in the data will be able to predict the different species.  I like to start with box plots which graphically show the mean, median, quartiles, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot(kind='box', figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `sepal_width` has outliers, but they are not extremely extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot(bins=25, alpha=0.2, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to have good variation, but it looks like there might be a mass point around `0` for `petal_width`.  Let's look into that in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.petal_width.plot(bins=25, alpha=0.3, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need to talk to a domain expert (biologist) about this. Or, it could be different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('species').hist('petal_width', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split in the histogram of `petal_width` appears to be caused by the iris's species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine bad data\n",
    "\n",
    "It is important to identify outliers and missing values, and then make a sensible decision about how to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "We saw from the box plots that only `sepal_width` has outliers.  Some options are:\n",
    "\n",
    "*  Drop them\n",
    "*  Use them unchanged\n",
    "*  Bin the continuous feature and add a bin for outliers (Check out `pd.cut` and `pd.get_dummies` for this case)\n",
    "*  [Winsorize](https://en.wikipedia.org/wiki/Winsorising) the outliers\n",
    "*  Truncate or impute the outliers in some other sensible manner\n",
    "*  Check if some other factor is causing outliers, such as defects in telemetry/instrumentation\n",
    "\n",
    "In the current case, using them unchanges is reasonable because the outliers are not very extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data\n",
    "\n",
    "Similarly, you need to identify missing values and decide how to deal with them.  There are a couple options:\n",
    "\n",
    "*  Drop rows with missing data if it is *missing at random* (How would you test this?)\n",
    "*  Impute missing values\n",
    "*  Bin the feature and add a bin for missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "\n",
    "Be very careful when cleaning data not to lose information which has predictive power.  It is very helpful to discuss cleaning and feature engineering with a domain expert.  Fortunately, this data doesn't need any further cleaning.  Often, you will have to identify and construct a label/target which makes sense and engineer other features.\n",
    "\n",
    "Dates cause a host of problems which you may need to deal with.  First, convert all dates to a `datetime` object using `datetime.datetime.strptime()`.  This will make time-series plots and models work correctly.  Next, you may need to correct for the number of (working) days per month.  Also, day, week, month, and day of week effects are common: you may need to create dummy variables to handle these shocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n",
    "Now that the data is ready, we can start building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a crappy first model\n",
    "\n",
    "Always start simply and build a first bad model. At this step, you want to check that your pipeline works and modeling is likely to succeed.  If the model performs too well, you should be suspicious of problems like information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, range(4)]\n",
    "y = df.iloc[:, 4]\n",
    "y = y.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check conversion of categorical data to numeric labels is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.species.cat.codes, df.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionCV(cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be suspicious of results which are too good.  If you observe them, you need to check for **information leakage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[y != y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs[y != y_hat, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate and refine your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment\n",
    "\n",
    "Deploy your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "EDA, cleaning, feature engineering, and modeling are an interative process.  At each step, you might discover something you missed and need to return to an earlier step.  Ultimately, understanding your data's strengths and weaknesses and engineering good features is usually more important that what model you choose.\n",
    "\n",
    "Now, you should focus improving your model until you meet the metric for success which you identified during *Business Understanding* section of CRISP-DM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
